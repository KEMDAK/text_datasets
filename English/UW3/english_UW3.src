The phase transitions in a single-crystal specimen of shape-memory alloys mani-
fest themselves in abrupt changes of deformation during loading or during changes
of temperature. The Landau-Devonshire model provides an analytic description
of such transitions. It characterizes the material by four parameters.
In a polycrystalline specimen the jumps of deformation are smoothed out,
because each crystallite responds differently to changes in load and temperature;
one may say that each crystallite is characterized by different quadruplets of para-
meters, These quadruplets are points in a four-dimensional space, which we call
the Preisach space in recognition of a similar construction by Preisach [l] con-
cerning ferromagnets. The quadruplets of all crystallites in the specimen fill a
by humans must be performed at the partial-pattern-matching level of compilation. We
then present techniques for representing and organizing such compiled knowledge that
have desirable time and space characteristics. We hope that this work is a step toward
understanding how people generate language as rapidly as they do and building practical
language-generation systems that achieve similarly impressive speeds.
This work was supported in part by a grant from the Center for Cognitive Science at
The Ohio State University.
would allow us to conclude the sentence ''Either Ed is the murderer or Jeff is the murderer''
because this follows irrespective of whether we believe John or Bill.
In the following definitions MAXCONS(P) and MAXCONS(P,IC) are maximal con-
sistent subsets of P and maximal consistent subsets of P with priority to IC. By maximal
consistent subsets of P with priority to IC, we mean maximal consistent subsets of P U
IC, which contain all elements of IC.
Definition 3.1 (Maximal Consistency). Let P be a theory, and IC be a set of integrity
constraints. A subset ( g P is said to be maximally consistent with priority to IC iff ? U
IC is consistent, and for every theory (' such that C 0' g P, it is the case that 0' U
IC is inconsistent. MAXCONS(P,IC) is the set of maximally consistent subsets of P with
priority to IC. When IC is an empty set, then MAXCONS(P,IC) is called MAXCONS(P)
and is the set of maximally consistent subsets of P.
Theorem 3.1 below shows that all theories possess at least one maximal consistent
subset with respect to any consistent set of integrity constraints.
Theorem 3.1. Suppose that P is any (possibly infinite) first-order theory and IC is any
consistent set of integrity constraints. Then P U IC has at least one (possibly infinite)
maximally consistent subset P' such that IC C P'. (If P is finite, then its maximal consistent
subsets are obviously finite.)
Proof. P U IC has at least one consistent subset that is a superset of IC, namely IC
itself. Thus, let CONS(P,IC) denote the set AlA gg P U IC and A is consistent and IC g
A), and let CONS(P) be defined as CONS(P, 2). Thus, CONS(P,IC) = because IC e
CONS(P,IC). We show below that every ascending chain of elements in CONS(P) has an
upper bound in CONS(P). The result then follows from Zorn's lemma.
Suppose that M g Ms g Ms C .., is an ascending sequence of members of CONS(P),
that is, each M, is a consistent subset of P U IC and IC C M,. Then M = UL)M, is an
upper bound for this ascending sequence. Moreover, M is consistent, IC M and M gC
P U IC, that is, M e CONS(P). The only nonobvious part is the consistency of M.
To see this, suppose that M is not consistent. Then, by the compactness theorem,
there is a finite subset M' g, M such that M' is inconsistent. Let M' = y, . . . . y,} for
some integer n. Hence, for each 1 s i s n, there is an integer, denoted o, such that yj 6
Maus, Let o = maxio(1),...,o(n)). Then M' g M,, Hence, as M' is inconsistent, M,,is also
inconsistent, thus contradicting our assumption that each M,, j a 1, is in CONS(P).
A weaker version of the above theorem has been established by Grant and Subrah-
manian (1990) (see corollary 3.1 below). An important point to note about MAXCONS(P)
is that it depends upon the syntactic nature of P. For instance, if we take P; = [a,b, -a)
and P4 = (a & b), -a), then MAXCONS(P,) = a,b), a,b)k, and so it is perfectly
reasonable to cautiously conclude b from P;. On the other hand, MAXCONS(P4) = [[(a
& b)], [a}} from which we cannot cautiously conclude b. Even though the theory [(a &
b)] is logically equivalent to the theory [a,b) according to classical logic, this equivalence
does not hold when we treat inconsistent theories nonclassically.
The following corollary of Theorem 1 shows that all theories have at least one maxi-
mally consistent subset.
This paper is concermed with the performance evaluation
of integrated voice/data multiplexers, in which voice
signals are transmitted in real-time and data messages are
buffered. Difficulties in the study of such systems are
generally related with the correlation property of voice
traffic'*% Voice traffic varies much more slowly than data,
and thus has a high correlation property. Because voice is
usually served with higher priority over data, this comelation
property of voice traffic has a direct effect on the data
buffer behaviour. When voice traffic increases, only a
small portion of the link capacity is available to data for a
relatively extended period and data packets pile up. To
account for this correlation effect properly, voice traffic is
usually modelled as a Markov process in the study of
voice/data integrated systemg'3
Studies on integrated voice/data multiplexers with
Markovian modelling of voice taffic have used both
continuous-time' ' and discrete-time parameters' %. In
the discrete time parameter analyses, gated service of
data packets is usually assumed, where data message4
arrived in a frame are momentarily blocked in that frame
and are transmitted in the succeeding frames. Dati
message length may have any general distribution in thig
analysis. A restriction of the discrete time parametei
analysis is that it is numerically untractable when the link
capacity is large. Itwas reported that numerical difficultie4
arise when the number of slots in each fixed length frame
is larger than 7'. Such a problem has partially been solvej
using an approximation. An expression estimating thi
parameters to destination parameters are fruitfully
discussed in Bernsten et a!.'?; The ATM OoS is not in
anyway affected by the requested LAN priority which in
all cases is preserved end-to-end, i,e, within the source
and destination LANs.
Before access to ATM takes place the AAdS.LPDU
(MACLSDU + FC + DA + FCS) is segmented and
delimited by the 8AdS protocol. The Basic Adaptation
Sublayer (BAdS) protocol is a further type of SAR protocol
devoted to supporting a subgroup of Class C services'' for
which the QoS offered by ATM is more than adequate''.
Thus the 8AdS protocol can support N-1SDN services
such as LAP-D signalling and interconnection of conven-
tional unslotted ANg-%
The functions of the BAdS protocol are segmentation
and reassembly as well as frame delimitation and cell loss
detection. These functions are provided by use of specific
subfields coded in the header of each segment. Similar to
the SAR type 3 and 4 protocols' we arrange for a 2 bit ST
(Segment Type) subfield and a 6 bit SN (Sequence
Number). The ST indicates the position of the segment in
the AAdS. PDU and is used for reassembly purposes. The
5N is used for cell loss detection purposes and is present
only in the first and middle segments of an AAdS..PDU.
In single segment messages and in the last segment of a
message SN is replaced by an Ll (Length Indicator) field
which declares the amount of user information in octets
conveyed by the segment. The total BAdS.PCI is one
octet instead of four octets as it is the case in SAR types 3
and 4,
NOte here that in the application of unslotted LAN
interconnection through the ATM network the MD
functionality provided by the proposed SAR type 4
protocol is not required. The use of MID is superfluous for
the following reasons'; no switching based in MID is
performed within the ATM network as already explained
before; priorities regarding segmentation of AALLLSDUs
are limited to queueing disciplines on a frame level, This is
a reasonable assumption, since in a possible realization a
high priority AALLLSDU does not intermupt the segment-
ation of a lower priority AALLLSDU. Thus multiplexing on
a segment level is justifiably excluded.
We adopt a broadcast star configuration (one-to-many,
many-to-one) between remote LAN/MANs, since it is
expected that splitting or merging functions will be
implemented with low cost in ATM networks.
BAdS segments pass to the ATM layer for ATM header
assignment and subsequent transmission. As regards the
allocation of VCIs/VPls to the outgoingATM cells and the
VPC4 interconnecting bridges, the following scenarios,
also applicable to the slotted MANs environment, are
considered:
In analogy with the scenario concerning unslotted LANs,
we here also assume that a semipermanent VPC has been
established between the IWUUs of slotted MANs. We thus
refer to case (i) of the discussion of scenarios for supporting
CL service in 8-ISDN above.
We assume that the interconnected MANs are all of
the DODB type and we consider the CL service provided
by DODB' The OA (Oueued Arbbitrated) slot size as well
as its PCI is compatible with the ATM cell. However
specific subfields within the OA header (we refer to both
the OA segment 4 octet-header as well as the OA slot 1
octet-header) are used to provide different functions.
Note for exarmple that in Figure 4 the first octet of the OA
slot is the ACF Access Control Field) which serves the
access mechanism of DODB while in the ATM cell case 4
bits are devoted to policing, ie, the GFC (Generic Flow
Control) field, and the remaining four bits are part of the
VP, The last two bits of the fourth of the OA slot are
devoted to controlling congestion within a segment
based multiport bridge by indicating the segment priority
(SP subfield). Similarly the last bit of the same octet in the
ATM cell header indicates a cell loss priority(CLP subfield)
useful for congestion situations within ATM.
We here consider the applicability of the ATM bridge
component for slotted MANs interconnection through
the ATM network, The internal layer organization of the
ATM bridge is illustrated in Figure 5. The segmentation
and reassembly functionality provided by the BAdS
protocol in the unslotted LAN case is not further required.
The main function of the relaying entity appearing also by
R in Figure 5 is the OA header conversion to an ATM cell
header, The reasons for this conversion requirement are
obvious: the OA slot will be routed along the ATM
network and relaying of VPls will be provided in DCCs in
the ATM manner. Thus the OAL.PCI should be converted
to a ecognizable ATMLPCl.
the mission and safety requirements, into mission and
safety operators, and mission and safety controllers.
By analysing the progression of events that result in a
disaster,'' a justification for the separation between the
mission and the safety requirements can be obtained.
The initiating event is an event that can put the system in
a hazardous state. A hazard is a physical situation,
expressed as a system condition, that can lead to a
disaster. A disaster is an unintended event or sequence of
events that cause death, injury,environmental or material
damage. The boundary between the safe and unsafe
states of a system is related to the initiating events of the
system. Thus, we define an unsafe state as a system state
which follows some initiating event that can lead to a
hazard, unless the safety controller and/or the safety
operator takes corrective action. We apply the mission
requirements to the behaviour of the system while it is in
a safe state, and the safety requirements to the behaviour
of the system when it enters an unsafe state.
There are some safety-critical systems for which the
separation between mission and safety issues is realised
at the level of the physical process, for example, the
identification of shutdown systems of nuclear power
plants. Other systems allow a separation to be made at
the controller level, such as railway systems.' Whether it
is feasible to separate the mission from the safety
requirements for a particular safety-critical system
depends on the ease with which a distinction can be
drawn between safe and unsafe states. The mission
controller is concerned with system behaviour while the
system is in a safe state, and the safety controller when
the system is in an unsafe state. Furthermore, the aims of
the two controllers are distinct. The mission controller
ensures that the mission is accomplished - this will also
require that the system does not enter into an unsafe
state. The safety controller is concerned with avoiding
hazardous states by dealing with the unsafe states that
precede these states; it is assumed that the system does
not start in a hazardous state.
Some of the benefits of making this distinction during
requirements analysis are: the resolution of potential
conflicts, detection of omissions and inconsistencies
between the mission and safety issues, the ability to'focus
on the safety-critical issues, and the simplification of
safety certification.
The basic concern in this paper is with the analysis of
requirements and not with their elicitation (the process
of acquisition of the relevant information from the user).
We deal with techniques that can be used to reduce (or
eliminate) the possibility of the occurrence of hazards
due to faults introduced during the requirements analysis.
In the following, two phases of the analysis of the
safety requirements are presented in terms of their main
characteristics. These two phases are called the Safety
Requirements Analysis and the Safety System Analysis.
The separation of the analysis into these two distinct
phases is intended to simplify the analysis task, permitting
an easier understanding and reasoning about the real
world properties, and how the system perceives and
manipulates them.
Within the overall framework, we assume that there
exists another phase - Conceptual Analysis - which
should be conducted before the two mentioned above;
this is intended to produce an initial informal statement
of the aim and purpose of the system and to determine
what is meant by safety for the system. As a product of
this phase we obtain the Safety Requirements, enumer-
ating the potential disasters, and the hazards related to
these disasters.
In this phase, the real world properties, in terms of
physical laws and rules of operation, are identified. As a
product of this real world analysis, the Safety Require-
ments Specification is produced, containing the safety
constraints and the safety strategies. A safety constraint
is simply the negation of a hazard modified to incorporate
safety margins, whereas a safety strategy is a means,
defined as a set of conditions over the physical process,
of maintaining the safety constraint. The general features
which are required for a formalism to be appropriate for
this phase are the following:' the specifications should
have a conjunctive character, in the sense that new
requirements can be added to the specification without
the need to reconstruct the full specification, the
behaviour of the system should be defined in terms of all
of its possible runs (i,e. a linear sequence of events
and/or states), and there is no need to explicitly specify
concurrency and non-determinism. In the proposed
framework each safety strategy is verified against the
relevant safety constraint, and the strategies are checked
for inconsistencies. The safety strategies are then vali-
dated against the mission requirements to ensure that
these do not conflict while the system is in a safe state.
The most appropriate formalisms for this phase are
logical formalisms, such as Temporal Logic,' Real-Time
Logic (RTL),' Real-Time Temporal Logic (RTTL),'%
and Timed History Logic (THL).''
In this paper we propose the use of THL during the
Safety Requirements Analysis, primarily because the
behaviour of systems expressed in THIL is defined by
imposing constraints over the set of all sequences of
states that the system can exhibit. This property gives
THL formulae a conjunctive nature. Therefore by using
THL the Safety Requirements Specification can be
constructed by considering each safety constraint and
safety strategy separately.
The activities to be performed during this phase include
the identification of the interface between the safety
controller and the physical process, and the specification
of system behaviour that must be observed at the
identified interface. Also in this phase a top level
organisation of the system is realised in terms of the
properties of the sensors and actuators of the system, and
the effects of possible failures of these sensors and
actuators. This phase leads to the production of the
Safety System Specification, containing the safety con-
troller strategies, A safety controller strategy is a
refinement of a safety strategy incorporating the sensors
and actuators, and their relationship with the real world.
Our aim is to construct a Safety System Specification
During the same period several other IBM
task forces and studies began looking at character
code, keyboard, and national language problems,
at least partially as a response to the user group
paper. An internal IBM Coded Character Set
Strategy was published in September 1983, which
outlined the known problems and established a
strategy for dealing with them. A critical compo-
nent of that strategy was the recommendation to
establish common sets of characters to be sup-
ported across all EBCDIC, ASCII and personal
computer codes used within given geographical or
cultural areas. The CECP multilingual character
set was recommended as the common set to be
supported in those countries which require the
Latin alphabet.
Shortly after, IBM established an organization
in Toronto which was given the mission of estab-
lishing an architecture to guide the development
of products in a way consistent with the overall
character code strategy. This group subsequently
clarified and expanded the strategy, and docu-
mented specific requirements for conforming im-
plementations.
The CECP recommendations of the IBM task
force were studied and debated throughout the
company, and even became the subject of a sec-
ond task force, but were ultimately agreed upon
as the most practical solution to the character
code problems. The codes were approved within
the company, and became internal IBM stan-
dards in 1984. Each contained the original 95
characters of its predecessor, in their original
positions, so compatibility with earlier products
and data bases was assured. As each had been
extended to include the full multilingual charac-
ter set, translation from one to another was al-
ways possible. Since the character repertoire was
drawn from the original multilingual word pro-
cessing code, translation between word process-
ing and data processing products and data was
also possible. The new CECP codes verified the
utility of the original multilingual set of charac-
ters, and established the principle that there is
value to standardization of a common set of char-
acters across several different codes, even if the
actual code values may differ.
Multilingual standardization efforts in ISO, the
International Organization for Standardization,
followed a somewhat similar course, in that they
began with an 8-bit, western European code stan-
dard, ISO 6937-2, approved in 1983, which pro-
vided a similar but larger set of characters, and
also ran into compatibility problems. The total
repertoire of characters supported by this stan-
dard is approximately 350, which is possible be-
cause of a somewhat controversial technique used
to code accented letters. A set of unique diacriti-
cal marks, or accents, was provided, each of which
can be used in conjunction with various letters so
that the combination of accent plus letter repre-
sents an accented letter. The two-character com-
bination of the codes for 'n' plus ' -' therefore
represents the code for an 'hi', for example. While
this use of 'non-spacing' diacritics was quite con-
venient and efficient for the transmission of data,
it did not gain wide favor with programmers and
engineers, who argued on technical grounds that
the code for a letter should always be one charac-
ter, regardless of whether the letter was accented.
The standard was for that reason less widely
accepted in the data processing industry than it
was with the public text communication services
for whom it was originally intended.
By 1983, then, the data processing industry
had made several attempts to resolve some of the
problems associated with sending text electroni-
cally from one country to another, and between
computers which supported different character
codes. Dissatisfaction with the nonspacing dia-
critic aspect of ISO 6937-2 led ANSI, ECMA and
ISO to independently explore the possible devel-
opment of a second 8-bit multilingual code stan-
dard. The new standard would include accented
letters only as complete characters, as had been
the practice in earlier national code standards.
IBM's experience with its EBCDIC multilingual
codes appeared directly relevant, and the com-
pany participated actively throughout the devel-
opment of the new standard.
Although the desired code would be able to
contain twice as many characters as one of the
older 7-bit codes, the choice of which 192 to
include was hotly debated over the next year or
so in meetings of the various organizations. The
national standards bodies of many countries were
involved, each with its own set of favored charac-
ters, Numerous companies were represented on
the committees and many, like IBM, had their
own candidates. Is the 'CE' ligature used in France
really a letter? Should the 'ii used in the Nether-
lands be considered one letter or two? What
right to preserve the integrity of a copyrighted work.P The amended
Copyright Law makes some specific exceptions to this rule when the
modifications are necessary to use the program in a particular computer
or to use it efficiently. This includes the right of an owner of a dupli-
cate copyrighted computer program to copy and adapt it to the extent
necessary for using it on his or her own computer. These provisions sig-
nify that a user of a copyrighted program may upgrade it and modify it
for the purpose of replacement.*
The 1985 Amendment confirms the broad interpretation of ''works
of authorship'' and ''reproduction'' given by the three court decisions
discussed previously.% A major change made by the 1985 amendment
although not touched by the court decisions is the registration require-
ment. Under the a mended law there are four kinds of registrations in
the Copyright Register: (1) registration of the author's real name;* (2)
registration of the date of first publication;* (3) registration of the date
of creation of a program work;* and (4) registration of the transfer or
other disposition of copyright. The expiration date of the copyright is
determined by reference to the date on which the work is created or
published. Because computer programs are often kept confidential, the
creator may register any date within six months after the actual crea-
tion as the date of creation.
Under the Copyright Law as amended in 1985, there are specific
legal effects that follow from registration. When a work of authorship
is published anonymously or under a pseudonym, the author may regis-
ter under his or her real name regardless of whether he or she owns
the copyright in the work. When the creator's real name is registered,
the registration is published in the Official Gazette. Upon registration
in a real name, there is a presumption of authorship. Registration of an
author's real name can be applied for after his or her death by a person
so designated in the author's will. However, many Japanese scholars
feel it is too early to tell if the Program Registration Law is of great
practical importance to the software industry.
The owner of a copyright has the right to seek remedies against
either actual or threatened infringement, The damages incurred by
system consisting of a Model 101 or successor CPU and require that the
Model 101 or successor CPU be configured in a unit or system contain-
ing a minimum number of CRT'e and I/O ports and specified minimum
capacities of memory storage, either in the CPU box or in one or more
peripheral devices. Such restrictions would all be aimed at preventing
the marketing of the software with a stripped down system consisting,
for example, of a single microprocessor chip.
A vendor might also consider imposing minimum price restrictions,
e.g., that the software may only be marketed as part of a system of
hardware and software where the total system price equals or exceeds
$25,000. Such a provision may raise two concerns, however. First, one
needs to be certain to avoid restrictions that might be interpreted as re-
sale price maintenance or price fixing. Second, because hardware prices
may drop substantially during the term of a VAR agreement, it is im-
portant that any such provisions take into account possible fluctuations
in price. A system that sells for $25,000 today may sell for $12,000 a
year from now.
A legitimate concern of some resellers, however, is that they be
permitted to sell their existing installed base. For example, if a VAR
has been granted rights to market certain software as part of a system,
the VAR may want the right to provide that software at a later date to
customers who do not initially acquire the software with the system but
decide to add that functionality later. Software suppliers may want to
limit a VAR's rights in this regard by, for example, permitting such
marketing only if it occurs within 120 days after the initial system sale,
or by requiring that the software be configured so it will not operate on
any system other than an authorized system, in order to help prevent
the VAR from competing directly with conventional dealers and
distributors.
It is common for a successful OEM or VAR to seek the exclusive
right to market certain key components in conjunction with a particular
line of equipment. For example, suppose ASI has a particularly success-
ful application program that is highly respected in a particular vertical
market and that runs on a number of different hardware platforms, in-
cluding minicomputers manufactured by Hye Tech, Inc. Acme VAR,
which specializes in marketing systems built around Hye Tech min-
icomputers, may seek from ASI the exclusive right to distribute that ap-
plication program on all equipment manufactured by Hye Tech.
Such an arrangement may give Acme VAR a significant competitive
advantage, because customers who want to acquire that application pro-
gram for use on Hi Tech computers will not be able to obtain it from
processorsimulator and a detailed mem-
ory simulator for the Dash prototype.
Tango allows a parallel application to
run on a uniprocessor and generates a
parallel memory-reference stream. The
detailed memory simulator is tightly
coupled with Tango and provides feed-
back on the latency of individual mem-
ory operations.
On the Dash simulator, Water and
Mincut achieve reasonable speedup
through 64 processors. For Water, the
reason is that the application exhibits
good locality. As the number of clusters
increases from two to l6, cache hit rates
are relatively constant, and the percent
of cache misses handled by the local
cluster only decreases from 69 to 64
percent. Thus, miss penalties increase
only slightly with system size and do not
adversely affect processor utilizations.
For Mincut, good speedup results from
very good cache hit rates (98 percentfor
shared references). The speedup falls
offfor 64 processors due to lock conten-
tion in the application.
MP3D obviously doesnot exhibit good
speedup on the Dash prototype. This
particular encoding of the MP3D appli-
cation requires frequent interprocessor
communication, thus resulting in fre-
quent cache misses. On average, about
4percent of the instructions executed in
MP3D generate a read miss for a shared
data item. When only one cluster is
being used, all these misses are serviced
locally. However, when we go to two
clusters, a large fraction of the cache
misses are serviced remotely. This more
than doubles the average miss latency,
thus nullifying the potential gain from
the added processors. Likewise, when
four clusters are used, the full benefit is
not realized because most misses are
now serviced by a remote dirty cache,
requiring a three-hop access.
Reasonable speedup is finally
achieved when going from 16 to 32 and
64 processors (77 percent and 86 per-
cent marginal efficiency, respectively).
but overall speedup is limited to 14.2.
Even on MP3D, however, caching is
beneficial. A 64-processor system with
the timing of Dash, but without the
caching of shared data, achieves only a
4,1 speedup over the cached uniproces-
sor, For Water and Mincut the improve-
ments from caching are even larger.
Figure 10 shows the speedup for the
three applications on the real Dash hard-
ware using one to 16 processors. The
applications were run under an early
version of the Dash OS. The results for
Water and Mincut correlate well with
the simulation results, but the MP3D
speedupsare somewhatlower. The prob-
lem with MP3D appears to be that sim-
ulation results did not include private
data references. Since MP3D puts a
heavy load on the memory system, the
extra load of private misses adds to the
queuing delays and reduces the multi-
processor speedups.
We have run several other applica-
tions on our 16-processor prototype.
These include two hierarchical n-body
applications (using Barnes-Hut and
Greengard-Rokhlin algorithms), a ra-
diosityapplication from computer graph-
ics, a standard-cell routing application
from very large scale integration com-
puter-aided design, and several matrix-
oriented applications, including one
performing sparse Cholesky factoriza-
tion, There is also an improved version
of the MP3D application that exhibits
better locality and achieves almost lin-
ear speedup on the prototype.
Over this initial set of 10 parallel ap-
plications, the harmonic mean of the
speedup on 16 processors in 10.5 Fur-
thermore, if old MP3D is left out, the
harmonic mean rises to over 12.8. Over-
all, our experience with the 16-proces-
sor machine has been very promising
and indicates that many applications
should be able to achieve over 40 times
speedup on the 64-processor system.
There are other proposed scalable
architectures that support a single ad-
dress space with coherent caches. A
comprehensive comparison of these
machines with Dash is not possible at
this time, because of the limited experi-
ence with this class of machines and the
lack of details on many of the critical
machine parameters. Nevertheless, a
general comparison illustrates some of
the design trade-offs that are possible.
Encore GigaMax and Stanford Para-
digm. The Encore GigaMax architec-
ture'and the Stanford Paradigm project'%
both use a hierarchy-of-buses approach
to achieve scalability. At the top level,
the Encore GigaMax is composed of
several clusters on a global bus. Each
cluster consists of several processor
modules, main memory, and a cluster
cache. The cluster cache holds a copy of
all remote locations cached locally and
also all local locations cached remote-
ly. Each processing module consists of
several processors with private caches
and a large,shared,second-level cache.
A hierarchical snoopy protocol keeps
the processor and cluster caches co-
herent.
The Paradigm machine is similar to
the GigaMax in its hierarchy of proces-
sors, caches, and buses. It is different,
however, in that the physical memory is
all located at the global level, and it
uses a hierarchical directory-based co-
herence protocol. The clusters contain-
ing cached data are identified by a bit-
vector directory at every level, instead
of using snooping cluster caches, Para-
digm also provides a lock bit per mem-
ory block that enhances performance
for synchronization and explicit com-
munication.
The hierarchical structure of these
machines is appealing in that they can
theoretically be extended indefinitely
by increasing the depth of the hierar-
chy. Unfortunately,the higher levels of
the tree cannot grow indefinitely in
bandwidth. Ifa single global bus is used,
it becomes a critical link. If multiple
buses are used at the top, the protocols
become significantly more complex. Un-
less an application's communication re-
quirements match the bus hierarchy or
its traffic-sharing requirements are
small, the global bus will be a bottle-
neck. Both requirements are restrictive
and limitthe classes of applications that
can be efficiently run on these machines.
1EEE Scalable Coherent Interface.
The IEEE P1596Scalable CoherentIn-
terface (SCI) is an interface standard
that also strives to provide a scalable
system model based on distributed di-
rectory-based cache coherence.'' It dif-
fers from Dash in that it is an interface
standard, not a complete system de-
sign. SCI only specifies the interfaces
that each processing node should im-
plement, leaving open the actual node
design and exact interconnection net-
work, SCI's role as an interface stan-
dard gives it somewhat different goals
from those of Dash, but systems based
on SCI are likely to have a system orga-
nization similar to Dash.
The major difference between SCI
and Dash lies in how and where the
directory information is maintained. In
SCI, the directoryisadistributedsharing
list maintained by the processor caches
MicroPatent's Who Invented What?
contains full-text abstracts of patents
issued during 1990-1991 on CD-ROM.
Users can search these patents by key-
word, inventor, patent holder, status,
patent number, issue date, application
number, application date, US refer-
ences, US classification, international
classification, and residence of first in-
ventor - Or anv combination of the
above.
Minimum hardware configuration is
an XT computer with 640 Kbytes of
RAM. 3 Mbvtes of available hard-disk
space, and an ISO 9660-compatible
CD-ROM drive. The disk and applica-
tions software cost $199,
MicroPatent's monthly automated
patent-searching CD-ROM provides
abstracts for $1.100 per year. A fully
searchable text of issued patents is
available for $1,450 per year. A week-
1y $5.500 series contains facsimile cop-
ies of patents with drawings that can
be viewed or printed on any PC and
laser printer.
Warner New Media's The Orchestra
lets users see orchestral instruments,
hear how they sound, and learn how
they are played.
Graphics highlight which instru-
ment is playing, and an illustrated
analysis explains how pieces are put
together. Features include a conduct-
ing lesson, a composing lesson in
which the user selects the instruments
to play and hears the result, and an ar-
cade in which the user plays such
games as Name that Instrument and
Music Trivia. Display features include
a full-color background and multiple
windows.
A time line teaches music history
with audio examples that range from
Gregorian chants to jazz improvisa-
tion. Other features include 500 extra
audio examples, a music guide, pro-
nouncer and sound indexes, biograph-
ical information, and a glossary.
Users need an Apple Macintosh
running System 6.0.5 or later with at
least 2 Mbvtes of memory, a hard disk
with at least 4.5 Mbytes of free space.
a CD-ROM drive, audio-playback
equipment, and HyperCard 2.0.
The CD-ROM disk costs $79.98.
The Institute for Scientific Informa-
tion is releasing a new version of its
Science Citation Index Compact Disc
Edition (SCI CDE). In addition to the
regular bibliographic information, the
reference product offers English-lan-
guage author abstracts for 83 percent
of the article information. SCI CDE,
which provides access to 3,100 interna-
tional journals in science and technolo-
gy, is also available without abstracts.
The abstracts version features refer-
ence searching that lets researchers
find recent articles that cite a known
relevant work. Indexing links all arti-
cles that have one or more references
in common. Author kevwords and a
function called KevWords Plus let us-
ers retrieve additional information.
Users will need an IBM PC or true
compatible with 640 Kbytes of RAM
(Nwith 416 Kbytes free), MS-DOS 3.1
or higher, and a drive with MS-DOS
CD-ROM extension 2.0. A Macintosh
version will be available in the third
quarter of 1992.
Current users can add the 1992 ab-
stract version for $3.833. The full
package costs $14,783 for the 1992
edition.
Version 3.0 of FCC Nuclear Struc-
ture Software for IBM PCs explains
the basics of nuclear structure theory
with graphics. A fully interactive mod-
eling capability lets users build and
modify specific nuclei by using a menu
and a mouse.
LUsers construct nuclei in 3D space
by using a default build-up sequence
and specifying the numbers of protons
and neutrons, or by building them
with nucleons and quantum subshells.
The position of valence nucleons can
then be altered to test the spin, parity,
binding energy, radius, coulomb ener-
gy, and magnetic and quadrupole mo-
ments of various configurations. Ex-
perimental data for 1.000 isotopes
comes on disk for comparison with
model predictions.
Numerical comparisons made among
the shell, liquid drop, and FCC models
of nuclear structure are graphically dis-
played. Users can change the color
coding of nucleons to emphasize quan-
tum values.
The software from TransTech AG,
Switzerland, runs on IBM PC XTIAT1
PS2s and compatibles with EGA or
VGA capabilities. A hard disk, math
coprocessor, and Microsoft mouse are
recommended but not required.
Novadyne Computer Systems' XT
series of multiprocessor systems are
powered by two or four 33-MIPS
88100 RISCs and run on a modified
version of Unix System V that sup-
ports parallel processing through mul-
tithreading operations. The Reality
operating system functions as a rela-
tional database management system
under Unix and provides disaster-
recovery capabilities.
The XT architecture contains a 100-
Mbyte/s Mbus used exclusively for in-
ternal traffic between processors and
memory, The architecture also in-
cludes a standard VMEbus for com-
munications, single or dual SCSI bus-
es for mass storage, and an Ethernet
channel for terminal connectivity. The
design minimizes contention between
high- and low-speed signals and fea-
tures symmetrical as well as parallel
processing.
The XT series starts at $70.000.
An overview will be given of different research pro)ects related to
rock mechanics in deep-mining in the Laboratory for Rock Mechanics at
the Faculty of Mining and Petroleum Engineering.
Main topics of the research are the investigation of complex material
properties and the application of such properties in calculations for
design and analysis of underground situations.
The material properties studied are: elasticity (including anisotropy
and non-linearity), failure criteria and failure and post-failure be-
haviour such as strain-softening, volume increase and changing of
elastic behaviour. Finally adjustment is made for the properties of a
rock-mass. Always is tried to analyse and understand the mechanisms
behind the material properties. (7). Acoustic techniques are an impor-
tant tool to study these complex material properties under uni-axial
and tri-axial loading conditions.
The thus found properties are applied in the analysis of existing and
planned mining situations as well as laboratory model tests. Calcula-
tions are made using analytical, finite element and finite difference
methods. When comparing these calculations with underground observa-
tions it is important to include observations of fractures and failure
Imechanisms.
Many mine galleries under severe stress conditions often show diffe-
rent types of failure mechanisms as a result of stress concentrations
near the walls. These mechanisms result in a gradual deterioration of
the gallery and serious convergence. Research is carried out in order
to develop the method of destressing the rock in order to prevent the
development of these mechanisms. (1,2,3,5).
In this way a detailed picture was obtaiied of the different stages of
the closure of the existing macrofractures. The closure of micro-
fractures could be observed by an increase of amplitudes of the first
arrivals; an 1ncrease of the velocities was observed as well. After
the hardening of the epoxy resins acoustic velocities were measured
comparable with those of intact rock salt. By core drilling after this
test, confirmation was obtained that the fractures were filled indeed.
Much experience was gained in carrying out measurements under these
underground conditions. Salt dust, elevated temperatures and the
sometimes extreme noise levels, due to mining activities, are the
typical features of a mining environment. Detailed results of the
acoustic measurements are given in thye report ''Acoustic P-Wave
velocity measurements of cataclastic effects in rock salt''. (5)
The G.S.F. München, Institut far Tieflagerung and the Netherlands
Energy Research Foundation are carrying out this large scale
demonstration experiment with support of the Commission of the
European Communities.
This test has to be seen as a pilot facility with regard to future
final disposal in a national repository in the Federal Republic of
Germany. The complete technical system of a high active waste
repository will be tested and proven as far as possible in a one to
one scale test. The present concept foresees the final disposal of
high active waste canisters in 300-600 m deep bore-holes from a
working level of 800 m depth. In order to simulate this situation, in
two galleries at the 800 m level bore-holes are drilled with a length
of 15 m. In six of these bore-holes thirty high level radioactive
waste canisters will be emplaced. (strontium-90 and caesium-137)
In two bore-holes forerunning tests will be carried out with
electrical heaters manufactured by E.C.N. The duration of the test
will be approximately five years and all the canisters are to be
retrieved at the termination of the test. An extensive measuring
program is developed a.o. for the validation of numerical
thermomechanical models. The program includes the measuring of
thermally and radiolytically induced water and gas release of the rock
salt, but also the thermomechanical behaviour. (6)
As can be seen, Ca had a positive influence on zinc dissolved from
material leached without reduction. Nearly no effect was observed for the
other case.
An explanation of these results is offered in terms of the Gibbs energy
values of equations 2-5 shown below (Outokumpu program, 1987) . The given
values all refer to a temperature of 750 'C. As seen earlier, because of
solid solution formation, rections 4 and 5 would possibly not be an
accurate representation of the possible reduction reactions.
Nevertheless, they are used here as a first approximation in the absence
of data for solid solutions.
During the ferrite synthesis stage, of the two possible reactions
(equation 2 or 3), the formation of calcium ferrite is thermodynamically
more feasible. If CaFew, is preferentially formed, then some 2no would
be left unreacted. This 2nO would then dissolve during leaching of the
unreduced samples. Synthesized samples are yet to be examined to test the
validity of this hypothesis.
aG' for reaction 4 is more negative than for reaction 5. Thus according
to thermodynamic data, CaFew; should have no significant effect on the
reduction of zinc ferrite to form ZnO which is subsequently leached. This
agrees with the experimental observation.
Thermodynamics gives the following data:
According to this data Fe,0, should prefer to react with Mn0, leaving some
2nO free which would dissolve from the unreduced samples. No zinc
dissolved from these samples however. The synthesized samples are yet to
be examined, but it is thought likely that all the 2n, Mn, and Fe must
have reacted to form a single spinel phase.
The relation between the time, during which the various fractions go to
the top of the bed and the l/w-ratio has not been quantified. It is
however clear that if the l/w-ratio increases, the separation of the Asg
becomes more difficult or for a high recovery of SiC even impossible. For
this reason the recovery has to be low for a high grade SiC product. With
increasing )ig time more SiC will move to the top, because in the last
stage of the demixing process the separation will take place based on the
difference in shape of the particles.
The 3-8 mm fraction of the jaw crusher product contains, according to
figure 2, 20 wt percent of the total Asg/Sic mixture. With 704 Sic in
the feed and 404 recovery, 1100 ton SiC can be extracted by )igging.
According to the German mother company of Elektroschmelzwerk Delfzij1
Results from field tests at Centralia and Hannah (thick coal seams of
5 to 10 m) and Pricetown (thin coal seams of 1 - 2 m) show, that the
behaviour of the cavity during Underground Coal Gasification (UCG) is
rather well known for shallow situated coal seams (up to 300 m depth).
Underground gasification of coal form a cavity behind a coal face. Due
to lithostatic pressures the roof of the cavity collapses and the
gasification chamber is filled with rubble. The flow of gas will be
determined by the permeability of the rubble bed (figure la,lb).
Underground coal gasification in Western Europe usually will take
place at a depth range of 800 m to 1500 m and up to now it has not
been proven a technically feasible prodess. The behaviour of the high
rank coal and overburden, to develop a gasification channel at such
depths, is important. The overburden pressure will compress the rubble
and compaction will be low in the vicinity of the burning coal face.
The overburden pressure increases toward the injection point
(Boswinkel 1983) . It is assumed that the rubble will be permeable for
injected gases (Fig. 1). Especially effects of overburden pressure and
temperature on cavity formation, roof collapse, spalling and rubble
permeability in the cavity, have to be mentioned. For this reason
prediction of petrophysical parameters for the overburden of deep
seated coal seams is necessary. This paper deals with high temperature
properties of overburden rock and temperature effects on rubble
permeability. One need to know, among other things, the mineralogical
and textural behaviour, dilatation and hardness of different types of
overburden sediments, during increase of pressure and temperature and
in different gas environments.
Carboniferous samples were taken from:
Three rock types have been used for laboratory tests:
deltaic) of a coastal area with occasional invasions of the sea. They
The bo re ho le simulator is a t r i a x ia l c e l l s im u la t in g r e s e r vo ir
conditions down to 4000 m depth. The rock samples are cylindrical with
a diameter of 400 mm and a length of 600 mm. A 60 mm diameter borehole
is presen t in the rock sample along the length axis.
An effec tive rock pressure is exerted on the grain contac t s while the
f 1 uid pres sure in the s amp le pores and borehole remains atmosphe ric.
The axial pressure can reach 435 b ar and t he rad ia l pre s s u re c an be
varied independe ntly up to 217 b ar. The apparatus and sample can be
heated to 150 'C.
As shown in figure 1 the following main parts c an be observed in the
b o r e h o le simulator : housin g, inner tube , lock - n u t, bo ttom p late,
s ample, plunge r, top cover and seals. The axial pressure is exerted by
p ump in g oil via conne c tion s in the top cove r into the ve sse l. The
resulting pressure is transmitte d via the p lunger on to the s amp le.
Th e radial pre s s ur e is e xe r ted by pump in g oil via one of the four
connections in the housing wall. The oil pres sure is tran sm itted via
t he thin inne r t ube wa ll on to the rock sample. In the bottom plate
and bottom of the housing a bore is present allowing trand uce rs to be
in s t a lled in the boreho le o f the s amp le . In the ne xt se c tion s the
d ifferent parts of the borehole simulator are discussed,
The housing is constructed from a mass ive fo rged block o f st ain less
stee l AIS I 4 10. The block was produced in Austria by the firm VEW and
100$ u ltrasone te sted for irregularities. The specific ations were
calculated us ing the ' Re ge ls voor toestellen onder druk' of ' Dienst
voor he t Stoomwezen ' . The outer diamete r o f the ho us in g is provide d
with a co llar to fas te n the hous ing in a binde r. Four radial bores
with sealing NPT- thread are present in the housing. They serve as oil
inlet and outlet and in one of them a displacement transducer measures
the radial dilatation. In the housing' s inner diameter a tr ap e ze
t h r e a d i s c o n s t r ucte d to fit t he lo c k - n u t. To avo id te n s io n
concentrations all transitions are provided with a radius.
T he mode conve rsions at the rock surface (and thus the recorded time
trace) depend upon the angle of incidence, 8. Fig. shows fo ur trace s
recorded on the same rock samp le at different angles of incidence. The
rock is an artificial sandstone sample cemented by araldite.
- 8 Ot (fig .A)
A t normal incidence no she ar wave s are gen e r a te d in the ro c k, The
r e c o r d e d tr a c e s h o w s a c o mp re s s io n a l w a ve ( a ) a n d m u ltip le
re flections equid is tant in time (d, e ). The slow P - wave (c) has a
ve locity which is approxim ate ly one - thir d the spee d of the fast
comp re ssion al wave. It therefore arrives close to the first multiple
re flection.
- 0 e %+s t. P
A s the ang le o f incide n ce is incre ased to 35* (i,e. close to the
critical angle of the fas t P - wa4 e ), the trace given in fig.5B is
re co rde d . The fa s t comp ressional arrival has we akened considerably
w h i le n o w t he s h e a r w a ve ( b ) is e x cite d s t r o n g ly . M u l t ip le
r e flec tion s of the fas t P-wave have disappeared from the trace and
the s low P - wave (c ) stands o ut c le ar ly. The fa s t P - wave shows a
time - s hift toward s an e arlier arrival time. The distance travelled
through the rock become s longer with incre as in g ang le o f in cide nce
a n d be cause the fa s t P - wave ve locity is highe r t han the speed in
water, the pulse arrives sooner.
- e 7 %+eit. P
The angle of incidence is increased beyond the critical angle of the
fa st P-wave in fig.5C. Only two waves remain : the shear wave (b) and
the slow P-wave (c). Both waves have a velocity that is s lowe r than
the ve lo city in water, so both wave s exhibit a time-shift towards a
later arrival time.
- 8 704
If the angle of incidence is increased still fur ther (fig. 4D , 8 =
7 0' ) , bo th the she ar wave and the slow P-wave remain present in the
recorded trace, The wave s are slower than the wavespe ed in water and
they the refore do not refract critically.
It may be difficult to identify the arrivals of the three bulk wave s
in o ne s in g le re co r de d trace. When the rock samp le is thin, the
separation in time between the various arrivals is min im a l. In this
c ase , it is conven ie n t to pe r form me asurements at several angles of
incidence and plot the recorded traces in one single graph. An example
is given in fig.5. The recorded acoustic traces are plotted versus the
an gle of incidence at incremen ts of 5 deg re e s . Th e s a m p le is an
artificial sandstone cemented by araldite. Its thickness is 29 mm. The
different waves are clearly identified. The first arrival is the fas t
P - wave. It d isappears after the critical angle of incidence of around
10' is reached. Its velocity is higher than the wave speed in water;
the ar riva l be n d s to t he left with incre asing an gle. The second
arrival is the she ar wave in the rock. It becomes visible at an an gle
o f incidence o f 155 - 20' and is strongly generated at angles above
li4%. Its velocity is less than the wavespeed in wate r ; the curvature
of the arrivals is to the right. The third arrival is the slow P-wave.
At small angles its arrival is d rowned by the strong arrival o f the
f irs t multiple reflection of the fast P-wave. At increased angles (in
this example at = 25' ) the multiple is not received anymore due to its
Compagnie de Signaux et d'Entreprises Electriques, Paris, France, has been awarded a contract by the
French Navy for 12 Sagaie naval countermeasures decoy systems slated for installation on the new anti-
aircraft corvettes, the Foch and Clemenceau aircraft carriers and the Suffren and Duquesne destroyers.
Sagaie is a fully automatic, passive countermeasures system designed to protect medium and large ships
from surface-to-surface and air-to-surface missiles. The system provides protection from missiles guided by
electromagnetic or infrared seekers, or any combination of the two. The system automatically operates
from the reception of a missile threat alarm originating from any of the surveillance systems, (radar, in-
frared, optical or electronic support measures) and will optimize the use of decoys in a very short reaction
time. It may be used alone (confusion and distraction)for jointly with a jammer by launching substitution
decoys, Ammunition for the Sagaie was developed by Societe La Croix, France.
Aeronautical and General Instruments Ltd. and Sabre Computers International of Surrey, England, have
signed an agreement with exclusive license to Sabre for the manufacture and marketing of the Milligan line
of ballistics instrumentation. The equipment has gained worldwide recognition for its dependability and
accuracy. The product range complements the ballistics instrumentation currently manufactured by Sabre.
The Milligan Sky Screen is particularly suited for use with the Sabre velocity and firing rate analyzer, an
advanced instrument accepted by NATO and other major authorities. The joint venture paves a logical
development for both companies and allows Sabre to offer comprehensive and advanced capability from
individual products to complete turnkey systems. Sabre will provide maintenance, technical support and
sales information to all existing users of the system.
A contract worth more than $70 million was awarded to Systems Development Corp. of Camarillo, Ca., to
modernize Thailands air defense system in a three-phase program. Under the terms of phase one of the
contract, Systems Development Corp. will upgrade and automate the Thai air force Air Operations
CenterjSector Operations Center. Also, radars located throughout the country will be modified to include
equipment that will enable radar data transmission to the centers via a newly designed microwave com-
munications system that will span central Thailand. The contract also calls for spare parts delivery, support
equipment tools, test equipment and technical data. Systems Development Corp. will install, test, integrate
and deliver the system to Thailand by mid-1989. Prior to delivery, the U.S. Air Force Electronic Systems
Division will oversee an extensive testing program that will include system operation by Thai personnel
Under the planned second and third phases of the program, Thailand's northern and southern air defense
sectors will be similarly upgraded and automated. This program is being conducted under the Air Force
foreign military sales program.
Operational evaluation of the British Aerospace Sea Eagle sea-skimming, anti-ship missile was successfully
completed at the U.K.s Joint Services Trials Unit, Boscombe Down, England, in January. The Royal Air
Force Buccaneers and Royal Navy Sea Harriers will be armed with Sea Eagle, a long-range fire-and-forget
missile, designed for use against large warships equipped with the latest countermeasures and air defense
systems. The Joint Services Trials Unit includes representatives from the Royal Air Force, Royal Navy and
U.K. Defense Ministry. During evaluation, the missile was subjected to a series of performance tests for its
capability and operational handling conditions. The evaluation included launching of l1 missiles from
Buccaneer and Sea Harrier aircraft under various operational scenarios. Missiles were launched singly and
in salvos from different altitudes against moving, stationary and electronic countermeasures targets at a
50-kilometer range. Sea Eagle derives much of its performance from on-board digital computers in its
active radar seeker and mission control unit. Target information is fed directly into the missile by the pilot
before launch. The missile is programmed with a complex flight path and arrives at the target area under
autopilot control. In its terminal phase, the active radar seeker switches on the missile to complete its
attack. Sea Eagle development and initial production cost was estimated at $280 million.
fter a rash of production prob-
lems and cost overruns exceeding
$100 million, the Navy is taking a
go-slow approach on the beleaguered
Submarine Advanced Combat System
(SubACS) program. Prime contractor
IBM Federal Systems Division in
Manassas, Va., will face competition
from a second-source contractor for
the second phase of the restructured
program.
The Navy awarded a $20 million firm
fixed-price system design definition
contract to RCA Missile and Surface
Radar Division for the new FY-89 sys-
tem in January, GE and Singer Libra-
scope will team up with RCA on the
project.
Once a three-phase program awarded
exclusively to IBM, SubACS was sup-
posed to integrate command and con-
trol, acoustic and weapons systems for
SSN-688 Lds Angeles-class and the new
SSN-21 Seawof-class fast attack sub-
marines, and include such sophisticated
technology as very high-speed inte-
grated circuits, a fiber-optic data bus
and software written in the Ada pro-
gramming language. But IBMs produc-
tion problems and cost overruns
prompted the Navy to restructure the
program into two much less innovative
phases. The first phase, the SubACS
Basic system, is an upgraded version of
the AN/BQQ-5 sonar suite and will be
installed on the SSN-688-class subs
authorized from 1983 through 1988.
The second and third phases of the
original SubACS program have been
converted into a separate program, the
FY-89 Submarine Combat System,
which will provide a more advanced
system for the new Seawolf-class subs
authorized for 1989 and beyond.
Although just what portions of the
advanced technology will make it into
the FY-89 system is not clear, a Navy
source did verify that the software that
runs the system will not be written in
Ada-despite the DOD mandate that
the language be used in all new mis-
sion-critical programs as a means of
controlling skyrocketing software main-
tenance costs. Officials at the DODY
Ada Joint Program Office refused to
comment, and Navy representatives
proved either unwilling or unable to
address the question.
But with or without Ada, the Navy
says, the FY-89 system will allow the
Seawof-class submarines to successfully
counter the projected submarine threat
in the complex combat environment of
the 1990s and beyond. The system will
accommodate more robust sensors and
provide a more rapid response from the
operators by enhancing the human
interface to the system and providing
automatic management of the informa-
tion from all the sensors and attack
orders to all weapons. The FY-89 sys-
tem will include the computer programs
and equipment necessary for acoustic
detection, tracking, classification, tar-
geting of torpedoes and missiles,
weapon presetting and launch control,
as well as actual command and control.
It will be the first system to introduce
the Navy's enhanced modular signal
processor.
Although General Dynamics Electric
Boat Division submitted a bid for the
potentially lucrative contract before the
Dec. ll deadline, a recent contracting
suspension kept GD out of the picture.
A Dec. 3 Pentagon directive barred
General Dynamics from competing for
government contracts in response to the
indictment of four current and former
GD executives and the company itself
for fraud. The suspension affected more
than General Dynamics, however.
AT&T Technologies, Gould Inc. and
Honeywell Corp. were to be subcontrac-
tors under GDs plan. A General
Dynamics spokesman declined to com-
ment on the SubACS award or the sus-
pension, saying only, 'We're having
ongoing discussions with the Navy to
resolve it[the suspension].''
As the winner of the system definition
contract, RCA will work as a follower
to IBM to 'iron out the problems with
The bull is still loose. That is the sentiment on the trading floor these days as the Dow Jones lndustrial Average and a host of other stock indicators continue to set
record hlghs. On Feb. 5, the Dow Industrials ventured above the 1600 level for the first time ever as once again investors focused on faliing oil prices and the antick
pation of lower interest rates. For the rmonth, the Defense Electronnics index easily outperformed all the other national indicators we monitor in Investor's Conmer, The
DE index skyrocketed 9) points (r8.14 percent) to ckose at 1186.95, compared to the rising Dow Jones Industrial Average, whch jumped 9& points (+627 percent) to
tinish the session at an all time high of 1613.42. Three stocks upped their dividends last month: Boeing increased its payout from $1.08 to $i.20, McDonnell Douglas
upped its dividend from $1.84 to $2.08 and Figgie International voted to increase its payout from 68 cents to 76 cents per share annually. Another especially note-
worthy milestone was set when the price of Digital Equipment stock surpassed that of IBM'g. DEC has been Wall Street's darling lately, as our most active issues
section indicates. For the month, DEC ended at $159, while lBM closed at $155.75. The last time DEC was higher in price than IBM was in the go-go days of the 1983
bull market. Six more cormpanies were added to our growing listt Dense-Pac, Heriey Microwave Systpms, International Technology Corp, Scientific Communications,
Varo Inc. and WIhitehall Corp., bringing the total number ot companies in Investor's Corner to 110. Tb be listed in our monthly financial barometer, call DE or Nordby
International Inc. For personal research reports on any of the 110 cormpanies we monitor in Investor's Cornwer, call Nordby lnternational.
Thornton himself, it should be pointed out, makes the
stimulus error, since in strict observation of the stimulus
error, it is neither the object colors, nor the lights re-
flected from them, which are metameric, but the percep-
tion which has the property of metamerism.
Thornton demands in his introduction, rightly I think,
that metamerism's ''terminology must be based on long-
standing practice . ,,'' This is explicitly what I have
attempted to do at each occurrence in these definitions. 1
have gone out of my way to define each applicable word
in terms of the specimen and in terms of the light reflected
or transmitted by it. This language is repeated over and
over again to the point of redundancy. This course of
action recognizes Thornton's admonishment that the defi-
nitions fit long-standing practice. Thornton calls this rec-
ognition obfuscation and muddling, but it is Thornton's
positions that the definitions be at once of long-standing
practice and not refer to the metamerism of object colors,
which are inconsistent with each other.
A second major difference appears to be Thornton's
concept that the recommended definitions for metameric
terminology are somehow intended by the author to be
bolstered by the recommendations having to do with
spectral decomposition theory. Nothing could be farther
from the truth. The article recommends terminology and
symbols for two separate subjects, metamerism and spec-
tral decomposition theory. The reason that these two
subjects appear together in the same article is that there is
a common thread, but that common thread is the recom-
mendation of terminology, not that one set of definitions
supports the other. When the ISCC Metamerism Com-
mittee was asked to write an article defining the terms
and symbols for spectral decomposition theory, it had
just finished some work on metamerism in which many of
the definitions applicable to the metamerism section of
the article were formalized. It was conceived to be a good
idea to publish those in the same forum. There was no
attempt that the two sets of definitions should form an
''alliance . . . between venerable metamerism and a re-
cent mathematical construct'' as Thornton states.
There are several smaller points which, 1 think, de-
serve to be rebutted lest the reader think Thornton's pos-
ture on them might have some validity.
(1) Thornton speaks in several places about what he
calls the confusion of ''light'' and ''specimen,'' Most ve-
hemently he seems to oppose the concept that the N vec-
tor can contain reflectance or transmittance factors, and
about this he states ''that tristimulus values must invari-
ably be computed from a light and a color-matching ftunc-
tion'' (emphasis his). On the other hand, I gave in my
article equations whereby one could precalculate the
color-stimulus function and integrate using color-match-
ing functions, or one could precalculate weight sets for
tristimulus integration and integrate using reflectance fac-
tors. In the section on tristimulus integration, I was care-
ful to point out that the teachings of the two equations
must be ''kept in dmind and one or the other imple-
mented.''
Thornton says, ''N cannot do both.'' I pointed out not
only that N can do both, but exactly how it can do both.
(2) Thornton says in his critique that reference viewing
conditions apply only to specimens. He seems to over-
look the fact that color-matching functions refer to the
equal energy spectrum which become part of the refer-
ence viewing conditions for metameric lights. Thornton
himself* normalizes data from his visual experiments in
which matches are made at arbitrary energy levels cho-
sen by the observer. He normalizes these data to the
equal-energy spectrum. Thus the reference viewing con-
ditions for a pair of metameric lights are the equal-energy
spectrum and typically one of the standard observers.
(3) Thornton feels that the fundamental of a metameric
pair changes for every new set of color-matching func-
tions introduced. Not very fundamental, says Thornton.
If a new set of color-matching functions is introduced, it
is quite unlikely that the pair is still metameric. The very
concept, metamerism, implies a singular set of reference
uiewing conditions for which the pair is metameric. It is
quite unwarranted for Thornton to change viewing condi-
tions in this usage.
This does speak to the problem some persons have had
with the use of the term ''fundamental'' for this con-
struct, however. Those persons seem to think that the
term fundamental implies something intrinsic, or underly-
ing, about this spectrum, as opposed to any other spec-
trum. It doesn't. The term derives from a coinage by
Helmholz.' Helmholtz's piece written in 1887 anticipates
Cohen by a hundred years. Helmholtz, in a treatise called
''Epistemological Analysis of Counting and Measure-
ment,'' under a subheading titled ''Addition of Heteroge-
neous Quantities,'' cites color as one example where
there is a necessity for components to have the same
direction in what we would today call a vector space.
Only then, says Helmholtz, can these components be
combined additively. He calls these three suitably chosen
colored lights ''fundamental colors.'' By this he means
what we would today call orthonormal primaries. The
term passes to us meaning only that the spectrum is pro-
jected to an orthogonal reference frame. This makes the
spectrum ''fundamental'' because its primaries are un-
correlated.
Thornton calculates, and reports in Fig. 7(a) of his re-
cent article,' the fundamental of a fluorescent white day-
light which he has given as a reference to his observers to
be matched with a set of primaries which are near orthog-
onal. He reports the results in terms of the ratio of the
energy at each wavelength in the fluorescent light to the
sum of the energies in the three-band matching light com-
prised of a variable wavelength and two of the three ap-
propriately chosen orthogonal primaries. He never recog-
nizes that he has derived the fundamental of his
fluorescent white experimentally, considering the funda-
mental only a ''recent mathematical construct'' lacking
venerability.
(4) A semantic objection applies to Thornton's asser-
tion that ''all the tristimulus information about each spec-
green, red, yellow, and purple in Fig. I to simplify refer-
ence to the specific chromaticities, and should not be
taken as accurate descriptions of the appearance of the
test stimuli, The luminance of the background and sur-
rounds were slightly higher than those of the test and
comparison stimuli to ensure a clear border between the
stimuli and background,' Nominally, the test and com-
parison stimuli were set at 9 cd/m and the background
and surrounds at 10 cdlm-. In practice, as shown in the
Appendix, the luminance levels were always within 2:0.4
cdim-and the chromaticity coordinates within a radius of
0.005 of the specified values.
For each test and surround condition, there were 17
comparison chromaticities. They consisted of a ''com-
parison test'' and 16 other chromaticities that differed
from the comparison test along four different directions
in the CIE 1976 (u', v') chromaticity diagram. The com-
parison test was the comparison chromaticity that had
the same coordinates as the test or had been determined
empirically to match the appearance of the test. It was
presented on each trial during a run in one of the compari-
son stimuli positions. Four of the other comparison chro-
maticities were presented in the remaining positions.
The chromatic discrimination data reported by Wys-
zecki and Fielder' were used to select the directions
along which the comparison chromaticities were located.
One direction corresponded to the average direction of
the major axes of discrimination ellipses in a similar part
of the chromaticity diagram. The remaining directions
were 90', 180', and 270' from this direction. Thus dis-
crimination was measured along two orthogonal axes in
the CIE 1976 (u', v') chromaticity diagram passing
through the chromaticity coordinates of the comparison
test. The actual directions along which the comparison
chromaticities were located are given in Table I for each
test and surround chromaticity. They are expressed as
the angular separation from a line parallel to the u' axis
and passing through the chromaticity coordinates of the
corresponding test colour. The comparison chromatici-
ties were located at four different points along each direc-
tion with the distance between two successive locations
the same along any one axis.
A run consisted of 120 trials and lasted approximately
10 to 15 minutes. During it, the observer was seated in
front of the CRT in a dark room at a distance of 122 cm
from the screen. Viewing time and fixation* were not
restricted. However, the time between the onset of the
stimuli and the observer pushing the button on the mouse
was recorded.
The observer was instructed to scan the display as
quickly as possible comparing the test, located in the
center of the display, with each of the comparison stim-
uli, located in the periphery, and to select the comparison
stimulus that was most similar in colour to the test.
To indicate his or her response, the observer used a
mouse to position the black cursor over the comparison
stimulus of choice and then clicked the center button on
the mouse. Between trials, the stimulus configuration
was replaced by the achromatic background for two sec-
onds to allow afterimages to fade.
The preliminary experiment was carried out in two
phases. The first phase was designed to establish an ini-
tial crude estimate of the chromaticities that would be
most similar in appearance to each test against each sur-
round and was carried out on a single observer (S). The
second phase refined those estimates and determined the
comparison chromaticities for each condition and ob-
server. The stimulus configuration, task, and procedure
outlined in the general method were used for both phases.
However, only the 20 chromatic surround conditions
were run, since for the control conditions, the match or
comparison test had the same chromaticity coordinates
as the test. The comparison chromaticities for the control
conditions were determined during the training runs that
preceded the preliminary experiment.
The methodology in this phase differed primarily in the
selection of the comparison chromaticities. Since the pur-
pose was to establish a preliminary match to each of the
test stimuli against each of the surrounds, there were no
comparison tests, Instead, the comparison stimuli were
composed of 20 chromaticities located at five different
separations from the chromaticity coordinates of the test
along four different directions in the CIE 1976 (u', v')
chromaticity diagram. The directions did not lie along
two orthogonal axes passing through the chromaticity co-
ordinates of the test. Instead, they were based initially on
the direction and extent of the shifts in appearance re-
ported by Ware and Cowan' for similar test and surround
chromaticities.
On each trial, the five comparison chromaticities lo-
cated along a single direction in the CIE 1976 (u', v')
chromaticity diagram were presented. Four different di-
rections were tested during each run. An example of a
chromaticity set for the green test against the green sur-
round is shown in Fig. 3(a). Since, on any trial, all of the
colours might have been dissimilar in appearance to the
test, the observer had the option, in this phase only, of a
$gUDDllGADLLEllS are discussed later in the paper. A flexible surface coating is repositioned with
piezoelectric or electrostrictive actuators to smooth out the flow pattern and eliminate flow noise.
äL9lLlßüLlUg can be carried out by localized heating beneath the surface. In this case the responding
actuator is a patterned composite resistor.
The remaining methods of reducing drag are mainly passive in nature, although they could be combined
with a compliant wall. SUillLSilIlrLEALGlSgLhlD&.GAUlDES prevent pressure build-up allowing
molecular interchange between the fluid and wall. Eiblcls and (4giSS&GIUDDEAUUS also act to streamline flow,
as do thhe sailll4lillllDlu4YSLLUgSlGIS called OLDS. Carefully designed geomeies like this can be built
into smart ceramics.
In a demonstration experiment we have shown that for external pressure fluctuations, the smart material
can become more compliant than the medium in which the pressure variations take place. This has the direct
consequence of reducing acoustic reflection from the surface of the smart material.
In our test experiment (Fig. 6), one actuator is used as a driver (noise source), and the other as the
responder. PIT disks are used as the actuator elements. Sandwiched between the two actuators are two
sensors and a layer of rubber. The upper actuator is driven at a frequency of 100) Hz and the vibrations are
monitored with the upper sensor. The pressure wave emanating from the driver passes through the upper
sensor and the rubber separator and impinges on the lower sensor. The resulting signal is amplified using a
low noise amplifier and fed back through a phase shifter to the lower actuator to control the compliance.
A smart sensor-actuator system can mimic a very stiff solid or a very compliant rubber. This can be
done while retaining great strength under static loading, making the smart material especially attractive for
underseas applications.
On the other hand, if the phase of the feedback voltage is adjusted to cause the responder to contract in
ength rather than expand, then the smart material mimics a very soft, compliant substance. This reduces the
force on the sensors and partially eliminates the reflected signal. The reduction in output signal of the uppe
sensor is a measure of the effectiveness of the feedback system. As shown in Fig. 7, we can reduce the
compliance of our actuator-sensor material by a factor of six compared to rubber.
No single piezoelectric material possesses the optimum properties for send-receive transducers, and the
statement is probably true for sensor-actuator devices as well. Putting together the best sensor with the best
responder makes a lot of sense. During the past year we have been experimenting with the send-receive
transducer shown in Fig. 8. It is a four-layer device consisting of a polyvinylidene fluoride piezopolymer, a
3-3 fried PZT-polymer composite, a poled PZT ceramic, and a 0-3 tungsten-polymer backing layer. In
connectivity notation, the transducer can be described as 2-2 (3-3) -2-2 (0-3). Metal electrodes are inserted
between all four layers to give control over the three piezoelectric layers [3].
There are several interesting properties of this transducer. The acoustic impedances are graded from hne
stiff PZT ceramic to the PZT composite to the complaint PVDF polymer. This eliminates much of the
reflection loss in transmitting ultrasound from P2ZIT to water. A second possible advantage is in
interrogating the outgoing signal and comparing it to the return signal. With he PZT as transmitter and the
composite and polyvinylidene fluoride layers as sensors, the outgoing beam can be sampled as it leaves the
ceramic. Since no two transmissions are identical, it is important to monitor the outgoing signal in order to
calibrate the intensity and frequency spectrum of the return signal.
Based on the physical properties of the constituent materials, the equivalent circuit of the trilaminate
composite transducer has been established. The frequency response of transmitting, receiving, and voltage
transition gain for different layers and their hybrid connection have been computed and compared with the
8perimental results. The frequency response of the composite transducer is considerably improved by
changing the connecting modes between the layers. Experimental results show that different layers make
different contributions to the frequency dependence of the sensitivity. The transducer impedance, sensitivity,
and bandwidth can be adjusted according to the application.
The response of a shell structure can be determined by the modal exrpansion method [ 15] in which
the total dynamic response is a summation of all participating modes U;y with individual modal participation
factor k.
where i= 1, 2, 3. Since for a distributed system, the number of modes are infinite. Thus, k goes from l to
infinity. This displacement will be used to estimate the distributed piezoelectric sensor output for feedback
controls. Simplification of the theory to a flexible beam/plate will also be demonstrated in a case study.
In this section, the piezoelectric theory is briefly reviewed. This leads to a development of a distributed
sensing theory for flexible shells.
The piezoelectricity is an electromechanical phenomenon which couples the electric field with the elastic
field. In general, a piezoelectric material responds to mechanical forces and/or pressures and generates an
electrical charge. This is called the direct piezoelectric effect. Conversely, application of an electric field to
the material can produce mechanical stress or strain which is called the reciprocal, or converse piezoelectric
effect. bn this paper, he direct effect is used for distributed sensing and the converse effect is for the active
distributed control of flexible structures. There are two fundamental equations representing the direct and
comrerre piezoelectric effects, respectively [ 16],
where (T) is e stess vector (ü.e., [T) = IT;; T;3 y 7;y y; T;3)5; [cP1 is he elasticity matris
evaluated at constant dielectric displacement, [S) is he strain vector; [h is he piezoelectric constant matris;
[D) is he electric displacement vector; [.]! indicates he matrix transpose; [E) is tuhhe electric field vector; [5A ]
is the dielectric impermeability matrix evaluated at constant strain; [S%] is the compliance matrix measured at
constant electric field; [ ] is the dielectric matrix evaluated at constant strain; and [d] is the piezoelectric
constant matrix. The constitutive equations formulated in Eqs. (8) and (9) are assumed to be instantaneously
both mechanically and electrically balanced and the two effects can be decoupled. In this research, the
piezoelectric material used is a piezoelectric bi-axially polarized polyvinylidene fluoride (PVDF) polymer.
The PVDF polymer consists of a large number of dipoles-regions with positive and negative charges. The
piezoelectric matri [d] of a PVDF polymer can be expressed as
If he PVDF polymer is electrically polarized without being mechanically stretched, the piezoelectri
coefficient d; is equal to d;s-
In the distributed sensing application, only he direct piezoelectric effect is considered. It is assumed hat
the distributed piezoelectric sensor layer is much thinner than that of the shell structure. The piezoelectric
sensor strains are assumed to be constant and equal to the outer surface strains of the shell. Assuming the
electric charge is balanced in the piezoelectric sensor layer (an insulator), one can derive an electrical charge
equation by using the Gauss hheorem [9,12],
where V is the differential operator. From the system configuration discussed earlier, only the transverse
electric field E4 is considered so that the strains and dielectric displacement D4 are independent of the o;.
According to Maxwells equation, the electric field can be related to the electric potential by
The voltage across the electrodes can be obtained by integrating the electric field over the thickness of
the piezoelectric sensor layer, ie.,
where hP is the thickness of the piezoelectric sensor layer; S;? and Sy? are the in-plane strains in a; and o
direction, respectively. Note that the superscript ''s'' denotes the distributed sensor layer. The in-plane
strains of a generic thin shell due to bending can be expressed as [9,15]:
where d;and dy* are the distances measured from the neutral surface; A; and Ay are Lame's parameters; u;
A4u4 are the in-plane displacements; R4 and Ry are radii of curvature, and ug is the transverse displacement.
Theelectric displacement D4P can be expressed in terms of the voltage t
IItegrating Eq. (12) over the electrode surface Ar yields a charge. Setting the resulting charge
%gression equal to zero gives an open-circuit voltage o? of hhe sensor,
Recently work has appeared [24, 25] relating to the use of special types of surface guided acoustic waves
for sensing fluid properties. In particular a leaky wave has been suggested [24] for use in a fluid
microsensor. The leaky wave, however, attenuates in the direction of propagation hence the effective path
length is small. Also theoretical work has suggested [25] that shear surface waves such as the
Bleustein-Gulyaev wave and the surface skimming bulk waves might also be used in a fluid microsensor.
A sensor which allows an acoustic wave to propagate through a fluid instead of simply interacting with
tUhe fluid-crystal boundary as in the case of the fluid BAW and plate mode sensors might offer the possibility
of being more sensitive to subtle changes in fluid properties. It is the purpose of this paper to investigate
both theoretically and experimentally two sensor configurations which allow the acoustic wave to not only
interact with the fluid-crystal boundary but also to propagate through the fluid layer.
In order to couple SAW energy into a fluid layer the two different geometries shown in Figs. 1 and 2
will be studied.
The geometry shown in Fig. 1 essentially consists of a single Y2Z-cut LiNbO4 SAW delay line upon
which a small amount of fluid has been deposited. The fluid delay path covers only a small fraction of tthe
delay path and is constrained at the top surface by a glass plate which is parallel to the LiNbO4 substrate.
Normal capillary action holds the fluid between the two solids. When the IDT is excited a SAW propagates
along the crystal surface and is incident on the fluid layer. A significant portion of the SAW energy is then
converted into a bulk compressional acoustic wave which radiates into the fluid at an angle, %;, with respect
to the normal to the crystal surface. This angle is defined as follows [26],
where v; = velocity of the compressional acoustic wave in the fluid and v, = SAW velocity.
The acoustic wave in the fluid is incident at the top solid and radiates some of its energy to acousti4
waves which are transmitted into the solid. The reflected acoustic wave from the top solid-fluid interfac
propagates through the fluid and is incident at the piezoelectric substrate at the angle, %,. At this point he
wave is converted to a SAW and propagates to the output IDT.
shows the result on a graph, with the active sensing
point's measurements indicated by line 1 and the
measurements of the sensing points behind it shown by
lines 2 through 4. This figure shows how the
measurements of the sensing points differ from one
another. Line l sharply indicates all variations in the in-
side wall surface temperature, even sudden brief shifts,
while lines 2 to 4 record lower temperatures and less
sharp variations. Thus the centermost sensing points
quite sensitively reflect temperature phenomena inside
the furnace.
To correlate pig iron temperatures with wall
temperatures, three kinds of information were process-
ed from the data collected by the brick thermometers
previously set in the wall and the FM sensor-Ts. These
included (1) the total sum of the temperature data
recorded by the centermost 28 FM sensor-Ts (FMCT),
() he FMET diferences per minute (FMT), and (3)
the brick temperature differences per minute (BR6T).
Here, 'difference' is defined as the value remaining
when the recorded temperature at the last sampling
time is subtracted from that recorded at the running
time.
Figure 4 shows the results of correlation analysis.
Though most of the data showed no definite correla-
tion, FMäT showed a clear (negative) correlation with
a six-hour delay. This result indicates that decrease in
heat level of the pig iron can be predicted by means of
4n earlier tap, using the FM sensor-T difference values
(FM&T). As mentioned in the introduction,
temperature variations on the inside wall of the blast
furnace seem to arise from channeling, wall deposit ex-
foliation, and excess peripheral gas flow. These
phenomena are the result of conditional changes such
as wall deposits, variation of the ore/ coke ratio at the
periphery caused by nonuniform charge distribution,
and sudden changes in blast conditions.
Solution C loss and the Ny content in the furnace
top gas are also conventionally used as heat-level in-
dices. A statistical method similar to the one used for
blast furnace wall temperature is used to calculate the
correlations between these indices and pig iron
temperatures. In fact, both solution C loss and Ny con-
tent demonstrate a clear correlation with only a five-
hour delay (see Figure 6). This suggests that the
greatest reliability of prediction would be obtained
through a combination of these conventional heat-level
indices with the FM3T.
In order to uncover a logic appropriate to the correla-
tion of combined indices, several possibilities were
tested by off-line analysis. The results are shown in
Table 1. Each value was checked to see whether or not
it exceeded the threshold level. During the period of
analysis, decreases in heat level occurred 19 times. A
combination of the conventional indices showed a
forecasting rate of 73.7% (the rate of successful
forecasting of decreasing heat level) and a 777% suc-
cess rate for predicting alarms.
In contrast, when the conventional indices of solu-
tion C loss and N content were combined with
temperature variation measured by the FM sensors, a
forecasting rate of 73.7% and an alarm prediction rate
of 87.5% were achieved. That is, the combination led
to higher accuracy.
Figure 7 shows an on-line method for signalling
decreases in heat levels. There are two condtions under
which an alarm is signalled. One is at a certain time
after FM3T and solution C loss or Ng exceed their
thresholds. The other is when solution C loss alone ex-
ceeds the threshold. The latter condition is intended to
detect heat-level decreases caused by variations in the
quality of raw materials being used.
Figure 8 shows the construction of the on-line
system, which consists of a personal computer, a floppy
disk unit, an M/T device and a printer. The personal
computer takes the temperature information from the
FM sensors and various data from the process com-
puter through a scanner, processes these data, com-
pares the processed data with the thresholds, and prints
out an alarm including a report on the details of the
alarm. The M/T device is connected to the personal
computer for off-line analysis.
For a period of 98 days (including 784 taps) an on-line
system for forecasting heat-level decreases was in opera-
tion at Kobe No. 3 B.F. Figure 9 shows an example of
its output. In this example, a temperature change was
detected by the FM sensors first, after which solution C
loss and N content exceeded the thresholds. At that
point an alarm sounded. After a subsequent interval,
the heat level of the pig iron fell. Table 2 shows the rate
of forecast of heat-level decreases. Successfully
predicted alarms in this case can be defined as occa-
sions on which the alarm sounded in a tap prior to the
decrease in heat level of the tap. With a target pig iron
temperature of 1495 C, heat-level decreases were defin-
ed in two stages. The total forecasting rate was 60%,
while the forecasting rate for the larger decrease in heat
level (below 1470C) was 67%. Table 3 shows the
percentage of successful forecasts for a total of 41
alarms. The five 'indistinct'' cases refer to the instances
when the heat level did not actually decrease by effect of
an action after an alarm. Among the 41 alarms, in 34
cases the forecast was successful, and in 2 cases the
forecasts proved to be false, giving a success rate of
83%.
Figure 3. Surface hardness was mainly dependent
upon the carbon content of steel; and other alloying
elements, the prior microstructure and cold forging had
little effect.
A steel's hardenability is usually assessed by calculating
the ideal critical diameter based on the chemical com-
position and the grain size. In this study, ideal critical
diameters were calculated using the grain size at the
case depth point. For the boron-containing steels,
calculations ignored the boron factor. Figure 4 shows
the results. The case depth related almost linearly to the
ideal critical diameter. As can be seen from the figure,
however, boron steel and boron-free steel showed dif-
ferent characteristics. Even with the same ideal critical
diameter, the boron steel showed a larger case depth
than boron-free steel.
The effect of boron was to increase the case depth
by about 35%. This value is less than the boron factors,
Fgg, from 1.7 to 2.0 for the present steels which would b-
observed by the [ominy test.
The effects of alloying elements were identical to thos
of the as-rolled steel, and no special effect was found
from quenching and tempering prior to induction
hardening.
In contrast to previous results, spheroidize-annealing
tended to reduce the case depth and increase data scat-
ter,
The relationship between case depth and alloying ele-
ment for the cold-forged, as-rolled steel was in-
vestigated. The results show that the case is slightly
shallower than that of the raw, as-rolled material. TH+
reason for the lower induction hardenability seems )
be the reduced grain size.
The grain size number of the cold-forged steel was
about 0.5 to 1 higher on the ASTM scale than that of
the standard material. However, the typical effects of
the alloying elements existed, and the case depth
related well to the ideal critical diameter.
The reason why the boron-containing steel did not
show satisfactory hardenability was considered. Unlike
other alloying elements, boron shows an unique
mechanism to improve hardenability. The equilibrium
segregation of boron at the austenite grain boundary oc-
curs, and it decreases the grain boundary interfac d
energy. Boron suppresses the nucleation of ferrite, a d
as a result hardenability is improved. To appreciate the
full boron effect, boron should be resolved, migrate to
the austenite grain boundary sites, and then form
precipitates.
With the assumption that the reason for the weak
boron effect on induction hardening was insufficient
time for resolution and migration of boron, the effect of
boron on case depth was examined by varying the in-
duction heating time. Using the as-rolled steel, induc-
tion heating was carried out for times of 5, 7.5 and 10
seconds, other conditions being the same as those in the
previous experiment. The effect of heating time on the
boron effect, F, is shown in Figure 5. As expected, the
effect of boron on induction hardenability approach -d
that found in the Jominy test as heating time increas d-
A temperature gradient is generated in the test
pieces by induction heating, and it changes with time.
T'o analyze induction hardenability, it is necessary to
know the thermal history of hardened sites. Ther-
mocouples were attached in the test pieces to measur
temperature at the point with 50% martensiti
microstructure. Figure 6 shows the temperature-tim<
diagrams. A parameter, P, was established as an indes
which shows the thermal diffusion over the a; transfor:
TE UNTEd NATONs Water and Sanitation Decade, now at its mid point, has not
attained the 'household word' status for which its sponsors might have hoped. However,
after early disappointment there is evidence that real progress has been made in the
provision of water supplies to under-developed countries, and whilst some 1850 million
people, about two-thirds of the world's population, lacked access to safe drinking water at
the start of the decade in 1981, some 300 million have benefited since.
Many governments have embarked upon long-term plans to develop water supplies,
but many more lack even the basic resources to prepare accurate surveys of needs, and
crippling foreign exchange shortages along with mountains of debt rule out any
expenditure on the materials needed for even simple schemes. Sierra Leone, in tropical
West Africa (Fig. 1) is firmly in the second category; it lacks indigenous fossil fuel and the
oil crises of 1973/74 and 1979 have had disastrous effects on an already precarious
economy, Electricity is diesel generated and regular power supplies exist now only in the
capital where power cuts are a daily feature, due both to lack of fuel and the age of the
equipment. Electrical machinery is subjected to wild and continuing voltage fluctuations
with consequent shortening of life. Transport is a real problem because of fuel shortages,
an ageing vehicle stock (for replacement or even repair of which there is no foreign
exchange) and unmaintained roads.
At the same time, the population, currently about 3.75 million, is growing rapidly and
the country has long since become a net importer of rice, the staple diet. Adult life
expectancy is low at 47 years and child death rate is tending to increase. Very few people
outside the capital, Freetown, have access to any organized water supply.
It was against this background that Water Aid elected to develop a programme of rural
water supply in Sierra Leone and the author used a spell of leave to make an exploratory
visit in mid 1982 to evaluate possibilities for Water Aid's involvement in water and
sanitation schemes. These were, as far as possible, to be community based and replicable.
Homer it was who said that if one wanted to tell a good tale, one should start in the
middle! It is very difficult to define a starting point for community-based water supply
schemes for, like most engineering projects, there is a considerable amount of iteration
involved as various factors are juggled.
Before becoming involved in engineering detail, two general concepts should be
discussed, namely 'community development and promotion' and 'appropriate tech-
nology'.
There are then the matters of motivation and organization. The tolly ot dumping a
ready-made water scheme or, for that matter, irrigation or tishery proiect upon a
primitive community is now well recognized. Aid projects in the past otten took little
account ot the needs and aspirations ot the users or of their traditions, habits and culture.
Worse still, little or no effort was put into teaching them how to operate and maintain the
new assets.
A vital starting point, therefore, is the community; confining Water Aid's or other
donors' funds to provision of engineering and materials supply, and encouraging the
village to carry out its own construction work, not only enables resources to be spread
much further, it also ensures that people in the community share in the problems and
satisfaction of creating something new and useful. Precious time, which at subsistence
level would otherwise have been invested in scraping a livelihood, is invested in the
project and this will do much to ensure its future use and upkeep. Perhaps as important as
any of these, the community, by having had to organize itself for the construction work,
will have developed its organizational ability and its self-confidence. It will feel better
able to help itself in other ways,
To brtng all this self-development about requires much effort or 'promotion' and the
thoroughness and care in this early work, which must be based on mutual trust and
sympathy, will determine the ultimate success of the project just as much as sound
engineering. In summary, the approach must be non-didactic and in the current jargon,
'bottom up' rather than 'top down'.
It follows naturally from what has been said above, that if schemes are to be
constructed and maintained by village labour, these must be simple. Only basic hand
tools will be available and whilst loads may have been lifted onto trunks at the railhead or
store, they must be capable of being off-loaded without such aids and be light enough to
manhandle to site several miles from the road. Moreover, the absence of mains electricity
and the extreme shortage of diesel fuel virtually rule out mechanically driven pumping
plant. Spares would, in any case, be very difficult to obtain. Shortage of foreign
exchange, referred to already, means that importation of water treatment chemicals is
unlikely to be possible and therefore sources need to be intrinsically safe or capable of
being protected.
Likely sources are springs, streams and groundwater. Shallow groundwater can be
easily exploited by wells but these are more likely to suffer contamination (50 coliforms
per 100 ml is usually considered a reasonable source). Deeper groundwater requires some
form of pump but will probably be of safer quality. Where it is possible to harness springs
or unpolluted perennial streams, simple gravity schemes can be developed with
convenient distribution to standpipes. Lined and covered shafts can be sunk to improve
swamp sources (Fig. 2).
In Sierra Leone it was first necessary to have the blessing of the Ministry of Energy and
Power Water Supply Division (MEP), for although this government department has no
funds, it is politically strong and no water project may proceed without its agreement. In
fact, Water Aid uses free office space in the MEP building and has also drawn on its
labour pool for specialist plumbing and other craft skills. Although it has no funds to
spend on water supply, a large workforce is maintained, albeit on low, often intermittent,
and always overdue, pay.
Having established a figure tor current population, future growth has to he considered.
Here the crystal ball can he particularly obscuredd and the designer must look hard at such
variables as proximity to a provincial town, local industry te ,g. fishing, farming, mining),
communications, relative prosperity, etc. At hest, a torward view ot only about l) vears
can be taken and a population increase of 3) per cent in this period may prove
conservative if clean water improves health.
Turning to per capita consumption, this is dependent upon whether provision is from a
well, a standpipe, or other source, and how fir the water has to be carried. Villagers with
reasonable access to standpipes (say within 15) m travel) are estimated to use 1) to 40
litres per capita per day (lcd), this being higher than where water is drawn from wells. The
author considers that 45 lcd is a reasonable target for standpipe based schemes,
It comes as something of a shock to the engineer used to UK based asset lives to realize
that most aid agencies consider a life of not more than 20 vears to be appropriate for pipe
based water schemes in small communities. This relatively short life is justified on the
grounds that the very existence of the community in 20 years' time cannot be guaranteed
and if it prospers, then it should be able to afford some amelioration of the provision.
There is more of a problem with village wells. Many aid agencies have encouraged
provision of simple wells, lined, if at all, perhaps only over the top metre or two. Costs
are minimized and funds spread as far as possible in primary provision, Unfortunately,
unlined wells are likely to have a useful life of only a few years before they collapse but
the provision of a full depth lining with facilities for deepening the well, as shown in Fig.
4, will extend its life probably beyond 20 years. Unfortunately, the cost is likely to rise
tenfold and the lined well shown, with its external protection against animal and bird
pollution, costs about f20 to E30 per capita. Nevertheless, the author is inclined to believe
that it is preferable to invest in a safely constructed, lined and covered well.
Pipe selection is dealt with later, but availability, transport and handling of other
materials, such as cement, sand and aggregate must be considered carefully. The only
sand available for the Peninsular Villages schemes is beach sand and the author wrestled
long with deeply instilled principles about the harmful effects of sodium chloride in
concrete before agreeing to the use of sand from the very back of the beach which had
been lashed by tropical rainstorms and was thus, hopefully, low in salt content.
Provision of sand and aggregate is customarily a community responsibility. The
traditional method of producing aggregate is by breaking stones with a hammer into even
smaller pieces until sufficient 'combined aggregate' is to hand. Considerable advance
warning is needed for the provision of anything more than one 'headpan' load. Often this
stone breaking is a task given to the older people no longer fit for active farming.
Climate has a severe effect on construction. In the rainy season, work becomes
impossible in rainfall intensities of 200 mm in 24 hrs-often more than 5 m in 3 months.
Works become entirely inundated and travel in any case becomes very difficult because of
washed out roads. In wet and humid conditions, laterized soils and rocks can become
highly corrosive. Strong ultraviolet radiation can be very harmful to plastic materials in
dry seasons.
There are no pipes manufactured in Sierra Leone and so these must be imported.
Cement is sometimes produced but is dependent on toreign exchange being available for
the import of clinker. Shipping costs and sailing times are therefore important
considerations and materials omitted from an order may take months to replace, with the
earlier. Where economics show that it is cheaper to provide storage at the village, then
the trunk main is, of course, designed for steady aserage flow. Where the trunk main is
very short or where a suitable tank site is not available at a village, then the main is
designed to accommodate peak flows.
The criteria used by Water Aid in Sierra Leone are the provision of one tap per 1(0
households with standpipes spaced not further apart than 150 m. The distribution system
is designed for peak flows of about 2: times average, this figure being widely considered
suitable for such systems.
The size of taps needs consideration; V2 inch BSP taps have the merit of throttling flow
so that when taps are left running wastage is reduced. On the other hand, ' inch BSP
taps are much more robust and are likely to survive longer. For the Peninsular Villages,
V2 inch taps have been adopted and their life will be monitored. Isolating stoptaps are
provided at standpipes to facilitate rewashering.
A great deal of thought was devoted to standpipe design; the essentials are a well
supported tap, a free draining bucket stand and proper arrangements to lead spillage
away to a drain or soakaway (to avoid mosquito breeding pools). The design is shown in
Fig. 7.
The cost of importing sectional tanks is fairly prohibitive and so two designs have been
evolved using local materials. Figure 8 illustrates the more straightforward tank consisting
of locally won, random masonry walls on a concrete floor and with a roof either of
galvanized steel sheets or of concrete. The latter is preferred as the former are valuable
and attractive and highly likely to vanish in the night.
At first sight the second design (Fig. 9) may appear extravagant since it employs
concrete blocks apparently to act as mere shutters. However, plywood is extremely
expensive, being imported, and would be very difficult to form to curves. Labour is very
cheap (free in the narrow context of the funding of these schemes, if not in the sense of
the village economy) and concrete blocks are readily made on site, One village has turned
out 1200 blocks in 3 days.
Where no restrictive (or safety!) regulations exist, it is surprising what can be made to
work. The author has examined a cylindrical tank, just over 2 m deep, constructed of a
single width of 150 mm concrete blocks, bound on the outside with post-tensioned steel
tendons. It was watertight!
Water Aid makes projections of probable maintenance costs and these are made known
to villagers during the promotion stage. At this same stage, a village water committee is
usually formed to administer revenue collection for maintenance, which will include
purchase of simple spares and reimbursement to the village maintenance technician(s).
Outside the Peninsular Area, this responsibility is usually handled by the existing native
administration.
Before work begins, monies are collected from the villagers by their committee,
equivalent to the estimated cost of the first year's maintenance and this money is
deposited in a trust account. So far no community has been found which is so abjectly
poor as not to be able to raise the small sums needed.
By these means, full responsibility for looking after the scheme which they themselves
have built, devolves upon the villagers. This must surely be the best possible means of
approach is to construct a demonstration latrine based on the Ventilated lmproved Privy
(VIP) developed in Zimbabwe (Fig. 10). Thereafter, yillagers are encouraged to copy this
design, with Water Aid providing the materials for the concrete cover slab and for the
vent pipe. The superstructure is made up from local materials (bamboo. grass, mud
blocks, etc.).
Communal latrines are not acceptable in the villages and provision has to be on a
household basis or, quite frequently, as a shared facility for an extended family (blood
brothers, grandparents, etc.). Sanitation is inevitably more difficult to 'sell' than water
supply!
The Peninsular Villages' project was Water Aid's first venture in the country. In the past
six months a second base has been established at Kenema in the Eastern Province (see
Fig. 1) and various village supply schemes are under way. In addition, Water Aid
personnel are working with a United Nations Development Programme (UNDP) mission
which has also selected the Eastern Province for a major project. This area may see
Water Aid's first venture into urban work, for the town of Kenema itself (population
55(000) is in dire need of assistance.
Increasingly, there is involvement with other agencies, such as with the Catholic Relief
Service for supplies to a Chiefdom village (Mabonto), and with church and school groups.
These notes are being written in February 1986 on departure from Sierra Leone. The first
disturbing news of a suspected cholera outbreak on Yelibuya Island on the north-west
coast began to circulate some ten days ago, with those afflicted dying within twelve hours
of feeling ill; personnel on both the EEC funded fisheries development project on the
island and the similarly funded agricultural development in the adjacent Kambia area
have been evacuated to Freetown; this has caused great dismay and foreboding in the
local communities. The disease appears to be spreading along the river Scerces and seven
days passed after the initial reports before the Ministry of Health managed to mount an
investigation mission. This is a poignant reminder of the tenuous nature of the lives of
hundreds of millions of people in underdeveloped countries.
It will take many decades to provide all the world's people with safe water and
sanitation but without the stimulus of the UN Decade the timescale would have been
immeasurably extended. The formation of Water Aid as a direct answer to the challenge
of the Decade has revealed a most remarkable capacity for caring and giving within the
water industry and its consumers, greatly reassuring in a world so often branded as selfish
and cynical.
This account of one small corner of Water Aid's work has, of necessity, been confined
to a somewhat bare description of project development. Behind the work lies the
dedication and perseverance of Water Aid's field personnel, living, working and
journeying in taxing and often frustrating conditions. Being unable to respond to the real
anguish of many of the communities visited can be a harsh and deeply wounding
experience and one which most certainly is never forgotten. Having to say no to a
community for whom the arrival of the Water Aid Vehicle is greeted as an answer to long
prayers is very hard. By contrast, the joy brought by a successfully completed scheme is
reward of a very high order. These successes, however small, mark steps on the road to
solving this immense problem.
Numerous methods are available in the literature for order reduction of single-
input-single-output (SISO) system models. Among them, error minimization in
the frequency domain has the distinct advantage that it is applicable to both
rational as well as irrational transfer functions (T.F). The method is also applicable
to the reduction of unstable system models.
This paper shows that the method, in its present form, must be augmented to
realize its full potential. The following major improvements are incorporated in the
method. Firstly, a logical procedure is employed for the selection of ''appropriate''
reduced-order transfer function structures prior to parameter optimization. This
substantially enhances the computational efficiency while eliminating the occasional
stability problems encountered in using the method. Secondly, an efficient fre-
quency domain quality criterion has been provided to rank reduced-order models.
This avoids the need for time domain simulations during the search for an optimum
reduced model.
It is shown that the method is particularly effective in reducing the T.F.s of full
order controllers to facilitate practical implementation. The effectiveness results
From the above example it is clear that for any index of quality to be meaningful.
it must indicate the closeness of an approximate frequency response to the original.
With this as the guideline, the following normalized index of quality, 7, is adopted:
Note that since n is computed in the frequency domain, it can be used for the
reduction of both stable as well as unstable systems. Also, if the original system is
stable (in view of the Parseval theorem ). n has the time domain interpretation of
''relative energy content'' of the impulse response error if o, is sufficiently large.
It is important to recognize the need for a quality criterion, 7, separate from the
error minimization index, J. The form of J as given in Eq. (III.1)and the weighting
function, W,, as given in Eq. (III.2) are chosen. These give relatively more import-
ance to the accuracy of the reduced model in those regions of o) where the magnitude
of G(jo) is relatively high but, at the same time, make this importance diminish at
high frequencies. However, if there are several T.F.s with widely different frequency
response characteristics, their reduction may entail dissimilar weighting functions.
In addition, their unnormalized energies may differ substantially. Under such a
situation, the minimums of J obtained for the various reduced T.F.s cannot be
used to compare their relative accuracies. Although n, as given by Eq. (III.3), may
not be the best index for error minimization in this context, it will supply the needed
measure of accuracy.
As discussed in Section II there are several techniques available to obtain the
parameters of a reduced-order T.F. once its structure is specified. The basic goal.
of course, is to find those values of the parameters which will give a minimum value
for the index, J, in Eq. (III. 1).
The iterative method developed by Sanathanan and Koerner (2) as modified by
Payne (6) and further generalized recently by Stahl (8) is found to be very effective
in dealing with the nonlinearity of J with respect to the parameters. Any T.F.
structure can be handled by the method. Also, attributes (such as the steady-
state gain) of the original model can be assigned directly in the reduced model.
Furthermore, the method has the advantage that an initial set of ''guess'' values
for the parameters is not required, and the rate of convergence is usually fast.
The primary disadvantages of the algorithm, as mentioned in Section II, are as
follows:
parameters, as well as ons, it is very possible that J has more than one local
minimum. If these minima arc located nearby, it is possible that the Lth iteration
is not necessarily converging to the minimum to which the (L - l)h iteration
was converging. Iherefore, an effective way to stop the oscillations is to freeze the
weighting function beyond a certain number of iterations, and let the parameters
converge to a local minimum.
h, The second disadvantage is more troublesome. Since the method is an uncon-
strained optimization procedure, the synthesizcd model could, at times, turn out
to be unstable even when the original system is stable. As discussed in Section
III.1, one of the reasons for this anomaly is an improper choice of the T.F.
structure. But, even with a reasonable structure, stability problems might be
encountered occasionally. It turns out that the synthesized transfer function
may have a pole-zero pair on the RHP almost cancelling each other. This
problem is discussed further in (8). Ignoring such pole-zero pairs, as suggested
in (7), seems to be just a quick fix to this problem, without any theoretical
motivation.
Procedures involving direct search techniques (21) are employed to overcome these
disadvantages. In particular, the iterative technique developed originally by Nelder
and Mead (22) is cffective for T.F. parameter optimization (23).
The simplex algorithm deals directly with the nonlinear index, J, of Eq. (III. 1).
In addition, the algorithm has the facility to accommodate constraints on the
parameters. This latter aspect is valuable, in that the stability problems can be
eliminated. The requirement of stability is translated into individual bounds on the
parameters of the denominator as given below:
where q, > 0 for i= 0, 1, 2,.., and q4 = 0 if the denominator degree is even. The
numerator is left as a regular polynomial with no constraints on the coefficients,
thus allowing both LHP and RHP zeros. This approach increases the nonlinearity
of J, but it seems to have an insignificant negative effect. Another advantage of the
above form for (?(s) is that when the original system model is unstable, a suitable
number of coefficients can be constrained to be negative when the structure is
established, thereby forcing a specific number of poles of the reduced model to be
gn the RHP (24).
A serious disadvantage of the simplex algorithm, like all other search methods,
& that it requires a set of initial guess values for the parameters. Although con-
Vergence is guaranteed, the rate of convergence is affected by these initial values.
$0 overcome this problem, a two-stage approach is adopted: in stage one, the
trix adaptation method (8) is employed and a specific number of iterations are
tade. With the resulting parameter values, the simplex method is initiated in stage
IWO.
Several examples are given to illustrate the wide range of applicability, effec-
Weness and, most importantly, the consistency of the improved frequency domain
ethodology.
for the transmitter and rcceiver transmittcr pair as for the recciver alonc, but this
is due to difficulties in measuring the true sound pressure input at the transmitter
earcap holes. F rom these comparisons we can conclude that all major dynamic
effects are adcquately simulated by the system bond graph. These models would
then be suitable for a design optimization procedure to improve the system's
frequency response.
natural numbers. The gray-tone band ranges from a lower limit of l, representing
''black'' to an upper limit for the x levels of L, representing ''white''. The cor-
responding reflectance for an image is 0% up to 100%. Thus, each of the mem-
bership functions corresponds to a language variable ''Y is black''. Note that, in
general, with increasing x, the corresponding degree of membership for the level
''black'' decreases. Therefore, we say that the grade of membership ,(x)decreases,
where A is used to denote a fuzzy subset named ''black''.
The formulas for each of the curves are given as follows :
Figure l(a) represents a curve based on the normal distribution :
with a positive coefficient k > 0.
Figure l(b) denotes a piecewise linear function:
Note that in the interval [a,,as], uu decreases linearly.
Figure l(c) is a curve with ridge-shaped distribution:
At the interval j4 4,a;], p decrcases by cosine law defined in the interval [0, ].
The curve shown in Fig. l(d) is defined by the function
This is an inverted S-type function. It exhibits a monotonic characteristic with
negative slope. We have selected these for distinctive distributions as the basis for
fuzzy membership functions. For convenience in citing these functions, we will
sometimes simply call them cases (a), (b). (c) and (d), respectively instead of
normal, p.w. linear, ridge-shaped and quadratic distributions.
We note in passing that another approach to determination of membership
functions has been statistically based (11). In that recently published study, the
piecewise linear curve of our Fig. l(b) was approximated by the membership
function corresponding to an exponential distribution function. That work is
IOCused toward noise contaminated linear communication systems.
The degree of fuzziness of a system with L gray-tone levels and given fuzzy
tmembership function u(x) can be measured by fuzzy entropy defined by DeLuca
and denoted by H(L):
here
Suitable computer programs were written and executed for the four distributions.
Oe of the computed results are listed in Table I. When L is large enough,
he corresponding entropy is denoted as H(oo), which renresents the available
WSormation content in the image. In our programs we consider L = 2500 as a large
nurmber for he gray-one kvel.
lhe following data are taken from Table I:
The seminal work of Cook [COOK71] and Karp [KARP72] has provided
us with a powerful tool, that of NP-completeness, to classify certain important
computational problems to be intractable (ie., not in P) with a high degree of
confidence tu.e., unless P NP. which is believed to be very unlikely ). While this
tool has been successfully applied to hundreds of computational problems, there are
some important problems which have eluded this approach. An important tech-
nique to handle problems that appear to be intractable, but have resisted a proof of
NP-completeness was proposed by Adleman and Manders [ADLE77]. We briefly
sketch their approach: Note that central to the theory of NP-completeness is the
notion of a reduction. Cook originally used reductions based on oracles (called a
Turing reduction ), but Karp found it sufficient to use many-one reductions, a
restricted form of Turing reduction. Many-one reductions have been most widely
used since. Such a reduction is carried out by a deterministic Turing transducer
(a Turing machine with an input and an output) running in polynomial time.
Adleman and Manders suggest the use of a nondeterministic transducer. Their
reduction, called a y-reduction, is defined as follows: A problem A is said to be
y-reducible to B (A s. B) if there exists a polynomial time-bounded nondeter-
ministic Turing machine transducer M which has the following properties called
''reliable'' and ''nice,'' respectively: For any input x: (i) at least one computation
path of M produces an output y and (ii) for every output y produced, x is in A if
and only if r is in B. It is easy to show that if A g. B for all A e NP, then B is not in
P, under the assumption NP w co-NP (a hypothesis widely believed to be true.)
This can be viewed as an evidence of the intractability of B. (B was called a
y-complete problem. ) Adleman and Manders showed some number-theoretic
problems to be y-complete, and these problems are not known to be NP-complete.
Our work was originally motivated by a class of problems in concrete complexity
theory (outlined in the following paragraphs ). We wanted to show that these
problems are intractable. We succeeded in showing that some natural and impor-
tant special cases are NP-complete. Then we wanted to handle the general problem
in a unified manner. It turned out that the tool we needed was a reduction known
as s4 !' reduction (strong non-deterministic Turing reduction ), a natural extension
of y-reduction, defined and studied earlier by Long [ LONG82]. The s ;' reduction
of Long is also a weaker form of a nondeterministic reduction studied by Ladner,
Lynch, and Selman [LADN75] and Selman [SELM78].
A problem A is said to be s ;' reducible to B (written A s, !' B) if there exists a
polynomial time-bounded nondeterministic Turing machine M using an oracle for
problem B having the following two properties (''reliable'' and ''nice''): (i) for any
input x, at least one computation path of M halts and (ii) every halting com-
putation on x leads to acceptance (rejection ) if x is in A (x is not in A). Observe
that a y-reduction is a many-one version of an s !' reduction. Let B be a problem
(or a language). If A g!' B for all A in NP, then B is said to be s ;'-complete for
NP. The intractability conclusion for the s '-reductions is the same as that of the
}-reductions, namely if B is s ;'-complete for NP, then B is not in P unless
NP co-NP. The primary goal of this paper is to show the s, !'-completeness for
NP of a class of combinatorial problems described below.
Our problem arises from the study of a simple model of computation called the
''comparator network'' [KNUT73 (network, for short.) This model owes its
THEOREM 4.2. Let I be any property such that the size of the smallest n-test set
for I is at least c -2' for some fixed c> 0 and for all sufficiently large n. Then P;, is
s P'-complete for NP.
Proof. Recall that we consider only properties II such that given H and o, it can
be decided in polynomial time whether H(o )e Il. Given H, a o can be guessed and
it can be verified that H(o )e IT. Thus P;; e NP. We shall prove that P;, is g'-hard
for NP. We show this by proving that Psnoo> P P,;. (Note that Psnoo+ s e ;-
complete for NP since s 5,-completeness for NP implies sC ;'-completeness for NP.)
By Lemma 4.1, this is equivalent to showing that Poowe NP A+co-NPr, Since
fsoow e NP, our claim is further reduced to showing that P,oo co-NP'r, ie., to
showing that P4;44 e Np*m, We show this below. In what follows, we present a
nondeterministic polynomial time algorithm AMOOP for Pg,,,,,, using an oracle
for P,,.
Let H be an instance for the problem P.,,,,,,, Let the number of inputs to H be
n, For simplicity, assume that n is even.
We show the correctness of the above algorithm. This requires showing: (1) If
H PAoon, then at least one path of the algorithm accepts, and (2) if H e P4oor
no path of the algorithm accepts.
Proof of (1). Suppose that H e Puoo. From Lemma 4.3, it is obvious that
there is at least one path which successfully constructs N and :. By Lemma 44(B),
for any o e |0, 1}M*'; (H'(o)lae- - d2s-: (H'(a))a+-- dG n+:-) for all i. We claim
that in Step 4, oracle P,, with input N' must respond ''no.'' Suppose not. Then there
is e [0, 1y'*' such that N,(d)e Ih. Since N,(d)= N(H'(8)), it follows from
Lemma 4.3 that (H '(d ))ae-- < d2--: 1 and (H'(d ))a4G.+ d2 +24 = 0 for some i
This contradicts Lemma 4.4(B ).
Proof of (2). Assume the contrary, ie., suppose H has an accepting computation.
Note that the only way to accept the input is in Step 4, which means this
computation has successfully produced N and r, and passed Steps 1 and 2 without
rejection. Also in Step 4, P;, must have answered ''no,'' thus N, e P;;. By the
assumption, H e Putoo so by Lemma 4.4, there is a string o such that H '(o )= r.
Thus N,(o)= N(H'(o ))= N(t )e II, a contradiction to the oracle's response in
Step 4.
This completes the proof. [
We conjecture that the bound on the size of the test set in Theorem 4.l can be
improved from c 2'' to 25' for any c> 0.
The main contribution of this work is the demonstration of the usefulness of s4 ''-
reductions as a practical tool. Note that many of the specific instances of our
problem can be shown NP-complete, but the general result appears to make a
crucial use of nondeterminism permitted in a s, ;'-reduction. One might encounter a
similar situation in other problem areas, such as graph problems, in which there are
hundreds of disparate NP-completeness results. One might hope to synthesize them
by applying similar techniques. Such a study, in addition to providing a tool for
establishing the complexity of problems on a wholesale basis, might also enable us
to understand better what causes a problem to be intractable.
The authors are very thankful to the referees for many useful suggestions. Special thanks are due to a
referee whose suggestions led to an improved and a corrected version of Lemma 4.4.
right to NP or co-NP. even with monadic relations [ BG86]. The difficulty has
been bypassed by considering first-order logic enriched with particular second-order
constructs, of computational or logical significance, such as fixpoint operators.
transitive closure operators, and (narrow) partially ordered quantifiers.
We have attempted instead to delineate subclasses of higher order formulas by
considering syntactic restrictions on the matrix of strict IT] formulas. We believe
that the two approaches shoud be viewed as complementary. The relation between
them is analogous to the familiar dual definitions of inductively generated sets:
on the one hand, as the intersection of all sets satisfying given closure conditions
and, on the other hand, as sets that can be generated constructively ''from within''
by a specified generative process. A non-trivial example of this duality is the
equivalence'' between two definitions of the hyperarithmetical sets: by a generative
process over recursive ordinals, on the one hand, and as the d; sets in the analytical
hierarchy, on the other hand. Each one of these two equivalent definitions
illuminates the other and is natural for particular applications.
The ''internal'' approach, of expanding first-order logic, has the advantage of
focusing on computationally interesting constructs. Our ''external'' approach, of
suitable restrictions on second-order formulas, is ''purely logical'' (no com-
putationally based constructs) and presents the technical advantage of operating
within the entire machinery of second-order logic. Our characterizations are
therefore both easy to derive and easily imply characterizations by computationally
driven operators.
The Rhöne river is the most important river entering the Mediterranean
Sea since the damming of the Nile river. It represents the major source of
''Po is of great radioecological interest because of its high toxicity
(Morgan et al., 1964) and the fact that it contributes more than 30% of the
total dose equivalent delivered to human tissues (Parfenov, 1974). It
results from the decay of Rn gas which diffuses into the atmosphere
where decay products attach electrostatically to particles which can be
carried to the soil, plant and aquatic environments by dry deposition, rain
and snow.
High levels of ''''Po can be registered in the atmosphere around uranium
mines as well as around lead, tungsten, coal and phosphate mines, where
the presence of this radionuclide is caused by the uranium present in those
minerals (Parfenov, 1974).
In soils, '8Poo content varies with soil type. For example, it is observed
that organic soils contain three times as much of this radionuclide as do
mineral soils (Berger et al., 1965). Normally, the 'P/ '%Pb ratio of
natural soils is equal to unity. Soils fertilized with superphosphates have
greater concentrations of Po and PH5, this being an additional source
of polonium for plants grown on soils treated with chemical fertilizers
(Parfenov, 1974). The soils transfer radionuclides to the vegetables by root
absorption and indirectly by dry and wet (rain and snow) deposition on
different parts of the plant followed by leaf absorption.
The accumulation of 'Po by vegetables depends on their morphologic-
al aspects and on plant age as well as on external factors such as soil type,
use of chemical fertilizers, frequency of precipitation and density of
deposition of aerosols (Parfenov, 1974).
Various authors have studied the contents of 'Po in soils (Hill, 1960;
Berger et al., 1965), in different vegetables (Hill, 1960; Mayneord et al,,
1960; Hanson, 1970) and principally in tobacco and derivatives (Radford
& Hunt, 1964; Berger et al., 1965; Francis & Chesters, 1967).
In Brazil, a monitoring program of the environment of the uranium
mining and milling facilities of Pogos de Caldas plateau (CIPC) was
developed by the Radioprotection and Dosimetry Institute of the Brazi-
lian Nuclear Energy Commission with the objective of estimating the
concentrations of uranium daughters in a variety of materials. Several
researchers have investigated the natural radionuclides of the uranium
series,''Ra and Pb, in the environment of the CIPC as to their mobility
and availability in soils and sediments and in the aquatic environment
(Franca et al,, 1982; Amaral et al., 1985) and foodstuffs (Amaral et al,,
1985; Vasconcellos et al., 1987).
With regard to Po, some investigation of seafood has been carried out
(Santos et al., 1983; Gouvea et al., 1987; Gouvea et al,, 1988). Some
tobacco products manufactured in Brazil and other countries have also
been assayed (Santos et al., 1970).
To complement the existing data, Po levels in foodstuffs cultivated in
the area close to that mining and milling installation where natural fallout
is intense (Franca et al., 1982) have been measured here, along with 'p6
measurements in associated soils and fertilizers.
As a control, a vegetable garden in Joinville, in the state of Santa
Catarina, was selected. Here the radioactivity level was normal, plant and
fertilizer selection was readily achieved and rainfall was high.
The uranium ore is mined in an open pit on a plateau at 1300 m altitude
(Fig. 1). The rivers Antas and Verde cross the region in which the facilities
are located and receive effluents from them. The waters of the Antas river
are used for crop irrigation and to water cattle (Amaral et al., 1985).
The Nuclebras vegetable garden is situated 1 km from the mill while the
SB farm sampling site is 5 km from the mill. The control vegetable garden
in the urban region of Joinville is located at sea level.
Vegetables and their associated soils were gathered every three months
at Pogos de Caldas, while the samples from the control area were collected
monthly. All samples were transported to the laboratory in plastic bags
and maintained below (0f'C until assay.
Once defrosted, the plants were washed, cut and dried at 105%C to
constant mass. After grinding and weighing, samples (3-5 g) were wet-
ashed in Erlenmeyer flasks with concentrated HNO, + HCIO, until com-
plete destruction of organic matter was achieved (15 h). Then 12 M HCI
was added, with heating, to transform perchlorates to chlorides, according
to the procedure described by Gouvea et al. (1987). The wet residue was
dissolved in 100 ml 0-5 Mt HCI for subsequent plating onto stainless steel
dishes.
The soils and fertilizers were dried, ground and weighed (1-0 g). They
were placed in an Erlenmeyer flask and 100 ml 0-5 s HCl was added with
continuous stirring for 24 h. The solution was then filtered.
For spontaneous deposition, 250 mg of u-ascorbic acid was added to the
HCl solutions in the stainless steel dishes (for details, see Gouvea et al,,
1987). The dishes were counted in a ZnS(Ag) scintillation system (Halden
& Harley, 1960).
The recovery was determined using pith cabbage and by adding 10 Bq of
1PPo standard at the moment of mineralization; a mean recovery of 98%
was obtained.
Since the ZnS(Ag) technique is not specific, the stainless steel dishes
were counted monthly in order to confirm the decay of Po. A further
five plates with activities in the range 10T*to 1 Bq were also analysed using
a surface barrier alpha detector coupled to a multi-channel spectrometer;
only a peak in the Po channel at 5-30 MeV was obtained, thus validating
the reliability of the separation process. The reagent blank background
gave a count rate of 1-25 3 0-22 cph over a 24h counting period.
The radioactive concentrations expressed in mBq.g''' (or Bq.kg''), dry
mass, were calculated via the relationship
the 0-9025 being a correction factor for radiochemical yield, radiometric
efficiency and the equivalence dph.mBq''' 'Po activity corrections were
made for decay since sampling the vegetables.
The plant-soil activity ratios were established by Hansen's method
(Hansen, 1971). The transfer coefficient is defined as: the polonium
concentration per gram of plant tissue (dry)!the polonium concentration
per gram of dry soil.
Table 1 provides the average concentration data for -Po in the edible
plant parts which were common to the three sampling sites. For the
vegetables, no significant activity differences are observed except for
lettuce, for which the averages for the CIPC samples are twice the control
site value. Comparison of data for similar species collected around the
CIPC show no systematic differences despite the quite different distances
of the vegetable gardens from the mill (Tables 1 and 2).
With regard to the associated soils, Table 1 shows -Po concentration
differences between the 3 collection points. Thus somewhat higher values
were found in soil from the mining region. This reflects the soil type here
and the long-term usage of the land for agricultural purposes, compared to
the vegetable garden control site, in which the soils are very clayey and
were recently tilled. Otherwise, the -8Po content of the soil depends on
local use and on the composition and quantity of the fertilizers used,
mainly here of superphosphates (Tso et al., 1964).
Analysis of fertilizers of phosphorus content above 20% yielded the
following '' Po data:
Nuclebras vegetable garden (NV) 290 3 49 Bq.kg'': dry
SB farm (SBF) 76 3 10 Bq.kg''': dry
Control vegetable garden (CV) 389 3 40 Bq.kg''; dry
Although there is a higher Po content in the fertilizer used in the
control region, i.e, relative to that used in the mining zone, there is a
smaller resultant accumulation of that radionuclide because of the age of
the vegetable garden. Thus -'Po is produced by its precursors in the decay
series and these tend to be accumulated by continued use of such products
(Tso et al,, 1964).
The observed Po soil-plant transfer coefficients are highly variable,
for example being quite different for carrots collected at the 3 sites
(SBF = 0-050; NV = 0-095 and CV = 0<112).
Table 2 shows that there are no significant variations in concentration
between similar plant parts from the CIPC region. Nevertheless, it is clear
that the parts directly exposed to natural fallout are those which contain
the greatest activities, especially for cabbage, in which the outer layers
accumulate more - Po than the pith. A similar observation was men-
tioned by Berger et al. (1965), who used it as an argument to suggest that
Po is not directly removed from soil by roots and subsequently translo-
cated to other parts of the vegetable.
Table 3 shows data for a variety of vegetables grown at the control site.
Here the prevalence of 8Po in the aerial parts of the plants, principally in
the foliage, is verified. Values, e.g. for radish, are comparable to those
obtained by other investigators (Hansen, 1971). The transfer coefficients
are of the order of 10' (roots of manioc = 0-243; Lepidium sp. = 0-278
and radish = 0-101) due to the low - Po content of the control zone soil.
The average 'Po concentrations observed in the soils here have a range
from 27-0 to 74-0 Bq.kg'', comparable to that reported in the literature
(Tso et al,, 1964; Berger et al., 1965; Francis & Chesters, 1967) and
confirmed by Hansen (1971) as a quite large range (namely 8- 14 to 128-30
Bq.kg''). This comparison is made with the reservations that these soils
are of different types, are obtained from sites of different depositions and
are leached with acid and saline solutions.
Finally, the results here suggest that the concentrations of 'Po in the
plants analysed depend more on the fertilizers employed than on the
natural deposition. It stands to reason that, for plants whose vegetative
cycle spans several months, deposition is a major contributor to the
accumulation and hence concentration of - P P6. This applies chiefly in
regions of radioactive anomalies, such as in the CIPC, and also at high
rainfall sites, such as at Joinville, the concentrations of 'PH and p5
being directly correlated with the precipitation index (Hill, 1960; Parfe-
nov, 1974).
The major concentrations of -Po verified in the upper parts of the
plants suggest the dominance of leaf deposition and leaf absorption. The
greater accumulation in the roots, relative to the stems, of some species
studied (Tables 2 and 3), suggests that 'Po arises from Pb absorbed by
the roots. It seems, however, that there is no movement of lead to other
parts of the vegetables. Some shrubs (Table 3) have stems with - P6
concentrations which are less than those in the roots and leaves, a
phenomenon which was also reported by Berger et al. (1965).
The authors are grateful to their financial sponsor, the Brazilian Nuclear
Energy Commission. They wish to thank Mr Zenildo Lara for valuable
technical assistance in sampling.
In 1987, Congress enacted the Surface
Transportation and Uniform Relocation Act
(PL.100-17), which allowed states to increase
speed limits on rural interstate highways from
55mph (88 km/h) to 65 mph (104 km/h). By
the end of the year, 38 states had increased the
speed limit on most sections of their rural
interstate highways (Table 1). In a previous
report, it was estimated that the first months
of the higher limits produced a 15% increase
in rural interstate fatalities (Baum et al.,
1989). The National Highway Traffic Safety
Administration (NHTSA), after adjusting for
the number of vehicle miles traveled on rural
interstates, estimated a similar increase in
fatalities of 16% due to the higher limits
(NHTSA, 1989). The increase in fatalities
translates into nearly 200 excess deaths
(Baum et al., 1989). It was also noted that
even more fatalities could be expected in
1988, because the states that changed speed
limits in 1987 would have one full year of
experience rather than just a few months.
Although several studies have tried to
estimate the effects of higher speed limits for
individual states (e.g., McCarthy, 1988;
Minnesota Department of Public Safety, 1988;
Virginia Department of Transportation, 1989),
national studies provide the best estimates of
the real effect of the higher speed limits. The
number of crashes and fatalities that occur in
any single state are usually too small for
meaningful statistical analysis. Even in states
where the numbers are larger, there is often
significant variation in fatality counts from
year to year for reasons that are not always
identifiable. Not surprisingly, the estimates of
the effect of higher speed limits in individual
states vary greatly and provide a misleading
and confusing picture of the actual effect of
higher speed limits.
iuring 1988, another two states adopted 65
mph speed limits on their rural interstate
highways. In addition, 16 states in response
to a 4-year Congressional demonstration
project (PL. 100-202) raised the speed limits
on 2,200 miles of interstate quality roads
(Table 2). Thus, in 1988 there were even
more miles of roadway subject to the 65 mph
speed limit than in 1987.
In addition, periodic monitoring by the
Insurance Institute for Highway Safety of
speeds on rural interstates in New Mexico, the
first state to post the higher limits, reveals that
speeds continued to increase throughout the
period of the higher limits. The percentage of
drivers exceeding 70 mph (112 km/h) rose
from 5% in April 1987, just after the speed
limit changed, to 29% in December 1988. If
this pattern of increasing noncompliance with
the new speed limit is common throughout the
United States, then it is likely that the effect of
the 65 mph speed limit would be higher in
1988 han 1987.
This report presents estimates of the effect
of the higher speed limits on fatalities in 1988
and on fatalities for the entire period of higher
limits, The extent to which the effect has
changed during the second year of the 65 mph
speed limit is also examined.
Data for this study are from the Fatal
Accident Reporting System (FARS), which is
a federally maintained computerized data base
of all fatal motor vehicle crashes on highways
in the United States since 1975 (NHTSA,
1988). Monthly counts of fatalities on rural
interstates were obtained for each year from
1982 through 1988. For comparison
purposes, similar counts were obtained for all
other rural roads and for all other roads.
Of the 40 states that had a 65 mph speed
limit on rural interstates in 1988, 38 had the
higher limit for all 12 months; Georgia
established the higher limit beginning in
February 1988 and Virginia in July 1988. To
facilitate comparisons with the previous
report, Georgia and Virginia were excluded
from most analyses. The states adopting the
65 mph speed limit on rural interstates during
1987 did so from April through November of
that year. To compensate for the differing
number of months of exposure of each state in
1987, fatality counts were aggregated for all
complete months after the month in which the
higher speed limit was instituted in each state;
these were termed ''postimplementation''
months (Table 1). For other years, the counts
for the postimplementation months for any
state were the same months as used in 1987,
Fatalities on rural interstates were tabulated
across states and across years. As previously
of rural interstate segments that subsequently
remained posted at 55 mph. In addition,
FARS coding does not allow for separation of
noninterstate highway mileage posted at 65
mph under the Congressional demonstration
project from other rural noninterstate mileage;
consequently, some of the fatalities on
comparison roads actually occurred under a
65 mph speed limit. This of course makes any
estimated effect of the 65 mph speed limit
conservative. In addition, the phenomenon of
speed adaptation suggests that higher speeds
on rural interstates will spill over to other
roads (Casey & Lund, 1987, 1988). To the
extent that higher speeds do result in more
fatalities, these factors would cause an
underestimate of the true effect of higher
speed limits on rural interstates.
Figure 1 illustrates rural interstate fatality
counts for the postimplementation months
from 1982 to 1988 for the 38 states that
changed their speed limits to 65 mph. The
number of fatalities did not vary greatly from
1982 to 1986. This was followed by a rise in
fatalities in 1987 and a further rise in 1988.
Comparing the average number of fatalities in
the postimplementation months on rural
interstates for 1982-1986 to 1988 revealed a
36% increase. Corresponding comparisons
for all other roads and other rural roads
showed only small increases in the number of
fatalities (about 4% for both comparisons).
Figure 2 illustrates annual rural interstate
fatality counts for 1982-1988 for the 38 states
that raised their speed limits in 1987. In this
figure, the fatality experience in 1982-1986
occured under 55 mph limits, that in 1987
under a mixture of 55 and 65 mph limits, and
in 1988 under 65 mph limits.
Table 3 compares the numbers of fatalities
on rural interstates with those on other rural
roads and on all other roads before and after
the speed limit change in the 38 states that
raised the speed limit in 1987. The first
section of the table shows the yearly fatality
data comparison. The odds ratio for annual
fatalities during 1982-1986 versus annual
fatalities during 1988, shows a 26% increased
risk (odds ratio = 1.26), when the experience
on rural interstates is compared to other rural
roads. The odds ratio was slightly higher
when all other roads were used as the
comparison (odds ratio = 1.29),4
Table 3 also shows the fatality comparisons
for the corresponding months before (1985-
1986) and after (1987-1988)) the speed limit
was changed. When these periods are
considered, the odds ratios show an increased
risk of 24% on rural interstates compared with
other rural roads and 27% compared with all
other roads.
When the fatality experience during the
postimplementation months of 1988 was
compared with the average experience in the
corresponding months of 1982-1986, the odds
ratios were slightly greater than those
obtained from the total annual counts (1.31
for the comparison of rural interstates with
other rural roads and 1.32 for that with all
other roads). These ratios were twice as high
as the comparable odds ratios estimated for
the postimplementation months of 1987
(Baum et al.,1989). This increase in the effect
of the speed limit is statistically significant as
indicated by the last column in Table 3: The
rural interstate fatality counts in 1988 were
13% to 14% higher than expected even when
compared with fatality counts during the same
postimplementation months in 19873
None of the odds ratios for the eight states
that retained 55 mph as the maximum speed
limit were statistically significant (Table 4).
The National Research Council (NRC)
review of the effects of the 55 mph national
maximum speed limit estimated that a return
to the speed limits that existed prior to 1973
would increase fatalities on interstate
highways by 20% to 25% (NRC, 1984).
However, data from 1988 suggest that the
National Research Council greatly
underestimated the effect of higher speed
limits. The estimated effect of higher speed
limits in 1988 based on all 40 states translates
into 531 to 566 deaths attributable to the
higher speed limit. The finding that the effect
of the higher limit doubled in 1988 compared
to 1987 suggests that even more lives will be
lost in 1989 if speeds continue to increase.
In addition to the negative effect of higher
speeds in the states that adopted 65 mph speed
limits, there has been concern that higher
speeds would spill over into the states that
retained the 55 mph maximum speed limit
(May, 1988). As a result, many of the eight
states that retained 55 mph speed limits have
adopted special speed enforcement measures.
For example, Massachusetts has increased the
fines for speeding and indexed them to the
amount of excess speed [$50.00 for the first 10
mph (16 km/h) over the limit and $10.00 for
each mph above that] (Ferson, 1988). In New
York, the maximum fine for speeding was
quadrupled to $400.00, and police in that state
and other bordering states have cooperated in
joint efforts to control speeds on selected
highways (James, 1989). So far, there has been
no statistically significant increase in fatalities
on the rural interstate highways in those eight
states, although the odds ratios were positive
but not significant for 1988.
As a final note, it should be reemphasized
that the effects of 65 mph speed limits on
rural interstate highways estimated here are
probably conservative. Comparing motor
vehicle crash fatalities on rural interstate
highways to fatalities on other highways
assumes that speeds are not increasing on the
comparison highways. In fact, there is reason
to believe that higher speed limits on rural
interstate highways have led to higher speeds
on connecting roads (Casey & Lund, 1987,
1988). Additionally, it is to be expected that
there have been increases in fatalities on some
of the comparison roads that have been posted
at higher speed limits under the congressional
demonstration project. These factors would
mask some of the effect of increased speed
limits on rural interstate highways as
measured by the present analyses. Thus,
although the estimated deaths attributable to
higher speed limits in 1987-1988 are a
national tragedy, it is likely that the true cost
of higher speed limits is even greater.
related to G E, This reduction process is continued with the equations in the set
until either (a) a true equation is found, or (b) the set is larger than a given bound
determined by GE. This process always terminates and we are able to conclude
that GE is satisfiable iff (a) occurs.
Firstly, we define a procedure COA/PLE TE that maps a given generalized
equation into a finite collection L of complete generalized equations CGE as
follows: let L initially contain GE. So long as L contains a noncomplete equation
GE, say bd, and bd, are two boundaries for which neither bd, bd, nor bd, bd,
appear in GE, remove GE from L and then add to L three new equations GE,,
GEs, and GE4:
The above process is continued until L contains only complete generalized
equations. Using a proof by induction on the number of pairs of boundaries in GE
which are unrelated, we easily have the
Next we define a procedure REDUCE that maps a given nontrivial complete
generalized equation CGE into a generalized equation GE. The idea here is that
the satisfiability problem for CGE is equivalent to that for G E. However, GE is a
somewhat ''simpler'' equation than CGE. Details are given below; meanwhile, the
procedure REDUCE is defined as follows:
Let CGE = (BS, BD, FF, BR), and let bs, be a distinguished base in BS such
that
In case there is more than one choice for bs,,, we may arbitrarily choose one. Let
bs, be any base different from bs, and whose left boundary is minimal amongst
those left boundaries z LEFT(bs,,). In case there is more than one choice for bs,,
we choose one whose length is minimal. Note that bs, may be an empty base, a
constant base, or it may be bs,. Intuitively, bs,, is the ''largest leftmost variable''
base and bs, is the ''second leftmost'' base. Clearly such bs,, and bs, exist in any
nontrivial complete generalized equation.
We proceed now by case analysis on the left boundaries of bs,,, 7S4,, and bs,. In
each case, we shall produce the desired G E = (BS', 5D', FF', 5R'), where the
functions LEFT'' and RIGHT' comprise FF'.
Ease 1. LEFT(bs,) > 0. All the nonempty bases whose left boundaries are
fithan LEFT(bs,) must be constant bases. Since CGE is nontrivial, all these
istant bases are of length one and so their right boundaries are less than or equal
EEFT(bs,). Essentially, GE is obtained from CGE by deleting these constant
as, deleting their left and right boundaries, and finally deleting all the boundary
tions involving these boundaries. Note that no boundary equations will be
Note here that we have set to zero the left and right boundaries of empty
bases whose left boundaries did not exceed LEFT(bs,,).
Figure 3 illustrates an example of the above reduction CGE into GE.
Case 2. LEFT(bs,) = LEFT(bs,,) = 0. The essential idea here is that the pair
pf bases, bs, and bss., are redundant. They can thus be henceforth ignored (and
3%do this by making them empty bases), and so too can those boundary relations
ich are relevant only to this pair of bases.
Suppose there are k 2 0 nonzero boundaries less than RIGH T(bs,,). For nota-
tional convenience, we assume that they are bd;, bd4,..., bdi and that bd; bdy
3-- < bdi appears in CGE if k > 0. Let J be the set of indices i e !1, 2, . ., k]
Rch that bd, is involved with bs, in a boundary equation. Since CGE is nontrivial,
these boundary equations must be of the form
(bd,, bs,, bd,).
The following thus appears in BR:
enough rewrite rules to deal with those FP elements that are neakly normalizable,
that is, those elements for which there is some sequence of rewriting rules that
terminates. However, using the fact that (just as in untyped -calculus or combi.
natory calculus) we can define an element fis in FP with the property that fis:f
reduces to f:(fix :f), it is easy to show that not all FP expressions are weakly
normalizable. To get around this problem, we construct a variant of FP, which we
call FP', on which, using the semantic domain, we can impose a notion of type,
Once we have done this, we can show that FP' is strongly normalizable (all
rewriting sequences eventually terminate). using proof techniques developed by
Prawitz [16] and Tait [20, 21]. (See [5] for a good description of these techniques.)
We can then ''pull back'' reduction sequences in FP' to FP, and use the information
thus obtained to prove completeness for FP,
In the latter half of the paper, we turn our attention to rewrite strategies, In order
to implement a programming language based on rewrite rules, it does not suffice
to know that there are in some sense enough rules in the language. An implementer
must have a strategy to determine the order in which these rules are to be applied.
But what constitutes a ''good'' rewrite strategy? And how can we find such strategies!
Our idea is to define notions of goodness for rewrite strategies corresponding to
notions of having ''enough'' rewrite rules. We define the notions O-safety, S-safety,
and L-safety for rewrite strategies in analogy to observable completeness, strong
completeness, and limiting completeness. We use our results on FP' to help us
completely characterize S-safety and L-safety in terms of FP', and then use this
characterization to show that S-safety implies L-safety, which in turn implies
O-safety. We also show that any fair strategy, intuitively, one that eventually
rewrites every redex, is S-safe. This lets us define a number of natural S-safe rewrite
strategies. It is interesting to note that some previously well-studied strategies such
as leftmost-outermost (cf. [11]) turn out not even to be O-safe in the context
of FP.
Our techniques for proving completeness are very similar to those used by
Wadsworth [23, 24] for proving limiting completeness for the untyped M-calculus.
However, FP presents two difficulties that do not appear in the M-calculus. The
first difficulty is caused by having two-sided infinite sequences. The second (and
more serious) difficulty is that the rewrite rules for FP have much more structure
than those of M-calculus. For example, in the M-calculus, any expression of the form
(Ax.M)N is a redex. Any rewriting done to M and N results in an expression of
the form (Mx.M ' )N ', which is still a redex. But in FP, we may start with an
expression of the form tl:x which is not a redex, rewrite x to an expression x' such
that tl:x' is a redex, and then rewrite x' further to an expression x' such that
tl:x' is not a redex. Additional problems are caused by the added structure
imposed by having sequences. The net result is that it is much more difficult to
carry out the proofs for FP than for M-calculus, and new, nontrivial insights are
required.
In other related work, Dosch and Moller [4] develop a set of rewriting rules
similar to ours, but they take for their semantic domain the term model underlying
the algebra of rules. Because their semantic domain is so intimately related to the
reduction rules, it does not seem possible for them to even state a completenesS
theorem of the same type as ours. For them, essentially by definition, the meanin8
of x is the same as that of y iff there is some z such that both x and y can be
reduced to z. (For these and other reasons we also feel that the semantic domain
we introduce here is a more natural one for programmers to use in reasoning aboul
their programs and for us to use in designing the language.)
e rest of the paper is organized as follows: In the next section we discuss the
itax and semantics of the version of FP that we use. which differs somewhat
pm the original FP of [1]. In Section 3. we present the rewrite rules for FP and
ifuss the notions of observable completeness and strong completeness. In Section
fei discuss strong and weak normalization for FP and show that strong com-
teness holds for a large subset of FP (namely, all those expressions that are
ikly. normalizable). In Section 5, we introduce FP', show that it is strongly
malizable, and use that fact to show that strong completeness holds for full FP.
Section 6, we consider good rewriting strategies. We define the notions of
ifety and S-safety for rewriting strategies, and characterize S-safe rewrite strat-
jes in terms of FP'. We also show that fair strategies are S-safe. In Section 7, we
jnsider limiting completeness and the corresponding notion for rewrite strategies,
safety. We characterize L-safety in terms of FP', and then use that characteriza-
tibn to show that S-safety implies L-safety. We conclude in Section 8 with some
idirections for future work and some remarks on the Church-Rosser property for
$P; We have written the paper with two audiences in mind: the researcher in the
d of programming semantics and the casual reader with little background in the
p of programming semantics. We have therefore attempted to make the paper
Bipletely self-contained, and have deferred many of the more technical proofs to
tappendices.
9r language is the result of a desire to extend the language of [1] to allow the use
ginfinite sequences, lazy evaluation, and programmer-defined functionals, and at
same time to preserve as much of the original flavor of FP as possible.
To accommodate these extensions, some modifications to FP are necessary; the
pimary differences between the language of [1] (herein denoted FP78) and our FP
Indeed, the best-known quantifier bound h(t, l) in the DMPR theorem has hi
form of an iterated esponential of tl Such a bound is implicitly contained in j
works by Adleman [l] and Adleman and Manders [3]. They analyzed the proof3
the DMPR theorem found in the paper by Matijasevi and Robinson [26]; g
especially Lemma IV in [3, p. 85] and the results surrounding it, or the mon
detailed analysis in [1]. See [24, pp. 179-184] for a further discussion of smg
aspects of the results in [3]; Manders and Adleman [24] provide no analysis of i.
quantifier bound in the DMPR theorem, but they make some remarks concerning
nondeterministic Diophantine machines, which is the main topic of [3]. Interesting
quantifier bounds for specific formulas, perhaps relevant in this context, i
described in [2], especially on p. 170. The question whether it is possible to haw
h(t, 1) = 1for all l is discussed by Paris and Dimitrocopoulos [27, p. 320] after the
proof of their Proposition 3. It is pointed out that an affirmative answer woukk
imply NP = co-NP. Gaifman and Dimitracopoulos [14] study the DMPR theorem
in fragments of PA.
ACKNOwLEDGMENTS. I would like to express my gratitude to Dr. Miklös Ajtai of
the IBM Almaden Research Center, Professors Andreas Blass of the University of
Michigan, Peter Gäcs of Boston University, Kenneth McAloon of Brooklyn Colleg
of CUNY, Kenneth Manders of the University of Pittsburgh, Jeff B. Paris of the
University of Manchester, England, and Richard Statman of Rutgers University,
with whom I had valuable discussions and correspondence concerning the above
material, and Spyro-Giorgio Mancivi of the City University of New York, who
pointed out some inaccuracies in an earlier version of this manuscript.
of Dr Charles Mitchell, the technical support of Ms Cecilia Kahle and Mr Bryan Castorina, and the comments of
Drs B. B. Boecker, Y.-S. Cheng, R. A. Guiimette, G. Kelly, P. J. Lipowicz, D. L. Lundgren, R. O. McClellan, C. E.
Mitchell, and H. C. Yeh in reviewing the manuscript. We also thank the Brush-Wellman Corp. for supplying the
bulk beryllium metal sample, the Mo-Sci Corp. for developing the glass density standards, Dr B. J. Greenspan for
the original probit analysis program. and the Associated Western Universities, Inc., for the support oiG. L. Finch as
a post-doctoral research appointee.
Knowledge of atmospheric aerosols is important for both environmental monitoring and
radiative transfer studies. In recent years, much attention has been devoted to regular
monitoring of atmospheric aerosols, as they directly influence the thermal state of the
earth-atmosphere system and have profound impact on the injection of natural and man-
made particles into the atmosphere. Direct measurements using particle counters and
impactors from aircraft (Khemani et al., 1985; Ottar et al, 1986) and balloon payloads
(Junge, 1961; Cadle et al., 1973) are quite expensive and non-continuous. Also, passive
remote measurements (ground-based as well as spacecraft) involve long path integrations
and limited spatial resolution (King et al., 1978; Quenzel, 1982; Russell et al., 1982)p
In contrast, active remote measurements using the lidar technique (Carswell, 1983;
McCormick, 1985) reveal aerosol characteristics over large atmospheric volumes from a
single surface location or over large regions by means of airborne or satellite-borne systems.
Several groups in the world (mostly at northern mid-latitudes) have been making more or
less continuous measurements, on a routine basis, using ground-based lidar systems (see
Carswell, 1983). A network of lidar stations is essential to understand the global aerosol
phenomenon. Such measurements are very much limited in the tropics. So far, some results
of lidar observations of atmospheric aerosols over a marine environment at Trivandrum
(8 N, 77E) have been reported (Parameswaran et al., 1984). In this context, lidar probing of
the atmosphere over Pune (18'N, 74'E), where meteorological conditions vary markedly
from continental (winter) to maritime (summer) environments, will yield valuable infor-
mation on atmospheric aerosols, leading to better understanding of various pro-
cesses prevailing in those environments. For this purpose, a CW, bistatic (where the trans-
mitter and receiver are separated by a distance), helium--neon lidar has been developed.
In this paper, we describe the experimental set-up and present preliminary results of
aerosol observations obtained during different atmospheric conditions.
The details of the experimental set-up and method of extracting aerosol information from
laser return signals are discussed briefly in this section, as they have been described in more
detail elsewhere (Devara and Raj, 1987). The system is composed of a Spectra-Physics
Model 159, helium--neon laser of power 5 mW as transmitter and a 25 cm Newtonian
telescope-photomultiplier assembly as receiver. The behavior of the received photons and
In addition, Yeh and Liu (1974) have given numerical solutions for a model based on the
K uwabara flow fieid. (A set of equations given by Stechkina et al. (1969) were later found to
be partly incorrect by the same authors (Stechkina et al., 1970) and will not be considered
here.)
These four model predictions are compared with experimental results obtained by Yeh
and Liu (1974) in Fig. 2. It is clear that the models of Stenhouse (1975) and Yeh and Liu
(1974) best predict the filter efficiency. Stenhouse's model has been adopted for the present
work since it offers the advantage of an explicit analytical equation.
The filter efficiency } was calculated from the single fibre efficiency, E, by sub-dividing the
size range into seven sub-ranges, since the aerosol was polydisperse. E, was calculated for
each sub-range j of average particle diameter d,,, from equations (4)-(11). The filter efficiency
Y was then calculated from the equations
and
where (n/n;;), and f, are, respectively, the number penetration and mass fraction of the j-th
size range of particles.
Under the microscope, it is seen (Part I) that the distribution within the filter of a liquid
which fully wets the surface of the fibre depends on the packing density (fibre volume
fraction). At all except the lowest packing densities (a < 0.04), a significant proportion of the
smaller interstices between the fibres is filled with collected liquid and not available for gas
flow. The fibres bounding these filled interstices are not available for aerosol collection. At
packing densities well below 0.04, however, the liquid is differently distributed. The
interstuces are too large to be spanned by liquid. Instead, the liquid exists mainly as clearly
defined droplets at fibre intersections. The rest of the fibre length appears little changed from
that in the dry condition.
heterodyne quasi-elastic light-scattering experiment (or extracted from a homodyne exper-
iment), is given by
where r is the correlation time and
Here ; is the momentum transfer in the scattering experiment, D the diffusion coefficient, k
the Boltzman constant, T the temperature, ; the viscosity coefficient and r the radius of the
particles. The quantity T is also called the characteristic linewidth associated with the
particles of radius r. Then in the polydisperse case, the first order correlation function is
given by
where G(T') is the normalized linewidth distribution function. The problem is the determi-
nation of G(T) from the measured values of jg'''()]. As follows from equation (2), the
determination of G(T') is equivalent to the determination of the distribution function of the
particle sizes.
Recent experiments (King et al., 1982; Chowdhury et al., 1984) have shown that there are
certain applications in aerosol science where the measurement of particle size may be made
by this method. In the references quoted the application was to sooting in flames. We should
note that the method is only applicable to aerosols where the geometry may be chosen so
that the Brownian diffusion of particles is significant compared with the other motions
possible in the plane containing the incident and scattered light beams. This will normally
cover the range of particles below one micrometer in diameter and therefore gives a method
complimentary to particle sizing methods using the angular dependence of total scattered
intensity. A departure of the data of Chowdhury et al. (1984)from a single exponential would
indicate a distribution of particle sizes present in the flame. In this paper we treat the
problem of estimating a particle size distribution in such an experiment and, as we have
already remarked, this is equivalent to the inversion of the Laplace transform.
There is a large amount of literature on this problem but a new approach was initiated by
McWhirter and Pike (1978), who introduced the eigenvalues and eigenfunctions of the
lLaplace transformation and were thus able to construct an 'information theoretic' inversion
procedure based on the well-known Nyquist sampling ideas of standard information theory.
This approach led to the consideration of a sampling scheme for a model inversion in which
the reconstruction was attempted with a resolution decreasing exponentially with increase
of the radius parameter (Ostrowsky et al., 1981). This is just the equivalent of the Nyquist
sampling of Fourier theory when the problem is dilationally rather than translationally
invariant. The corresponding method of inversion, based on simple linear least-squares
procedure, is known as the 'exponential sampling method'.
As experience with this method built up it became apparent that the resolution achievable,
although not high, was significantly better than that predicted by the eigenvalue spectrum.
The origin of this discrepancy was shown by Bertero et al. (1982) to be due to an implicit
restriction of the domain where the reconstructed solution is assumed to be different from
zero: in practice finite limits must be set for numerical calculations and these, of necessity,
add some a priori knowledge of the position and extent of the solution. This knowledge can
be used explicitly to calculate the extra resolution possible by an extension of the method of
eigenfunction expansions known as singular function expansions, well-known in pure
mathematics since the last century but little used in physics until quite recently. This method
is so flexible that it can also be used for the problem with discrete data and for adding several
kinds of a priori information in terms of profile functions. The application of this method to
Laplace transform inversion will be described in this paper. Let us stress the fact that the
method applies to more general linear inverse problems when the signal detected for a
monodisperse suspension is no longer a simple exponential function. In particular, our
discussion of the limitations inherent to inversion procedures is also relevant for other
problems in particle sizing such as Fraunhofer diffraction, analysis of extinction data etc.
The problem of determining the resolution limits in Laplace inversion was solved by
McWhirter and Pike (1978) using the eigenvalues and eigenfunctions of the transformation.
Here we follow an equivalent approach which is based on the use of the Mellin transform.
Using slightiy different notations, equation (3) is a first kind Fredholm integral equation of
the following form
where L is an integral operator given by
When K (x)=exp(- x) the integral operator L is just the Laplace transformation. It is
interesting to point out that a similar integral equation, with a different kernel, must also be
solved in the case of the particle sizing by Fraunhofer diffraction or by extinction
experiments (Bertero et al., 1987a,b). In all these applications the function K (x) is real and
such that IT*K(x) is integrable. This guarantees that the operator L is bounded and self-
adjoint(Reed and Simon, 1970)in the space L'(0, + od)of square-integrable functions on the
semi-axis (0, + oc).
The Mellin transform of a square integrable function is defined by (Titchmarsh, 1948)
and, using well-known results (Bertero et al., 1982), the solution of equations (4) and (5) can
be written in the following form
where g(o)) and K(a)) are the Mellin transforms of g(p) and K (x) respectively. The inversion
formula (7) however, is affected by numerical instability since | K (o)|--0 when ]o|-- op. In
particular, in the case of the Laplace transform we have
In order to get stable approximate solutions one must use regularization techniques which
have been developed for the treatment of ill-posed problems (Tikhonov and Arsenine, 1977).
The most simple of these techniques, which is also known as optimum filtering, is the
following. Ifwe assume that the data function g(p)is corrupted by additive,zero-mean, white
noise with power spectrum ef and that the unknown solution f (t) is also from a zero-mean
white noise process with power spectrum E', then optimum filtering consists in restricting
the integral in equation (7) to the interval [ - wa,wa] where a,, is the solution of the
6quation
Let us denote by ,(t) the approximate solution provided by the procedure above. Then, in
the case of noise free data, it is easy to show that the following relation holds true between
G(t) and the true solution f(t) (Bertero et al., 1985a)
application, among many others, is as low power consumption devices sucl
as watches. The lithium button cells currently used employ a plastic crim;
seal and the main question is the long term reliability of such cells whill
driving devices to the maximum capacity of the power source, Unde
very low drain, the current is too low to break down the passivating laye
on the lithium electrode. To accelerate the passivation phenomenon furthe:
the cells used in the present study were measured after storage, (.e., withou
drawing any current.
The internal resistance and the weight change were measured as a fu
tion of storage time under three experimental conditions: 20 'C, 40 C/9 1
RH and 55 'C. The increase in the internal impedance is shown to be mainl
controlled by water permeation into the cell through the plastic crimp seal.
Fitty CR 2016 (20 mm dia., 1.6 mm thick) Li/MnOg cells from Sany:
Electric Trading Co., Osaka, lapan (January 1985), and fifty BR 2016 Li/CT
cells from Matsushita Battery Industrial Co., Osaka, Japan (November 1984.
were used. The initial capacities were 92 : 2 and 78 S 2 mA h, respectively.
at a cut-off voltage of 2.0 when discharging into 30 kS2 at 20 C with mea:
voltages of 2.9 and 2.8. In a button cell configuration the anode, thz
separator, and the cathode are parallel planes perpendicular to the axis of
symmetry of the cell. The seal is usually a crimped polypropylene plasti:
and a polymeric sealant [3, 4].
Two cells of each type were stored under each of the following cor
ditions, defined in international standards [5]:
The internal resistance of the cells was measured at regular timi
intervals. After 720 days of storage, resistive discharges were performed t
determine the self-discharge.
Two techniques were used to measure the cell impedance. First th
a.c. impedance was determined using a frequency response analyzer anl
an electrochemical interface controlled with a (Hewlett Packard 9825 A
calculator. The alternating voltage was 2.5 mV rms. Five measuremenß
per frequency decade were performed between 0.1 Hz and 100 kHz at th
open circuit potential. Analysis in the complex plane gives the serii
resistance, R,, and the activation resistance, I.a, to within s2%, assumir4
a simple Randles equivalent circuit.
The second method involved the measurement of the ohmic drop aft
10 ms following a current pulse ranging from 0.1 to 2 mA. This galvanostat
method was applied to study the effect of current density and to examiM
the possibility of using a simple, inexpensive, and fast method to determi
the passivation resistance of lithium cells. The results on several types
cells, especially those with a high internal resistance, have shown that the
best correlation with the I.4 Value obtained by the a.c, impedance method
was obtained from the ohmic drop determined at a current value of 0.1 mA.
The resolution of the digital voltmeter was 1.0 mV. The ohmic drop cannot,
therefore, be determined with a resolution better than 10 2.
The measurements were performed at 20 'C: the reported values are
the average of two cells for each experimental condition, and therefore
no statistical data can be given.
In addition to the impedance measurements, five undischarged cells of
each type were stored at 40 'C/93% RH, and 55 C and regularly weighed
with an accuracy of :0.02 mg.
The impedance data given by both the a.c. and the galvanostatic pulse
techniques are reported in Figs. 1 and 2 for the Li/MnOg and Li/CF cells,
respectively. The ''as-received'' Li/CF cells showed a fairly well defined semi-
circle at high frequencies (Fig. 2). As the storage time increased, the dia-
meter of the semi-circle, I.24, Increased and its centre moved below the real
axis, The impedance locus for the Li/MnO, cells showed a depressed semi-
circle (Fig. 1). The activation resistance is given by the diameter of the semi-
circle, but for simplification it has been taken here as the intersect of the
semi-circle with the real axis, irrespective of the position of its centre.
The results reported in Figs, 1 and 2 for the fresh, undischarged cells
are in agreement with published data [2, 6, 7]. The increase of activation
resistance with time after storage has also been reported and attributed to
the growth of a passivating film on the lithium electrode [2, 8].
The self-discharge of lithium cells comes mainly from the reaction of
the lithium electrode with impurities, especially water, oxygen, and other
impurities in the electrolytes. Although water vapour permeation may be the
main contributor to both the self-discharge and the activation resistance,
self-discharge is expected to be in relation to the amount of lithium which
has reacted with water, while I,+4 may not be directly related to the thick-
ness of the passivating layer [1]. Thus, the self-discharge does not neces-
sarily relate to the I.4 Value after 720 days of storage, as shown in Table 1
The increased thickness of the cells is apparently related to ti e
humidity and temperature of the storage environment. The greater t:ae
thickness, the higher the activation resistance (Table 1). At a given state-of
discharge, the water vapour might increase the volume of the cell by swel-
ling the plastic seal. This will decrease its mechanical strength and thus
contribute to the thickness increase during discharge, due to the net volume
increase of the electrodes. The swelling of the seal increases its gas per-
meability and contributes to the increase in the internal resistance.
The permeation of solvent from the electrolyte out of, and of water
into, the cell occurs simultaneously but at different rates depending on t.e
temperature and humidity of the environment and on the type of cell. The
plastic crimp seal of the Li/MnOg cell exhibits high permeability for the
solvent and a low permeability for water, whereas the opposite is observed
for the Li/CF cell. The main contribution to the increase of the activation
resistance is the water permeation. After the storage times investigated, the
permeation of the solvent does not raise the series resistance of the cell
significantly.
The consequences of water permeation on the useful life of lithium
cells will be investigated as a function of the state-of-discharge in Part Il
of this series [9].
The author is pleased to thank F. Züllig for his skillful assistance in the
experimental work, R. Jeanmonod for discharge measurements, and ETA
Inc., Grenchen, member of the SMH Group, for its interest and suppork
A summary of this work was presented at the 174th Meeting of the Electro:
chemical Society, Chicago, IL, Oct. 11, 1988, and will be published in 4
proceedings volume of this society.
The activation resistances were significantly higher than those of the
Li/MnOg cells (Fig. 7). After storage at 20 C, they were between 150 and
600 2 (Fig. 7(a)), compared with 90 {2 for the Li/MnOg cells (Fig. 1(a)).
At discharged states lower than 40% Cown. N reached a maximum value
after 30 - 180 days, while at 60% - 80% discharge the activation resistance
increased linearly to reach a stable value after about 180 . 360 days of
storage. At 40 C193% RH, R,A increased rapidly after short storage times
then levelled off between 1 and 2 kS2 for medium states-of-dischar .
whereas for the undischarged and 80% discharged cells the activation
resistance exceeded 3 k2 after 270 days of storage (Fig. 7(b)). At 55 C,
a sharp increase was again recorded at short storage times, followed by a
decrease (with the exception of the 80% discharged state), and a sharp rise
(Fg.7(e)).
The increase in the activation resistance after storage times longer than
360 days at 40 C/93% RH and 55 C was higher for the undischarged and
highly discharged states than for those between 20% and 60% (Fig. 8).
After storage at 40 C193% RH the series resistance, I,, increased
almost linearly with time up to about 400 days from 4 - 5 $2 to 6 - 8 2 for
small states-of-discharge, whereas the rise was exponential for the 60% and
80% discharged cells (Fig. 9(a)). A similar behaviour was measured at 55 'C
with a rapid increase of R, at shorter storage times (Fig. 9(b)). In most
cases R, was very small compared with I.a4. At 20 C the increase of R,
with storage time was negligible. As already observed, for the Li/MnOg
cells, there was no universal correlation between I.g4 and R,; the relation-
ship depended on the storage conditions and state-of-discharge.
The results obtained with the galvanostatic pulse method were in
agreement with those given by the a.c. impedance technique when I.a.
was small, ie., after storage at 20 C (Figs. 7(a) and 10(a)). By contrast,
when the activation resistance was larger than 500 {2, ie., after storage at
40 C193% RH and at 55 C, the total cell impedance, R,, measured with
the galvanostatic pulse method, was significantly smaller than F.a
determined by the a.c. impedance technique (Figs. 7 (b), (c) and 10 (b), (:))
For example, after storage at 40 C/93% RH, the cell impedance measure4
by the galvanostatic method exceeded 1 ki2 only for the 80% dischargel
cells after 150 days of storage, whereas I.4 was higher than 1 kS2 after 3l
(0% D) to 360 days (60% D).
The complementary discharges performed after 720 days of storagt
gave homogeneous results (Table 2) from which the following self-dischar
over 720 days can be calculated: 1% - 3% Casa (1.6 mA h) at 20 C; 16%
25% Csa (16 mA h) at 40 C1935 RH; 12% - 18% Csa (12 mA h) 4
55 C. The final thickness after the complementary discharges varied si@
nificantly with the storage environment and the state-ofdischarge (Fig. 6).
It has been shown that the changes in the internal impedance anß
weight of Li/MnOg and Li/CF button cells could be explained by wat4
permeation into, and solvent permeation out of, the cell [1]. The increas
in K.44 1s mainly due to water permeation through the plastic crimp sed
In the following section, the effect of the state-ofdischarge and environme
will be discussed and an attempt will be made to predict the useful lifetim
for a given application.
The increase in the activation resistance of standard commercial 2]3 1
size Li/CF cells with time after discharge at ambient temperature has alreadf
The complexes MCl, xL (x = 2, M = U, Np and Pu, L P ((-C,H4)4O:
M = Np and Pu, L = CH4CON(IC4H4)4 and CH,CON(IC4R4)4; M = U, L =
P(C,H,CH4)4O(tbzpo); M = Pu, L = P(CH4)4C,H,)O(dmppo); x = 2.5, M =
Np and Pu, L = HCCN(CH4)g(dmf); x = 3, M = U, L = tbzpo; M = Np and Pu,
L = P(CH4)(C,H,)4O; x = 3.5, M z Np, L = dmppo) have been prepared. The
IR and near IR--visible spectra of the complexes are reported.
Complexes of thorium and uranium tetrachlorides, MC1,: xL, have
recently been reported for the alkaryl phosphine oxides [l] PMe,PhO-
(dmppo) (Me = CH4; Ph = CHi: x = 3.5, M = Th and U or x = 2, M = U) and
PMePh4O(mdppo) (x = 3.5 and 3, M s Th and U or x = 2.5, M s U) and
the bulky amides [2] MeCONPr';(dipa) Pr' = i-C4H4: x = 3, M = Th) and
EtCONPr';(dippa) (Et = C4H,: x = 3, M s Th: x = 2, M s U). Phosphine
oxide complexes of neptunium and plutonium tetrachlorides, MC1, xR4PO
(M s Np and Pu, x = 2, R = CH, or (CH4)N and x = 6, R = CH4; M = Np,
x = 2, R s CH4) have been recorded [3], whereas only a few amide com-
plexes, MC1, xL, are known for these elements (x = 2.5, M Np and Pu,
L = MeCONMe;(dma) [4]: x = 2, L = Me,CCONMeg(dmpva) [5]). It was
therefore of interest to attempt to prepare complexes of neptunium and
plutonium tetrachlorides with the more bulky phosphine oxides (dmppo,
mdppo and P((-C,H,)4O(tibpo)) and amides (dipa and dippa), and with the
4mall amide ligand, HCON(CH4)g(dmf), for which the complex UCl, 2.5dmf
ls known [6], in order to examine the effect on the stoichiometries of the
Droducts caused by the decreasing M(IV) radius across the actinide series.
All the experimental work was carried out in dry atmosphere, nitrogen
filled glove-boxes as described previously [5]. Solvents were dried as
reported earlier [7] and dmf (BDH, Ltd.) was used as supplied. Cs;NpCl,
[4], Cs;PuCl, [4], UC1, [8] and the ligands [1,2] were prepared by pub-
lished methods.
Spectroscopic measurements were made as described previously [S].
(a) UC1,<2tibpo was prepared by adding an excess of tibpo (0.17 g,
O.77 mmol) to a suspension of Cs,UC1, (0.215 g, 0.3 mmol) in dichloro-
methane (5 cmf); after stirring for 7 days the supernatant was evaporated
to dryness. The pale green residue was washed with n-pentane (3 X 10 cmf)
to remove any excess ligand and then vacuum dried, The yield was 56%.
Analysis showed 28.5% U, 35.4% C, 6.7% H, 17.2% C1 and 7.6% P.
UC44H.Cl,/D4P4 reauires 29.2% U, 35.3% C, 6.6% H, 17.4% Cl and 7.6% P.
The bright green complexes NpCl, 2tibpo (yield, 54%) and PuCl,
2tibpo (yield, 44%) were prepared in the same way from Cs;NpCl, and
Cs;PuCl, respectively, except that the final products were washed with 2-
methylbutane (2 X 10 cmf). Analysis of the neptunium complex showed
28.8% Np and 17.35 C1. NpC44Ra4C1i04F4 reauires 28.85 Np and 17.4% Cl.
The plutonium complex was identified by its IR and near IR--visible solution
spectra.
(b) NpCl, 3mdppo was prepared from Cs;NpCl, as described in (a)
except that the supernatant was evaporated to 2 cm' and the salmon-pink
complex precipitated on the addition of 2-methylbutane. The yield was 78%.
Analysis showed 22.6% Np and 13.5% C1, NpC44H44Cl4O4P4 reauires 23.1%
Np and 13.8% C1.
Brown NpCl, 3.5dmppo (yield, 91%) and salmon-pink PuCl, 3mdppo
(yield, 87%) were prepared in the same way from Cs;NpCl, and Cs,PuCl,
respectively. Analysis of the neptunium complex showed 25.5% Np and
15.1% C1, Np4CJ44C140P4 reauires 25.9% Np and 15.5% CI. PuCli
3mdppo was identified by X-ray powder photography. The known com-
plexes [1] UC1, 3mdppo and UC1,3.5dmppo were also prepared from
Cs;UCl, by the above procedure.
(c) PuCl, 2dmppo was prepared by stirring a misture of Cs,PuCl.
(0.497 g, 0.69 mmol) and dmppo (0.449 g, 2.91 mmol) in boiling methyl
cyanide (5 cm*) for 10 min, The supernatant was evaporated to dryness and
the yellow-green residue was washed with a mixture of dichloromethane
and 2-methylbutane (1:1 by volume, 5 cm*), with 2-methylbutane (10 cmf)
and finally vacuum dried for 4 h. The yield was 81%. The complex wa
identified by X-ray powder photography. The yield from this reaction i
dichloromethane (method (a)) is very poor.
The complexes described in (a) and (b), and the known complex
NpCl, 6tmpo (tmpo s PMe,O) [3] were also prepared by this method;
NpCl, 6tmpo crystallises from the supernatant on cooling, PuCl, 2dippa was
prepared in the same way, except that the methyl cyanide supernatant was
evaporated to small bulk and the yellow-green complex precipitated on addi-
tion of a 7:3 by volume mixture of 2-methylbutane and dichloromethane.
The solid residue was also extracted with methyl cyanide and the complex
precipitated from the extract after evaporation as described above. The yield
was 48%. The complex was identified using K-ray powder photography.
(d) PuCl,- 2dipa was prepared by stirring Cs;PuCl, with a large excess of
dipa in methyl cyanide for l week, The yield of the blue-green complex was
63%. It was identified using N-ray powder photography.
(e) UC1, 3tbzpo was prepared from UClias in (b) except that the green
complex was precipitated by the addition of n-pentane; the product was
washed with n-pentane (2 X 10 cmf) and vacuum dried for 8 h. The yield was
84%. Analysis showed 17.9% U, 55.4% C, 4.8% H, 11.0% C1 and 6.9% P.
UC4R.4CD4P4 reauires 17.85 U, 56.4% C, 4.75 H, 10.6% Cl and 6.9% P.
UC1,-2tbzpo (tbzpo = (C,H4CH4)gpo) (green) was prepared in the same
way, using the stoichiometric quantity of tbzpo. The yield was 82%. Analy-
sis showed 22.8% U, 49.05 C, 4.1%H,13.6% Cl and 6,4% P. UCi4.4C14OP4
requires 23.3% U, 49.4% C, 4.1% H, 13.9% Cl and 6.1% P. These complexes
could not be obtained by the methods described in (a) to (c).
(f) UC1, 2.5dmf, a known complex [6], was prepared by stirring
Cs;UC1, (0.454 g, 0.63 mmol) with dmf (3 ml; 63 mmol) for 4 days; addi-
tion of a 70:30 by volume mixture of 2-methylbutane and dichloromethane
to the supernatant yielded a green oil which solidified on trituration in 2-
methylbutane. The pale green solid was washed with dichloromethane
(5 cm') in which it is sparingly soluble and with 2-methylbutane (2 X 10
cm'), and finally vacuum dried. The yield was 72%. Analysis showed 41.9%
U, 16.6% C, 3.2% H, 24.6% Cl and 6.2% N. U4C;,H4N4Cl.O, reauires 42.17
U,15.9% C, 3.5% H, 25.1% Cl and 6.25 N.
NpCl, 2.5dmf (blue-green; yield, 73%), PuCl, 2.5dmf (blue-green;
Yield, 72%), NpCl, 2dipa (brown; yield, 65%), NpCl, 2dippa (pale-green;
yield, 63%) and the known [2] complexes UCl, 2dipa and UC1, 2dippa were
prepared in the same way. All of these complexes were identified using K-ray
I9Wder photography, by comparison with authentic samples of the uranium
(IV) compounds.
The analyses of the neptunium and plutonium complexes were carried
out as described previously for (nC4H,)4PuC1 [9].
The uranium, neptunium and plutonium tetrachloride complexes were
Prepared by reaction of the hexachlorometallate(IV), CsMC1,, with the
appropriate ligand either alone (amide complexes only), or in dichloro-
methane or boiling methyl cyanide, or (UC1, xtbzpo; x = 2 or 3) from UCla
and the ligand in dichloromethane. PuCl, 2dmppo could only be obtained in
reasonable yield from the reaction of CsPuCl, with dmppo in boiling methyl
cyanide.
Atomic Energy Authority, Atomic Energy Research Establishment, Harwell
for additional support and for the provision of research facilities.
as aircraft and ships. Hence, the framework ot Tcrramechanics, comparable in principle
to fluid mechanics, was conceived. During this period, inspired by the work of F. Nessen
published in Germany in 1940, he also became fascinated by the possibility ot analysing
complete systcms ot locomotion. This led him to the development of the systems analysis
approach to the evaluation ot terrain-vehicle systems.
He subsequently moved to the United States and during the period of 195(0-1952. he
established graduate courses and a laboratory in the mechanics of off-road locomotion
at the Stevens Institute ot Technology, utilizing the experience gained in Poland and in
Canada. There he focused on the development of the pressure-sinkage equation and
on the characterization of the shear stress-shear displacement relationship for soils. In
addition. he further pursued the concepts of articulated vehicles and vehicle trains. The
course notes he prepared at Stevens were later published in the book. Theory of Land
Locomotion, by the University of Michigan Press in 1956. During the years of 1952-1954.
he was with the Research Office of Johns Hopkins University. He concentrated on the
development of the organizational and functional schemes for off-road mobility research,
and finalized the development of what became known as the Bekker pressure-sinkage
equation for soils.
In mid-summer 1954, he was given the mission of establishing a new laboratory, the
lLLand Locomotion Laboratory, for the U.S. Army Ordnance Tank Automotive Command
in Detroit and became its founding Chief. During his period in Detroit, he focused on
the experimental verification of the stress-deformation relationships for soils which he
developed earlier and on the development of algorithms for evaluation of design and
performance parameters of a multitude of vehicles, missions and environments. He
also initiated research programs on the dynamic response of off-road vehicles to terrain
roughness, based on the stochastic approach. His outstanding achievements during this
period were summarized in his second treatise, Off-he-Road Locomotion-Research
and Development in Terramechanics, published by the University of Michigan Press
in 1960.
In 1960. with the decision in the United States to land the first man on the moon
by the end of the decade, Dr Bekker accepted a new position at the General Motors
A.C. Defence Research Laboratory, Santa Barbara, California. He expended most of
his efforts on the problem of lunar surface locomotion there. Out of this research, came
the successful development and operation of the first extra-terrestrial man-carrying
vehicle, the Lunar Roving Vehicle, for the Apollo missions. To summarize his overall
contributions to, and experience in, the development of land locomotion mechanics, he
published his third treatise, Introduction to Terrain-Vehicle Systems in 1969. As Dr Alan
Reece in his Journal of Terramechanics review put it, ''it is a very complete guide to
the work in the whole of this field up to the end of 1966. It is excellently written and
takes an impressively wide view of vehicle development . ... Its author, Dr Bekker,
has once again demonstrated his capacity to point the way forward. Not, this time, in a
new and minor branch of engineering theory, but in the much more important matter of
a whole new technique for engineering advance''. Dr Bekker retired in 197(0. However,
he continued to pursue, with vigor, the discipline that he helped found.
I had the good fortune to work with him on a number of occasions. He was
invited to Carleton University to deliver the first public lecture in a series on
''Overland Transportation Technology and Environment'' in 1971. The other lecturers
for the series were Professor N. W. Radforth and Dr A. R. Reece. Their presentations
were subsequently published in a special issue of the Journal of Terramechanics, Volume
11, No. 2, 1974
We jointly offered a special professional development program ''Terrain-Vehicle
Systems Analysis'' at Carleton in 1976. 1977, 1978 (with Dr A. R. Reece), 1980 (with
J. R. Radforth) and in 1985. In the summer of 1978, at the invitation of General Stig
Therefore, it is clarified that the wear rate of the tyre M, (mm/SMH) is proportional
to the function of power spectral density S (mlclm) for the frequency of which the
wave length is approximately the same as the length of tread of OR tyres as,
on an actual roadway.
At a limestone quarry site, l1 heavy dump trucks were operated to carry the blasted
debris from the bench cut surface of 15 m height to the unloading point. The average
distance of transportation was about 400 m, and almost all the roadways were located
on horizontal flat terrain.
The classification of 206 OR tyres 21.00-35-36/40PR scrapped during 15 years showed
the ratio of scrapped tyre due to tread wear: 46.1%, cut burst: 35.5%, tread separation:
8.3%, shock burst: 5.3%, and side cut: 2.4% etc. The average wear rate of these OR
tyres scrapped due to tread wear was (8.77 8 2.71) x 10- mm/SMH, and the average
wear life was 4706 3 1536 SMH during those years. In recent years, the wear life of
OR tyres tends to increase due to improvement of rubber material, optimum control
of tyre air pressure and tyre rotation.
The road bed for the heavy dump trucks was covered by crushed limestone
which was compacted and made smooth by a vibratory roller. The physical and
mechanical properties of the limestone were determined as: apparent specific gravity
2.68, specific gravity 2.75. Shore hardness 35.1 8: 2.4, uniaxial compressive strength
42.4 8 13.1 MPa, radial compressive strength 3.8 8 1.0 MPa, longitudinal elastic wave
velocity 2988 3 441 mls and amount of Los Angeles abrasion 34.0%. As the longitudinal
elastic wave velocity of the rock mass of limestone was 1042 8 45 m/s, the coefficient
of crack of the road bed was calculated to be 0.878. The grain-size distribution of the
surface soil sampled at points A and Y showed average grain-size: 3.5 and 22.0 mm,
maximum grain-size: 19.1 and 32.S mm, and coefficient of uniformity: 3.46 and 4.45,
respectively.
Next, the friction test between the tyres and the road surface was executed by
use of a 100 cm- rubber block at point A and a 900 cm- rubber block at point Y,
respectively. As shown in Fig. 5, the coefficient of friction shows a higher value at
point A where consists of fine grains.
As shown in Fig. 6, the roughness of the road surface was measured at points
A and Y, by use of a 10 cm diameter rolling wheel, the up-down movement of
which was automatically recorded. The measured data were analysed by means of the
power spectrum method with fast Fourier transformation. Figure 7 shows the relation
between frequency (clm) and power spectral density (mc/m) which is calculated as
second powers of amplitude of surface roughness. The parallel line group in this figure
means the evaluation of roughness of road surface which is defined in ISO [5]. As shown
in these figures, the power spectral density has a peak value for 10 c/m frequency at
point A and for 20 cim frequency at point Y. respectively. In both cases, the states of
the road surface are evaluated to be ''bad'' road.
In the limestone quarry site the variation of amount of wear with time and the wear
rate were measured for 3 kinds of heavy dump truck tyre. Figure 8 shows a general
view of loading operation of heavy dump truck with wheel loader in the quarry site.
Table 1 shows the specifications of the heavy dump truck and the dimensions of the
OR tyre. The empty and loaded vehicle weights and their distribution to front and rear,
and right and left tvres were measured several times in the field. For rear tvres, the axle
load on dual tyres was measured directly. As the effective rolling radius of an OR tyre
varies with the load in the dump truck, both the number of rotations of the tyres and
the moving distance of the dump truck were measured to calculate the effective radius
of rotation. Figure 9 shows an example of an OR tyre.
The contact area of ellipse F can be calculated as follows [6],
Here, R is the radius of tread, D is the diameter of the tyre, and f is the
amount of tyre deflection which can be calculated from the tyre spring coefficient and
the axle load.
The real contact area F can be calculated as,
Here, a is the filling ratio of tread in the contact ellipse.
investigated. Figure 12 (b) shows an example of two wear history curves of the front
tyre used under 686 kPa air pressure and four wear history curves of the rear tyre used
under 637 kPa, and the results show that the wear lives of front tyres are almost equal
to those of rear tyres.
Six OR tyres 33.00-51-58PR of a capacity 1176 kN heavy dump truck were tested.
Figure 12 (c) shows an example of six wear history curves of the front and rear tyres
used under 686 kPa air pressure. The result is that there is almost no difference between
the wear lives of front and rear tyres.
For almost all the tyres rolling on flat terrain, the wear history curve could be
expressed by some straight line. That is, the residual depth of groove of tyre tread
decreases linearly with time. So the wear rate could be defined as the gradient of the
straight line, i.e, the amount of wear divided by the service meter hours.
Next, the relations between the wear rate and the axle load of tyre might be
considered. A total of 98 series of wear history curves of the three kinds of heavy dump
truck tyre could be utilized to calculate the wear rate. The axle load on each tyre could
be calculated as an average value of the loads in the empty and loaded states. Figure 13
shows the relation between the wear rate and the axle load for various kinds of tyre,
The wear rate depends on the axle load on the tyre, the tyre size, the air pressure and
the tyre position. That is, the wear rate of 21.00-35-3640PR tyre under 568 kPa air
pressure shows a higher value, but the wear rate of 33.00-51-58PR shows a lower value
in comparison with the large axle load of the tyre.
As a result of the multiple regression analysis for all tyres rolling on the flat
terrain, the wear rate M, (mm/SMH) could be expressed in the following equation;
Here, Iß,,, is the tread width (cm), p is the real contact pressure (kPa), is the air
pressure of tyre (kPa), and K is 9.06 x 10% for left tyre and 7.77 s 10% for right tyre.
The coefficient of multiple correlation of this equation is 0.70. The above equation is
centers 3.; for the vehicle with single-axle steering is greater than for equipment with
articulated frame steering. The greater this difference 3(, the greater is the kinematic
irregularity that can lead to problems in vehicles with mechanical power transmission
to the wheels (Fig. 6). Kinematic irregularity can be generally defined with the use
of the condition known from mechanical engineering referring to inseparability of a
rigid body: ''For any rigid body the components on the straight line connecting any
two points must be equal''. In practice this condition is frequently not satisfied for
all-wheel-drive vehicles.
The following factors are primarily responsible for kinematic irregularities:
Kinematic irregularity leads initially to loss of play in the drive train components, then
possible elastic deformations in drive components, tires and the ground are exploited,
and finally the wheels begin to slip. Certain machine operating conditions are particularly
prone to kinematic irregularities; in fact, it is even possible in this regard that wheel slip on
the front axles works against that on the rear axles. This means that the circumferential
the machines with articulated-frame steering moves towards the front of the machine and
towards its line of overturn when the steering angle y of the machine increases (Fig. 1b).
For instance, the four-wheel drive steering loader can provide full lifting power even
for the smallest turning radius, while the lifting power of an articulated-frame loader is
decreased proportionately to the lock angle. In spite of these advantages, earthmoving
machines with four-wheel steering are rarely produced because this kind of equipment
On the premise that it is possible to improve deep snow trafficability for commercially
available off-road vehicles through preconditioning, this study was aimed at developing
techniques appropriate to field application. The objectives of the study were:
Both laboratory-scale and full-scale experiments were carried out to study various
preconditioning techniques, the findings of which are addressed in this paper.
The program of experimentation consistcd of two parts, as follows:
This was established to explore alternative preconditioning techniques on a laboratory
scale, to determine practical test procedures and to provide better control of test
conditions. The parameters studied included:
All laboratory tests were conducted using artificial snow which was prepared by
pulverizing ice cubes into fine grains, similar to the technique previously used by Yong
and Fukue [2]. Artificial snow was adopted because of the need of producing replicate
snow samples for control testing. The grain size distribution of the artificial snow was not
significantly different from certain types of natural snow [2,3]. Similarly, the load-resisting
behaviour of the artificial snow was not much different from that of natural snow [2]. Thus,
the use of the artificial snow in the laboratory equipment provided the fundamental basis
for planning field study.
The artificial snow, which produced a typical distribution of particle size as
shown in Fig. 1, was deposited loosely into a bin (1219 mm long by 102 mm wide by
1006 mm deep) having one transparent wall made of plexiglass, as illustrated in Fig. 2.
The long length of the snow bin permitted to conduct several tests on precisely the same
snow condition, thereby minimizing the effect of variation in snow properties. Tests
performed included measurements of temperature and density, Rammsonde hardness,
footing (plate) pressure and resistance to cone penetration. The load-carrying capacity
of the artificial snow was measured by footing penetration which was carried out using
a 74 mm by 99 mm rigid plate. The width of the footing (74 mm) was much smaller than
the depth of the artificial snow (1006 mm) so as to satisfy the 'deep snow condition',
which required that pressure bulb development in the snow below a loading platform
should not intersect the bottom rigid boundary. The development of a pressure bulb
during footing penetration was visually observed by monitoring the deformation of a
network of gridlines inscribed onto a vertical face of the artificial snow sample prior
to testing.
Fine diamonds (0.5-3.0 micron) were added to a commercially available cordierite powder to
enhance the mechanical properties of the hot-pressed matrix ceramic material, The fracture
toughness was determined using the indentation strength in bending technique and it was found
that at low additions of diamond the toughness increased, levelling off at a value of 3.1
MPa,m'' at concentrations in excess of 20 volume percent. Scanning electron micrographs of
the cracks emanating from the corners of the Vickers indentation indicated a crack bridging
mechanism. However, closer examination of the results indicated that crack deflection and the
presence of compressive zones in the matrix were the dominant toughening mechanisms.
Diamonds were added to hot-pressed cordierite and aluminium nitride to improve the thermal
conductivity (measured as diffusivity) of these materials. Thermal diffusivity measurements at
both room and elevated temperatures have shown that improvements over the matrix alone
can be achieved at diamond loadings up to 30 volume percent. The experimental results were
compared to the Maxwell prediction for the thermal conductivity of a dilute two-phase
composite and reasonable agreement was obtained. Specific heat determinations were also
carried out at various temperatures and it was shown that the specific heat of a diamond-
ceramic composite can be estimated from a rule-of mixtures approximation.
trioxide. Because of its toxicity, arsenic trioxide has considerable potential for producing
environmental damage.
Arsenic has commercial application in pesticides, wood preservation, glass production, alloys
and electronics. However use of arsenic in industry is decreasing, exacerbating the need for
a suitable disposal method.
This poster describes a method of disposal of arsenic trioxide waste based on stabilisation in
Portland cement and other pozzolanics, This approach has two objectives:
To achieve these goals, the waste form should have the properties listed in Table 1, together
with relevant quality control tests, Using these techniques, the phase assemblages and
distribution of arsenic among cement phases was determined.
Many methods of coating bioactive hydroxyapatite (HAp) onto strong materials have been
studied for increasing its mechanical strength. This paper reports the characterisation of HAp
coatings, prepared using a radio-frequency (R.F.) thermal-spraying process, on ZrO,-3 mole%
Y4O, ceramic substrates. High HAp contents in the coated layer were obtained using lower
plasma powers. The surface of the coated layer contained many pores in the size range 20-50
the grain boundary phase change with sintering conditions (temperature, cooling rates and gas
atmosphere). The amount and type of impurities in the starting powders also play a major role
in determining this grain boundary behaviour.
Electrode materials to be used in high temperature solid oxide fuel cells (SOFC) were
evaluated. Their electrical and mechanical properties were investigated and promising
candidates were used in a small planar unit. Emphasis was put on the perovskite group
(ABO,) of materials which were fabricated using a number of wet chemical routes.
In the majority of SOFC designs, La;.,Sr,MnO4; and La;.,Ca,MnO4; are used as cathode
materials, Fabrication problems appear to be inherent in the higher dopant cathode materials,
even though they have superior electrical conductivities. As a consequence of thermal
mismatch and mechanical stresses which build up in the cell, especially in the planar system,
System analysis and the need to reduce the cost of balance-of-plant are compelling
development groups to lower the temperature of operation with an ultimate target of 750
(s50*C). Strategies to attain this turget will be reviewed, and the possible replacement of
zirconia electrolytes by alternative solid oxide systems will be discussed.
Finally progress in protonic SOFC systems using methanol fuel will also be considered.
integrated engineering and automated production. This will lead to a cost effective, high volume,
export-oriented industry, adding high value to Australian resources.
The knowledge and expertise gained in research, development and manufacture of advanced
ceramic components will be directly relevant to other ceramic producers and will also provide
opportunities for major end user industries to improve their international competitiveness.
The main thrust of the research and development program is to assist the existing and emerging
advanced ceramics manufacturing companies to become competitive in the efficient mass
fabrication of high quality, high performance, high value added ceramic components and
devices.
The research program will develop intelligent manufacturing processes for the production of
advanced ceramics. The key requirement is to obtain control over the forming and firing
processes such that components require little or no final machining to achieve engineering
tolerances.
The program comprises four interactive research streams:
Therefore, the research focus will predominantly be on:
The partners have recognised that if no corrective action is taken the future growth of the
advanced ceramics industry in Australia will be restricted by a lack of suitably trained personnel.
Australia currently has no pool of labour with the necessary skills or experience in advanced
ceramics. The centre's education plan is hence an important factor in assuring the industry's
future in a time of rapid expansion demanded by the world operating environment. The new
skills made available will be highly relevant to the downstream user industries generally.
Since the pioneering contributions of Laplace, Y oung and Poisson to the
mathematical formulation of the equilibrium configurations of liquid menisci and the
early experimental work of Plateau, capillary fluid mechanics has undergone an
intense study and development.
Owing to the simplicity of the governing equations, the stability limits and free
vibrations for simple geometries (spherical. cylindrical. or flat equilibrium shapes)
3ere the first issues under study, An excellent and thorough review of the literature
on this field can be found in Myshkis et al. (1987). These early studies involved a
linearization of both the equation of the free surface given by the balance of
9namical pressures, and the equations of motion of the fluids. Thus. after the
Apropriate choice of coordinates, the equation of the perturbation of the interface.
hich in turn determines the complexity of the problem, can be analytically solved.
However, although many mechanical features of capillary menisci came to light, the
Pptions made in dealing with these simple geometries were quite restrictive.
For many axisymmetric equilibrium configurations, the effect of the presence of
s forces such as gravitational (Pitts 1974 1976). isorotational (Brown & Scriven
48Oa, b) or electromagnetic fields (Basaran & Scriven 1989: Gonzalez et al, 1989) as
M4l as thermal effeets (Busse 1984: Chifu et al, 1983) have also been investigated in
e nesr abs regme r 6o sngle and eompouna aops. +her 4aiaua o
%4ve. However. the presence of these body forces was investigated only in the
s when they could be treated as small perturbations, or in such a way that they
region of the shear layer, Since (i) the actual dissipation must always be positive and
(ii) errors in the measured triple yelocity transport are certainly lower than 100 '',.
we have to conelude that the pressure-velocity correlation component of the energy
transport must be significant. Indeed, it must in those regions be of the same order
as the velocity triple-product transport components. to ensure e > 0. It therefore
seems very likely that pressure transport is important across the whole tlow. at least
at these stations, and there appears to be no guarantee that this is not the case nearer
reattachment. although here the deduced dissipation remains positive (figure 1Sc).
The 'dip in the apparent dissipation at ,r/L, = 0.86 may well be caused more by
changes in the size of the pressure transport term, rather than any true multi-peaked
behaviour of the actual energy dissipation. It is interesting that the energy balance
obtained by Driver & Seegmiller (19985) in the recireulating region behind a
backward-facing step also has a distinct dip in the dissipation profile, although it
does not become negative, so the (neglected) pressure-velocity transport may be
relatively smaller in that flow.
As indicated earlier, one of the features of nearly all the published work on
nominally two-dimensional separated regions is the finding that a distinct low-
frequency motion exists, whose characteristic timescale is very much longer than
that associated with the large eddies in the separated shear layer. This motion is
usually most clearly identified just downstream of separation, where the cor-
responding spectral peak is well separated from the shear-layer turbulence, and has
been noted by Castro (1981). Eaton & Johnston (1982). Kiya & Sasaki (1983). Cherry
et al. (1984). Barkey-Wolf (1987) and CH. among others. To find whether a similar
Imotion occurred in the present case, autocorrelation measurements were made at
a/L,. = 0.2, using the techniques described by Castro (1985). Typical results are
presented in figure 21 and the resulting cross-stream variation in integral timescale.
defined by
is shown in figure 22. r' is the time to the first zero crossing of R(r), or a: if there is
no zero crossing and T, has been normalized by SU and . The maximum value of
T, occurs in the centre of the shear layer and is somewhat larger than the maximum
value of about 10 implied by the data of CH and Kiya & Sasaki (1983) at an
quivalent distance from separation (r/lL, = 0.2). The immediate implication is that
Asimilar low-frequency motion is present. This is corroborated by the wall pressure
9ectrum obtained at the separation position, shown in figure 23. Because of acoustic
haterference, this kind of data was not easv to obtain : in fact, similar measurements
a little further downstream were rather more uncertain (largely because the total
-square fluctuating pressure was much smaller there), so are not presented,
Whough the low-frequency peak evident in figure 23 was just as clear.
IIn terms of the equivalent autocorrelation timescale (see CH for a discussion of
ch equivalence) this low-frequency peak corresponds to a T,3U/A value which lies
Mmarkably close to that obtained by extrapolating to the wall the timescales
otained from the velocity autocorrelations. as shown in figure 22. This is rather
aller than the timescale deduced by Kiya & Sasaki (1983). also from a pressure
trum, and it appears that although the maximum value in the centre of the layer
they toeuused attention only on internal tlows with wall waviness characterized by a
scale of the same order of magnitude as the characteristic viscous length. As a
consequence results described in Ralph (1986. 1988) and Sobey (1980. 1982. 1983)
cannot be used to obtain even qualitative informations on oscillatory flow at the
bottom of sea waves where the constraints present in internal flows are absent and
ripples are characterized by wavelengths much larger than the bottom boundary-
layer thickness.
In the present ppaper we determine the flow field close to a sea bottom covered with
ripples. WWe attempt to overcome the difficulties described previously by employing
a numerical approach based on spectral methods and finite-difference approxi-
mations which allows us to obtain detailed results in a range of parameters of
physical relevance by means of a limited amount of ('[' time.
ln particular the numerical approach is able to describe both the irrotational part
of the flow and viscous boundary layers for relatively large values of the Reynolds
number. Thus the dynamics of the vortices generated by flow separation at the ripple
crests is analysed along with the dynamics of the yortex structures generated by flow
separation along the ripple protile induced by pressure gradients, Some of the results
obtained by Longuet-Higgins (1981) are thus recovered. In particular the present
results show a vortex pair shed from the ripple crest every half-cyele, The inclusion
of viscous effeets allows us to obtain accurate quantitative results and detect some
new interesting phenomena :
These phenomena cause the flow field to assume rapidly a complex behaviour
which requires suitable averages to be performed in order to gain a clear picture of
the process.
In the next section we formulate the problem. The numerical approach is described
in $3 while results are presented in $&. In $4 an asymptotic analysis, which holds for
Bmall times, is presented and used as a check of the numerical model.
Let us consider a two-dimensional gravity wave of small height H. length L* and
period T'* in shallow water of depth D* propagating over a wavy bottom. Let us
denote by p and water density and kinematic viscosity respectively.
It is well established that the flow can be modelled as irrotational except within
the unsteady boundary layers adjacent to the bottom and to the free surface. Since
We are interested in the interaction between fluid and bottom, we focus our attention
On the former layer and use linear wave theorv to describe the motion outside this
T8gion. We assume the characteristic thickness of the bottom boundary layer to be
where QT stands for terms quadratie in the disturbance quantities p -- P4, pj, and
u-- u,, This indicates that the pressure contribution subtracted from 3 W,,,; in (53)
ineludes the rate of work done by the ambient pressure against the disturbance
velocity and its counterpart. the rate of work done by the disturbance pressure
against the ambient velocity. NSimilarly. the extra terms also include the simple
convection of ambient total energy by the disturbance velocity and of disturbance
total energy by the ambient yelocity.
In viow of its completely general form, (46) is probably of most value in
conjunction with special cases for which some of its terms are absent. As indicated
earlier. the author has applied a highly specialized version of (46) in a study of
nonlinear quasi-one-dimensional waves pOpagating through a near-sonic throat
flow. l)etails of that work contirm the fact that the general energy corollary leads to
a consistent aceounting for the transport of disturbance energy in a situation where
linearized theory is inadequate to deseribe the disturbance field. Here, as an example
of the utility of (46). a much simpler nonlinear problem will be discussed, and that
only in enough detail to illustrate the application of the corollary,
Consider plane wave propagation in an ideal gas, As is well known, even in the
absence of explicit thermoviscous effocts, shock waves which develop in such fields
give rise to cnergy losses which attenuate waves as they travel, This is a situation in
which (46) with s-- s,, == )can provide a convenient means of accounting for the losses.
The ' piston problem. in which data are specified on r = 0 and plane waves move out
into a > 0. has been analysed in detail by Whitham (1974) for the case of finite-
amplitude disturbances travelling into a uniform, quiescent state, So long as the data
are such as to generate suffieiently weak shocks, a first approximate solution for the
disturbance field is obtained by assuming that the flow is homentropic. In this case
equations (1) are equivalent to
in which [f are the Kiemann invariants,
and the subscript notation for derivatives is employed. The pressure and density in
the flow are determined from
where y is the specific heat ratio and the subscript 0 again denotes the ambient state.
For the current purposes. however. a somewhat different problem will be
considered. Assume that the disturbance propagates upstream into a gas which is in
uniform motion at speed ' in the negative r-direction. Define a dimensionless
Velocity perturbation u and sound speed perturbation a according to
and assume that the basic steady flow Mach number .lI = U/c,, is near unity. This
introduces a small parameter e = 1--.lI into the problem, and it becomes amenable
to a formal perturbation analysis which describes waves of small amplitude
$r0pagating into an oncoming near-sonic flow, The details of this analysis will be
Omitted here. but it is noted that such an approach is of value because it leads to an
One of the most important functions of the dialyzer
is the elimination of waste in blood by filtration,'
It is always carried out naturally in healthy persons,
but not in patients of functional disease in the kidney.
The artificial elimination in the patients, therefore,
has to be done by using the dialyzer at regular
intervals, during which they must keep quiet. Then,
it is quite necessary to reduce the eliminating time.
The authors made a preliminary test to increase the
efficiency of dialysis by irradiating ultrasound on the
dialyzer model, because the following ultrasonic
mechanical actions are expected to increase clear-
ance; that is, the repetitional sound pressure increases
the relative velocity of particles and induces the
ultrasonic straight flow, microstream, and cavita-
tion which effectively stir the inner or outer solutions
of the dialysis membrane. A remarkable en-
hancement of the efficiency of dialysis was found in
this test. The reports on the effect of ultrasound on
the dialysis efficiency, however, are hardly found.
The purpose of this study is, therefore, to clarify
the causes of the ultrasonic effect on the dialysis
efficiency.
The concentration of solute, waste molecule,
denoted C, in the solution, which flows inside a
hollow fiber tube for dialysis as shown in Fig, 1,
may be regarded as a function of time, t. The con-
centration per unit distance, therefore, may be rep-
resented by C(t) at a certain position in the flow
direction, The change with t in the concentration
of solute inside the hollow fiber tube, hereinafter
abbreviated as the tube, can be considered to be
proportional to the difference between the solute
concentrations in the inside and the outside of the
tube, and if that in the outside of the tube is much
smaller than that in the inside, the following equation
is given;
where K is a rate constant of the solute which
passed through the tube membrane. Hence, the
solute concentration at the inlet of the tube, t = 0, is
defined as Cs, and then the solute concentration at
the outlet, Ci, is given by
where V is the total effective volume of the inside of
the tube, is the volume flow of the solution per
unit time. The solute molecule may pass through
three layers, i.e. the dialysis membrane and the
inner and outer concentration boundary layers on
the dialysis membrane'' as shown in Fig. 2. These
three layers can be safely regarded approximately
as a plane, where the layers are much thinner than
the radius of the tube. Hence, the resistance of the
solute transport through the tube membrane, R, is
represented by the diffusion theory as follows;
where R,,, K., and R.,, are the solute transport
resistances in the inner concentration boundary
layer, membrane, and outer concentration boundary
layer respectively. The permeabilities which are
the reciprocal of R, therefore, also consist of the
following three terms;
where ki,, ks, and ka,, are the permeabilities in the
inner concentration boundary layer, membrane, and
the outer concentration boundary layer respectively.
These permeabilities are related with the rate con-
stant K, as the mol numbers of the solute which
passed through are equal to each other in the three
layers, Therefore, the following equation is given;
where As, Aia, and Aa, are effective areas of the
tube membrane, the inside and outside of the mem-
brane, and C, Cs, and Ci are the concentrations on
the inner and outer surface of the membrane, and
in the bulk of the dialyzer as shown in Fig. 2 re-
spectively. Therefore, the following equation is
given;
where Ai., d4, and Aa,, are represented by
respectively. Assuming 8,,[Yi and d.,,/Ti< 1,
Ai, A4 Aa,i, where 44, i,, and d.,, are the thick-
ness of the tube membrane, the inside and outside
of the membrane respectively, and Y, is an inner
radius of the tube, and L is an effective length of the
tube. Hence,
and then since Y is constant, the following relation
is given;
Both of clearance, C., and dialysance, D,, are the
parameters which represent the dialysis efficiency
per unit time, and these are defined as follows;o%
The dialyzer model of hollow fiber kidney type
consists of a bundle of hollow fiber tubes (1) set up
in a 50 ml beaker (2) and a screw (3) to stir the
dialysate in the beaker as shown in Fig, 3. The
following equipments are connected to the dialyzer;
a small reservoir (50 ml) (4) for the waste solution
instead of blood; a pump (5) to supply the solution
to the hollow fiber tube; a large reservoir (1 1) (6)
for the dialysate; a pump (7) to circulate the dialy-
sate; a pump (8) to circulate the thermostatted water
(9) in the whole equipments. Poly-vinyl chloride
tubes of 2 mm in an inner diameter (10), which were
covered with polystyrene foam sheet (11) for the
maintenance of a constant temperature, were used
in the transports of the waste solution and dialysate
through those equipments. A rotational speed of
the pump (5) for the supply of the waste solution
was controlled particularly by an outer DC stabilized
supply. Since an electromotive force of a driving
motor of the pump was kept below 3 0.1 %( in a
fluctuation, which was observed with a digital volt
meter, the fluctuation of rotational speed of the
pump might be also assumed to be less than 3 0.1 '.
On the other hand, the pump (7) controlled only
by a bild-in DC stabilizer circulated the dialysate
at the volume flow of 70 (85%4) mllmin. 50 ml
dialysate, which was always preserved in a 50 ml
beaker (2) of the dialyzer, was stirred by the screw
(3) jointed to a stepping motor (12). The stepping
motor, which was driven by a driving amplifier with
a crystal pulse oscillator, SMD-2, produced by
Kairiku Denpa Co. Ltd., Hiroshima, was maximum
1,500 RPM in a rotational speed, S,, and less than
80.1%; in the fluctuation of S,. The hollow fiber
tube, the product of Asahi Medical Co. Ltd. was
made of copper ammonium rayon, 200 gm in an
inner diameter, and 11 gum in a wall membrane
thickness. The dialyzer was made of a bundle of
20 tubes and cut to an effective length, L, of 16 and
4 cm. Those bundles were loosely fixed by a
helically shaped vinyl-coated-copper wire, 0.5 mm
in a diameter, where the heiix of the wire was 6 cm
in an inside diameter as shown in Fig, 4. The
bundie of L= 16 cm was fixed particularly on the
three PMMA, poly-methyl methacrylate, posts of
3 mm in diameter erected on the bottom of PM MA
cap of 50 ml beaker (2). Ultrasound was irradiated
from the bottom of 50 mi beaker (2) by an remodeled
ultrasonic cleaner, Pet Cleaner 52, produced by
Cho-onpa Kogyo Co. Ltd., Tachikawa, Tokyo.
The frequency of ultrasound was 41 3 0.5 kHz and
controlled in P, 0--30 W by the applied electric
voltage, less than AC 100 V, where P, is an average
electric power input to the transducer (13). The
value of P, was measured by an analog electric power
meter with the accuracy within 5%, connected to
input terminals of an output transformer. The
stabilities of P, in the experimental conditions were
all within 5%4.
The 1.00 x I0-%, aqueous solution of creatinine,
CHiNC(: NH)NHC(O)CHe, molecular weight =
113.12, which is one of the wastes, was used instead
of blood. Ion-exchanged water was used as the
dialysate in order to simplify the dialysis system.
The concentration of creatinine was determined by
the absorption measurement of ultraviolet light of
wavelength 234 nm. In a series of measurement of
change in C; with at constant S, and P,, an in-
crease in C, was less than 2%( of C, and the difference
between values of C;, and D; was extremely small,
so that the dialysis efficiency will be expressed by thc
clearance, C., hereinafter. The value of 0 was
collected in a sample tube (14) in Fig, 3 and its
dropping time. The density of the creatinine solu-
tion, which was very diluted, was equal to that of
Aater. A viscosity of the solution was measured
with a rotational viscometer. All the measurements
were made at 30C,
4.1 Increases in Clearance by Ultrasonic Irradia-
tion
The value of C, in the hollow fiber tubes used in
this experiment decreased with time and reached
a minimum stationary value in about a week in the
condition of S., P,, and Q =constant. The study on
the decrease in C, with time is very interesting as
another subject. Therefore, the bundle of hollow
fiber tubes, abbreviated again as tube hereinafter,
soaked in water for a week was used in this experi-
ment. The measurements of C,, were made mainly
by using the bundle of the tubes of L= 16 cm,
because C, changed largely with and S,. The
experimental values of C; increased with ? as shown
in Fig, 5. The position of curve of C, vs 2 went
up with the increase of the rotational speed of screw,
S., which was proportional to the stirring speed of
water, dialysate, without ultrasonic irradiation.
When the ultrasound was irradiated, the position of
curve went up more with the increase of the ultra-
sonic power, P,, and reached the maximum position
at P,= 10 W at constant S,= 1,500 RPM. The
following contributions of the ultrasound to the
increase in C, may be presumed;':* i) A diffusion
velocity of creatinine molecule increases with the
relative velocity of the molecular particle which is
increased by the repetitional sound pressure; ii) The
thickness of concentration boundary layer on the
inner surface of membrane of the tube is decreased
by exchanging the solution in the neighborhood of
the membrane to that at the central part in the tube
by the ultrasonic cavitation and microstream; iii)
The thickness of concentration boundary layer on
the outer surface of the membrane is decreased by
exchanging the water in the neighborhood of the
membrane with that in the bulk by the ultrasonic
straight flow; iv) The thickness of concentration
boundary layer on the outer surface of the mem-
brane is decreased by the similar exchange of the
water to iii) by the ultrasonic cavitation and micro-
stream; and so on.
From Eqs. (2) and (12) the following equation is
given;
Equation (14) represents a linear relation of
- In (Ci/C.) with 1[2 and has an intercept of
-In (Ci/C)=0. The points of - In (CJC) ob-
tained from the experimental values increased
linearly with 1/Q but the intercept shifted slightly
upward from - In (Ci/C4)=0 as shown in Fig, 6.
The increase in the inner diameter of the tube with
?, the non-uniform distribution of concentration of
creatinine, and so on in the extremely large 2 may be
presumed as causes of the intercept of -- n (Ci/C)=
O, but the data are insufficient to evidence the cause.
However, Eq. (14) is rewritten in the range of 2 in
this experimental condition as;
and V is constant when ? is stationary, so that C,
depends on K. Hence, the contributions of ultra-
sound to the enhancement of C, i-iv), may be
examined by the value of K or k, in Eq. (15).
The slope of the straight line increased with the
ultrasonic power, P,, and reached the maximum
position at P,= 10 W at constant S,= 1,500 RPM,
as shown in Fig. 6. The diffusion coefficient is
proportional to the relative velocity of solute
molecule which is almost proportional to the am-
plitude of ultrasound. The slope of the line, KV,
therefore, must increase with /P, if i) were the
cause. However, KV did not increase with /P,
at P,10 W, This fact means that the contribution
i) may not be relevant to the explanation of this
experimental result.
The thickness of the concentration boundary
layer on the inner surface of the tube membrane
related to ki, is considered here. On the assump-
tion that the concentration boundary layer is thinner
than that of the velocity, the theoretical calculation
on the inlet pipe flow can be applied. The growth
length, xi, in axial direction where a fully developed
Poiseuille flow is attained is shown as
where y is a coefficient of kinematic viscosity and U
is an average flow velocity of the solution,'? As
Ai is proportional to flow volume ?, and the mean
thickness of the layer can be considered inversely
proportional to the thickness of the layer. The
following similar relation with the concentration
thickness is obtained;
Ri, in Eq. (3) and 1/ki, in Eq. 4) seem to be prop-
ortional to ö;,, Hence,
The value of ki,, therefore, must change also with
1[Q. However, since K is independent of 1/0 as
can be seen in Fig. 6, 1/k,, in Eq. (4) may be ignored
under this experimental condition, and 64, may be
ignored. Therefore, the contribution ii) may not
be relevant to the explanation of the experimental
result.
As revealed in the above consideration the re-
sistance of solute transport through the tube mem-
brane, R, is given as follows under this experimental
condition;
The clearance, C., increased with the rotational
speed of screw, S, as can be seen in Fig. 5. This
fact shows that the average thickness of concentra-
tion boundary layer on the outer surface of the
membrane, ,.,, decreases with the flow speed of
water, and I[ka,, also decreases with the flow speed
of water. The decrease in 1/k.,, by the ultrasonic
straight flow, macrostream, of water, therefore,
must also be taken into account. However. since
the flow speed of the macrostream is much slower
than that by the screw, this effect may scarcely con-
tribute to the decrease in 1/ka,,, Hence the con-
tribution iii) may not be relevant to the explanation
of the experimental result.
The outer surface of the 20 tubes may be studded
with dead water regions due to the existence of
narrow space between the tubes and other fluid
mechanical contributions. The area of dead water
region generally increases with the flow speed of
fluid. Since the dead water regions exist on the
outer surface of the membrane, the average thick-
ness of the outer concentration boundary layer,
.,i, dose not seem to be 0. The dead water regions
may be removed by a mechanical action of the
ultrasonic cavitation and microsteam. In this
experiment the slope of the line in Fig. 6 did not
increase with ./P, at P,2 10 W, so that dead water
regtons were removed by the ultrasonic mechanical
action at P,= 10 W, and 4,,, may be about 0. Hence,
R may be given here by
The increase in C,, by ultrasonic irradiation may
lead to the effective exchange of the water in the
neighborhood of the outer surface of the membrane
with that in the bulk by ultrasonic cavitation and
microstream. The contribution iv), therefore, is
the most reasonable for explaining the experimental
result.
The value of K4, the rate constant in the mem-
brane, was calculated to be 8.1 min* from the slope
of line, KV, at P, 10 W In Fig. 6. The diffusion
coefficient of the solute molecule in the membrane,
D4, is related with K,, and k, as follows;
The values K,= 1.4x I0-- Yi 100 y I0'cm,
and 6= 11 x I0'cm are substituted in Eq, (21).
D, is estimated to be 7.7 x 10-' cms, which is about
1/10 of that of the diffusion coefficient of creatinine
in water, 7 x 10 cm s, obtained by the empirical
formula.'' The magnitude of D, value may be
reasonable, if the free microspace in the membrane
medium of the tube is presumed to be about I[10
of that in water. The proportion of the volume
occupied by free space in the membrane mediums,
about 1/10, may be the reasonable value as judged
by a steric structure of molecule of hollow fiber,
cellose, swelled in water. On the other hand, the
value of D4g calculated by unsteady axisymmetric
diffusion equations was 1.1 x 10 cm s', which is
not so different from the above obtained value.
The bundle of the tubes of L= 16 cm is slightly
complicated in shape compared with the ideal
straight tube as can be seen in Fig. 4, so that the
liquid flows both in the inside and outside of the tube
are also complex. Since it is not somewhat certain
whether the complex flow affected the value of D,
calculated by the theoretical equation for a straight
tube or not, the expcriment similar to the above
was made by using the simpler bundle of the tubes
of L=4cm at S,= 1,500 RPM. The shape of the
bundle of the tubes of L =4 cm is almost linear in
the effective part of dialysis as shown in Fig, 4. The
experimental values of C; changed with (0 as shown
in Fig, 5 in the same manner as in those L = 16 cm.
The position of curve of C;, vs 0 in L=4 cm reached
the maximum at P, = 15 W, in contrast with that in
L= 16 cm. This may be considered to be due to
the heterogeneous intensity of sound in the beaker
(2) in Fig. 3. The values of K, and D, were cal-
culated to be 1.3 x 10*4*5 and 7.2 x 10' cm%s,
respectively, which were almost equal to those in
L= 16 cm, from the slope of line, K V, which were
obtained by plotting - In (Ci/C.) vs 1/ in P,=
15 W, These values indicate that the theoretical
values are scarcely affected by changing the shape of
the bundle of the tubes, here.
As mentioned above, it was clarified that dialysate,
water, could be exchanged effectivcly by the ultra-
sonic mechanical actions, cavitation and micro-
stream, and then the efficiency of dialysis increased.
However, ultrasonic irradiation can not be applied
readily to the medical dialyzer, because many tissues
which may be destroyed by ultrasonic irradiation
exist in blood. The investigations, therefore, are
expected to be made regarding the effect of ultrasonic
irradiation on tissues in blood. In any event, since
a limitation exists in the exchange of dialysate by
mechanical stirring, it is very useful to enhance the
efficiency of dialysis by ultrasonic irradiation,
The authors are grateful to Prof, K. Yoneda,
Medical College of Oita, for reviewing and refining
the manuscript. Special thanks to Asahi Medical
Co. Ltd., for supplying the hollow fiber tubes for
dialysis and its data, and aiso to Cho-onpa Kogyo
Co. Ltd., Tachikawa, Tokyo, and Kairiku Denpa
Co. Ltd., Hiroshima, for the technical assistance.
The fundamental task of many systems for classification, recognition, and diagnosis
is to provide a belief assessment on a set of n hypotheses h;,. . . , h,, based on the
presence of a subset of evidence e;,. . . . ,,. The belief assessment is done on an
ordered scale. The belief scale may be continuous, as the real line, or discrete, as
lnguistic quantifiers. Partially ordered scales are also possible, one example being
multiple index assessment. We shall discuss ordered assessment only in this paper.
Suppose the system is designed to respond to all the possible combinations of M
pieces of evidence. That is, for each hypothesis h, and each subset of evidence E, the
Bystem provides a degree of belief B,(E). In this case, a complete specification of the
8Ystem involves a tabular presentation of 2f items. This table is usually too big to
fillfor a reasonable M. Among common alternatives is a partially-defined, stereotype
table with a mechanism to induce other items of the table from the stereotypes. Then
the table-filling task is reduced into the specification of a set of stereotypes and a
mechanism. The simplest form of this reduction is to have the stereotypes in the forms
of B,( }) and B,(;e, } ). Thus, nm items based on single pieces of evidence and n before
getting any evidence, are assessed individually and the rest of the table is derived from
these stereotypes and a mechanism.
The mechanism that reduces any general belief assignment into a function of these
stereotype belief assessments is often called the formula for combining evidence. There
are two common practices in finding a formula for a decision-making system or an
expert system. The first approach is to construct a formula based on an interpretation
of the belief, say, conditional probability,
Then, from a well-established theory for the interpretation, say, the theory of
probability, a relation is found to relate this with P(h, ) and P(h, 'e,). The advantage
of this approach is that it has a well-established model. The disadvantage is that when
a particular interpretation does not exactly correspond to the real situation, a correct
theory may lead to an irrelevant formula. Even within a well-established model, the
derivation of a formula may require some assumptions, say, the statistical indepen-
dence assumptions, which may be either invalid or difficult to verify. Since most
complex systems do not have idealized interpretations, this approach is often replaced
by the second one.
The second approach avoids immediate interpretation of the belief. It starti
from the testable and desirable properties the system requires. These properties are
called axioms and this approach is called an axiomatical one. Interpretation is nol
considered until the formula has been constructed and is optional. This kind of
construction approach often seems ad hoc. A well-known example is the formula use4
in the original version of the expert system MY CIN. The advantage of this approac?
is that the formula thus generated fits the particular application better. The dir
advantage is that there is no good study on the properties of these formulae and.
hence, these formulae may face consistency, uniqueness, and other unforesee
problems.
This paper is an attempt at the formalization of the axiomatical approach o
constructing formulae for combining evidence. We hope this will bring attentio4
to a more theoretical study of evidence combination and provide guidelines fof'
knowledge engineers who have been inventing ad hoc formulae for their systemk
Section 2 is an overview of the procedure of simplifying a belief assignment. Bina7
decomposition without memory is chosen as the subject of this paper. Section
discusses some common properties or axioms for these formulae. Some commd
properties make these formulae operations in ordered semigroups. Section 4 is a brif'
survey of the existing mathematical work on connected ordered topological se
groups which will be used as the basis of our discussion of formulae on the u
interval. Two different types of use of the unit interval depending on whetM
Omy sm = =swme4 na 4mre onouiae a geeesea sauo'
and 6.
Let us consider the procedure of simplifying the tabular belief assignment with n2'*
items B,(E). Consider the assignment for one hypthesis only, and thus the subscript
iis ignored. The first step in simplification is to view the set function B(|e,.-. . . <.)
as a function of the strength of each piece of evidence B({e,}), j = 1,..., m. That
is, we can replace one evidence by another as long as their strengths are the same.
Thus, we can write the B function as a collection of functions on strengths with
various numbers of arguments:
where a, = B(5e,}), the available stereotype assignment. Notice that because B is a
set function, the F. functions must be such that the ordering of their arguments does
not affect the function values.
The second step of simplification is to decompose these F, functions into binary
operations or functions of two arguments. This is a transformation from batch
processing to sequential processing. Evidence can be treated piece by piece and belief
can be updated based on new evidence without going over the complete combination
again. For example, the set function
with the summations over all the combinations of subscripts, can be reduced into the
binary operation
In other words,
This binary operation f will be called Bernoulli's rule because in 1713, James Bernoulli
proposed it for combining beliefs. It is used in MY CIN as the rule for combining
measures of belief. An important feature of this decomposition is that as a sequential
4dating process, one cannot and need not recall the history of the establishment of
the background belief, which may be the result of a few observations or hundreds of
9bservations. Indeed, it is treated equally as the newly collected evidence, which may
t based on one observation only. We will call this kind of decompositions binary
Rcompositions without memory.
A different example is the set function of taking average,
Ts set function has a binary decomposition with memory:
also applicable in many fields, such as maintenance robots [l], more advanced
working robots, free-flying service robots in space [2], and more evolved flexible
automation [3], etc. The robot based on this cell-structured concept is also considered
to be a distribution system of the Vroid, as previously reported [4-6], and so has a
lot of incentives for intelligent robotic technology [7].
There have been some robots or studies based on ideas similar to those given
in this study, c.g., the machining center in FMS and maintenance robots for a
nuclear power plant [1, 3]. They can only replace part of their tools with othe
tools, depending on a given task. But the tool parts are only instruments without
versatilities, so that a large variety of tools are required for each operation. Naturally,
they do not have any functions that can be called intelligence and other intellectual
data bases. Therefore, we propose a new robot system that satisfies the followint
items:
A cell is defined here as a fundamental component of a robot structure and mechanis4
and has a single mechanical function: joint, end-effector, adjustable part of the arm
length between joints of a manipulator, mobile mechanism, and power, etc. These ce
are classified according to their general purpose when they construct manipulatorsd
mobile robots.
The combination and detachment between cells is carried out by mobile cells whr[
can be transferred by themselves, or by the joint cells which are already combined l
a manipulator.
Most of the present industrial robots have four to six degrees of freedom and a
large workspace according to their general work purposes. However, in some cases,
it is doubtful that those configurations are the optimum. Cells of the DRRS can
construct optimal configurations, since they can freely change combinations and
choose degrees of freedom for given tasks. For example, it is easy to decompose
a whole robotic system and carry the cells of the robot based on a spacecraft
capability when it is transported from Earth into space. This robot can flexibly
adapt itself to given works by using the combination and detachment between cells
with a restriction of working environments: closed space, like a tank which has
only a small entrance (Figure 1), and space that the entire configuration of a
mobile robot cannot pass through because of some obstacles such as pipes or
handrails. Or if the same type couplers with cells are set on a floor or a wall, this
robot can move along them, and can be fixed at the base of a manipulator with any
position or orientation in a given situation. Figure 2 shows the serial-parallel
manipulator which uses branching cells of Level 2 described above, and can generate
a higher torque. This robot can replace failed cells with new ones or can cover
troubled ones with normal ones in the case where some cells are suspected of
failing. So this robotic system has a fault-tolerant feature which is to keep, to
some extent, the original functions. There is a lot of merit in a robot which
can decompose into 'parts' called cells, although such a system has not yet been
proposed, because of difficulty in realization. However, such a type of intelligent
robotic system as DRRS is considered to be a kind of ideal robotic system and
In this paper we have attempted to describe a general framework for studying anc
modelling information systems for flexible manufacturing. The information ceh
model in conjunction with Petri net representations constitute the major componens
of the framework. In addition, the design requirements of an expert system for thi
framework are outlined. The framework can be used to describe functions anc
subfunctions of the information systems requirements in a hierarchial fashion such
that modelling can be carried out to any desired level of detail in a structural way. Th
expert system component allows the analyst/designer to experiment with the system
under study and to design and redesign its subfunctions and the structure of thes
subfunctions. The use of Petri nets allows considerable generality and flexibility n
designing, structuring, and the ability to reflect a broad spectrum of situations iha:
are encountered in the information components of an FMS, Time and cost measures
that can be used in conjunction with a model are given, The expert system uses thes
measures to assess the relative performance of various configurations of the FMS1
information system under study.
For the near future, we are concentrating our effort on building a prototype of ths
general framework as an interactive computer system. Such a system will allo
experimentation with FMS information systems design issues.
The section 'Book Reviews' will be an important complement to the main body of the
Journal. The editor opens this section in the present inaugural issue and welcomes
book review submissions on any topic within the subject matter of JIRSTA.
This book is concerned with the AND/OR Process Model for parallel interpretation
gf logic programs. It provides a framework for implementing parallel interpreters,
which contribute to the ultimate goal of designing large-scale parallel processors. The
book presents an intermediate level of abstraction between hardware and semantics,
Damely a set of requirements for a parallel interpreter running on a multiprocessor
srchitecture. The AND/OR Process Model is an abstract parallel interpreter based on
the concepts of accuracy, scalability, and modularity. One can regard this model as an
itor's system, where many objects communicate through messages and update local
RAte information in asynchronous operations. The asynchronous operation of objects
rplaces the sequential steps of PROLOG interpreters, whereas the local states of
0bjects substitutes the centralized type of environment produced in the execution of
8OLOG programs
The book involves seven chapters. Chapter 1 (Introduction) describes the four
hierarchical'top-down' layers of abstraction (namely: Theory, Operation, Implemen-
lon, Machine) which define the model of computation and the formal semantics of
ie programs.
hapter 2 (Logic Programming) provides a thorough introduction to logic
9$amming, ie. to programming based on statements that use formulas of first-
.predicate logic. Starting with the definition of syntax and formal semantics of
programs, proceeds to a description of the standard sequential control strategy
by most inerpreters, and ends with a detailed presentation of PROLOG,
ns lernaive control straegies used PROLOG sysems.
mpter 3 (Parallelism in Logic Programs) is concerned with the parallel execution
i< programs represented as a goal tree. The OR parallelism, referring to a
=lseareh stragy,s frsconsidered. Then, ue AND parallelism.corresponding
wllel construction of the tree branches, s studied.
pter 4 (The AND/OR Process Model) wogeher with Chapter 5 (Parallel oR
m===++0-As6+44i+44++@+4-++is+
n AND rocsss <ead w som=eoasaemenuaomouncuon otoneor
=. >+or =+w4bANe4++e==+ ==
Some applications of the finite element method demand
integration over the boundary of the body under con-
sideration. To carry out this integration the boundary
surface should be defined by grouping the boundary
nodes into boundary elements. Heat conduction
problems with boundary conditions of the third kind are
a typical example of this class of problem. The standard
approach used in structural mechanics of converting
boundary loads into nodal forces has no simple equiv-
alence in this case.
The definition of the boundary surface can be also
useful in input and output graphic programs where
hidden line removal is often required. The knowledge of
the bounding surfaces allows an efficient transition from
the wireframe model defined by the domain connectivity
array into a solid model described by a boundary connec-
tivity array.
The domain connectivity array, grouping the nodes
into finite elements, comprises a considerable portion of
data needed to run an FEM code and the chance of
committing errors when preparing this matrix is high. It
is therefore important to check consistency of this array.
The developed algorithm performs this task by cross
checking, e.g. whether the surfaces generated by process-
ing the connectivity matrix are closed and band the
domain
The present paper is an extension of a previous one dealing
with 2D problems.' It consists of three stages:
The appendix contains the list of appropriate routines.
This computer implementation of the algorithm can deal
with linear (8 nodes) or quadratic (20 nodes) brick
elements as well as with all kinds of transition linear -
quadratic ones.
The local nodes numbering pattern of a finite element
used in routines listed in the Appendix is shown in Fig. 1.
The local numbers of nodes within a face are shown in
Fig. 2. Other numbering schemes can be easily incor-
porated into the program.
The first step of the algorithm is executed within the
routine UNPACK. All fsces bounding a given finite
element are generated from data stored in the domain
connectivity matrix IX. The scheme of unpacking
elements into faces is coded by the local nodes numbering
pattern stored in the array IFACEPATTERN, initialized
by a DATA statement in the UNPACK routine. The
result of the unpacking, i.e. the global numbers of nodes
associated with a given face, are stored in the face con-
nectivity matrix IFACE. Each column of this matrix
corresponds to one face of a finite element.
Elements having triangular faces are generated by
doubling appropriate nodes. An example of this kind of
element is shown in Fig. 3 in which the face defined by
points 1,2.3,4 in Fig. 1, has been degenerated to line 3-4.
Obviously, the element shown in Fig. 3 can also be
obtained by compressing the upper face. A column of the
domain connectivity matrix corresponding to this
element should contain following the numbers: 4,3,3,4,
1.2.6,5,1.0,1.0.7,8,9,10.12,13,14,15]
Elements having triangular faces contain faces
degenerated to edges. These kind of faces are discarded
from the resulting IFACE array.
The second step of the algorithm is based upon a simple
fact that all faces of the finite elements appearing only in
one finite element should belong to the boundary of the
entire domain. These faces will be referred to as
boundary elements. The task of finding the boundary
elements from all faces stored in the IFACE array
generated by the UNPACK routine is performed in the
SEARCH routine. The computations of this routine are
arranged in three nested loops. The external one is over
all faces stored in IFACE array. The medium loop is over
all faces not marked as processed ones. The internal loop
is over all corner nodes of a given face. In case the face
considered within the external loop is spanned over the
same nodes is the current face, both faces are marked as
internal faces (not belonging to the boundary), otherwise
the external loop face is boundary element and as such it
is stored in the boundary connectivity matrix IXB.
The hird stage of the algorithm is a rearrangement of
the IXB array obtained in the second stage. The pro-
cedure employs the obvious feature of boundary
elements where each edge should be common to two
elements. All boundary elements having common edges
form a close boundary surface referred to as a physical
boundary. More than one physical boundary occurs in
case of multiconnected regions. Appropriate compu-
tations performed in routine SORT are arranged in three
steps:
The routines should be executed in following turn
The FORTRA N routines for generation of the boundary
connectivity array are listed in the Appendix.
Following input variable should be defined before
entering the routines:
Routines demand one working array (can be used for
other purposes once current computations are com-
pleted).
The output from the routines are:
Within the output array the boundary elements sorted
into physical boundaries, i.e, first columns are occupied
by boundary elements belonging to the first boundary
surface. Boundary elements belonging to the second
physical boundary (if any) are stored in next columns.
Procedures detect following errors in input data and
print appropriate messages.
- more than two boundary elements have one
common edge. Program terminates.
In case the routines are used to limit the number of faces
considered when determining hidden lines the testing of
various aspects of consistency of the 1X matrix can be
skipped.
For data corresponding to a discretized domain depicted
in Fig. 4
NEMAX = 20
NUMEL = 12
MAXNFACE = 72
NBMAX = 8
NBMAX2 = 10
NBMAX3 =
IX array is given in Table 1.
the routines should return following values:
NBC =
NBE = 29
the values of array IXB are given in Table 2.
The developed routines can be incorported into each 3D
FEM pre and postprocessor. In preprocessors the
routines can be useful both to check the consistency of
the connectivity matrix as well as for generating
boundary surfaces. The knowledge of the boundary
surface connectivity matrix is essential when the contri-
butions from boundary conditions are calculated by inte-
gration over the boundary surface. The boundary con-
nectivity matrix can be also useful when plotting the 3D
meshes at it enables one to speed up the procedure of
.,,the whole of analysis discovered is dependent in
great part upon modified algorithms of certain fixed
quantities...
Leonhard Euler, 1764!
his opinion was quoted in 1822 by Babbage, in a paper
on notation which was related to functional equations
(p. 1/344).'In thisarticle Iput forward a general thesis about
Babbage's work as a mathematician,engineer, and scientist,
of which several elements are exemplified by the quotation.
My claim is as follows:
The content of this thesis will become clearer in and after
the discussion (in the next section) of certain mathematical
trends in Babbage's time, but some preliminary expansion
on the word ''algorithm'' is necessary here. I intend it to
refer, in a very general range of contexts, to ideas, theories,
or procedures in which prominence is given to successive
repetitions of a process or maneuver, its reversal, its com-
pounding with other processes, andlor its substitution into
itself. In mathematical contexts the words ''iteration'' and
''combination'' will also be used (and indeed, quoted).
In addition, two related notions have to be included.
First, ''algebra'' refers both to the branch of mathematics
in which Babbage worked (specifically, functional equa-
tions) and to certain features of algebraic thinking and
proof which also arise elsewhere in his activities. Second,
''semiotics'' denotes theories of signs, symbols, and nota-
tions as such (in mathematics and elsewhere), in which is
stressed their importance in a theory and in its philosoph-
ical basis. The word was not used in Babbage's time,' but
it can be applied to several of his concerns and those of
some contemporaries.
The thesis, then,is that Babbage consciously followed an
algorithmiclalgebraiclsemiotic approach in his choice and
solution of many of his problems and deployment of analo-
gies, and that his historians should give it proper attention.
For convenience I shall coin the word ''algorithmism'' to
refer in general to this characteristic.
that Babbage played in the reform of mathematics at Cam-
bridge and his researches in functional equations and some
other areas of mathematics. The section ''Calculations by
hand and by handle'' starts with mathematical tables and
moves on to the Difference and Analytical Engines.''Indus-
try and science'' notes a miscellany of other examples in
manufacturing,cryptography,and physics. The final section
draws some conclusions about the importance of algorithm-
ism in Babbage (including a contrast with Boole) and spec-
ulates upon its origins. Reference is made in places to ''the
figure,'' which appears in the last section.
I give details of the principal pertinent events in or
related to Babbage's life in the box. For more details,I refer
the reader to A. Hyman's excellent biography.'I rely almost
entirely upon Babbage's published writings and on certain
manuscripts that appeared posthumously, for enough mate-
rial is to be found there for my purpose. Many unpublished
documents reinforce the thesis, and a few have been cited.
The references will be found in the references list, but in the
text I cite by volume and page number from M. Campbell-
Kelly's fine new edition.' For example, in the reference at
thestartof thisarticle to an 1822publication,''p. 1/344''cites
page 344of Volume l of the new edition, but the superscript
-2'' cites the original publication. Dates associated in the
text with items are normally those of first publication. A
page range not preceded by a volume number and a slash
refers to the work indicated by the superscript number that
precedes it.
.,,the dotsof Newton,the d's of Leibnitz,or the dashes
of Lagrange.
Babbage, 1864 (p. 11I19P
One part of Babbage's life is well known; he played a
major part in the conversion of British mathematics by the
Analytical Society from Newton's fluxional calculus to the
Continental notation. However, this''fact'' is not a fact; it is
also misleading in connotation. A revised account will be
briefly summarized here.'
First, before that Society set to work in 1812, reforms in
calculus teaching had been under way, at least among the
staff, in various British institutions: in Scotland, in the circle
around J. Playfair and also W. Spence; in Ireland, at Trinity
College, Dublin,in moves initiated in 1812by H. Lloyd;and
in the Home Counties of England, at the Royal Military
College and the Royal Military Academy (with P. Barlow,
O. Gregory,C. Hutton, J. Ivory, W. Leybourn, and W. Wal-
lace). At Cambridge itself, R. Woodhouse had become
acquainted with, and even the current occupant of Newton's
chair of mathematics, I. Milnor (a quite insignificant math-
Calculus of Functions.
This was my earliest step, and is still one to which
I would willingly recur if other demands on my time
permitted... It is very remarkable that the Analytical
Engine adapts itself withsingular facility to the devel-
opment and numerical working out of this vast de-
partment of analysis.
With her usual acuteness, Lady Lovelace had also stressed
this possibility in 1843 (p. 3!116);'
In studying the action of the Analytical Engine, we
find that the peculiar and independent nature of the
considerations which in all mathematical analysis be-
long to operations, as distinguished from the objects
operated upon and from the results of the operations
performed upon those objects, is very strikingly de-
fined and separated.*
Menabrea had gone a little too far in his 1842 account of
the engine. In a fit of outdatedness he invoked Lagrange's
belief(Equation l)in the Taylor series to stress the (alleged)
generality of its range (p. 376).'* Neither Lovelace nor
Babbage picked up this detail in her transiation (p. 3/1071'%
and earlier we saw that Babbage may not have known of
Cauchy's refutation of the belief.
In an 1826 paper on ''expressing by signs the action of
machinery'' (akin to the paper on mathematical notation'?
cited earlier, incidentally), Babbage varied his use of
Carnot's curved and straight overbars by deploying vertical
lines and curled left brackets to distinguish different types
of motion of parts of Difference Engine no. I (pp. 3215-
216)''' Later, in his account of the Analytical Engine, he
used Lagrange-like predashes as in Equation 1 to distin-
guish the axes; for example, in 1837, F, 'F, and''F were used
(p.317).Later,in the pamphlet'''of 1851, he lettered parts
of the engines on his working drawings in a manner extend-
ing this practice to sub-, super-, and all-over-the place indi-
ces, which indicated the type of part involved and its rela-
tionship to other parts,
Most important of all for semiotics, Babbage developed
a ''mechanical Notation'' for all his engines, by means of
which ''the drawings, the times of action, and the trains for
the transmission of force, are expressed in a language at
once simple and concise'' (p. 11179) It included rules on
using upright,italic,and small-font letters for different kinds
of referents (pp. 11I107-110)' Had he managed to construct
his engines as envisaged, he might well have developed
these ideas further in producing the envisaged printing
mechanisms. There are obvious cross-influences between
these concerns and his printing of mathematical tables (dis-
cussed in the earlier subsection on mathematical tables).
Babbage's style is evident in concerns other than math-
ematics and computing, as we shall now see.
It is not a bad definition of man to describe him as a
tool-making animal.
Babbage, 1851 (p. 10/104yy
To the modern view it is an irony that Babbage, much
concerned as he was with various applications of probability
and statistics, failed to notice their place in production
engineering. Instead, the semioticist won:''Nothing is more
remarkable, and yet less unexpected, than the perfect iden-
tity of things manufactured by the same tool,'' he wrote in
1835 (p. 8/47, italics inserted).P' We may have a clue here
about his failure to complete any of his engines: A lack of
understanding of production processes led him to waste
time and money on the excessively precise manufacture of
some of their parts.
Babbage's statement was made in the most influential
book that he published, On the Economy of Machinery and
Manufactures (the 1835 edition is cited here). There was
much concern at that time,especially among engineers, with
the mathematical analysis of economicquestions,especially
concerning optimization' and equilibriurmP (Chaps. 2-3).
However, in his usual lateral and algorithmic way, Babbage
focused instead upon the processes that take place: To the
extent that optimization is treated, it is usually in the form
oftime-saving or time-consuming. A wide variety of produc-
tion procedures and problems was given in the book, of
which one is worth noting here: an extensive account of
Adam Smith's principles of the division of labor and their
use by de Prony to manufacture his logarithmic and trigo-
nometric tables (pp. 8/124-126, 135-139)P
Algorithmicthinkingisevidentin Babbage's ideas on the
postal services. His proposal ''for transmitting letters en-
closed in small cylinders, along wires suspended from posts,
and from towers or from church steeples'' (p. 111447) has
the characteristics of compounding and reversal, and the
little model that he made around the mid-1820s in his own
house shows that he took it seriously. Again, in his book on
manufactures, he criticized the poor way in which letter
boxes were indicated and advocated a semiotically much
superior system: ''at each letter-box, to have a light frame of
iron projecting from the house over the pavement, and
carrying the letters G.P., or T.P., or any other distinctive
sign''(pp. 832-33)M
'*I was much struck with the announcement'' of F,
Arago's researches of 1824 ''on the magnetism manifested
by various substances during rotation,'' recalled Babbage in
his autobiography (p. 11/339)7 and he and Herschel re-
ported their own researches in a joint paper'' of 1825 and
Babbage in one of his own''? a year later. One may presume
that the repeated oscillations inherent in the phenomena
attracted the attention of this natural algorithmist.
In 1851 Babbage published his study of another case of
repetition, this time an optical one: He proposed occulting
systems for lighthouses,in which ''Itwould only be necessary
to apply a mechanism which should periodically pull down
an opaque shade over the glass cylinders of the argand [sic]
burners.'' Further, a lighthouse could identify itself by ex-
hibiting its identification number in a temporally semiotic
manner,in terms of the appropriate numbers of occultations
for each digit. For example, the lighthouse numbered 253
would signal
[time --]
0000000000.0 . 0000.00.0000.0-0.0000000000...
where the raised points indicate the (2, then 5. then 3)
interruptions of shining of the light (pp. 10/62-65).
Another late interest of Babbage lay in cryptography; it
exhibits algorithmism and semiotics very clearly, especially
in the transposition and rearrangement of the letters of the
alphabet. In attending to the coding and decoding, he
thought once again of going forward and backward. He did
notpublish much on it(principally an 1854article'').but O.I.
Franksen''' has shown recently from the manuscripts that he
gave it a great deal of attention. It also provides another
feature for the figure.
The next morning I breakfasted with Humboldt. On
the previous day I had mentioned that I was making
a collection of the signs employed in map-making.
Babbage, 1864 (p. 11I149y
Babbage's eclecticism is not as random as it might ap-
pear: The constant concern with algorithms, semiotics, and
algebraicthinking functioned together and gave his work far
more interconnections than are obvious at first. This is the
thesis proposed in the first section of this article, and it is
strengthened not only by the content of the cases exhibited
but also by their choice: That is, Babbage preferred to work
on problems and contexts in which they were prominent
rather than on the numerous other situations in which they
were not (so) evident.
A further common factor can be pointed out, as I fulfill
the promise of providing the figure. It shows four illustra-
tions of what I call an ''iterative array'': that is, some kind of
repeatable process or its product. Further, one passage from
a paper of 1819 on infinite series (see the subsection on
functional equations) proposed a mathematical analog:
namely, ''the method of expanding horizontally and sum-
ming vertically'' when a sequence of series were to be
summed together (pp. 1/248-249, 268).-*The wide range of
contexts highlights the strength of his liking for algorithmic
thought.
Thus a personal explanation seems to be required, cen-
tered primarily on Babbage himself - 'psychological.'
maybe, though the scare quotes are there to scare off the
psychohistorians.* For some reason algebra came naturally
to Babbage, and the contexts of the time extended that
inclination into a lifelong interest in matters algorithmic and
semiotic.* In the quotation set at the head of this subsec-
tion, he stressed the importance of the semiotic aspects of
his early orientations. The quotation ''What is there in a
name?'' is the firstsentence of his autobiography.' And the
quotation on ''tool-making animal'' -- serving as not a bad
definition of man - is certainly a very good definition of
Babbage himself. Can we see him as a naturally algorith-
miclalgebraiclsemiotic mathematical thinker patched into a
practically oriented personality? Relative to the French
background,was he a fusion of the interests of Lagrange and
de Prony?
Material related to this article was presented at the Babb-
ageiFaraday Bicentenary Conference, held at Cambridge,
England, in July 1991: and at the International Conference
on the History and Prehistory of Informatics, which took
place in Siena, Italy, in the September following. Ouestions
asked on these occasions led to valuable additions. Com-
ments on a draft were received from M. Panteki and O.I.
Franksen, and two anonymous referees. For permission to
quote from Lovelace's letter I am indebted to Taylor &
Francis Ltd., and the St. Bride Printing Library.
The interaction occurring between fluid and bluff bodies has been
known for sometimes by many designers. One outcome of such an
interaction is the tlow induced vibration phenomena which is en-
countered in various engineering applications such as flow in-
duced vibrations of pipes, cables, heat exchangers, bridges, wings
of aircraft, tall buildings and many more. Although there seems to
be numerous amounts of experimental data on the flow induced
vibration of a single cylinder such as those reported extensively
by Blevins'. Chen and Parkinsorn''. the understanding of the
various parameters effecting the flow induced vibration of a single
cylinder still needs further investigation.
Various system parameters affecting the stability of the flow in-
duced vibration of a single cylinder have been reported in Refs 4-
6. These studies indicate that the lock-in region exhibits a single
resonance peak.
To the best of the author's knowledge the only work conducted
on the flow induced vibration of a single cylinder subjected to
more than one stream of a fluid is that of Jubran and Hamdant',
They investigated the general effects of a parallel secondary injec-
tion on the flow induced vibration of a smooth single cylinder
elastically mounted at both ends. Their results indicate that for
certain combinations of injection velocities and locations and re-
duced velocities the dimensionless vibration displacement ampli-
tude may be reduced by as much as 50%,
The present investigation is an extension of this work) and
aims to study the effects of injecting another stream of jet air par-
allel to the cantilever cylinder, but at right angles to the main
stream, on some characteristics of the flow induced vibration. A
secondary aim of the present investigation is the potential of a us-
ing secondary jet for the control of flow induced vibration over
cylindrical structures.
The experimental set up used in this investigation is basically a
modified version of that used by the Jubran et afA), It consists of
an open suction type wind tunnel with a square cross section area
of 30 cm x 30 cm and of length equal to 200 cm, Fig. 1, The free
stream velocity was varied from 5 to 35 m/s, with the free stream
turbulence intensity level of (0-35%%). The test cylinder was placed
at 1-3 m from the inlet of the test section where the flow was
found to be fully developed. The resulting Reynolds number range
was about (6-8 x 10- 46 x 10, In this range the Strouhal num-
ber for the test cylinder is 0-2.
The test cylinder was of an aluminum tube with cross section of
outer diameter D = 21-5 mm, wall thickness 1 = 0-5 mm and
length L = 440 mm. This combination yields an aspect ratio L.D
of 20-5 and mass per unit length m of 0-1662 kg/m.
The cylinder was positioned in the wind tunnel horizontally:
one end is tixed, while the other is free, The cylinder passed
through two slots of (28 mm x 28 mm ) of the wind tunnel. Care-
tul consideration was given to ensure the two dimensionality of
the vortex wake. This was achieved by making the height of the
slots at the two sides ot the wind tunnel to be much less than near-
ly four times the diameter of the test cylinder, as was recommend-
ed by Graham', This was contirmed by preliminary tests.
The secondary injection jet flow is injected from a settling
chamber to ensure uniform flow through the injection mechanism.
Fig. 1. The settling chamber is 900 mm x 650 mm in area and 160
mm in depth. The flow is supplied by a fan through two inlet pipes
of (20 mm) in diameter tixed at the two sides of the settling cham-
ber at the lower end on the third part of the height. Four sheets of
pertorated mesh are placed in the upper remaining two third of the
settling chamber over the supplying pipes. A Perspex plate of 3
mm in thickness was used to cover the settling chamber with a
perspex pipe of 17 mm in diameter and 300 mm in length fixed at
the middle ot the root of the settling chamber through which the
flow is injected. The resulting circular jet from the settling cham-
ber is injected into one side wall of the wind tunnel downstream
trom the mounted cylinder. The arrangement of the injection is
shown in Fig. 1. A movable stand carrying the settling chamber is
used to inject air at different locations downstream from the
mounted test cylinder. The injection velocity was varied by
changing the discharge of air from the fan.
The displacement amplitude of vibration in the transverse di-
rection (lift force direction) and the displacement amplitude of vi-
bration in the streamwise direction (drag torce direction) were
measured using the arrangement shown in Fig. 2. The free stream
velocity was measured using a constant temperature hot wire
anemometer CTA) type d55MO1). The frequency spectrum and
the wavetorm of the velocity fluctuation in the wake of the oscil-
lating cylinder, which show the vortex shedding frequency, were
monitored by simultaneously feeding the signal from the hot wire
probe placed at 1D in the horizontal direction and 2D in the verti-
cal direction from the static equilibrium position at the mid point
of the oscillating cylinder to the frequency analyser and to a digi-
tal oscilloscope.
The natural frequency (F) and the logarithmic decrement (6) of
the test cylinder was determined by impulsive test, wherein the
cylinder was set into vibration by slightly tapping its centre at
zero flow condition.
Care was taken to note possible sources of error and an error
analysis based on the method of Kline and McClintocklP was
carried out. The error analysis indicated a 3z4% uncertainty in the
vibration amplitude and a 35% in the velocity. Reduced velocity
within the vortex shedding was obtained by tuning the natural fre-
quency of the test cylinder to about 56 Hz for which the logarith-
mic decrement was found to be about (0-(083, obtained from a sim-
ple impulsive test, wherein the cylinder was set into vibration by
slightly tapping its centre at zero flow condition. The damping
factor K, is found to be 4.93.
The variations of the non-dimensional RMS vibration ampli-
tude A, /D, averaged over 20 ms, in the transverse direction (lift
force direction) with the reduced velocity at various locations
downstream the cylinder for various secondary injection velocities
compared with the response in the absence of injection flow (re-
terred to as single cylinder response) are shown in Fig. 3. It can be
seen trom Fig. 3(a) that the general trends of the dynamic re-
sponse of the test cylinder in the vicinity of the cylinder, siD = 1,
is to increase A, iD for all injection velocities and reduced veloci-
ties used except at very limited range of injection velocities name-
ly 13 U, 17 where there was a reduction in the dimensionless
amplitude. Another noticeable feature of the vibration curves is
that as the injection velocity is increased the start of the lock-in re-
gion is shifted to the right, i.e, the lock-in region occurs at an in-
creased reduced velocity. The effect of increasing the injection ve-
locity from 20-2 m/s to 65-9 m/s in the reduced velocity range
17 : U, 22 is to reduce the maximum peaks of the amplitude of
the vibration of the cylinder with injection, yet the vibration am-
plitude for all injection velocities is still higher than that without
injection. For U, > 22 increasing the injection velocity tends to in-
crease A,/D significantly.
When the injection location is at st0 s 2, it can be seen from
Fig. 3(b) that for high injection velocity, U, = 319 m/s and 65-9
m/s and U, c 20 the injection of air tends to reduce the dimension-
less amplitude. At the higher reduced velocities U, > 20 the effect
of injection is almost negligible.
A possible explanation of the increase and the decrease in the
dimensionless amplitude in the transverse direction can be made
by examining the vortex shedding mechanism responsible for the
induced vibration. Nakagawal1 suggested a formation mecha-
nism for the vortex shedding behind a circular cylinder. He ob-
served a symmetry of vortex pair with respect to the wake axis be-
hind the cylinder. It was found that such symmetry is necessary
tor the alternative vortex shedding responsible for the induced vi-
bration. In the presence of injection velocity in the vicinity of the
test cylinder, s/D = 1 and dependent on the momentum of injec-
tion jet, the degree of symmetry of the formed vortices is affected
and in general, at siD = 1 the jet tends to enhance the vortex shed-
ding formation and symmetry which in turn results in an increase
in the dimensionless amplitude. On the other hand for sD > 1 the
jet effect on the formation and the symmetry of vortex is to weak-
en such formation and disturb the symmetry and hence reducing
A,D.
The etfect ot injection jet velocities on the dimensionless am-
plitude in the drag direction, A,,/V is shown in Fig. 4 for various
locations downstream of the test cylinder. The effect of the injec-
tion jet velocities is to increase the values of A,/D over that in the
absence of injection, for all distances downstream of the test
cylinder. The effect is signiticant in the vicinity of the cylinder
and diminishes as the location ot the injection jet is moved away
trom the test cylinder, Fig. 4a, b). The maximum increase in AD
occurs at U, = 18 and U, = 20-2 m/s when the injection is located
at s:D 2. A possible explanation of the increase in the dimen-
sionless amplitude A,/D may be due to the increase in the drag
torce resulting from the injection of the jet specially very close to
the cylinder.
Flow visualisation will be undertaken to provide physical ex-
planation of the flow induced vibration of a cantilever mounted
across two streams of air at right angle.
An experimental investigation was conducted to explore the effect
of a jet injection on the transverse and streamwise flow induced
vibrations of a cantilever cylinder. The outcome of the experimen-
tal results suggests the following:
A large number of problems in AI and other
areas of computer science can be viewed as
special cases of the constraint-satisfaction
problem (CSP) (Nadel 1990). Some examples
are machine vision (Chakravarty 1979; Davis
and Rosenfeld 1981; Mackworth 19775; Mon-
tanari 1974; Hummel 1976), belief mainte-
nance (Dechter 1987; Dechter and Pearl
1988b: Dhar and Croker 1990), scheduling
(Dhar and Ranganathan 1990; Fox 1987; Fox,
Sadeh, and Baykan 1989; Petrie et al. 1989;
Prosser 1989; R)t 1986), temporal reasoning
(Allen 1983, 1984; Dechter, Meiri, and Pearl
1991; Vilain and Kautz 1986; Tsang 1987),
graph problems (McGregor 1979; Bruynooghe
1985), floor plan design (Eastman 1972), the
planning of genetic experiments (Stefik
1981), the satisfiability problem (Zabih and
McAllester 1988), circuit design (de Kleer and
Sussman 1980), machine design and manu-
facturing (Frayman and Mittal 1987; Nadel
and Lin 1991; Navinchandra 1990), and diag-
nostic reasoning (Geffner and Pearl 1987).
The scope of this article is restricted to
those constraint-satisfaction problems that
can be stated as follows: We are given a set of
variables, a finite and discrete domain for
each variable, and a set of constraints. Each
constraint is defined over some subset of the
original set of variables and limits the combi-
nations of values that the variables in this
subset can take. The goal is to find one assign-
ment to the variables such that the assign-
ment satisfies all the constraints, In some
problems, the goal is to find all such assign-
ments, More general formulations of CSP can
be found in Freuder (1989); Gu (1889); Mack-
worth, Mulder, and Havens (1985); Mittal
and Frayman (1987); Mittal and Falkenhainer
(1990); Freeman-Benson, Maloney, and Born-
ing (1990); Navinchandra and Marks (1987);
Shapiro and Haralick (1981); and Ricci (1990).
I further restrict the discussion to CSPs in
which each constraint is either unary or binary.
I refer to such CSP as binary CSP. It is possible
to convert CSP with n-ary constraints to
another equivalent binary CSP (Rossi, Petrie,
and Dhar 1989). Binary CSP can be depicted
by a constraint graph in which each node
represents a variable, and each arc represents
a constraint between variables represented by
the end points of the arc. A unary constraint
is represented by an arc originating and ter-
minating at the same node. I often use the
term CSP to refer to the equivalent constraint
graph and vice versa.
For example, the map-coloring problem can
be cast as CSP. In this problem, we need to
color (from a set of colors) each region of the
map such that no two adjacent regions have
the same color. Figure 1 shows an example
map-coloring problem and its equivalent CSP.
The map has four regions that are to be col-
ored red, blue, or green. The equivalent CSP
has a variable for each of the four regions of
the map. The domain of each variable is the
given set of colors. For each pair of regions
that are adjacent on the map, there is a binary
constraint between the corresponding vari-
ables that disallows identical assignments to
these two variables.
CSP can be solved using the generate-and-test
paradigm. In this paradigm, each possible
combination of the variables is systematically
generated and then tested to see if it satisfies
all the constraints. The first combination that
satisfies all the constraints is the solution. The
number of combinations considered by this
of the domain of V, might no longer be com-
patible with any remaining members of the
revised domain of V,. For example, in CSP In
figure 2, arc (V4, V) is initially consistent.
After arc (Vy, Vy) is made consistent by delet-
ing green from the domain of Vy, arc (V;, Vy)
no longer remains consistent. The following
algorithm, taken from Mackworth (1977a),
obtains arc consistency for the whole con-
straint graph G:
The major problem with the previous algo-
rithm is that successful revision of even one
arc in some iteration forces all the arcs to be
revised in the next iteration, even though
only a small number of them are affected by
this revision. Mackworth (1977a) presents a
variation (called AC-3) of this algorithm that
eliminates this drawback, This algorithm
(given here) performs re-revision only for
those arcs that are possibly affected by a pre-
vious revision. The reader can verify that in
AC-3, after applying REVISE(V, V,,), it is not
necessary to add arc (V,44, Vi) to OQ. The reason
is that none of the elements deleted from the
domain of Vg during the revision of arc
(Vg V,,4) provided support for any value in the
current domain of V,a;,
The well-known algorithm of Waltz (1975)
is a special case of this algorithm and is equiv-
alent to another algorithm, AC-2, discussed in
Mackworth (1977a). Assume that the domain
size for each variable is d, and the total number
of binary constraints (that is, the arcs in the
constraint graph) is e. The complexity of an
arc-consistency algorithm given in Mack-
worth (1977a) is O(ed) (Mackworth and
Freuder 1985). Mohr and Henderson (1986)
present another arc-consistency algorithm
that has a complexity of O(edA). To verify the
arc consistency, each arc must be inspected at
least once, which takes O(d) steps. Hence,
the lower bound on the worst-case time com-
plexity of achieving arc consistency is O(ed).
Thus, Mohr and Henderson's algorithm is
optimal in terms of worst-case complexity.
Variations and improvements of this algo-
rithm were developed in Han and Lee (1988)
and Chen (1991).
Given an arc-consistent constraint graph, is
any (complete) instantiation of the variables
from current domains a solution to CSP? In
other words, can achieving arc consistency
completely eliminate the need for backtrack-
ing? If the domain size of each variable
becomes one after obtaining arc consistency,
then the network has exactly one solution,
which is obtained by assigning the only possi-
ble value in its domain to each variable. Oth-
erwise, the answer is no in general, The
constraint graph in figure 3a is arc consistent,
but none of the possible instantiations of the
variables are solutions to CSP. In general,
even after achieving arc consistency, a net-
work can have (1) no solutions (figure 3a), (2)
more than one solution (figure 3b), or (3)
exactly one solution (figure 3c). In each case,
search might be needed to find the
solution(s) or discover that there is no solu-
tion. Nevertheless, by making the constraint
graph arc consistent, it is often possible to
reduce the search done by the backtracking
procedure. Waltz (1975) shows that for the
problem of labeling polyhedral scenes, arc
consistency substantially reduces the search
space. In some instances of this problem, the
solution was found after no further search.
Given that arc consistency is not enough
to eliminate the need for backtracking, can
another stronger degree of consistency elimi-
nate the need for search? The notion of K
consistency captures different degrees of con-
sistency for different values of K.
A constraint graph is K consistent if the fol-
lowing is true: Choose values of any K - 1
variables that satisfy all the constraints among
these variables, then choose any K'th vari-
able. A value for this variable exists that satis-
fies all the constraints among these K variables.
A constraint graph is strongly K consistent
if it is J consistent for all [ s K.
Node consistency, discussed earlier, is
equivalent to strong 1 consistency. Arc con-
sistency is equivalent to strong 2 consistency.
Algorithms exist to make a constraint graph
strongly K consistent for K > 2 (Cooper 1989;
Freuder 1988). Clearly, if a constraint graph
containing n nodes is strongly n consistent,
then a solution to CSP can be found without
any search. However, the worst-case complex-
ity of the algorithm for obtaining n consis-
tency in an n-node constraint graph is also
exponential. If the graph is K consistent for K
c n, then in general, backtracking cannot be
avoided. Now the question is, Are there cer-
tain kinds of CSPs for which K consistency
for K c n can eliminate the need for back-
tracking? Before answering this question, I
define some terms.
An ordered constraint graph is a constraint
graph whose vertices have been ordered lin-
early. Figure 4 shows six different ordered
constraint graphs corresponding to the given
constraint graph. Note that in the backtrack-
ing paradigm, the CSP variables can be
instantiated in many different orders. Each
ordered constraint graph of CSP provides one
such order of variable instantiations. (The
variables that appear earlier in the ordering
are instantiated first. For example, in figure 4,
for each ordered graph, the variable corre-
sponding to the top node is instantiated
first.) It turns out that for some CSPs, some
orderings of the constraint graph are better
than other orderings in the following sense:
If the backtracking paradigm uses these order-
ings to instantiate the variables, then it can
find a solution to CSP without search (that is,
the first path searched by backtracking leads
to a solution). Next, I define the width of a
constraint graph that is used to identify such
CSPs.
The width at a vertex in an ordered constraint
graph is the number of constraint arcs that
lead from the vertex to the previous vertices
(in the linear order). For example, in the left-
most ordered constraint graph, the width of
the vertex Vy is 1, and the width of V; is 0.
The width of an ordered constraint graph is
the maximum of the width of any of its ver-
tices. The width of a constraint graph is the
minimum width of all the orderings of the
graph. Hence, the width of the constraint
graph given in figure 4 is 1. The width of a
constraint graph depends on its structure.
Theorem 1: If a constraint graph is strongly
K consistent, and K > w, where w is the width
of the constraint graph, then a search order
exists that is backtrack free (Freuder 1988, 1982).
The proof of the theorem is straightfor-
ward. If w is the width of the graph, then an
ordering of the graph exists such that the
number of constraint arcs that lead from any
vertex of the graph to the previous vertices
(in the linear order) is, at most, w. Now, if the
variables are instantiated using this ordering
in the backtracking paradigm, then whenever
a new variable is instantiated, a value for this
variable that is consistent with all the previ-
ous assignments can be found. Such a value
can be found because (1) this value has to be
the complexity of backtrack search (Bitner
and Reingold 1975; Purdom 1983; Stone and
Stone 1986; Haralick and Elliot 1980; Zabih
and McAllester 1988). Several heuristics have
been developed and analyzed for selecting
variable ordering. One powerful heuristic,
developed by Bitner and Reingold (1975), is
often used with the FC algorithm. In this
method, the variable with the fewest possible
remaining alternatives is selected for instanti-
ation. Thus, the order of variable instantiation
is, in general, different in different branches
of the tree and is determined dynamically.
This heuristic is called the search-rearrangerment
method. Purdom and Brown extensively stud-
ied this heuristic, as well as its variants, both
experimentally and analytically (Purdom
1983; Purdom and Brown 1981, 1982; Purdom,
Brown, and Robertson 1981). Their results
show that for significant classes of problems,
search-rearrangement backtracking is a sub-
stantial improvement over the standard back-
tracking method. For the n-queens problem,
Stone and Stone (1986) experimentally show
that the search-rearrangement heuristic led to
an improvement of dozens of orders of mag-
nitude for large values of n, With this heuris-
tic, they were able to solve the problem for n
s 96 using only a personal computer. The
standard backtracking method could not
solve the problem in a reasonable amount of
time even for n = 30. Nadel (1983) presents
an analytic framework that can be used to
analyze the expected complexities of various
search orders and select the best one. Feld-
man and Golumbic (1989) apply these ideas
to the student scheduling problem and sug-
gest some further extensions.
Another possible heuristic is to instantiate
those variables first that participate in the
highest number of constraints. This approach
tries to ensure that the unsuccessful branches
of the tree are pruned early. Freuder and Ouinn
(1985) discuss an ordering that is somewhat
related to this heuristic. A set of variables
with no direct constraints between any pair
of them is called a stable set of variables. In
this heuristic, the backtracking algorithm
instantiates all the members of a stable set at
the end. The instantiation of these variables
contributes to the search space only additively
(that is, not multiplicatively, which is usually
the case). If the constraint graph has n vertices,
and a stable set of m vertices exists, then the
overall complexity of the backtrack is bound-
ed by dn-m md as opposed to d''. Hence, it
makes sense to find the maximal stable set of
the constraint graph before deciding on the
order of instantiation. Unfortunately, the
problem of finding a maximal stable set is
NP-hard. Thus, one has to settle for a heuristic
algorithm that finds a suboptimal stable set.
Fox, Sadeh, and Baykan (1989) use many
different structural characteristics of CSP to
select variable order and show its utility in
the domains of spatial planning and factory
scheduling.
Recall that the tree-structured constraint
graphs can be solved without backtracking
simply at the cost of achieving arc consistency.
Any given constraint graph can be made a
tree after deleting certain vertices such that
all the cycles from the graph are removed.
This set of vertices is called the cycle cutset. If
a small cycle cutset can be found, then a good
heuristic is to first instantiate all the variables
in the cycle cutset and then solve the result-
ing tree-structured CSPs without backtracking
(Dechter 1986). If the size of a cycle cutset of
an n-variable CSP is m, then the original CSP
can be solved in d + (n - m) d steps,
Once the decision is made to instantiate a
variable, it can have several values available.
The order in which these values are consid-
ered can have substantial impact on the time
to find the first solution. For example, if CSP
has a solution, and a correct value is chosen
for each variable, then a solution can be
found without any backtracking. One possi-
ble heuristic is to prefer those values that
maximize the number of options available for
future assignments (Haralick and Elliot 1980).
By incorporating such a value-ordering
heuristic in Stone and Stone's algorithm for
solving the n-queens problem, Kale (1990)
developed a backtracking-based algorithm
that can be used to solve the problem with
little backtracking even for large values of n
(= 1000). Without incorporating Kale's
heuristic, Stone and Stone's algorithm is
unable to solve the n-queens problem for n
much larger than 100.
Minton, Johnston, Philips, and Laird (1990)
use similar value- and variable-ordering
heuristics in a somewhat different framework
to obtain solutions to the n-queens problem
for n = 1,000,000. In their scheme, backtrack-
ing starts after a good initial assignment of
values to the variables in CSP (that is, an
assignment to the variables that violates only
a few of the constraints) has been obtained
through some greedy algorithm. Now the
values of the variables that conflict with other
variables are systematically changed in the
backtracking paradigm. Minton and his col-
leagues present empirical results using this
scheme for many problems as well as an ana-
lytic model that explains the performance of
this scheme with different kinds of problems.
This method was originally developed as a
hill-climbing method (that is, nonbacktracking
method). It starts with a reasonable assign-
ment of values to variables and then contin-
ues to repair the values of variables until a
correct solution is obtained. Sosic and Gu also
developed a similar algorithm (Gu 1989).
Another heuristic is to prefer the value
(from those available) that leads to CSP that is
easiest to solve. Dechter and Pearl (1988a) dis-
cuss one way of estimating the difficulty of
solving CSP. In this method, CSP is converted
into a tree-structured CSP by deleting a mini-
mum number of arcs; resulting CSP is solved
for all solutions. The number of solutions
found in corresponding tree-structured CSP is
taken as the measure of difficulty of solving
CSP (higher the solution count, easier CSP).
They also present an experimental evaluation
of this heuristic, as well as its variations, on
randomly generated CSPs and show that a
variation of this heuristic helps in reducing
the overall search effort. Good value-ordering
heuristics are expected to be highly problem
specific. For example, Sadeh (1991) shows
that Dechter and Pearls value-ordering heuris-
tic performs poorly for the job shop schedul-
ing problem. Sadeh presents other variable-
and value-ordering heuristics that work well
for this problem.
CSP can always be solved by the standard
backtracking algorithm, although at substantial
cost. The reason for the poor performance of
backtracking is that it does not learn from the
failure of different nodes. The performance of
a backtracking algorithm can be improved in
a number of ways: (1) performing constraint
propagation at the search nodes of the tree,
(2) performing reason maintenance or just
intelligent backtracking, and (3) choosing a
good variable ordering or a good order for the
instantiation of different values of a given
variable. By performing constraint propaga-
tion, given CSP is essentially transformed into
different CSP whose search space is smaller. In
the process of constraint propagation, certain
failures are identified, and the search space is
effectively reduced so that these failures are
not encountered at all in the search space of
transformed CSP. In the second technique,
CSP is not transformed into a different prob-
lem, but the search space is searched careful-
ly. Information about a failure is kept and
used during the search of the remaining
Space. Based on previous failure information,
whenever it is determined that search in
some new subspace will also fail, then this
subspace is also pruned. Good variable order-
ing reduces the bushiness of the tree by
moving the failures to upper levels of the
search tree. Good value ordering moves a
solution of CSP to the left of the tree so that
it can be found quickly by the backtracking
algorithm. If applied to an extreme, any of
these techniques can eliminate the thrashing
behavior of backtracking. However, the cost
of applying these techniques in this manner
is often more than that incurred by simple
backtracking. It turns out that simplified ver-
sions of these techniques can be used together
to reduce the overall search space. The optimal
combination of these techniques is different
for different problems and is a topic of cur-
rent investigation (Dechter and Meiri 1989).
I want to thank Charles Petrie, Ananth Grama,
and Francesco Ricci for a number of sugges-
tions for improving the quality of this article.
An earlier version of this article appears as
MCC Corp. document TR ACTAI-041-90.
HEN AN ANTENNA ARRAY of /N elements is
subject to undesired interference, such as
jamming plus the thermal noise in each of
the N receivers, the interference power can be reduced,
relative to the power in some desired signal, by forming
as the system output a suitably weighted sum of the
waveforms observed on all the antenna elements, We call
this process nulling, Usually the choice of suitable weights
must be made adaptively. The choice of weights that
maximizes the signal-to-interference ratio (SINR) ob-
served in the system output is the solution to a well-
studied least-squares problem. The number of arithmetic
steps required to solve this least-squares problem, for
almost any algorithm chosen, is proportional to the cube
of the number of antenna elements, Furthermore, be-
cause the statistical characteristics of the interference
change with time, the adaptive weight determination
must be performed repetitively and in real time.
When the antenna array is on board a satellite, the
number of antenna elements that can be nulled in prac-
tice is limited by the combination of the real-time re-
quirement driven by satellite motion and the cubic de-
pendence of the computational cost of adaptive weight
determination. A previous study of computational cost
set this limit at N 26, based on an assumed conven-
tional digital signal processing architecture. This limita-
tion is not absolute, because it depends on the resources
we are willing to allocate to a nulling processor. Using
more resources, however, is not an efficient way to handle
a large number of antennas.
In this article we describe a specialized adaptive-nulling
processor, called the Matrix Update Systolic Experiment
(MUSE), which is capable of determining he weights
for N 64 antenna elements, Because of its novel archi-
tecture, it can be compactly realized by using restructur-
able wafer-scale integration; the resulting system fits in
a 4-in square. MUSE is substantially smaller and light-
er than a conventional processor, and it uses substantial-
ly less power. Although the MUSE processor is special-
ized for N 64 antennas, the MUSE design concept
can be applied to the design of a similar processor
for an antenna array with a different number of
elements,
We briefly explain the mathematics of adaptive null-
ing and the use of Givens transformations for voltage-
domain computation of a Cholesky factor. We explain
Coordinate Rotation Digital Computation (CORDIC)
rotations and show how they are suitable for realizing
the Givens transformations needed to update a Cholesky
factor. We also develop the idea of a systolic array of
computing elements, each of which is composed of
three CORDIC rotation cells operating together. An
array of connected computing elements shares the work
of updating a Cholesky factor, and this work is the
largest part of the computational task. By using a tech-
nique that we call latency-controlled interleaving, we
show how to modify the systolic array to make it 100%
eficient for the Cholesky update aask.
Once a Cholesky factor is found, the nulling weights
are the solution of a set of linear equations whose constant
coefficients are the Cholesky factor. Our CORDIC cells
are also used for the solution of the linear equations. We
describe a modification of the timing of our systolic
array so that it can be used for both the Cholesky update
and the solution of the linear equations.
We hen consider how the MUSE systolic array can
be adapted for implementation by using Restructurable
Very Large-Scale Integration (RVLSI) technology Il].
We describe a design that uses 96 identical cells and
the plan that provides intercell discretionary
connections.
Figure 1 illustrates a ypical adaptive-nulling system.
The outputs of the NV antenna elements are down-con-
verted and the NV waveforms are simultaneously sampled
and digitized. At the nth sampling instant a complex
number x, comes from the ih antenna. We collect the
simultaneous samples together in a column vector X(n).
These vector samples are passed to a module that performs
the actual nulling, and some samples are passed to another
module that adapts the weights.
The adapted weights can be treated as a column
vector WC Although W varies with time, it is not recom-
puted for every sampling interval. The nulled out-
put y(n), which is a scalar, is formed by computing
the dot product of the vector W with the vector X(y).
We express the nulling operation by the vector
equation
where ()Tis the Hermitian transpose.
The adaptation process shown in Figure l involves
antenna samples X(n) and a lower triangular matrix l,;
that is regularly updated and used to determine W. The
interval within which W i held constant is defined as a
6äc Within each block, Wis chosen optimally with re-
spect to the statistics of interference appropriate to that
block. The statistics that matter are the correlations of
interference observed from one antenna element to an-
other; these statistics are arranged into a matrix R, which
we do not know in practice because it is a statistical
expectation. Let
where [(n) is the interference component of X(p). We
estimate R by using samples of the vector X(x). The
matrix R varies with time, but we have neither the time
to gather enough samples to estimate R perfectly nor the
computational resources to use all the samples we have.
We must use a limited amount of data to obtain an
estimate of Rfor each block, hen gather another limited
set of data to update R for the next block, and so on.
Let us therefore collect all the samples of X() that
we intend to use to determine the optimum weights
for a given block and refer to this collection of data as A.
There is no implication that all the samples used come
from within the given block-we actually envision using
samples from within a sliding window. If the num-
ber of samples of X(p) is M and each sample is an
Nelement vector, the M vectors can be arranged into a
matrix of M columns and N rows, The estimate of the
correlation matrix based on this matrix of raw data is
given by R, where
To optimize SINR we must first characterize the
Sgnal, given that we now have an estimate of the inter-
ference statistics. Let S be a known constant vector. It
relates to the desired signal as follows: if all interference
were absent and only a signal were present, we would
expect to observe in X(n) some scalar time function
times the vector S. Then the optimum choice of W g
known from the literature [2] to satisfy the set of linear
cquations
These equations tell us, first, that the only informa-
tion we need from the matrix X to determine the
optimum weight vector is the information neces-
sary to estimate R. Hence, if we intend to estimate
R by using R in Equation 1, we can comfortably trans-
form A in any way as long as that transformation
leaves XX' unchanged. Second, his equation tells us
that we can findd W fom R by solving linear equations.
and hence it suggests that we might accept as a good
estimate of W the solution to the same equations
wsing R.
where R is the estimated correlation matrix in Equation
1 UI.
How can we transform X while leaving XX'
unchanged! Suppose we postmultiply X by a unitary
matrix gQ:
By definition a unitary matrix has the property QQ'= I.
so we can insert QQ'berween and X'in the equation
fr R/
Therefore, we can transform A into X without chang-
ing the correlation matrix that we estimate from it.
Furthermore, we can repeat this transformation with
another unitary matrix, and repeat it again, as often as
we like, until the final transformed version of X has a
convenient form, with the nonzero elements confined to
an N x N lower-triangular submatrix, which we call L
on the left. For example.
conversational speech messages described in the follow-
ing section. The goal in speech-message information
retrieval is more modest; such a system attempts only
to extract the most general notion of topic or category
from the message. Ihe purpose of this paper is to demon-
strate the feasibility of a speech-message information-
retrieval system.
In configuring the system to a particular task, we
assume that both speech and text corpora exist that
represent the speech messages in each message category.
While the speech corpus is used for training statistical
hidden Markov acoustic models for the word spotter.
the text corpus, which contains text transcriptions of
speech messages, is used for training the second-stage
message classifier. The next section describes dhhe speech-
message classification task, along with the speech and
text corpora used to define the task.
The automatictechniques and experiments for speech-
message information retrieval are described in two parts.
First, a perfect acoustic front end is assumed, and the
attention is focused on the message classifier. The section
entitled 'Message Classification'' describes the message
classifier model and the techniques used for training his
model. Results of message classification from text
transcriptions of speech messages are also presented. The
second part of the paper concerns the complete problem
of information retrieval from speech messages. The section
entitled ''Information Retrieval from Speech Messages'
describes the acoustic word spotter and techniques for
integrating the acoustic front end with the second-stage
message classifier. Results are presented for both word-
spotting performance and information-retrieval per-
formance from speech messages.
The most difficult problem in performing a study on
speech-message information retrieval is defining tthe task.
The definition of a message class is a difficult issue. and
the relevance of a particular speech message with respect
to a message class cannot always be determined with
certainty. We were fortunate in that a speech corpus
already existed that was suitable for this study. In this
alarm rate of 5.4 false alarms per keyword per hour
(fEu/kwlhr). The false-alarm rate is given as the total
number of false alarms normalized by the number of
keywords and the duration of the message. This false-
alarm rate corresponds to a total of approximately 550
true hits and 1050 flse alarms in the evaluation dataset.
A standard figure of merit used in evaluating word-
spotter performance is the average probability of detection
when averaged over false-alarm rates between 0 and 10
falkwlhr. Computing this figure of merit gave 50.2%4
average probability of detection over 0 to 10 f/kwlhr,
highlighting the poor performance at low false-alarm
rates,
This section addresses the integration of the maximum-
likelihood acoustic word spotter and the mutual-
information-based message classifier. The stream of key-
words decoded by the word spotter form the partial
message transcription that is input to the message classi-
fier. The partial transcription is inaccurate in that it con-
sists of keyword insertions (false alarms) in addition to
correctly decoded keywords (true hits). The interest
here is in devising a keyword detection mechanism
that requires little supervision and can easily be adapt-
ed to changing acoustic conditions. A network is de-
scribed that learns the detection characteristics for all
keywords simultaneously through an error metric based
on the global message-classification task.
Keyword detection is generally accomplished in word
spotting by using a Neyman-P'earson criterion, in which
the probability of correct keyword detection is maxi-
improvement gained from corrugation ring loading
would be best at the center of the large overall band-
width. In our case, the two useful bands are widely
separated and are located at the edges of the overall
band, not at its center.
In a corrugated-horn design, N, the difference in
wavelengths between the spherical wavefront and plane
aperture, is a useful parameter for characterizing the
radiation pattern. (Note: 2 = [al(2A)]tan(6;2), where d
is the horn diameter and 6; is half the flare angle.) A
value of N greater than 0.75 ensures wideband operation
[13]. Thus, for the design of the corrugated horn, we
selected N equal to unity at the low-frequency end of the
low frequency band. In addition, we chose the semiflare
angle to be 46', and the aperture diameter to be 2.88 in.
These dimensions gave the desired beamwidths. In the
high frequency band, we chose the width of the corruga-
tions to be less than A/2, Fourteen corrugations were used;
the corrugation depth was 0.217 in, he pitch 0.120 in,
and the tooth thickness 0.015 in. The horn and a por-
tion of the K-band circular waveguide was machined
from an aluminum block.
Because a corrugated horn with a wide flare angle will
produce spherical waves at the horn opening, we used a
spherical curved surface for the radome design. The
radius of the inner surtace of the radome was selected to
be 2.1 in, and the thickness to be one wavelength at the
center frequency of the high frequency band. Because
this thickness was close to a half wavelength in the low
frequency band, good dual-band transmission charac-
teristics (i,e., losses less than 0.1 dB) could be obtained
for radomes made of either Teflon or polyethylene. We
chose polyethylene for the present application because
of cost considerations, and we fabricated a polyethylene
radome with a thickness of 0.176 in, PPolyethylene mate-
rial of a forest-green pigment was also used and tested,
and we observed that the pigment had no measurable
RF effects, A radome made of Teflon, which has a
slightly better water-shedding property, would require a
thickness of 0.183 in.
We designed the Q-band components with standard-
size waveguides: the circular-guide WC-19 and the rect-
equipment, but requires a large amount of circuit over-
head for control and memory, and the flexibility in
signal routing is limited. Arrays that use voltage-pro-
grammable links enable the selective connection of
electrically programmable vias between two levels of
interconnect. Although closest in technology to the la-
ser-programmable devices that are the subject of this
article, voltage-programmable arrays differ in three im-
portant respects. (1) Extra access lines and high-voltage
transistors are required to distribute the programming
voltages to the links. (2) After programming, dhe links
have significantly higher resistance han laser links. (3)
There is no provision for making cuts: i.e., only additive
links are provided [6].
As part of a program in restructurable wafer-scale VLSI,
we have developed a number of connective laser link
structures [2]. This connective capability represents a
significant advance beyond previous laser-programmable
technologies that could only cut first-level andlor sec-
ond-level metal. Cut-only technologies have two main
disadvantages. First, the unprogrammed chip must be
fabricated with every possible connection made, which
renders the configuration untestable. Thus every part.
including those that have defects, must be programmed
first before any tests can be performed. With additive
link technologies, chips are testable prior to program-
N 1953 THE sOVIET UNION exploded their first ther-
monuclear device, and the possibility of their join-
ing that device with a long-range ballistic missile
became a real threat for the United States [l]. Govern-
ment and research facilities realized that this threat called
for an increased research effort to study reentry phenom-
ena. Discussions in 1958 with representatives of the then
National Advisory Committee for Aeronautics (NACA),
located at Langley Air Force Base, Va., revealed that
NACA was designing a test vehicle to reenter small pay-
loads into the earths atmosphere at velocities in the
neighborhood of 20,000 f/sec. At the same time, Lin-
coln Laboratory, motivated by the threat posed by the
Soviet Unions successful development of long-range
ballistic missiles, became interested in the problem of
warhead reentry and in the radar and optical phenomena
associated with hypersonic reentry into the earths
atmosphere.
The importance of controlled experiments with known
aerodynamic configurations to study reentry phenome-
na became obvious, and Lincoln Laboratory joined forc-
eswith NACA in a cooperative Reentry Physics Research
Program. NACA, which became the National Aeronau-
tics and Space Administration (NASA) on 1 October
1958, was responsible for the design and launch of reen-
try vehicles, and Lincoln Laboratory was responsible for
the development and construction of the radar and opti-
cal instrumentation to observe the reentries, along with
the gathering and interpretation of data.
Inn 1958 the knowledge of the phenomena associated
with the hypersonic entry of a body into the earths
atmosphere had been obtained only by observing natural
meteors, By using optical and radar techniques, meteor
astronomers had accumulated considerable knowledge
of the effects produced by meteors interacting with the
earths atmosphere. The earliest radar observations of
meteors had been made at long wavelengths (approxi-
mately 30 MH2) by using both pulsed and continuous-
wave transmissions. At that frequency only the meteor
trail could be seen. Later, because of increased power
generation capability at higher frequencies, the signals
reflected from the ionized region in the immediate vicin-
ity of the meteor were observed at 200 MHz and above.
The production of an ionized trail by meteors was
well known, and the degree of ionization in the trail
(whether it was underdense or overdense) was well un-
derstood. At frequencies of 200 MHz and greater. so-
called bead echoe, or radar reflections from the ionized
plasma in front of the reentry body, were observed. The
head echoes traveled at the same speed as the meteor, and
the combination of the Doppler frequency and the range-
versus-time data of the head echo provided a direct
measure ot meteor velocity.
Results of meteor research led to the following con-
the fourth priority addresses airport capacity. Priorities 2
and 5 are intended to improve the situational awareness
of controllers and pilots, and priority 5 recognizes the
need to assign the pilot a greater share of responsibility in
preventing runway incursions. Priority 2 depends on the
availability of the new ASDE-3 high-performance sur-
face radar. This unit is currently in early production and
is scheduled to be installed at 29 of the largest domestic
airports within the next two years.
The ASTA program currently being developed by the
FAA has three overlapping phases: ASTA-1, ASTA-2,
and ASTA-3. ASTA-1 is a radar-based safery system that
includes the automatic detection of runway incursions
and other movement errors, audible and visual alerts for
controllers, and a system of automatic runway-status
lights intended to improve the situational awareness of
pilots and vehicle operators. In addition, the FAA is
undertaking an earh implementation of another auto-
matic alerting system known as the Airport Movement-
Area Safety System (AMASS). AMASS will provide the
foundation for the implementation of ASTA-1 capabili-
ties, ASTA-2 adds a surface beacon surveillance system
for positive identification of all Mode-S and other tran-
sponder-equipped aircraft and vehicles. ASTA-2 also in-
cludes initial functionality for an integrated traffic-man-
agement system designed to reduce delays, improve
airport capaciy, and reduce controller workload intensi-
ty. ASTA-3 adds a two-way digital data link between
tower and cockpit as part of the surface Mode-S capa-
bility. This link makes possible a number of safety and
traffic-management functions, including automatic di-
rect cockpit alerts, automatic taxi guidance and moni-
toring, delivery of surface traffic information to pilots
to reduce movement errors, delivery of information on
potentially hazardous weather, and error-free transmis-
sion of flight-route clearance data.
The control of traffic from the tower is based on three
major components: (1) surveillance to determine the
traffic situation, (2) processing to formulate a plan for
managing the traffic, and (3) communications to issue
clearances to implement that plan. The ASTA program
provides additional tools to the controller in all three of
these areas.
The primary mission of air traffic controllers is to insure
safety, and safety depends on adequate physical separa-
tion between all airborne and surface traffic. A controller
achieves separation through the use of surveillance, either
electronically by radar or visually by direct observation.
For the en-route or terminal-area controller, electronic
surveillance is the only surveillance available. Although
most air traffic can be detected by primary radar, which
relies on simple skin reflections, the trend is to place a
greater reliance on beacon radar.
Beacon radar, which is also known as the Air Trffic
Control Radio Beacon System (ATCRBS), is based on
the concept of equipping cooperating targets with a
transponder. The transponder receives interrogation pulses
from a beacon radar and emits a reply containing (as a
minimum) a 4-digit octal transponder code entered by
Seven seconds after this com-
munication the accident occurred.
Ironically, the 727 cockpit crew was
communicating with the same
ground controller when the DC-9
crew became lost, and thus over-
heard most of the exchanges. Fol-
lowingthe NWA1482 transmission
at 184006 when the DC-9 pilot
said they had missed taxiway oscar
six and thought they were on fox-
trot (taxiway fox), the cockpit voice
recorder in the 727 captured an
inter-cockpit exchange in which
the pilot said, 'He sure did' (i,c.,
miss oscar six) and the copilot add-
ed,''Idoh hink so' (ie.,he DC-
9 was on foxtrot now; it wasnt).
the pilot. The three ypes of ATCRBS transponders now
in use are Mode A (the oldest and most basic), Mode C,
and Mode S (the most advanced). In addition to the
identity code, Mode-C and Mode-S units also send the
pressure altitude, and Mode S adds a Mode-S address
that uniquely identifies the individual aircraft. Mode S
also provides a two-way digital data link between ground
and cockpit: this data link can be used for a number of
communications purposes.
All traffic at major domestic airports is required to be
equipped with either Mode C or Mode S (a small but
decreasing number of aircraft still use Mode A). Over the
next few years, Mode S transponders will be installed
in all transport aircraft with more than 30 seats, as
part of an FAA-imposed requirement for a collision-
avoidance system. One feature of the Mode-S system
that greatly increases surveillance capacity is the ability
to interrogate selectively any single aircraft within a
crowded airspace.
Although Mode S is basically a surveillance system,
like Mode A and Mode C, the additional availability of
the digital data link will have significant impact on air
traffic control in the years ahead. As suitable cockpit and
ground equipment becomes available, he data link will
permit advances such as the substitution of electronic
messages for voice traffic and the automatic delivery of
in-flight weather, heading, airspeed, and other data from
the aircraft to ATC control centers In the future, ATC
ESEARCHERS HAVE BEEN STUDYING machine vi-
sion (MV) technology for more than 50 years.
As a result of their work, a standard technology
for computer vision has evolved. The most rigorous of
the conventional MV methods comes from D. Marrs
work at MIT in the 1970s (see the box 'Conventional
Machine Vision Design'). Yet, to some researchers and
potential users, the performance of conventional MV
systems is disappointing. Io establish the context of our
work, we quote from a recent review by A. Rosenfeld [l],
a founder and leading MV figure:
... Standard vision techniques for feature detection,
segmentation, recovery, etc., often do not perform
very well when applied to natural scenes.
Ideally, he [vision process] stages should be close-
ly integrated; the results obtained at a gven stage
should provide feedback to modifv the techniques
used at previous stages. This is rarely done in existing
vision systems, and as a result, little is known about
how to design systems that incorporate feedback be-
tween stages.
. .. Hfumans can recognize objects-even com-
plex objects whose presence was unexpected-in a
fraction of a second, which is enough time for only a
few hundred (') ''cycles' of the neural 'hardware' in
the human visual system. Computer vision systems
have a long way to go before they will be able to
match this performance.
Our main goal was to develop a general MV architec-
ture that would work on a variety of image types without
significant changes in the algorithm. This robustness
contrasts with the current practice of tailoring an MV
system to a specific application. Such tailored systems
have not performed well in situations unforeseen by the
designers.
In many respects the human vision system-known
for its high performance over a wide range of objects and
situations-is tar superior to current MV systems. Ihus,
in developing the architecture of our system, we decided
to model the human vision system. Although this idea
TO A MAJOR EXTENT, conventional
machine vision (MV) technology
evolved from the work of one man,
D. Marr, at MIT in he 1970.
Developed from the information
theory. cybernetics, and digital com-
puter technology of that era, Marrs
contributions were made within the
field of artificial intelligence. His
work led to numerous papers on
vision and, finally, to the book
Vion [1], which was published
after Marr died of leukemia in 1980
at the age of 35. Reference 2 pro-
vides a recent summary of Marrs
work in relation to that of others.
An acknowledged contribution
of Marrs was his attempt to clarifv
the thinking about vision systems
or, more generally, information pro-
cessing systems. In his work, Marr
introduced the distinction among
three levels of explanations: (1) the
computational theory, (2) the algo-
rithm, and (3) the hardware im-
plementation. Consideration of
these levels and associated issues
leads to a sequence of questions that
guides he design.
At the time, Marrs explicit ideas
somewhat puzzled researchers in
vision because other approaches
used concepts that were undefined,
or they used descriptions rather
than explanations. Marr attained a
high degree of rigor because his ap-
proach produced ideas that could
be directly checked by computer
simulation.
At the computational-theory lev-
el, Marr asserted that the key issue
is to determine both a goal for the
computation and strategies for
achieving that goal. By explicitly
stating the goal and accompanying
strategies, we can describe what the
machine achieves and characterize
the constraints, Knowledge of the
constraints, in turn, allows the
processes to be defined. At the al-
gorithm level, the key issue is how
the input and output are represent-
ed, and the actual algorithm for
transformation. The algorithm will
depend partially on the nature of
the data representation. At the im-
plementation level, the key issue is
how the machine actually works.
The concern here is with the hard-
ware of the machine, and the na-
ture and operation of its compo-
nent parts.
Forexample, applying Marrs ap-
proach to optical sensors and two-
dimensional processing leads to a
modular design with the following
consecutive processing stages:
Three-dimensional scenes are an
extension of two-dimensional ones.
For the extension to three di-
mensions, stages l and 2 should be
replaced by a method to find the
surfaceorientation of each pixel. The
process will produce a representa-
tion map called the 29-D sketch.
Further extensions of Marrs
method add one or more of the
following stages: (1) cleanup of in-
put pixel values with image-restora-
tion techniques, (2) production of
multiple images for stereomap-
ping and motion analysis, (3) ad-
justment of the processing by
feedback from later stages oo earlier
stages, and (4) recognition of ob-
jects by matching them with mod-
els made up of composite parts.
by K. Brodmann in 1909.)
Certain modules in the midbrain
also belong to the visual system: for
example, the lateral geniculate nu-
cleus (LGN), shown in Figure A
(bottom). The optic nerves relay
the images captured by the eyeballs
to the LGN, which has processing
functions and acts as a buffer.
The retina has about 1.25 x 10'
receptors, Data compression by ret-
inal processing is about 125 to 1,
which gives a resolution near the
fovea of about 1000 x 1000 pixels.
Assuming 7 bits of relative discrim-
ination of stimulus frequency per
pixel [2] and a 100-Hz pulse fre-
quency along the optic nerve, we
find that the data rate to the visual
cortex is about 700 Mblsec, less
than the capacity of fiber optic
channels.
Researchers have mapped over
30 pathways among the visual areas
but the actual number probably is
much larger because there are many
connections to areas that have not
been studied. A basicfinding is that
with few exceptions the pathways
connect the modules in reciprocal
fashion.
Evidence shows a hierarchical
structure for the vision system [5]-
for the dozen visual areas, the over-
all cortical hierarchy has six levels.
Anatomical, behavioral, and physi-
ological data show two distinct
channels for classifying and locat-
ing. In Figure A (bottom), the clas-
sification channel consists of the
LGN, A17, A18, A19 (not shown
in the figure), A20, and A21; he
location channel consists of the
LGN, A18, A7, andA8.
The two channels separate at the
retina and they have their own reti-
nal cells, labeled X and Y. (Another
cell type not shown, the W cell,
goes to the midbrain areas to coor-
dinate the FOV to head and eye
movements). At the cortical and
midbrain levels, the channels re-
main separate. Evidence shows
the classification channel analyzes
form and color while the location
channel analyzes visual motion
across the FOV.
In A17, A18, and A19, three
types of cells work like feature de-
tectors, The features in primate vi-
sion are stationary or moving edges,
skts or lines, and their respective
ends [1]. (In comparison, the fea-
tures in current MV technology are
much more varied: corners, human
faces, spatial frequencies, and re-
sponses of matched filters are typi-
cally used).
The population of retinal cells
that feed into a given feature cell are
not scattered about all over the
retina but are clustered in a small
area. This area of the retina is
called the receptive field of the fea-
ture cell. The size of the recep-
tive field of simple cells is about
one-quarter degree by one-quarter
degree at the fovea.
Research shows that feedback
takes place in the human vision sys-
tem [4]. In normal operation, stim-
ulating A20 changes the receptive
fields of A17. This result wggests
that A20 exerts feedback control
over the feature detectors.Thus rec-
ognition is likely an active feedback
process that restructures the feature-
extraction stage. The restructuring
continues until the transformed in-
put matches some known class of
stimulus. Research also suggests that
there is a mechanism for directing
attention within the FOV. In short,
windouingtakes place. Windowing
focuses attention on small details
and also suppresses notice of other
objects in the FOV. Researchers sus-
pect that the midbrain directs the
windowing process automatically,
perhaps by using cortical inputs.
In summary, human vision is a
system in which a small number of
serial stages (sensor-preprocessor-
feature-extracting-classifier) pro-
cess large arrays of data in parallel.
The architecture has two channels
that use feedforward and feedback
signals.
The 14 to 16 hour day and the 6 day workweek of the
1700's was reduced in the early 1800's, by union pressure
and in spite of management fears, to a 10 to 12 hour day.
This first major reduction of the workday was a chain
reaction which resulted from a directive by President
Jackson that all shipyard workers in Philadelphia be put on
a 10 hour day, It is significant to military managers that
government workers were among the first to benefit from
this new workday. After the Civil War some craft workeni
obtained an 8 hour day. By the 1920's when most worker.
including government employees, had finally achieved the ß
hour day, a drive for a new and shortened workweek begal.
This was the idea of a 5 day workweek. The Ford Motor
Company adopted the plan in 1927 and the Monday
through Friday workweek soon became the industrisl
standard.' Since WW II, private industry has been chippint
away at this standard.'
The 5 day workweek has been slowly yielding to a
everincreasing campaign by management, labor, and uno
to test, evaluate, and implement, where feasible the 4 day
workweek. Many companies have successfully transitioned
to some variation of the shortened workweek and by 1971
+-4i<+4++i-i-äi+4'
A survey in that year of 300 corporate presidents and boar
chairmen conducted by Duns magazine indicated stron
agreement that the shortened workweek is the wave of the
future for American industry.?
Management views the shortened workweek as having
,fowis m=ior adsantees: nweased roductivity,
sw4 nf- rm84 ==4W.4===4 r==r
itnnover. 4eereased setup and shut-down tmes, and
rnn= <wiuns r4ons' Mns nms =nwn
ät =n me riation o he day week reront
gessed productivity; ninety percent of them report
putity inwease4 rnsina from 2 t 2$ reren- The
äining companes are service-oriented and productivity
gghot be directly calculated. The service-oriented firms
e increased their profitability through increased sales,
Rer customer service, and a reduction in both
gnteeism and worker turnover. In general, firms which
the 4 day week realize a large portion of their
$fuctiity increases through a reported 20 percent
jsge reduction in set-up and shutdown times.' The
gsl of the concept is not limited to economic gains for
Bagement, how ever, employees too perceive benefits.
When the largest private employer in San Antonio, the
ed Services Automobile Association, primarily a
Mte-collar operation, converted to the 4 day workweek in
ober 1972 employees throughout the firm stood and
ered the official announcement.' It is generally
pted that the personal benefits of the 4 day, 40 hour,
kweek boost employee morale. Some of the more
ificant benefits noted are; more leisure time; more time
medical, dental, and legal appointments; a more
mnded and thorough rest period; more time for family
iities and short vacations; easier and cheaper com-
ing; and all this at no loss in pay. Employees who work
firm where the 4 day schedule has been found feasible
often the envy of friends working the 5 day week.
While management and employees in a wide variety of
ate firms appear to be generally well satisfied with the
ept and seem willing to merge their goals within the
imework of the apparent benefits of the 4 day, 40 hour
kweek, its labor union and legal aspects could present
sificant impediments to the apparent goal congruence of
pagement and labor within the federal sector.
Civil Service rules, governed by Title 5 of the U.S.
s which deal with the standard 8 hour workweek and
time pay requirements for government employees,
m the most significant legal barrier to the adontion of a
Ehour workday and a 4 day workweek. These rules
94 the payment of wages at overtime rates for any time
ied in excess of 8 hours per day. Thus, ii the 4 day 40
9 Pan was adopted by the Air Force today,government
9Yees whould be entitled to 8 hours of overtime pay
$Rtandard 40 hour week. In this era of reduced budgets
4n adoption would seem unlikely. Additionally, the
$mme wages could easily nullify the economic
tions of the plan. Therefore, it would seem that a
$ of this legal barrier would be reauired before the
could be beneficially used in the federal sector. A
R4ent exists. In the industrial sector, where this rule
not apply, managers and employees bave jointly
w on a 40 hour week based on a straghttime pay sale
f each 10 hour day.
Another potential legal problem could arise because
some state statutes prohibit more than an 8 hour workday
for women. However, where this barricr has been
encountered, management has, in some cases, been abie to
secure waivers from state lawmakers. Unfortunately, we
cannot be sure such a uooperative attitude would be
forthcoming from all labor union leaders.'
Some union leaders appear skeptical about the
shortened workweek. Perhaps they fear that the new work
schedule will reduce their bargaining position with respect
to further reductions in the total workweek hours. Union
leaders may also fear that management will eventually use
the 4 day 40 hour schedule as a means of getting around
the overtime issue which they consider an economic
necessity. In spite of these misgivings, Some unions have
added the shortened workweek to their list of bargaining
goals. While it does appear that in some instances labor
unions present a significant barrier to the shortened
workweek, it is equally likely that economic pressures and
employee demands will eventually force unions to permit
testing some variation of the concept where the schedule is
feasible.
Our review of the 4 day week indicates the concept
might have economic and behavioral benefits in an Air
Force application. However, there are potential legal and
union barriers and there are also other potential problem
areas. Many people question whether, given the current
wage structure, workers can afford an extra day of leisure.
Others state that transportation schedules often do not
coincide with the requirements of a 10 hour workday
schedule. Some firms which have tried the 4 day plan have
dropped it because it was not feasible in their operations.
The study by students of AFIT was primarily
concerned with an investigation of the feasibility of
aligning the labor resources of certain ALC maintenance
activities with the 4 day 40 hour workweek. Feasibility
involves many factors. For example, can transportation
arrangements for the odd work hours be arranged? Can
legal waivers be obtained? Can a workable production
schedule be developed? However, these are really problems
of implementation. There are two very basic questions
which must be answered in the affirmative before
management seeks solutions to the implementation
problems. Therefore, the focus of the study was on these
two basic questions:
The primary data sources for this study were the
employees and managers of the Electronics Division,
Directorate of Maintenance, Ogden Air Logistics Center,
Hill Air Force Base, Utah, and selected managers and labor
The phenomenon of learning is widely recognized' in ths
study of production facilities. Due to such factors as
increased skill and efficiency of the operator, machine
modifications, changes in materials handling, and a host of
other possible sources of improvement, the time required to
produce one unit of product can be expected to continually
decrease. Although a particular improvement may result in
a quantum change in production time, the fact that such
improvements are likely to be distributed throughout the
entire production life cycle permits the overall cumulative
effect of these changes to be described by a learning curve.
Simply stated, a learning curve shows the production time
per unit falling sharply as the first few units are produced
and then leveling off somewhat as more and more units are
produced. For this phenomenon it turns out that every
time the cumulative production doubles, the production
time per unit of product is reduced by a constant multipliet
called the decimal learning ratio, L55, which typically takes
on a value in the neighborhood of 0.8. Thus the quantity
l5 reflects the ''average'' effect due to the various factos
which, at one time or another will contribute to a reduction
in production time for a unit.
Such a development is standard in the literature'. One must
take care, however, to proceed with caution, That is, 4
given task may be thought of as consisting of two parts--
one that is affected by learning and the other that is not.
Consider for example, a mechanic performing a mainte'
nance task such as lubrication on an unfamiliar vehicle. At
first, he will have to study a repair manual to ditcover the
gns of the arease fttings, tvne of oil flter reauired,
mrouan wevwrl enlications ot me task he wii need o
l r4 w w e mww= =n4 meretore ou me
feauired for the iob. There wil, however, be a lower
Peiat he cannot imnrove unon - a timit dictated by
io5=Pwps=+
iss. ete. This oircurmstance csn be antcoaed n
ii in that a oven rocess has sme basic time
bion that cannot be overcome.
he purpose of this oaner to combine the above notions
itlmevalue-ofmoney considerations to model a certain
i of production facilitv. The necessity of these
derations follows basically from the fact that monies
gt be borrowed or invested for a period of time before
iitting revenues are received. There is a cost associated
i holding borrowed money, or a desired return on
srment, that varies with the time it is held. Conversely,
s s a similar benefit or income realized from having the
ff revenues. Therefore, a discounting procedure can be
i to equivalence a cash quantity relevant at one time to
gfferent amount at some other standard time (such as
0). By combining this procedure with the concept of
ming it is anticipated that the model will aid in decision
iing regarding alternative processes. The emphasis of the
idel is economic; the quantity evaluated is the present
th of net cash flow.
s model is restricted to facilities which produce N units
a product one at a time. (See Figure 1) The costs
sidered in the model are as follows:
nrder to perform the discounting procedure previously
$Ided to, it is necessary to determine the time of
Bnpletion of each unit of production. It is assumed there
e two contributions to the unit production time for the
Amth unit;
Thus 0 can reflect intentional or unavoidable delays
between production of consecutive units as well as
completion of tasks which take a fixed time. Note that the
ratio b/T, can give one an idea of what to expect from
learning contributions. If the ratio /CT, is large, then the
effect of learning will be minimal. However, if b/CT , 2C1,
then learning may be very significant (depending, of course,
on the value of L in Eq. 2, e.g, if Lg) 1, then there is no
learning at all and 2T4 = 2T,).
ln keeping with the above discussion, the intervals subject
to learning decrease according to the expression
wnere N = I09;4 kD)lA9,,,4). (2)
It follows then from Figure l and Eqs, l and 2 that the
time to the completion of the m th unit is given by
As previously mentioned, it is standard practice to evaluate
a variety of expenditures and incomes occurring at various
times by using some method of discounting to calculate an
equivalent value at time zero, the so called present worth of
net cash flow. The contributions to the present worth
correspond to the expenditures and incomes indicated in
Figure 2.
For convenience, continuous discounting is used, imply ing
for example, that some revenue having value Y at time T (in
years) has present worth Yexp( -RT) where 100R is the
discounting factor in percent per year, Applying this result
for the various discrete terms in Figure 2 and making use of
the factor [1 - exp(-RT )] /R for a constant flow, (3, 4) an
expression for the present worth of the net cash flow in
Figure 2 for the production of N units is
where
Gg Fixed initial cost [$]
C =fixed cost incurred at beginning of production
of each unit [$]
S =selling price or revenue received upon
completion of each unit [$]
=flow rate of continuous cost [S/year]
R =discounting factor [year] 8
T; = completion time of %r unit [years] .
*The factor R cann be found to correspond to a nominal interest rate
i from the expression R = Iogail+i).
Combining Eqs 3 and 4 yelds the present worth for the
project as
where b is the portion of production time not subject to
learning Iyears] and where the quantity x is related to the
decimal learning ratio L by Eq. 2. Note that Eq. 5 takes
on a particularly simple form in case there is no learning
One can achieve this either by setting Lg) = 1D- y=0 or by
choosing DT, = 0. For the latter choice the result is
This model was developed primarily to be used as an aid in
decision making, especially for industries employing
significant arnounts of manual labor andlor ''young''
technology. For example, if one is interested in comparing
two different processes, either of which could be used to
manufacture a particular unit of product, he may find Eq, 5
applicable to each. Usually the parameters in Eq. 5 will be
different in each case and in general, different present
worths will be calculated. If the total project times TN; are
approximately equal, then one selects the alternative having
the greatest present worth.
If the project times are not similar for the competing
alternatives, the cormparison of present worths is perhaps of
questionable value (2, 6, 7). The basic difficulty is that such
a comparison ignores what one does for the balance of time
(the difference between the shorter and longer alternatives)
following completion of the shorter alternative, assuming
that was one's choice. In case a particular alternative can be
repeated an indefinite number of times, one can apply the
transformation of present worth to an equivalent annual
cost (3). Here the desirable alternative is the one with the
greatest equivalent annual worth. lt is anticipated for the
present problerm that use of equivalent annual worth will
rnost often be disallowed. In any event the transformation
is simple and is not pursued here, The recommended choice
is thus that of present worth. An analysis of various return
evaluation techniques can be found in (6, 7).
For a given set of parameters the present worth Eq. 5 can
readily be evaluated with a programmable desk calculator
or a small computer. For this reason and for maximum
flexibility, Eq. 5 was left in its most general form. The
desire to exam ine variation of pararmeters in Eq. 5 for any
given problem, however, may dictate additional constraintt.
For example, if one associates mainly with labor rate and
C with materials cost, the ratio (ßTy;)/(NC) representing
total labor cost/total rmaterials cost may necessarily fall in a
known range. There may also exist relationships between 7
and L5 or between f and CT,. That is, high labor rates will
usually buy skilled, enthusiastic help which may yield a
high learning rate. Or perhaps somewhat the opposite might
be expected with large ß implying a relatively small CT, but
with a correspondingly weak learning rate.
In order to briefly acquaint the reader with the effects of
parameter changes, a few computations of present worth
using Eq. 5 have been performed in the vicinity of the basi
parameter set shown in Table 1 Various departures from
this basic set are listed in Table 1 for four parameters, , R,
&), and b. In each case all parameters but one were held
constant at their ''basic'' values. The results are shown in
Figures 3 to 6.
ghould be noted that the present worth varies finearly
parameters G4, C. S and and thus it is not necessary
gtsmulate the sensitivity of these parameters. Changing b
Fgure 3 suffices to reduce or increase its importance
he to OT,. Note from Figure 4 that the present worth
ggles rapidly only in the vicinity of small values of N.
A model has been developed which combines the concepts
of learning and time-value-of-money to determine the
present worth of net cash flow for a facility producing a
number of identical units one at a time. The resulting
rmodel involving nine parameters, can be easily and quickly
evaluated with a programmable calculator.
Use of the model allows one to make the most economical
choice among competing processes. The simplicity of the
calculation does not restrict its application to production of
expensive items. For example, a job shop may have received
an order for a hundred copies of a custom-made jig which
could perhaps be produced entirely on any of a number of
different machines in the shop. The model can be used to
decide which machine is most economical to use. The
model may also be applied to situations that do not involve
a great deal of manual tasks. Even a process which relies
upon numerically controlled machines is subject to learning
through programming changes and other technical improve-
ments.
There are effects not considered in this model which may
be important in some cases. For example, inflation may
cause values such as C, S, and to vary with time'. If the
revenue pattern can be mathematically described, the
approach presented in this paper can be modified with the
help of the methods suggested by McNichols and
Wortharn''. On the other hand, growth effect may
expand the market, increasing the original value of N. In
addition perhaps some of the parameters should be
considered stochastic. Inclusion of these effects may
provide basis for further work.
important to be left to staff specialists alone. The
practical effects of this lesson upon the Department of
Defense during the past quarter century have had a
tremendous impact upon military organization, project
management, life cycle weapons planning and pro-
tessional career development. Should not this lesson
teach us something useful for the management of the
nonmilitary sector of our economy as well?
If there are any consistent threads running through
the pattern of rapid change in today's world, they are
the pivotal questions surrounding the subject of resource
utilization. Most economic and social issues currently
raging in America and other lands are debates involving
different ways in which it is believed that human,
financial and material resources should or should not be
employed.
How resources should be allocated -- to what ends
- is the job of the politician and strategist. How they are
used in actions to reach these ends is the task of the
tactician and operator. How resources are marshalled
and managed is the job of the logistician.
In other words, strategy defines the job, tactics does
the job, and logistics provides the means to get the job
done.
The resources required to perform any job of
significance are more than beans, barges and black oil.
They include all the resources needed to carry out an
operation - materials, facilities, people, money and
information. The basic cost and availability of any of
these resources, of course, are primarily determined in
the political and economic arenas; however, the
efficiency of the process by which available resources are
provided is a logistical responsibility.
Clearly, no strategy can be successfully executed
and no operation effectively sustained without effective
resource management, that is to say, without adequate
logistics. No enterprise can succeed without its logistical
bill of rights: the right men and materials in the right
place at the right time in the right condition at the right
cost .., and with the right information. This is no mere
military slogan. The private enterprise system with its
doctrine of profit maximization will never reach its full
potential as the most efficient and responsible allocator
of resources unless modern logistics assumes its
necessary role in the top management activities of
gOVernment and industry.
Too often, top business executives have tended to
leave many important aspects of logistics to others.
bome have felt to their everlasting regret that ''if you
uan make it and sell it the rest will take care of itself.''
Besides, strategy and tactics are far more glamorous than
the dull, complex and tedious details of logistics, Indeed,
it has only been with the advent of systems analysis,
operations research and computer science during the
past two decades that we have had the tools even to
begin to understand the complex interrelationships of all
the elements of logistics. Integration of the principal
logistics functions and coordination of strategy, tactics
and logistics are tasks that can be delegated by top
management only at considerable risk.
Several developments have been steadily emerging
since the early 1950's that portend the transformation
of the ancient art of logistics into a new management
science which will benefit producers, consumers and the
public interest alike.
e In the field of business logistics ''physical
distribution management'' has gained increasing recog-
nition as a viable approach to the more efficient
distribution of raw materials and finished products. A
new professional society, the National Council for
Physical Distribution Managment (NCPDM), has grown
rapidly since its creation ten years ago, and has had its
greatest impact among organizations engaged in
transportation and warehousing and among shippers of
bulk supplies and nondurable consumer goods.
e Because U.S. companies typically spend from
30-70% of their cost of sales on materials and their
management, the concept of ''materials management''
has also attracted a large number of advocates in recent
years. The difference between this approach and the
physical distribution concept is a blurry one. Materials
management, however, embraces the purchasing
function and places more emphasis on requirements
planning and production scheduling. The concepts of
physical distribution, materials management and systems
coordination have been effectively integrated in two
recent business logistics books: Logistical Management
by Donald 1. Bowersos an4 EE5EESS2SEEESE. ty
Heskett, Glaskowsky and Ivey. The International
Materials Management Society (IMMS) and the
American Production and Inventory Control Society
(APICS) are two prominent professional organizations
that also have made significant contributions in this area.
The ability of companies to sustain a given level of sales
with relatively lower levels of inventory today is
substantially attributable to increasing professionalism in
this field.
e The ''integrated logistic support'' (ILS) concept
has evolved from experience with large military and
aerospace programs. ILS is essentially a management
discipline that integrates the pianning and application of
all required resources -- materials, facilities, manpower,
funds and information - to the support of a product,
system or service during each phase of its life cycle from
conceptual design through production, distribution and
final disposal or recycling. In his new book on Logistics
Engineering and Management, Benjamin Blanchard
presents a comprehensive analysis of all the elements of
the ILS approach. Professor Blanchard is Chairman of
the National Education Committee of the Society of
Logistics Engineers.
The proper distribution of finished product is often
a complex and always an interesting challenge to those
involved in a large marketplace. For Kodak, this means
providing the right product to the right place at the right
time in good condition and at reasonable cost.
The task of serving the product needs of customers
is not so simple. That is why we have come to think of
what we do as being the art and science of product
management. To perform this function, Kodak has
developed a highly integrated worldwide distribution
system. Dealers and customers are those buying our
products. They're the part of the system over which we
have little or no control, but that are, of course, the
basic reason for which the system exists. It is entirely up
to them what, when and how much they order.
To supply these customers in the domestic market
are seven Regional Distribution Centers located across
the country. In addition to the Regional Distribution
Centers, there are about 40 District Distribution Centers
which supply product primarily to the Business Systems
Markets.
Through the use of computerized inventory control
systems the inventory at these regional locations are
automatically replenished from central Distribution
Centers - one located in Rochester with over a million
and a half square feet of space - and provision for more
than doubling that area as needed - the second is in
Windsor, Colorado, adjacent to the new manufacturing
facilities located there.
These central Distribution Centers are in turn
Supplied by the various manufacturing organizations
located in Rochester and Colorado. Maintaining a proper
balance between inventory levels and desired customer
Service requires accurate demand estimates and sound
scheduling concepts. This, too, is Distribution's responsi-
bility and is supported by mathematical forecasting
Systems to aid the estimator in preparing quantity sales
estimates, large-scale computerized production
scheduling systems to aid in the development of item
level production transfer schedules, and of course, the
numerous information systems to provide the data and
information necessary to control the operations and
measure our product management performance.
The name of the game is profits. If we don't keep
our inventories close to demand, money is in the wrong
place. If we don't run operations as efficiently as
possible, costs are up. It hurts, too, when you have to
scrap outdated film or paper, or when you can't move
items made obsolete by shifts in customer demand.
Indeed, the distribution of Kodak's products to the
domestic market is a tremendous responsibility.
Add to this the additional complexities of the
international marketplace and the role of Kodak's
Distribution organization becomes even more
interesting. Moving product to the top of the world or to
way down under, incorporates the extra dimensions of
distance, government and culture. However, regardless of
these additional international complexities, it is our
responsibility to make certain that customers in all parts
of the world, no matter where they happen to be, can
have our products when they need them.
The Kodak international organization consists of
eight manufacturing companies located outside the
United States that produce product not only for their
domestic market but also for export to other
international Kodak locations. Each of the manu-
fscturing countries also has a distribution system similar
to the one just described for our domestic market.
For example, Kodak Pathe has regional branches which
8upply their dealers throughout France. In addition,
many of the products needed overseas are manufactured
in Rochester. Each of these manufacturing countries, in
turn, exports a significant proportion of its products to
the International Photographic Division non-
manufacturing companies. There are approximately 40
non-manufacturing facilities located within Kodak's
International Photographic Division responsible for
marketing and distributing our products.
The complexities of Kodak's international distri-
bution system are obvious. Attempting to provide an
4xtremely wide range of products manufactured in ten
factories to over 40 marketing/distribution installations
Around the world calls for sophistication, coordination,
and cooperations.
Let's begin our analysis by tracing the international
tmovement of Kodak's goods. First let's take a look at
one of the non-manufacturing companies. Italy, for
example. Kodak Italy, like many overseas marketing
units, employs a computer to assist in proper inventory
management. In today's competitive international
marketplace, having the product in the right place when
it's needed has a profound influence on acceptance by
foreign customers. We have developed a computerized
inventory management system for common use by all of
our international companies with computers.
This inventory management system can best be
described in terms of two sub-systems. One is called the
automatic replenishment system. It is the system that
determines when to place an order on one of the
manufacturing companies and what quantity to order.
This inventory management system automatically
reviews, every day, the stock position of each item. If
stock is found to be below a predetermined reorder
point, the system prepares a replenishment order which
is reviewed by the local planning department. Experi-
ence has shown that the vast majority of suggested
replenished orders are accepted. Thus, the stock planner
need only advise the computer of modifications. This
replenishment system enables the stock planner to spend
his time looking only at those items which require
attention. Thus, the planner spends less of his time in
the clerical review of the product line looking for
replenishment orders and more of his time in a true
planning function. In addition to reviewing the product
line for required replenishment orders, the system is
responsible for automatically establishing control points
used in this replenishment cycle. This is the second part
of the inventory management process. Basically, it is a
question of balancing desired customer service with
minimum inventory investment. This is accomplished by
first estimating expected future customer demand. Each
week, actual customer demand is used to update average
demand figures which are maintained in a computer file
for each item at each location.
Actual supply times from the factories are
monitored and used to create updated lead-time
averages. Also included are management's service
objectives; that is, the desired level of protection against
future backorders. These demand and lead-time
parameters are combined in a mathematical model with
monthly sales seasonals and management's service
objectives. The result is a reorder point and minimum
order quantity required for the automatic replenishment
process. This tool provides each of Kodak's com-
puterized facilities overseas with an effective and
efficient means of maintaining properly balanced
inventories. In the competitive marketplace today, this
often means the difference between future sales lost to a
competitor and satisfied customers who will continue to
come to Kodak for products.
Y ou may be thinking what's new about an
inventory control system -- there are all kinds of
inventory control systems that have been around for a
long time. And that's the point. Kodak doesn't have all
kinds of inventory control systems in its international
locations. There is one - a common system . . , every
location is using identically the same decision rules, the
same definitions, etc., in fact the program statements
themselves are the same. This is important where you are
trying to coordinate product management on an
international scale.
We have traced in general the computerized
inventory management system employed in most of our
overseas companies to automatically replenish their
inventories. The end result of this system is replenish-
ment orders to the Kodak manufacturing plants. Let's
take a look at how these replenishment orders are
forwarded to the manufacturer. In the past, replenish-
ment orders were used in a multitude of ways. Orders
arrived in the form of letters, telephone calls, telegrams,
and standard order forms, All of these ordering methods
required a significant amount of manual effort. Clerical
effort on the part of the ordering location was required
in typing the order, and manual effort was required at
the receiving location to translate the order into a
format appropriate for entry in an order processing
system. Obviously, this type of order communication
wastes valuable manpower and time. Efficient processing
of replenishment orders by the supplier is hindered by a
multitude of undefined order forms. Considering the
distance between the supplying and ordering locations in
the international marketplace, time is a very crucial
factor. To overcome some of these problems, a standard
system has been developed for preparation of replenish-
ment orders by our international companies. Basically,
each company prepares the order automatically in
computer card format following automatic replenish-
ment. Each day, the cards are mailed to the supplying
factory where they are entered into a computerized
order processing system. The result is manpower and
time saved at both ends of the order.
This process is not as easily accomplished as it may
look. To do this it was necessary to develop a system of
common product master data among all Kodak
companies. Obviously, accurate product identification is
necessary for effective communication among the
operating divisions of the company. With the trend
toward mechanization, and the advent of computers,
various product numbering systems were originated as an
aid to either manufacturing, distribution, or company
computer operations. Marketing, on the other hand,
tended towards the use of descriptive nomenclature
rather than numbers to identify the product. As a
result, we ,found different systems of product identi
fication throughout the company, each serving the
specific needs of manufacturing, marketing, or distri-
bution, but not necessarily being compatible or
adaptable to operations or to computer systems on a
company-wide basis. The foundation for developing a
common master data system is the creation of an
international product number. Each product manu-
factured by a Kodak company is assigned a unique,
unstructured number. This number becomes the
common identifier for the product. For each inter-
national product number, a complete set of common
master data is maintained by each company unit but is
coordinated centrally in Rochester for all locations. This
master data contains descriptive information, packing
sizes and other information required by the various
systems involved. Therefore, ltaly's computer system,
for example, knows the correct product number,
description and packing sizes of Kodacolor film which it
orders from France. Replenishment orders are efficiently
and accurately prepared by Italy in card format for
issuance to the distribution center in France. Also,
should the source of supply be changed from France to
say Rochester or London, the master data in Italy is
quickly updated from the central coordinating group in
Rochester - again, a common system.
We have recently expanded this concept of sending
replenishment orders from one location to another. For
the last four or five years, transmission of computerized
information has been relayed over the telephone lines in
Northern Europe between six Kodak non-manufacturing
companies. Basically, this has been done to reduce
computer costs and to eliminate duplication of program
development. One large computer in Sweden services
smaller computer systems in Denmark, Norway, Finland,
Belgium, and Holland. We have taken advantage of this
teleprocessing capability recently, and starting in 1972
Kodak Sweden and Rochester were linked via a
teleprocessing network through New York City. Every
night, all replenishment orders from these six Kodak
companies are transmitted from Sweden to New York
City and then to Rochester. At 8:00 a.m., the orders are
processed through the order entry system in Rochester.
The advantages are potentially great. Reduced leadtimes,
in this case about five days, result in reduced inventories
at the ordering locations. Currently, the teleprocessing
network is being expanded to cover all of Europe and to
include additional data.
What happens once the replenishment orders are
received by Rochester? First the orders are entered into
the export computer system. Then, if sufficent product
exists in the central Distribution Center, notification is
generated to release the order. If for some reason the
order cannot be released immediately, notification of
backorder is sent to the ordering company. The order
then goes to the warehouse where the appropriate
products are assembled.
Occasionally, a code entered into the card order
indicates that special packing materials be used; dry ice
for example. Packing information such as case numbers,
weights, etc., are entered into the export computer
SyStem and when the product is packed and ready to go,
notification of shipment is the necessary export
declarations, shipping advices, and invoices required by
the company, which ordered the product to clear
customers at his port of entry. Thus, orders generated
automatically by the computer system in Italy are
entered directly into the export computer system in
Rochester. Efficiency, awwuracy, and timing are the main
considerations in our attempts to maintain a proper
worldwide balance of inventories.
lLet's take a look at the actual shipment of Kodak's
goods to the foreign market. Basically, the product is
shipped to New York City by truck. From here, it goes
overseas by either ocean freight or air freight. The
coordination of this product movement is the
responsibility of our Traffic Department. Intransit time
accounts for a significant proportion of the overall
elapsed time between the issuance and receipt of an
order by one of the overseas companies. During this
time, the product is of no use to any Kodak company.
Therefore, the Traffic Department must attempt to
make this time as short as practical and the product safe
4s possible during the intransit time interval. We have
found that the use of containerized shipments in either
20 or 40 foot vans goes a long way to accomplishing
Traffic's goals. First, using piggy-back containers means
the product does not have to be unloaded from the
truck in New York and reloaded by hand on the ship.
The entire van is placed on the ship. This means that no
one outside Kodak has a chance to handle the product
from the time it is opened for customs clearance at the
customer's port of entry. It is not always desirable to use
ocean freight as a means for shipping our product
overseas. Rush orders or orders for products with a short
life are often sent by air freight. It is the responsibility
of the Traffic Department to schedule the timing and
method of shipment to insure that we are getting the
best possible rates for the service we are using.
We have traced our international distribution system
from the preparation of an automatic replenishment
order through the order processing system at the factory
and finally through shipment of the product to the port
of entry. This whole process is based on one very simply
but all important fact - that the supplying factory has
sufficient inventory in its Distribution Center to fill the
order. This is the most important link in the entire chain
of events. Accurate inventory management in Italy loses
some significance if the supplier is not able to satisfy the
requirements made on it. Who is responsible for insuring
that the factory Distribution Centers will have product
when it is required? This is the function of the
Distribution Division in conjunction with the marketing
and manufacturing divisions. First a quantity sales
estimate must be established by our Estimating
Departement in Distribution. The quantity estimates are
then presented to the marketing people for approval.
The estimates are combined with management's service
objectives, manufacturing constraints, and current stock
positions to yield the necessary production schedules.
This means that the Estimating Department in
Rochester must be in close contact with the
international marketplace to see that sales requirements
for the foreign market are properly incorporated into
the marketing, distribution and manufacturing chain of
events. World Estimating Department performs this
function. To help them perform their job, an
international information system has been developed.
This information system consists of three parts. One part
provides information to World Estimating each week for
a select number of key items which may be under some
stress in the market -- the introduction of the Kodak
provide training, solve systems problems, and consider
new needs to continually improve the systems.
Overall coordination of all international systems is
provided by the Kodak International Photographic
Division Systems Coordinating Committee. Membership
in this committee consists of top systems management
from the major overseas companies as well as members
from appropriate Rochester units including Distribution.
The goal of this committee is the ''optimization of
international application systems.'' This group concen-
trates on evaluation of needs and the coordination of
systems efforts required to satisfy these needs. It
provides the coordination and control required to
develop and implement effective international systems.
Distribution systems in the international market-
place - identifying the objective is easy enough, that is,
doing what is necessary to insure that Kodak's products
are always where they are needed. Moving towards the
accomplishment of this is more complicated. It takes
sophistication, coordination, and a lot of cooperation.
Kodak Pathe - one of the eight mantufacturing
plants located outside the United States.
One of the non-manufacturing facilities within
the Kodak International Photographic Division.
This is our building in Singapore.
The attempt to describe the temporal evolution
of a chemical reaction in a continuous flow stirred
tank reactor (CSTR) leads to an autonomous dyna-
mical system of ordinary differential equations. Such
systems, dependent on parameters, have been the
subject of extensive research for a long time. It is
well known that the structure of steady states as well
as the local and global dynamic features change when
a parameter crosses a critical surface.
Uppal, Ray and Poore ': %. using a two-dimen-
sional model of a nonisothermal CSTR with a first
order irreversible exothermic reaction, have dispiayed
the various dynamical features that occur around
some bifurcation points such as saddle-node or Hopf
bifurcations which give rise to a new branch of steady
state solutions or the creation (or extinction) of
periodic solutions. They have also shown that there
are homoclinic bifurcations in which the unstable
manifold of a saddle returns to itself to give a solu-
tion with infinite period. In a two-dimensional au-
tonomous system however, no behavior more com-
plicated than a simple periodic motion can occur,
as is known from the elementary theory of differen-
tial equation '?, Much more complex dynamics
can be expected in a system of three or more dimen-
sions.
Some chemical reactorg?:% have been observed
to exhibit chaotic behavior and to pass through
related bifurcation sequences. Some models are
composed of complex reaction steps and many
elements, often developed from the Belousov-habo-
tinsky reaction. The earlier models of B-Z system
involved reactions among eleven chemical species and
some of the proposed systems ':' are of rather
remote chemical relevance even though they display
a variety of mathematicaily interesting features. A
recent model for chaotic behavior has been the
consecutive reaction scheme A -- B -- C taking place
in a CSTg . This is much simpler in structure
than some of those mentioned above, but even so
exhibits such complexity of behavior that a complete
description of the whole parameter space is virtually
impcssible. The most significant work has been done
by fiudson and will be referred to in detail in the
clostng remarks since it has to do with coupled
reactors,
Chaos is the name given to a type of time evolu-
tion of a dynamical system with a seemingly sto-
chastic character of self oscillation, on which all the
paths are unstable and behave in a complicated and
tortuous fashion. In many cases this is reached by a
succession of period doublings of a stable periodic
motion. In chaotic motion there is an extreme sen-
for the map are proportional to that of the flow
weighted by the mean iteration time r as
Now we may consider the one-dimensional map
constructed from the Poincare section, from which
the largest Liapunov exponent can be computed P
where F'(U;) is the derivative of the map at U;, and
can be obtained from the discrete data of one-dimen-
sional mapping. This method also has some problems
as Roux et ail already noted. They are related to
the sensitivity to the noise or data fitting procedtire,
yielding a resulting value of o,, that fluctuates from
positive to negative. Also, there may be a problem
due to the fractal nature of the mapping, i.e. the
dimension of mapping appears to be greater than one.
Therefore, to apply the Eq. (27) to tabulated data
a fitting function is needed.
In ou computation we check the derivative F'
at every point U; by finding the nearest point among
the previous points of the mapping, so that the
nearest point may be considered as the nearby tra-
jectory in the flow. This method appears to be very
consistent and more powerful at least in commputing
time in comparison with the methood of Benettin
et o ' Figure 7 shows the Liapunov exponent for
the chaotic motion shown in Figure 2.
Since the chaotic nature of dynamics is expressed
in the divergence of nearby trajectories, the Liapunov
exponents always show positive values. While, for a
periodic orbit of period T, the Liapunov exponcnts
will obviously die away when the trajectory returns
to the same point at every time T. When the Lia-
punov exponents converge to a negative value, we
refer to it as the case of non-periodic attractor. A
notable thing is that the Liapunov exponents are
known to be in good agreement with the metric
entropy, shown as Eq. (12), for the Lorentz at-
tractor ', So here we use o,; for mapping as the
measure of chaos without any special modification.
We begin our investigation with the search for a
homoclinic orbit which acts as a critical noint of
periodic bifurcation, and around which very com-
plicated dynamical featurcs are found in high dimen-
sional systems*' Iin relation to the homoclinic
bifureation theorem, some complex dynamics are
discussed by Silikov and otherg '1 ->>
A homoclinic orbit is tound when a system pos-
scsses a hyperholic saddle point p whose stable and
unstable manifolds intersect each other transversally
somewhat as in the torus map in Figure 8, We may
suppose that there is a point q E WP(p) W(p)
with ; 9 p: then since (.''(q) -- p as n -+ t = there
must he an infinite set of such points. The orhit
(.''(q); of ; is called a homoclinic orbit and plays
an important rcle in the global dynamics. In par-
ticular, the violent winding of the global manifolds
W'! (p ) and W(p) in the neighborhood of p leads
to a sensitive dependcnce of orhits on the initial
conditions, sn that the presence of homoclitnie
orhits tends to promote erratie hehaviot
Figures 9 and 10 show the situation of steady
States whichi we are contronting m our investigat ion
of the dynamics. lfere the steady states are quant-
tied by the temperature ot the first CSTR und the
parameter is the Dumkohler number, Da. The other
system parameters arc: 6 = 1.0. E = DaiDa' = 10.0.
The reliability of data is of significance if they
are to be used effectively in process monitoring
for operational optimization, control and identi-
fication. In experimental studies of chemical reac-
tors at steady state the data are subject to errors
so that the measured values (flow rates, volumes,
concentrations) ot the species entering and leav-
ing the reactor do not satisfy the law of conserva-
tion of the elements. In these cases we say that
material balance data are not consistent. This charac-
teristic is either rooted in the nature of the process,
or it is a consequence of inevitable uncertainty re-
lated to the experimental equipment, ie ., the meas-
urements errors. It is common practice in a data
reconciliation procedure to assume that all measure-
ments are subject to random experimental errors and
correct the raw data in such a way that the required
adjustments are minimum in some sence.
In practice, process data mmay also contain other
types of errors, which are caused by nonrandom
events, for instance instruments biased may malfunc-
tion, presence of unknown side products, etc, We
shall refer to these biases as gross errors, The presence
of gross errors invalidates the statistical basis of any
data reconciliation scheme and it is impossible to
prepare and adequate process model on the basis of
erroneous measurements. In order to avoid these
shortcomings, we need to check and identity the
presence of gross errors in the measured data.
Several previous works in the literature have at-
tempted to deal with the general problem of data
rectification (Vaclaveck', Mah, et al.'. Nogita',
Ripps', Romagnoli and Stcphanopoulus':''. Ro-
magnoli', Crowe et al,', some of them dealing, in
particular, with the experimental data related to
the material balance of a chemical reactor (Madron
et al,', Murthy''-3 However, there are some
special characteristics of the problem that have not
been fully exploited, which can be very useful to
redefine the mathematical formulation making it
more attractive for computational purposes and
for dealing with systemuatic gross errors.
In the present work an alternative formulation
for the statistical treatment of material balances is
suggested. This alternative leads to the mathematical
decomposition of the original problem. Such decom-
position allows, with redundancy condition satisfied,
the construction of a reduced estimation problem
with subsequent computational advantages. Further-
more, the proposed equivalent formulation is ideal-
ly suited for the identification and rectification of
gross biases in the measurements. Consequently, a
The early stages of metal deposition is usually
associated with two-or three-dimensional nucleation
processes, which rate is strongly dependent on
overpotential. Different physical models have been
considered which account for the various features
of the nucleation procesg''
The electrochemical phase formation of metals
on various substrates have been extensively studied,
however no information is available on palladium1
electrodes. Copper appears to be a suitable metal for
such study since both Cu deposition and stripping
occur before the onset of oxide formation and com-
plication owing to the superposition of both proces-
ses is avoided * '
This paper reports the results of a study carried
out to determine the nature of the initial stages of
Cu deposition on Pd. Experimental data are inter-
preted in terms of both the classical and atomistic
models of electrolytic nucleation.
Experimental
The working electrode was a polycrystalline palla-
dium rod (Materials Research) sealed in a polyethyl-
ene holder in order to expose only the top surface of
the metal to the solution. The electrode surface was
mechanically polished to a mirror finish with dia-
mond paste up to l u and washed ultrasonically.
A HglHg4SO., NagSO. (sat) electrode, con-
nected to the cell by a Luggin capillary served as re-
ference electrode, however all potentials given in this
paper are referred to the reversible hydrogen elec-
trode.
Measurements were carried out at 25*C in an
electrolytic cell made from Pyrex glass. The electro-
lytes were prepared from analytical grade chemicals
and triply distilled water and deaerated by bubbling
with purified nitrogen.
Voltammetric measurements were performed
with a potentiostat (Bank Elektronik LB 75L) driven
by a triangular wave generator (Bank Elektronik VSG
72), while rectangular potential pulses were imposed
by a pulse galvanostat (Megaphysik MP-IM400) con-
nected to the potentiostat through an ohmic resistor.
Current-time transients were obtained by pulsing
the electrode from the equilibrium potential for cop-
per deposition, E,, to different final potentials, Ey.
The overpotential was defined as E, - Er.
Linear sweep voltammograms obtained at dif-
ferent sweep rates, v, on a Pd electrode inmersed in
a solution containing 10* % M Cuf are shown in
Figure 1. The main features are the steep rise in cur-
rent as the cathodic peak potential is approached and
the sharp fall in current after the anodic peak which
is characteristic of the stripping of an insoluble specie
from the electrode surface. The anodic and cathodic
Most of the reaction products condense at the
exit of the reactor and non-condensed gaseous pro-
ducts are retained by washing traps containing an
NaOH solution 0.125 M.
To determine molybdenum recovered, the solid
products are quantitatively extracted with an alkaline
solution similar to the above one. This solution, as
well as the one coming from the washing traps, are
analyzed according to the atomic absorption spec-
trophotometry technique by means of a Varian AA-
275 instrument, using the standard aggregate method.
The attack of sample to know the total amount
of molybdenum and cobalt was carried out by two
techniques: total dissolution of catalyst with aqua
regia, Ruiz de Olsina et al.', and total volatilization
with chlorine and carbon monoxide for 180 minutes
at 973 K.
Molybdenum and cobalt determinations, Ruiz de
Olsina et al.', Ginzberg et al.', in the solutions com-
ing from both attacks were carried out by atomic
absorption. Values obtained were 16.3% of molyb-
denum expressed as trioxide and 4.3% of cobalt ex-
pressed as oxide.
A spent catalyst in the form of extrudate,
000150003 m in diameter and 00020D05 m in
length, was used in all trials.
The amount of uatalyst used in each assay was
0D03 kg.
The chlorine and carbon :onoxide :nixture, in
a 1: 1 ratio, was fed at a flow velocity of 004 dm'
min. In a previous paper, Rivarola et al,', it was
proved that the 1: 1 ratio between the two gases was
the one that allowed to obtain better recovery values.
Molybdenum recovery trials were followed for
periods of time ranging from 15 to 90 minutes at a
temperature interval from 523 to 673 k.
The reaction system was entirely purged with air
before starting the attack of sample with the chlorine
and carbon monoxide mixture. Recovery values oh-
tained showed no differences with the ones obtained
trom assays in which the system was purged with
nitrogen.
Trials were carried out m order to identify pro-
bable products from chlorination reaction. Through
IR spectroscopy the solid product ot reactor out
put was analyzed. The presence of phosgene and
carbon dioxide in the efluent gases was investigated
by using reactive paper N.BP.' and solution of
barium cation at appropriate pll, respectively.
The separation and purification of molybdenum1
recovered were carried out oin solutions coming from
attacks performed under identical working condi-
tions (623 K and 90 min). These solutions were
treated as follows: it was alkalized at pH values from
6 to 10 with NH, OH. At this pH range, aluminum.
iron and cobalt hydroxides precipitate, provided
some of it was present, Burriel et al,'. Molybüenum
remains in solutions.
The precipitate was filtered and washed with an
NHOH diluted solutions. The resulting solution was
evaporated up to almost dryness and then dried at
333 K in a vacuum drier until constancy of weight:
A white solid was obtained, which was quali-
tatively analyzed by IR and X-ray fluorescence,
Schwing-Weill et al.'.
Figure 1 shows expert mental results.
The maximum extractions obtained after 90
minutes were 78, 90 and 96% at temperatures of
523 K, 573 K and 623 K, respectively.
All molybdenum present in the catalyst is
recovered after 60 minutes at a temperature of
673 K.
Under the working conditions studied, cobalt
is not extracted and th aiumina imatris is sarcely
attacked,
The quantitative analy sis mude on the produucts
of the chlorination reaction through the teciuiques
described above, indicated the presence of MoOC1g.
MoOC1,, Cl;CO and CO4 as reaction products. Then,
in our working conditions, chlorination reaction
could be represcnted by:
where: x; and xg are the number of moles of MoO4
in each reaction.
In the electrodeposition process, the character-
istics of a coating are affected by the resistance of the
metallic phase. This effect is higher when the thick-
ness is thinner, such as in the manufacture of printed
circuits, or when the current density is higher, g.,
the chromium plating. In this way, the electrode
region near the current feeder has a higher overvolt-
age than the regions located far from this point.
Consequently, there is a current density distribution
over the electrode that causes non-uniform coating
with variable properties. Taking into account that
the methods of predicting the distribution of current
density over electrodes are of great interest in ap-
plied electrochemistry, it is justified the study of the
effect of electrode resistance on current densitv dis-
tribution, The electrochemical reaction has, in this
case, a combined diffusion and charge-transfer con-
trol.
In the iterature, there are some mathematical
analysis''' evaluating such systems, In' '', the effect
of a lineal polarization is included in the model.
Other authorg''' -? have considered a Volmer-Tafel
equation for the kinetic. Alkire '8-''' also takes into
account the diffusion of the reactants, though few
experiments have been performed ' 1. -? verifying
the proposed models.
In a previous work '' carried out in this laborato-
ry, a mathematical treatment was developed assuming
an axial isopotential of the electrolyte, with an elec-
trochemical reaction controlled by charge-transfer.
A close agreement was obtained between the ex-
perimental and calculated values. In this paper, the
mathematical model is solved for an electrochemical
reaction with a combined control by diffusion and
charge-transfer. The theoretical data are compared
with experimental results by employing the reduc-
tion of ferricyanide as test reaction, in order to con-
firm the validity and reliability of the model.
Figure 1 shows schematically a cylindrical rod
(length: L; radius: r,) fed at its lower end (e = 0);
the balance of current for the region x, x x is:
conclusion that it may be convenient to present the
results by combining the variables into a dimension-
less group. In this way the Wagner number '' was pro-
posed; it was defined as the ratio of polarization and
electrolyte resistance. In an earlier paper '', in which
the effect of electrode resistance on current density
distribution has been investigated by considering that
the electrochemical reaction is controlled by charge-
transfer at high overvoltages, it was analytically
proved that the controlling variables of the current
distribution can also be lumped in an unique dimen-
sionless parameter, the so-called modified Wagner
number. This one, together with the mean relative
deviation, b,, has been an essential parameter to
check the validity and reliability of the proposed
model, With this purpose, in the present paper it is
computed the modified Wagner number, defined as:
where
R,, : polarization resistance:
R,,,; resistance of the metal phase,
the expressions of which are given by:
and
By introducing Eqs, (.0) and (21) into Eq,(19),
Taking the derivative of equation (15) with
respect to nn, introducing the result into Eq. (22).
and solviang at x = 0 it was obtained the modified
Wagner number given in column 8 of Table 2. Fig-
ure 7 summarizes the results obtained, the mean
relative deviation, , (defined by Eq, (18)), was
represented as a functionn of the dimensionless varia-
ble Waff, (0), givn by Eq. (22) and calculated using
na, (0).
When Figures 6 and 7 and Table 2 are analyzed,
it must be noted that there is a close agreement
between experimental and theoretical results which
is as better as higher is the value of the modified
Wagner number.
The dashed line in Figure 7, represents the
theoretical effectiveness factor as a function of the
Way (0). It can be seen that the relationship bet-
ween Y and Wafi; (0) is a monotonically increasing
function. In practice, when an electrochemical
reactor is designed, a high Y value is desirable. Ac-
cording to Figure 7, this is achieved when Waf, (0)
is high. Under this condition, it has been shown that
the model predicts more reliable results. Therefore,
it can be properly used to design reactors operating
under such kinetic conditions and its predictive
capability is better as y is higher.
In the same figure, the points ''o'' correspond
to the hydrogen evolution reaction in 1 M NaOH at
30*C. These results, informed in a previous paper ''
have shown that, for the solutions with high electrical
conductivity -such as 1 M NaOH - and test-reaction
with charge-transfer control, the agreement between
the theoretical and experimental values is excellent
when Waf, (0) is greater than 15 X 10 '. In this
work, it was investigated a test-reaction with a
combined diffusion and charge-transfer kinetic
control and the validity of the proposed model for
high modified Wagner number was confirmed.
The reliability of the proposed model, for high
modified Wagner number, was demonstrated com-
paring experimental results and theoretical calculated
data; thereby verifying the use of this mathematical
treatment to design such reactors.
The author would like to thank Stiftung Volks-
wagenwerk, Federal Republic of Germany and Con-
sejo Nacional de Investigaciones Cientificas y Tecni-
the light atoms in the soft tissue of our
jowls do not stop x rays as well as
the heavy mercury atoms in the dental
amalgam used to fill teeth. Although
this phenomenon is useful to the dental
profession, it is often an embarrassment
for scientists measuring atomic posi-
tions.
X rays are scattered by the electrons
surrounding the nucleus of an atom.
As a result, heavy atoms with many
electrons (such as mercury) scatter x
rays more efficiently than light atoms
(such as oxygen or, worse, hydrogen).
Thus, x rays pass right through light
materials without being greatly attenu-
ated or deflected. It is for this reason
that the structure of the much-heralded
high-temperature superconductors was
not determined by x-ray diffraction-
in spite of the fact that most university
physics departments worldwide have an
x-ray machine. One of the first high-
temperature superconductors discov-
ered contained yttrium and copper, both
of which are heavy ahd scatter a rel-
atively large percentage of the x rays
incident on a sample. Unfortunately,
the superconductors also contained oxy-
gen, whose feeble scattering of x rays
is swamped by that of its heavy neigh-
bors. It was impossible to determine the
positions of the oxygen atoms using x-
ray diffraction because the x rays passed
through the superconductor almost with-
out noticing the oxygen.
We might try to find atomic posi-
tions by ''seeing'' with electron beams.
After all, quantum mechanics tells us
that particles have wave properties,
and the wavelength of electrons can
easily be matched to interatomic dis-
tances by changing the electron en-
crgy. However, as anyone who has
ever rubbed a balloon on the family cat
knows, the interaction between electrical
charges is strong. Not surprisingly then,
a charged particle, such as an electron
or a positron, does not travel far through
solids or liquids before it is attracted or
repelled by the electrons already in the
matter. This makes electrons unsuitable
for looking inside bulk materials: they
suffer from the same opacity problem
as light, and specially prepared, thin
samples are required for electron mi-
croscopy.
What about neutrons? They have no
charge, and their electric dipole mo-
ment is either zero or too small to be
measured by the most sensitive of mod-
ern techniques. For these reasons, neu-
trons can penetrate matter far better than
charged particles. Furthermore, neutrons
interact with atoms via nuclear rather
than electrical forces, and nuclear forces
are very short range-of the order of
a few fermis (1 fermi = 1(0 meter).
Thus, as far as the neutron is concerned,
solid matter is not very dense because
the size of a scattering center (nucleus)
is typically 100,000 times smaller than
the distance between such centers. As a
consequence, neutrons can travel large
distances through most materials with-
out being scattered or absorbed (see the
opening illustration to ''Putting Neu-
trons in Perspective''). The attenuation,
or decrease in intensity, of a beam of
low-energy neutrons by aluminum, for
example, is about l percent per millime-
ter compared with 99 percent or more
per millimeter for x rays, Figure 1 illus-
trates this point for other atoms and for
electrons as well as x rays and neutrons.
Like so many things in life, the neu-
tron's penetrating power is a two-edged
sword. On the plus side, the neutron
can penetrate deep within a sample even
if it first has to pass through a container
(necessary, for example, if the sample
is a fluid or has to be kept at low tem-
peratures or high pressures). The corol-
lary is that neutrons are only weakly
scattered once they do penetrate. Also,
detection of a subatomic particle in-
volves the observation of that particle's
interaction with some other particle, so
neutron detection requires a certain in-
genuity (in practice, detectors make use
of one of the few atoms, such as boron,
helium-3, or lithium, that absorb neu-
trons strongly to produce ionizing radia-
tion). To make matters worse, available
neutron beams inherently have low in-
tensities. X-ray instruments at synchro-
tron-radiation facilities can provide
fluxes of 10'8 photons per second per
square millimeter compared with I(0'
neutrons per second per square millime-
ter in the same energy bandwidth for
powerful neutron-scattering instruments.
The combination of a weak interac-
tion and low fluxes makes neutron scat-
tering a signal-limited technique, which
is practiced only because it provides in-
formation on the structure of materials
that cannot be obtained by other means.
This constraint means that no generic
instrument can be designed to examine
all aspects of neutron scattering. In-
stead, a veritable zoo of instruments has
arisen with each species specializing in
a particular aspect of the scattering phe-
nomenon.
In spite of its unique advantages, neu-
tron scattering is only one of a battery
of techniques for probing the struc-
tures of materials. All of the techniques,
such as x-ray scattering and electron mi-
croscopy, are needed if scientists are to
understand the full range of structural
properties of matter. In most cases, the
different methods used to probe material
structure give complementary informa-
tion because the nature of the interaction
between the radiation and the sample
are different. For example, neutrons in-
teract with nuclei, whereas x rays and
electrons ''see'' only the electrons in
matter (Fig. 2). To a certain extent the
method of choice depends on the length
scale of the structure to be investigated
(Fig. 3). When two techniques address
the same scale, additional information,
such as the size and chemical composi-
tion of the sample, is required to choose
the optimal technique.
The scattering of neutrons by nuclei
is a quantum-mechanical process. For-
mally, the process has to be described in
terms of the wave functions of the neu-
tron and the nucleus. The wave function
of the neutron, as its name suggests, has
the form of a wave-that is, a function
that oscillates sinusoidally in space and
time. The square of the amplitude of
this wave at any point gives the proba-
bility that the neutron will be found at
that point. It does not matter whether
we talk about the wave that represents
the neutron or the probability that a par-
ticle called the neutron is at a given
location. Both descriptions will give
rise to the same mathematics and are,
therefore, equivalent. Sometimes it is
convenient to refer to the neutron as a
wave because the picture thus conjured
1s easier to understand. At other times it
is more useful to think of the neutron as
a particle. We can switch from one de-
scription to the other at will, and if we
do the mathematics correctly, we will
always get the same answer.
The neutrons used for scattering ex-
periments usually have energies simi-
lar to those of atoms in a gas such as
air. Not surprisingly, the velocities at
which they move are also comparable
with those of gas molecules-a few
kilometers per second. Quantum me-
chanics tells us that the wavelength of
the neutron wave is inversely propor-
tional to the magnitude of the neutron
velocity v = lv] (throughout the text
we will use a bold variable to represent
a vector quantity and a nonbold ver-
sion of the same variable to represent
the corresponding magnitude). For the
neutrons used in scattering experiments,
the wavelength, A, turns out to be a few
angstroms (l angstrom = 1()1PP meter).
It is often useful to work in terms of the
so-called neutron wave vector, k, which
is a vector of magnitude k = 2/A that
points along the neutron's trajectory.
The vectors k and v are collinear and
related by the equation
where h is Planck's constant, m is the
mass of the neutron (1.67495 s 1()=27
kilogram), and mv is the momentum of
the neutron.
The scattering of a neutron by a sin-
gle nucleus can be described in terms of
a cross section o, measured in barns
(1 barn = 1(0 square meter), that
is equivalent to the effective area pre-
sented by the nucleus to the passing
neutron. If the neutron hits this area, it
is scattered isotropically, that is, with
equal probability in any direction. Why
isotropically? The range of the nuclear
potential is tiny compared to the wave-
length of the neutron, and so the nu-
cleus is effectively a point scatterer,
(X rays, on the other hand, do not scat-
ter isotropically because the electron
clouds around the atom scattering the x
rays are comparable in size to the wave-
length of the x rays,)
Suppose that at an instant in time we
represent neutrons incident on a fixed
nucleus by a wave function e'', which
is a plane wave of unit amplitude ex-
pressed in terms of the position vector
r. Note that the square modulus of this
wave function is unity, which means
the neutron has the same probability of
being found anywhere in space but has
definite momentum mv e hk/2. The
nodes of the wave-that is, the points
at which the phase k - r is equal to n,
where n is an integer-are the straight
where r, and r; represent the positions
of atoms labeled j and k in the lattice
and ba4,g IS the coherent scattering length
of those atoms.
Equation 2 is the scattered intensity
that would be measured in a neutron-
diffraction experiment with a real crys-
tal, and is often called the structure fac-
tor, S (Q). As we count through the
atoms of a lattice performing the sum
in Eq. 2, the real and imaginary parts
of the exponential function both take
values that are distributed essentially at
random between plus and minus one.
Because many atoms are involved, the
sum usually averages to zero, except at
certain unique values of Q.
Obviously, the values of Q for which
the structure factor, S (Q), is nonzero are
rather special, and it is easy to imagine
that not many values of Q satisfy this
condition. Further, those values are in-
timately related to the structure of the
crystal because the vectors r; - r; in
Eq. 2 represent the set of distances be-
tween different atoms in the crystal.
We can determine the values of Q
at which S (Q) is nonzero and at which
diffraction occurs by consulting Fig. 6.
Suppose Q is perpendicular to a plane
of atoms such as Scattering Plane 1 in
this figure. If the value of Q is any in-
tegral multiple of 2 /d, where d is the
distance between parallel, neighboring
planes of atoms (Scattering Planes 1
and 2 in Fig. 6), then Q - (r; - r0) is
a multiple of 2 and S (Q) is nonzero
because each exponential term in the
sum in Eq. 2 is unity. Thus, Q must be
perpendicular to planes of atoms in the
lattice and its value must be an integral
multiple of 2 /d. For values of Q that
do not satisfy this condition, S (Q) = 0.
and there is no scattering.
The values of Q at which neutron
diffraction occurs are governed by the
Same law that was discovered for x
rays in 1912 by William and Lawrence
Bragg, father and son. To see this, we
apply the condition described above
(Q = n(2/d), where n is an integer)
to the scattering triangle for elastic scat-
tering. Then using the relationship be-
tween (0, %, and A shown in Fig. 5, the
condition can be rewritten as
This equation, called Bragg's law, re-
lates the scattering angle, 26, to the in-
terplanar spacing in a crystalline sample.
Bragg's law can also be understood in
terms of the path-length difference be-
tween waves scattered from neighboring
planes of atoms (Fig. 7). For construc-
tive interference to occur between waves
scattered from adjacent planes, the path-
length difference must be a multiple
of A, the wavelength. Applying this
condition to Fig. 7 immediately yields
Bragg's law in the form given in Eq. 3.
Many of the results described in the ar-
ticles in this issue will fall back on this
point of view.
Diffraction, or Bragg scattering, as
it is sometimes called, may occur for
any set of atomic planes that we can
imagine in a crystal, provided the wave-
length. A, and the angle, 9, between the
incident neutron beam and the planes
are chosen to satisfy Eq. 3. Bragg scat-
tering from a particular set of atomic
planes resembles reflection from a mir-
Now let's return to powder diffrac-
tion, In a powder-diffraction instru-
ment at a reactor source (Fig. 9), a
monochromatic beam of neutrons im-
pinges on a powdered sample, and the
neutrons scattered from the sample are
recorded as a function of the angle 26.
Each Bragg peak in a typical scattering
pattern (Fig. I0) corresponds to diffrac-
tion from atomic planes with a differ-
ent interplanar spacing, or value of d.
Many peaks can be recorded simultane-
ously by placing detectors at a variety
of scattering angles (such as the sixty-
four helium-3 detectors in Fig. 9).
In a powder diffraction instrument at
a spallation source (Fig. 11), the sam-
ple is irradiated with a pulsed beam of
neutrons having a wide spectrum of en-
ergies. Scattered neutrons are recorded
in banks of detectors located at differ-
ent scattering angles, and the time at
which each scattered neutron arrives
at the detector is also recorded. At a
particular scattering angle, the result
is a diffraction pattern very similar to
that measured at a reactor, but now the
independent variable is the neutron's
time of flight rather than the scattering
angle. Because the neutron's time of
flight is proportional to its wavelength
and, for constant scattering angle, wave-
length is proportional to the spacing
between atomic planes (Eq. 3), the mea-
sured neutron scattering can be plotted
against either time of flight, A, or d-
spacing (Fig. 12). (The resemblance be-
tween Figs. 10 and 12 is obvious. The
patterns are equivalent ways of prob-
ing Bragg's law, and in fact, diffraction
data obtained at reactors and spallation
sources can be plotted on the same scale
by simply using ( = 4 sin 8/A as the
independent variable.)
As in the reactor case, detectors at a
spallation source can be placed at dif-
terent scattering angles, allowing many
patterns to be measured simultaneously.
Detectors at small scattering angles pro-
vide information about widely spaced
atomic planes, whereas those at larger
angles record data relevant to small
spacings. There is usually some overlap
of information provided by the different
detectors.
UUsing patterns like those of Figs. 10
and 12, the atomic structure of a poly-
crystalline sample may be deduced from
Eq. 2. In practice, however, one guesses
the atomic positions, evaluates Eq. 2,
and from a comparison of the calcu-
lated and measured diffraction patterns,
refines the atomic coordinates. This
type of procedure is described in de-
tail in the article ''X-Ray and Neutron
Crystallography-A Powerful Combina-
tion'' by Robert Von Dreele.
Another way of thinking about coher-
ent elastic neutron scattering is shown in
Fig. 13. One can imagine the incident
and scattered neutron waves setting up
a ''probe wave'' in the sample-much as
two misaligned picket fences generate a
set of moire fringes. One can alter the
wavelength of the probe wave, Ayrobe. by
changing the angle between the ingoing
and outgoing waves (that is, the scatter-
ing angle) or by increasing or decreas-
ing the wavelength of the neutrons used.
To obtain information about structures
by coherent elastic scattering, Apobe
must be chosen to be approximately the
same as the size of the structure. For
crystallography this means that Ayobe
needs to be of the same order as inter-
atomic spacings. We already know this
from Bragg's law. A little trigonom-
etry applied to Fig. 13 will show that
NpoEe * Aaeuusa/2 sin 68, so that when
pros @uals the distance between two
adjacent scattering planes, Bragg's law
is satisfied.
The probe-wave idea shows us how
we can measure structures that are larger
than typical interatomic distances. We
simply arrange for AwH&e t0 be large,
either by decreasing the scattering an-
gle or by increasing the neutron wave-
length. In practice, to examine some
of the larger structures displayed in
Fig. 3-polymers, colloids, or viruses,
for example-we need to use neutron
wavelengths greater than 5 angstroms
and scattering angles less than 1 de-
gree. Because of the latter constraint,
this technique is known as small-angle
neutron scattering, or SANS.
The Van Hove formulation for neu-
tron scattering may be manipulated (see
'The Mathematical Foundations of Neu-
tron Scattering'') to provide the follow-
ing equation for the intensity of neu-
trons scattered at small angles (that is,
for small values of 0):
where the integral extends over the
entire scattering sample and b(r), the
scattering-length density, is calculated
by summing the coherent scattering
lengths of all the atoms over a small
volume and dividing by that volume.
In many cases, samples measured by
SANS consist of particles with a uni-
form scattering-length density b, that
are dispersed in a uniform matrix with
a scattering-length density b,4, Exam-
ples include pores in rock, colloidal dis-
persions, biological macromolecules in
water, and many more. The integral
in Eq. 4 can, in this case, be separated
into a uniform integral over the whole
sample and a term that depends on the
difference, b, -- b44, between the scat-
tering length of the particles and that of
the matrix, This difference is called the
contrast factor. If all the particles are
identical and their positions are uncorre-
lated, Eq. 4 becomes
where the integral is now over the vol-
ume V, of one of the particles and N,,
is the number of such particles in the
sample.
The integral above of the phase factor
f over a particle is called the form
factor for that particle. For many sim-
ple particle shapes, the form factor can
be evaluated without difficulty: the ex-
pression for spherical objects was first
derived by Lord Rayleigh in 1911.
Equation 5 allows us to understand an
important technique used in small-angle
scattering known as contrast match-
ing. The total scattering is proportional
to the square of the scattering contrast
between a particle and the matrix in
which it is embedded. If we embed the
particle in a medium whose scattering
length is equal to that of the particle,
contrast matching
An isotopic-labeling technlque based on the
dramatic difference between the scattering
lengths of hydrogen and deuterium, which
is particularly useful in neutron-scattering
studies of complex biological molecules in
aqueous solution. The technique involves
matching the scattering from the solvent with
that from one component of the biological
molecules by replacing the hydrogen atoms
in the solvent or the component or both with
deuterium. The observed scattering is then
due to only the unmatched components.
netic moment. In this situation the clas-
sical equations that describe the motion
of the neutron moment are similar to
those of a rotating top that has been
pushed by a force from the side and
so begins precessing about its original
axis of rotation. The neutron does the
same thing-its moment starts to pre-
cess about the local field direction at a
rate known as the Larmor frequency,
which depends on the magnitude of the
field inside the flipper. By choosing the
thickness of the flipper and the strength
of the field in the second coil appropri-
ately, one can arrange for the neutron
moment to rotate precisely 180 degrees
during its passage through the flipper.
Clearly, if a neutron's moment was up
before the flipper, it will be down after
the flipper, and vice versa.
Now suppose we have a spectrome-
ter with polarizers before and after the
scattering sample. If flippers are in-
serted on either side of the sample, we
can measure all of the neutron scatter-
ing laws-up to down, up to up, and
so forth-simply by turning the appro-
priate flipper on or off. This technique,
known as polarization analysis, is useful
because some scattering processes flip
the neutron's moment whereas others do
not.
Scattering from a sample that is mag-
netized provides a good example. Mag-
netic scattering will flip the neutron's
moment if the magnetization responsi-
ble for the scattering is perpendicular to
the guide field used to maintain the neu-
tron polarization. If the magnetization
is parallel to the guide field, no flipping
occurs. Thus, like the dipolar interaction
described earlier, polarization analysis is
a technique that helps determine the di-
rection of electronic moments in matter.
Incoherent scattering that arises from
the random distribution of nuclear spin
States in materials provides another ex-
ample of the use of polarization anal-
ysis. Most isotopes have several spin
tates, and the scattering cross section
for a nucleus varies with spin state. The
random distribution of nuclear spins in
the sample gives rise to incoherent scat-
tering of neutrons. It turns out that two-
thirds of the neutrons scattered by this
incoherent process have their moments
flipped, whereas the moments of the re-
maining third are unaffected. This result
is independent of the isotope that is re-
sponsible for the scattering and of the
direction of the guide field. Although
incoherent scattering can also arise if a
sample contains a mixture of isotopes of
a particular element, neither this second
type of incoherent scattering nor coher-
ent nuclear scattering flip the neutron's
moment. Polarization analysis thus be-
comes an essential tool for sorting out
these different types of scattering, al-
lowing nuclear coherent scattering to be
distinguished from magnetic scattering
and spin-incoherent scattering.
Polarization analysis has been partic-
ularly useful in the study of magnetic
phenomena because it has helped to de-
termine the directions of the magnetic
fluctuations responsible for scattering.
Without this technique, many of the el-
egant experiments that have provided
confirmation for ideas about nonlinear
physics (see ''Nonlinear Science-From
Paradigms to Practicalities'' by David K.
Campbell, Los Alamos Science No. 15,
1987) could not have been performed.
The three-axis spectrometer of Fig. 15,
for example, is equipped for polarization
analysis.
Magnons. Another important aspect
of magnetized materials is the fact that
the directions of the atomic moments
in a material such as iron can oscillate
like the pendulums considered earlier
for lattice vibrations. Here again, there
is a coupling between magnetization at
different atomic sites, and a wave of
magnetic oscillations can pass through
the material. These magnetic excita-
tions, or magnons, are the magnetic
analogue of the phonon displacement
waves described earlier. Not surpris-
ingly, magnon frequencies can be mea-
sured by inelastic neutron scattering in
the same way as phonon frequencies.
Since the magnetic oscillations that
make up the magnons are perpendicu-
lar to the equilibrium direction of the
atomic moments, the scattering causes
the magnetic moment of the neutrons to
be flipped, provided the neutron guide
field is parallel to the equilibrium di-
rection of the atomic moments. This,
of course, allows one to distinguish be-
tween phonons and magnons.
So far we have described only exper-
iments in which the structure of bulk
matter is probed. One may ask whether
neutrons can provide any information
about the structure of the surfaces of
materials. At first sight, one might
expect the answer to be a resounding
''No!'' After all, one of the advantages
of neutrons is that they can penetrate
deeply into matter without being af-
fected by the surface. Furthermore,
because neutrons interact only weakly
with matter, large samples are generally
required. Because there are far fewer
atoms on the surface of a sample than
in its interior, it seems unreasonable to
expect neutron scattering to be sensitive
to surface structure.
In spite of these objections, it turns
out that neutrons are sensitive to sur-
face structure when they impinge on
the surface at sufficiently low angles.
In fact, for smooth surfaces, perfect re-
flection of neutrons occurs for almost
all materials at angles of incidence (the
angle between the incident beam and
the surface) less than a critical angle,
denoted Y;. This angle is proportional
to the coherent scattering-length density
of the material and the neutron wave-
length. For a good reflector, such as
nickel, the critical angle measured in de-
grees is about one-tenth of the neutron
wavelength measured in angstroms-
it is well under a degree for thermal
neutrons. As the angle of incidence in-
creases above the critical angle, less
and less of the incident neutrons are re-
flected by the surface. In fact, reflectiv-
ity, which measures the fraction of neu-
trons reflected from the surface, obeys
the same law, discovered by Fresnel,
that applies to the reflection of light: re-
flectivity decreases as the fourth power
of the angle of incidence at sufficiently
large grazing angles.
However, Fresnel's law applies to re-
flection of radiation from the smooth,
flat surface of a homogeneous material.
If the material is inhomogeneous and
there is a variation of the scattering-
length density perpendicular to the sur-
face, the neutron reflectivity, measured
as a function of the angle of incidence,
shows a more complicated behavior. By
keeping the reflection angle, , small,
neutron reflectometry can be used to
probe density variations in the surface
to depths of a few thousand angstroms
with a resolution of a few angstroms.
Most of today's technical gadgets
are either painted or coated in some
fashion to prevent corrosion or wear.
Reflectometry can often provide useful
information about such protective lay-
ers, Figure 17, for example, shows the
reflectivity, measured on the LANSCE
Surface Profile Analysis Reflectome-
ter (SPEAR), from a 1500-angstrom
layer of diblock copolymer (polystyrene-
polymethylmethacrylate) multilayer
deposited on a silicon substrate. The
spacing of the undulations in this result
provides a direct measure of the aver-
age thickness of the polymer layers in
the film. When the detailed shape of
the reflectivity profile is compared with
theoretical predictions, the density and
thickness of the polymer layers, as well
as the thickness of the interface between
layers, can be deduced.
Neutron reflectometry is a relatively
new technique. It is also one ideally
suited to spallation sources. In the next
few years I expect the method to pro-
vide new information on subjects as di-
verse as the recycling of polymers, mag-
netic recording media, and the cleanup
of oil spills. For someone like me who
has been associated with neutron scat-
tering for more than twenty years, the
birth of this new technique is a happy
event. It means that there are still qual-
itatively new ways in which neutrons
can help unravel the complex struc-
tures of the materials on which we de-
pend.
shorter is the neutron pulse and the
lower its intensity. The location of the
poison is the feature that primarily dis-
tinguishes the two ''high-intensity'' water
moderators at LANSCE from the single
''high-resolution'' water moderator. (The
liquid-hydrogen moderator contains no
poison.)
Figure 1 illustrates another important
point: Production of moderated neu-
trons is extremely wasteful. Most of the
spallation neutrons wander off into the
reflectors and the shielding without ever
encountering a moderator or a beam
tube. By changing the relative arrange-
ment of the various moderator, reflector,
and neutron-absorbing materials, it is
possible to increase somewhat the neu-
tron flux emerging from the moderators.
For example, the LANSCE split tar-
get yields a higher neutron flux than a
single target because it simultaneously
feeds neutrons into the moderators from
above and below. We are fortunate to
have access at the Laboratory to the best
computer codes-not to mention a few
Crays-to optimize the configuration
of the target assembly. The optimiza-
tion cannot be performed analytically;
Monte Carlo computations are the only
recourse for tracking neutrons and im-
proving the performance of spallation
neutron SOurces.
The LANSCE split target is unique
worldwide, thanks to the conceptual de-
sign of Gary Russell and his colleagues.
The assembly was installed in August
1985 and has since operated reliably
with no target or moderator changes.
What makes the LLANSCE source so
special is that it is very efficient and
very ''clean'': efficient because the mod-
erators are fed with neutrons by both the
upper and lower targets, and clean be-
cause only a small fraction of the spalla-
tion neutrons escape along a beam tube
without first being moderated. The lat-
ter point is important because unmoder-
ated neutrons, which are not useful for
neutron scattering, degrade experimenta.
results by contributing background
signals. More important, high-energy
neutrons can damage living cells and ar.
better kept within the crypt.
The high-energy spallation neutrons
produced at LANSCE would pass right
through the biological bulk shield at a
reactor, which is typically made of con-
crete containing a neutron-absorbing
material such as boron. The LANSCE
crypt is surrounded by a 3.7-meter-thick
bulk shield containing a core of iron en-
circled by a layer of concrete. The bulk
shield, in combination with the nickel
reflectors within the crypt, reduces the
radiation exposure of researchers to very
low levels.
All neutron-scattering instruments
have certain common requirements.
on at LANSCE for a few days after the
experiment to complete an analysis, or
at least to get the data into a form more
convenient for analysis back home. In
most cases, the working relationship be-
tween visitors and LANSCE scientists
quickly develops into a true collabora-
tion, one in which ideas and knowledge
are freely exchanged.
LANSCE provides the target assem-
bly, the data-acquisition and -analysis
systems, the neutron-scattering instru-
ments, and the required support services.
The Medium Energy Physics Division
operates the PSR that delivers proton
pulses to LANSCE and the WNR fa-
cility. Support for LANSCE and PSR
is given by the Office of Basic Energy
Sciences of the DOE and by the Labo-
ratory. Operation of the LAMPF accel-
erator itself is handled by the Medium
Energy Physics Division with support
from the DOE-OER Office of High En-
ergy and Nuclear Physics.
In July 1989 the Laboratory's
neutron-scattering center was officially
dedicated in honor of former New Mex-
ico Congressman Manuel Lujan, Jr.
When now Secretary of Interior Lujan
arrived at LANSCE for the dedication
ceremonies, he noticed pictures of the
early days of Los Alamos hanging on
the wall. Secretary Lujan went straight
to the picture of the post exchange and
said he worked there making sodas dur-
ing the war. It seems appropriate that
a research center at Los Alamos should
be named for a native son of San Ilde-
fonso and long-term member of the
House Committee on Science, Space,
and Technology.
LANSCE has demonstrated the
unique capabilities of a high-intensity
pulsed neutron source and has fur-
thered the development and refine-
ment ot neutron-scattering instrumen-
tation. It is truly an outstanding tool for
research in many areas of condensed-
matter science. In addition, LANSCE
plays an important role in the techno-
logical advancements of our society and
provides a unique educational oppor-
tunity for graduate students in a wide
range of disciplines. m
The authors wish to thank the staff at LANSCE
who built the instrumentation described in this
article.
The biography of co-author Roger Pynn appears
on page 31.
chemicals, changes in the availability
of raw materials, potential restrictions
on the availability of noble-metal cat-
alysts, and the desire for new products
have pointed up the need for a clearer
understanding of catalytic processes.
Much of the research into cataly-
sis is directed toward metals, because
they catalyze many important reac-
tions. Metals may be catalytically ac-
tive in the form of finely divided parti-
cles, organometallic compounds in so-
lution, or ions bound to large biologi-
cally active molecules, such as enzymes.
The catalysis may be heterogeneotus in
the sense of involving more than one
phase (solid metal and gaseous reac-
tants, for example) or it may be homo-
geneotus in the sense of involving only
one phase (such as a solution ). What-
ever the form, when the metal binds to
a reactant molecule, it almost always
alters the chemical bonding in the re-
actant. If that alteration is favorable to
some particular reaction, then the metal
is a catalyst for that reaction.
To understand catalytic activity, or
to tailor a catalyst to do a specific job,
we need to know the individual steps in
the catalytic process in great detail. For
example, consider the hydrogenation of
ethylene to ethane,
which can serve as a prototype of re-
actions used in producing synthetic fu-
els. The production of synthetic fuel
from coal, for example, involves various
series of reactions, including the step-
wise hydrogenation of carbon to form
acetylene (HCCH), ethylene, and ethane.
as well as the stepwise hydrogenation
of carbon chains with more than two
carbon atoms. The hydrogenation of
ethylene shown above is a particularly
useful reaction to study because it can
be carried out at moderate temperatures
in the presence of a metal catalyst. The
various steps to the reaction are repre-
sented schematically in Fig. 1, and a
full understanding of the hydrogena-
tion process requires knowing many
details about each step. Knowledge of
the spatial relationships of the adsorbed
species and the metal atoms at the cat-
alyst surface may enable us to identify
reactive sites on the surface. Determi-
nation of the changes in bond angles
and distances of the reactant molecules
when they are adsorbed should make
it possible to understand theoretically
the changes in electronic structure that
occur when the reactants are activated.
Because the adsorbed species must dif-
fuse on the surface to react and form
new molecules, we need to know how
this occurs, (We might wonder, for ex-
ample, whether the adsorbed hydrogen
diffuses only over the surface of the
metal or also into its interior.) Finally,
identification of reaction intermediates
is crucial to understanding the entire
process,
Unfortunately, these details of struc-
ture and dynamics cannot easily be de-
termined in a ''real-world'' situation-
that is, during an actual catalytic reac-
tion. Catalytic processes usually pro-
ceed under conditions that preclude the
direct application of many powerful an-
alytical techniques-or at least make
such application very difficult. Consid-
erable effort has therefore been devoted
to the study of so-called model systems,
which are designed to reproduce the
critical relationships as accurately as
possible. One useful model system is
a single crystal of a metal for which
the surface arrangement of atoms is
known. Others that have been widely
used are synthetic molecules consisting
of a metal atom (or a cluster of metal
atoms) surrounded by stabilizing lig-
ands, usually carbonyl groups (CO) or
other more complex organic groups.
When a reactant molecule such as ethy-
lene or benzene binds to such a syn-
thetic molecule, we can assume that, to
some degree, the configuration of the
resulting complex resembles that of the
same reactant adsorbed on a metal sur-
face. The complex can be studied with
several spectroscopic techniques, and its
crystalline form can be characterized by
x-ray and neutron diffraction, which re-
veal details of its architecture with great
accuracy.
The more closely the properties of the
model system approximate the proper-
ties of the real-world system, of course,
the better. As a result, model systems
are often structurally modified to refine
their properties and bring them closer in
line with the more complex system of
interest. However, such modifications
can complicate the structural character-
ization of the model system. For exam-
ple, as the model system becomes larger
and more complex, the chances increase
that some portions of the molecule will
be disordered or less easily defined. The
necessity of modeling the disorder can
decrease the precision of the results for
the metal-hydrogen interaction, which
is the feature of most interest. In effect,
the results become slightly fuzzy and
less precise.
Besides being useful in the study of
catalysis, metal complexes are highly
suitable for theoretical studies of chemi-
cal bonding between the bound molecule
(ligand) and the metal atoms. They
are therefore of fundamental interest
to researchers studying chemical bonds
from first principles. Finally, metal-
cluster complexes can stabilize cer-
tain molecules that are unstable in pure
form. For example, cyclobutadiene can
be stabilized by binding to iron car-
bonyl, Fe(CO)y; and ethylidyne, CH4-C
(a highly reactive intermediate formed
in the hydrogenation of ethylene), can
be isolated by reacting with cobalt car-
bonyl to form the metal-cluster complex
CH4C-Co4CO)4.
The kind of information available
through the study of model compounds
is illustrated by the case of the clus-
ter compound HFeCo4(CO))4. Diffrac-
tion studies show that the single hydro-
gen atom is located at a site of three-
fold symmetry, that is, just outside
the triangle formed by the three cobalt
atoms (Fig. 2). The vibrational spec-
trum of hydrogen in this compound is
very similar to that of hydrogen atoms
chemisorbed on a nickel or a platinum
surface. Since the vibrational spectrum
of a molecule or atom strongly reflects
the way in which it is bound to other
atoms, the similarity here allows the
inference that hydrogen chemisorbed
on a catalyst surface is located at a site
of threefold symmetry. We can further
infer that the catalytically active sur-
face is the so-called (111) plane of the
metal, because that is the only crystal
plane having threefold symmetry. This
information could not have been easily
obtained in any direct way.
How does one then study the model
systems? There are many experimen-
tal techniques, each especially suited
for a particular aspect of the problem,
and neutron scattering is one of these.
However, even the most intense neu-
tron sources produce fluxes far below
those commonly available from sources
of photons (x rays, ultraviolet, visible
light, and infrared), and so neutron
scattering is not one of the principal
tools of surface science. Nevertheless,
when the systems include hydrogen
or molecules containing hydrogen-as
do the more important types of com-
pounds involved in industrial catalytic
processes-neutron scattering is ex-
tremely useful.
The singular utility of neutron scatter-
ing is in locating the all-important hy-
drogen atoms and highlighting the vibra-
tional and rotational motions associated
with them. This strength is a result of
the fact that neutrons scatter as strongly
from hydrogen as from most other el-
ements (see ''Neutron Scattering-A
Primer'' by Roger Pynn). Although it
is nearly impossible to ''see'' hydrogen
atoms in the presence of heavy met-
als using x rays, x-ray diffraction can
sometimes implicitly locate hydrogen
atoms bound to or interacting with metal
atoms. If a site in a metal complex is
usually filled, an apparent vacancy at
that site, together with other physical
and chemical evidence, can lead to the
inference that hydrogen occupies the po-
sition, Neutron scattering, however, is
needed to confirm the actual presence of
hydrogen. Thus, the structures of com-
pounds of interest are typically deter-
mined by first applying x-ray diffrac-
tion to locate the heavier atoms and
then neutron diffraction to obtain pre-
cise metal-hydrogen distances and bond
angles.
Historically, single-crystal neutron
diffraction has been more difficult than
x-ray diffraction. Neutrons can travel
large distances through material without
being scattered, so neutron diffraction
requires a much larger crystal. This
problem has been partly alleviated by
the availability of more intense sources
of neutrons. Furthermore, the time-of-
flight wavelength measurements possi-
ble at pulsed-accelerator-based neutron
sources makes all neutrons in each pulse
usable. Area detectors make it possi-
ble to collect large volumes of data at
one time and make feasible full struc-
tural determination from polycrystalline
material.
For the observation of molecular vi-
brations, optical techniques (infrared
absorption and Raman scattering) are
far more common and much easier to
use than neutron scattering. Once again,
however, the difference in the nature of
are possible. The vibrational spectrum
of hydrogen in a regular octahedral site
would show a single excitation, a triply
degenerate hydrogen-metal stretching
mode. If, however, the hydrogen were
to also move significantly off center,
additional peaks would appear in the
vibrational spectrum. It is in just such
cases that vibrational spectra are of
great value in obtaining structural infor-
mation. An inelastic neutron-scattering
study of the cesium salt of the same
cobalt cluster showed primarily a single
excitation at a frequency of 1056 recip-
rocal centimeters (cm'''), confirming the
central location of the hydride ligand in
the octahedral site.
Prompted by the results of some in-
frared spectroscopic studies that showed
interesting changes in the spectra of the
[HCo,(CO);4] cluster as the crystalline
environment was altered, we recently
investigated the vibrational spectrum
of the cluster combined with the very
much smaller counter ion K', The data,
shown in Fig. 4, were obtained by the
differential technique on the Filter Dif-
ference Spectrometer at the Manuel
Lujan, Jr. Neutron Scattering Center
(LANSCE) at Los Alamos. Two sam-
ples were measured, one with hydrogen
and one with deuterium as the ligand.
Because the scattering cross section for
deuterium is much smaller than that for
hydrogen, the vibrational spectrum of
the deuterated compound serves essen-
tially as a ''blank'' to be subtracted from
that of the protonated compound. The
resulting differential spectrum is free of
all the many vibrational modes of this
large molecule that do not involve mo-
tion of the hydrogen and thus highlights
the vibrational modes that do involve
hydrogen.
The features shown in the differential
spectrum can be immediately identitied
with hydrogen vibrations and suggest
that all the hydrogen atoms are by no
means located at the center of the oc-
tahedron of cobalt atoms. The broad
band in the region between 1050 and
1100 cm''' may certainly be assigned
to the stretching vibration of hydrogen
at the interstitial site, but the band at
950 cm T' must then be indicative of
hydrogen at a different site-one bridg-
ing either two or three cobalt atoms. In
either case, a second vibrational line at
higher frequency would be expected.
The data are not conclusive in this re-
spect, but if the band at 950 cm'T' is the
symmetric stretching vibration for the
doubly or triply bridged hydrogen, then
the high-frequency shoulder just above
1100 cm''' has about the expected fre-
quency for the asymmetric stretch of
triply bridged hydrogen.
The spectrum thus appears to reveal
an instance of the flurionality of the
hydride ligands in cluster compounds.
Fluxionality-commonly detected in
nuclear-magnetic-resonance studies-
refers to the movement of hydrogen
from one site to another. Because the
movement occurs on a time scale that is
many orders of magnitude greater than
the time scale of a typical vibration,
the hydrogen can be ''caught'' vibrating
rapidly at more than one site. However,
if the binding energy is much larger at
one site than at others, such fluxionality
is unlikely.
In any case, the remarkable result of
our studies is that the position of the
hydride ligand in these metal clusters
apparently depends on the nature of the
counter ion used to crystallize the com-
pound. This fact suggests that the bind-
ing strengths for hydrogen at the vari-
ous sites differ by only small amounts
and may, in fact, be affected by the
charge balance between the complex
ion and its counter ion. Such a conjec-
ture is needed to explain the observed
change in fluxionality of the hydrogen
atom in the cluster. Moreover, the con-
jecture is in agreement with nuclear-
magnetic-resonance observations of the
[HCo(CO);4]' ion in solution, which
show that the hydrogen can easily leave
the octahedron and exchange with pro-
tons of the solvent molecules.
The factors that govern fluxionality of
the hydride ligand in cluster compounds
may, of course, differ considerably from
those that determine the diffusion of hy-
drogen between the metal surface and,
as the H-H or W--H distances; com-
parison with experimental values then
serves as a check on the validity of the
theory. The current problem, however,
is sufficiently complex that structural
information is used as input to simpli-
fied theoretical models. Whether or not
the theoretical model is derived from
first principles or from a combination of
structural data and a theoretical model,
it is highly desirable to have other ex-
perimental information on the nature of
the chemical bonding that can be used
to gauge the theoretical picture.
The nature of the bonding between
the dihydrogen ligand and the transition
metal is of major significance because
the complex represents the first exam-
ple of a sigma-bond complex, that is,
a complex in which the ligand binds
through interaction of a metal center
with a o-bonding electron pair. The-
oretical studies of this three-center,
two-electron bond indicate that both
the bonding and antibonding orbitals of
hydrogen (Fig. 9) may be involved. The
primary interaction between Hy and the
metal atom is donation of electron den-
sity from the H-H o bond to an empty
orbital in the metal atom (Fig. 10); how-
ever, the same studies indicate that, to
a lesser degree, backbonding between
a metal orbital and the Hy antibonding
sigma orbital (o ') also occurs, Back-
bonding stabilizes the side-on orienta-
tion shown in Figs. S and 10 rather than
an end-on orientation (in which the Hs
molecule would have its bonding axis
pointed straight at the tungsten atom
with one hydrogen atom much closer
to the metal atom than the other). The
side-on coordination ultimately facili-
tates cleavage of the H-H bond to give
dihydride complexes in oxidative addi-
tion reactions. These theoretical predic-
tions, of course, require experimental
confirmation.
Rotational Dynamics. Hydrogen in
the side-on coordination mode can un-
dergo a remarkably wide variety of lig-
and dynamics, including torsional os-
cillations, or librations, about its equi-
librium orientation and much slower
180-degree reorientations by tunnel-
ing through the rotational barrier. Es-
tablishing the presence of a significant
electronic energy barrier to rotation
would provide confirmation of metal-
to-Hy backbonding. Such a barrier is
too small to be observed by standard
nuclear-magnetic-resonance techniques.
Inelastic neutron scattering, however,
is highly sensitive to hydrogen mo-
tions because of the very large neutron-
scattering cross section of protons and
the typically large amplitude of the mo-
tions. In fact, this technique is routinely
used to study rapid rotational motion
(for example, of methyl groups and of
solid or liquid hydrogen or molecular
hydrogen in zeolites).
The nature of the rotational motion of
the bound hydrogen molecule may be
described with the aid of a diagram that
shows the energy levels that the dihy-
drogen ligand may occupy as a function
of the height of the barrier hindering the
rotation. These levels are the solutions
to the Schrödinger equation chosen to
represent the rotational motion of the
bound hydrogen molecule. In particular,
the equation includes only one angular
degree of freedom because we assume
that the relatively strong three-center
metal-dihydrogen bond keeps the hydro-
gen ligand essentially in a plane during
its rotational motion. The complex may,
in fact, be the first example of hydro-
gen rotation with only one degree of
rotational freedom, a situation first de-
scribed by Pauling as an approximation
for solid hydrogen. If any mixing with
vibrational modes can also be neglected,
evident from the fact that they remain liquid all the way down to absolute zero.
In quantum mechanics the momenta of atoms in a finite-sized box are quantized
in integer multiples of WLT', where L is a dimension of the box. This quantization is
a consequence of the requirement that the wave function of an atom must be zero at
the boundaries of the box and therefore must have an integral number of nodes. Thus
the momentum distribution for the system is discontinuous (unlike the continuous
Maxwekl-Boltzman distribution of classical systems) and becomes continuous only for
a box of infinite size.
Now consider the momentum distribution for a non-interacting (ideal) gas of
spin-0 atoms. At high temperature the atoms are thermally excited, and the proba-
bility of an atom being in any particular momentum state is inversely proportional
to the size of the system (L') and proportional to a Maxwell-Boltzmann distribution.
However, as the temperature is reduced, the preference of bosons for occupying the
same momentum state causes deviations from the Maxwell-Boltzmann distribution.
As the temperature is further reduced below a critical Bose-condensation temperatitre,
a significant fraction of the atoms begin to occupy the lowest (or zero) momentum
state. The fraction, no, is called the Bose-condensate fraction, and its value is inde-
pendent of the size of the system. In the momentum distribution n; shows up as a
delta function, of weight no, at p = 0. The width 2p of the momentum distribution
for the remainder of the atoms is on the order of y/if,T and goes to zero at zero
temperature. On the other hand, tio approaches one as the temperature approaches
zero; that is, the entire system becomes a Bose condensate. Figure l shows plots of
l-n(k) versus k for a system of non-interacting bosons at various temperatures. (In
this article momentum p and the wave vector k sE p/ are used interchangeably. The
natural unit for wave-vector magnitudes is the inverse angstrom, A '.) Figure 2 is a
plot of the Bose-condensate fraction n versus temperature.
London reasoned that the superfluidity observed in 'He was a macroscopic con-
sequence of the microscopic Bose condensation of 'He atoms into the zero-
tion and the ''apparent'' momentum distribution that would be inferred by analyzing
final-state-broadened data as if the impulse approximation did not require corrections
for final-state effects. In the absence of final-state effects, the discontinuity in n(p)
would cause a sharp change in slope in J;4(T . 3) at f = py/l. Unfortunately, the
new theory of final-state effects predicts that such a change in slope will not be di-
rectly apparent in the data. Nevertheless, experiments can distinguish the HNC the-
ory from other theories that do not have a Fermi-surface discontinuity (such as the
pairing theory of Lhuillier and Bouchaud). The HNC theory would predict a different
Gaussian width for J (Y . 0) than would other theories. Experiments on 'He are par-
ticularly difficult because 'He is a strong absorber of neutrons (and is therefore the
primary component of thermal-neutron detectors! ). Nevertheless, we hope that theory
and experiment will resolve the question of the Fermi-surface discontinuity in 'He in
less time than the more than twenty years required to confirm the existence and size
of the Bose condensate in 'He.
At the beginning of this article, we mentioned that 'He becomes a superfluid
at 3 millikelvins. Does that mean a Bose condensate forms in *He despite the spin-
statistics relation'? The answer is no. Superfluidity in 'He is caused by the formation
of Cooper pairs of 'He atoms and is thus somewhat analogous to superconductivity
in metals, which is caused by formation of Cooper pairs of electrons. The differ-
ence is that the Cooper pairs of 'He atoms are uncharged and form a relative p-wave
bound state, whereas the Cooper pairs of electrons in metals are charged and usually
form a relative s-wave bound state.
The present experimental and theoretical techniques for determining momentum
distributions in helium can also be applied to questions concening the pressure and
temperature dependence of the condensate fraction, the non-existence of a Bose con-
densate in solid 'He, the non-existence of a Fermi surface in solid 'He, the behav-
ior of n(p) at high jp] (which many-body theories predict is exponential rather than
Gaussian even in the normal quantum liquid), the larger nm predicted for 'He in a
porous medium (which is expected to behave like a low-density Bose system), the
complex momentum distributions expected for mixtures of 'He and 'He, the pre-
dicted absence of a Bose condensate in the two-dimensional 'He systems produced
by physisorption of 'He on surfaces, and so on. All such experiments will benefit
from the epithermal neutrons provided by pulsed neutron sources or by hot sources
at reactors because they all require measurements at high (? values. Although con-
ditions suitable for applying the impulse approximation may never be reached, the
final-state corrections will be understood.
prior). In the coin-tossing problem, if we are completely ignorant about the coin, we
would assign a uniform prior, Pr(H ) = constant = 1 for all values of h between
O and 1, to indicate that a priori all possible values of H are equally probable. If
we do have other prior information, perhaps the results of previous data, then this
information should be reflected in the nonuniform character of Pr(H ). (Actually our
statement of Bayes' theorem should read Pr(H ][data},1) > Pr([data}]H .1 )x Pr(H ]I ).
where I represents other prior information or prior assumptions.) Figure 3a shows
three possible assignments for Pr(H/ ), each reflecting a different assumption about the
coin: the uniform, or ignorant, prior; a prior that assumes the coin is most likely to
be double-headed or double-tailed: and a prior that assumes the coin has a head and a
tail and is probably fair.
Having specified our prior, we need now to consider the other probability dis-
tribution in Bayes' theorem, Pr([data}]H ), which reflects the nature of the ''experi-
ment.'' This probability distribution can be computed because it involves deductive
logic. It is called the liielihood function because it tells us how likely it is that we
would have obtained the data that we did if we had been given the value of H, For
our problem we are told that a coin was flipped n times and came up heads r times,
If we assume that the data are independent (that is, the outcome of one flip did not
affect the result of another) and that the bias-weighting is H, then the likelihood
function is simply a binomial distribution:
where ''C, = n!/r!(n -- r )! is the number of ways of picking r objects (independent
of order) from a choice of n. (Figure 1 shows such a binomial distribution.)
Multiplying Pr(H ) and Pr({data}]H ). we obtain the posterior Pr(H ][data}),
which summarizes all that we can infer about the value of H given the data. Fig-
ure 3b shows how the posterior for each of the three priors in Fig. 3a changes as we
are given more and more data. The data in this example were generated by using a
random-number generator in a computer and setting H to (0.2. We find that as we ob-
tain more data, we become more confident in our prediction for the inferred value of
H (that is, the width of each posterior decreases) and our prior state of knowledge,
as expressed in Pr(H ). becomes less important (that is, no matter what our prior as-
Sumptions were, the posteriors converge to the same answer when enough data are
available).
The power of Bayes' theorem is that it effectively provides the only consistent
bridge between the inductive logic (or indirect probabilities) required for scientific
inference and the deductive logic (or direct probabilities) that we know how to use.
Generalizing, we see that Bayes' theorem encapsulates the process of ''learning'':
PrC'hypothesis''idata}.1) s Pr4data}|''hypothesis''.1) s. Prc'hypothesis'''1).
where hf,{)l is estimated from Eq. 7 and t(f,,) is estimated from Eq. 11. The unlagged
coherency function for stations aligned parallel and transverse to the epicentral direction are
plotted in Figures 14 and 15, respectively. For other station orientations, Eq. 12 should be
evaluated. The unlagged coherency for randomly oriented stations is shown in Figure 16. If
the epicentral direction is not known, then the curves in Figure 16 should be used.
accelerogram. Because of the difficulties associated with
recovery of the record, these response spectra are not
uniquely determined. As with the acceleration and velocity
traces, qualitative uses may be permitted only with the
utmost caution.
Figures 4 and 5 compare the Victoria record with
several other well known accelerograms, all plotted on the
same scale. The recording at Victoria stands out
particularly for its sustained duration of intense
horizontal accelerations, and for its sustained high level
of high vertical acceleration.
Maximum horizontal accelerations of .98g and .87g, and
a duration of shaking of approximately 15 seconds, are
comparable to those for the recent Superstition Hills
earthquake (.9g ,.7g and 17 sec.), described as ''among the
largest and longest ever recorded in the 55 year history of
the program'' (Porcella et. al., 1987). Comparison of the
records (Fig. 5) shows many similarities, and suggests that
similar records are to be expected in this type of geologic
environment. Peak horizontal accelerations for both records
Within the expected experimental errors, the agreement
is sufficiently good that we consider the use of the foam
rubber modeling technique to be validated for modeling the
response of more complex structures.
Foam rubber of the 1.8 lb/cf (0.029 gm/cc) class comes
in a wide range of elastic wave velocities and is,
therefore, a good candidate for modeling layers of soil in
the earth. To construct the model, foam types with relative
shear moduli representing the change in the earth's shear
moduli with depth at the TSB site are selected using data
obtained by SW/AA through their 1976 geotechnical
investigation (Figure 5). The thickness of the layers is
determined once the scale is decided. The scale is defined
by the ratio of the wavelength in the model to that in the
earth, that is,
where k. is a typical wavelength in the earth, and k,, is the
corresponding wavelength in the model. With
then
where B and v are the shear wave velocity and frequency
respectively. Therefore, frequencies in the model are
related to those in the earth by
Societies that exist in seismic regions must deal with how to improve
the earthquake safety of existing buildings. Design standards for new
buildings are continually being modified to incorporate the latest
and mechanism of their second and largest source to represent the mainshock in
his paper. The aftershock that occurred on 4 October 1987 was located 2 km
northwest of the mainshock at a depth of 13.3 km, with a strike-slip mechanism
on a vertical plane (Haukkson and Jones, 1989).
The 11 stations for which CDMG digitized strong motion records of both the
mainshock and the aftershock are shown in Figure 1. The stations range from
almost directly above the earthquakes to 50 km distant. Five of the stations are
sited in the free field (157, 196, 303, 399, and 436); the other six stations (368, 400,
401, 402, 403, 461) are sited in one-story buildings.
Acceleration spectra for the mainshock and aftershock are shown in Figure 2.
The S waves are windowed into 4 sec and 2.5 sec segments for the mainshock
and aftershock, respectively, and tapered, and then Fourier transformed. The
frequency of the peak acceleration ranges from 1 Hz for station 368 (at Downey) to
6-8 Hz for station 399 (Mt. Wilson). In general, the two spectra from each of the
11 stations are similar, although the mainshock generated a higher level of
acceleration. Since Mt. Wilson is a hard rock site 18 km from the epicenters, the
spectra at this station may represent the source spectra with relatively little
contribution from path and site effects (Houston and Kanamori, 1990), although,
as we discuss below, there is evidence for a directional site resonance near 8 H2z at
Mt. Wilson.
at the 1/4-scale structure is about 65', but due to the
refraction at the layer boundaries, the incident angle will
be smaller. For comparison we assume the angle of incidence
in the earth is the same as in the model.
Due to the symmetry of the problem, we may compare
results of the foam rubber (recorded along ARM--1) with the
observation (recorded along ARM-2). Fig. 8 for event 14
shows the comparison of the observed transfer functions with
the corresponding ones obtained in the foam rubber model.
The agreement is good when the low strain assumption is
used.
The fact that the high strain assumption does not
adequately predict the building response during the May 20,
1986 earthquake (event 7) has been attributed to the
existence of stiff backfill (2]. They argue that due to the
stiffness of the backfill, the resonant frequency of the top
layer (' 2.8 hz) does not feed energy into the building,
thus an apparent reduction of the building response around
that frequency.
There is no reliable data available for the properties
of the backfill. However, at the suggestion of Enrique Luco,
the backfill was constructed by replacing the soft foam
around the containment with a stiffer one (shear modulus of
backfill is approximatley three times the shear modulus in
the top layer) . Fig. 1 shows the plan view of the
backfill. Fig. 9 shows the transfer functions from the
control motion to the top of the containment for events 6,
7, 8, 14, and 16. The dashed lines represent the respective
transfer functions in the foam-rubber model with the stiff
backfill. For small earthquakes (events 6, 8, and 14) the
introduction of a stiff backfill does not improve the foam-
rubber prediction; on the contrary, it worsens the
agreement. For events 7, and 16 the amplitude of the
predicted transfer functions (high strain model) seems to
agree well with the observation. However, there are
significant differences between the shape of the predicted
and observed transfer functions.
At this point it is difficult to decide whether the
existing backfill at the Lotung site is stiffer than the
surrounding soil. The forced vibration test in the presence
of a still backfill has a resonant peak at about 4.9 hz, 39
percent higher than the observed peak. Also, the absence of
4 stiff backfill is suggested by the predicted transfer
functions for small earthquakes (events 6, 8, and 14). In
foam-rubber model the structure is perfectly bonded to the
9il, therefore the previous argument, supporting the
absence of a stirr backfill breaks down if there were a gap
as shown on Figure 2. This is consistent with the results of Idriss, who found definite
jfferences in spectral shape between deep and shallow focus earthquakes (Idriss, 1978).
These differences are thought to be due to reduced inelastic attenuation, greater stress
drop, and general suppression of surface waves for earthquakes with greater focal depths
(Crouse, et al., 1988; Idriss, 1978). These differences should cause an increase in the
high frequency components and a decrease in the longer period components of ground
motion from subduction zone earthquakes compared to shallow-focus earthquakes. This
means that Joyner and Boore's predictive equations should underestimate the response
at short periods and overestimate it at long periods for deeper focus earthquakes. This
is demonstrated in Figure 2.
It would be possible to use the predictive equations to estimate a target response
for the input time history if the effects of the soils could be subtracted. Kawashima's
predictive equations for subduction zone earthquakes (Kawashima, et al., 1984) for rock
and soil sites were compared to estimate the effects of soils on base rock motion. The
spectra constructed from Kawashima's predictive equations tend to have a greater
response, especially in the high frequency range than do spectra measured from strike
slip events. A sample comparison for a magnitude 8 earthquake at a depth of 31 miles
(50 km) is shown on Figure 3. This may not be representative of the actual differences
due to the method used in this analysis, but a trend is evident. Amplification occurs for
soil sites at longer periods with the maximum amplification occurring around a period
of 10 second. Comparison of curves developed from the Joyner and Boore predictive
equations for rock and soil sites shows the same general effects (see Figure 3). These
results can be compared to those obtained by Seed, et al. (1976) and Mohraz (1976),
who show not only an increase in longer period ordinates, but a decrease in higher
frequency ordinates as the soil deposits become thicker and/or softer, as can be seen in
Figure 4. This would imply that the response of the input time history selected should
have larger ordinates at higher frequencies and lower ordinates at longer periods than
the corresponding response on soil sites.
To establish an appropriate target response for the input motion from the
predictive equations for subduction zone earthquakes, it would be appropriate to
increase the high frequency components and decrease the long period components to
account for effects of soils. With respect to the Joyner and Boore equations, the target
tresponse should have a shape intermediate between rock and soil with larger high
frequency ordinates and smaller long period ordinates. A comparison of standard
sectral shapes shows that the spectral shape developed by Seed et al. (1976) for stiff
soil conditions generally meets these requirements, as shown on Figure 5.
The question that remains is whether to have more than one spectral shape. Two
Areas require consideration: the difference in spectral shapes between deep and shallow
arthquakes, and the difference between near and far earthquakes. While there are
Aparent differences between the spectra of deep and shallow earthquakes, most
thquakes occurring in the Puget Sound area are at depths greater that 6 miles (10
k). 0ss, eswou4eeneras beesonzeaaaeef sas,Tea sannge
offocal dephs to consider, but he response trom ue larger, deeper evens encompasses
in which Uj, denotes the Fourier transform of he longitudinal or tmnsverse motion recorded
atthe center station on the i level during the j test (Table 3).
The estimates of the normalized rocking response of the base in the longitudinal and
transverse directions were calculated from
where Vi; and V,; are the Fourier transforms of uhhe recorded vertical motion at the east and
+%Wää4GäS4i4ä44G5444,G4.
represent the corresponding vertical motions at the north and south stations on the basement
during the first transverse test (Test no.4). The other constants are H 37.40 m, !; =
13.55 m and t; = 36 m.
region the corresponding ground motion is amplified the most. Based on this, Nes]
mark and Hall (1969) estimated constant amplification factors for the acceleratioj
velocity and displacement period ranges using accelerograms recorded during thgg
1934 and 1940 El Centro, California earthquakes and the 1952 Kern County, Cl
fornia earthquake. Multiplication of the peak ground motions by these amplificatk4
factors resulted in acceleration, velocity and displacement spectral ordinates whi
can be plotted on four-way log paper to produce a smooth response spectrum.
Following the 1971 San Fernando earthquake, more records became availd4
making it possible to obtain better statistical estimates of amplification factogi
(Blume et al, 1972; Newmark et al, 1973). These studies resulted in 50th and 8i
percentile amplification factors for peak acceleration, velocity and displacementii
for several damping ratios. The distribution of the amplification factors was fousi
to be lognormal so that the 50th percentile corresponds to the median and the 8
percentile corresponds to the median plus one standard deviation. Further studi
in the mid 1970s resulted in more refined estimates of amplification factors (Hali
al, 1976; Newmark and Hall, 1978). These are given in Newmark and Hall (198
and reproduced in Table 1.
There are at least two variants of the original Newmark-Hall method of oogi
puting a scaled spectrum. These are procedures A and B in Table 2. In additioni
methods A and B, which scale amplification factors by peak ground acceleratioi
velocity and displacement in different period ranges, a scaled spectrum mayel4
be computed by procedure C in Table 2 where amplification factors are scaledi
peak ground acceleration only. Typically these amplification factors are presenßi
in graphical form as plots of P$A for unit gravitational acceleration versus natui
Fig. 5, which is simil4r to an approach suggested for bridge columns by the Applicd
Tächnology Council [20] and others [21]. Three situations are identified in Fig. 5. When
he shear corresponding to flexural strength V;; exceeds V,, from Eq. (14a), a brittle shear
failure is expected. The strength is V4, and' the ductility is u = 1. When the shear
associated with flexural strength is Vt $ V; s V4,then the strength is V:, with ductility
given by
When the shear corresponding to flexural strength is Vr; S Vug, then the strength is Vr;
with full ductility. That is, y = 6 providing 'good'' detailing exists.
The ductility capacity of a beam sway mechanism should not necessarily be taken as the
lowest ductility of any hinge in the mechanism. Further, it is contended that gravity load
moments should be ignored in assessing ductility capacity (but not potential hinge location).
On the first count, exceeding the ductility capacity of a single hinge in a beam sway
mechanism is unlikely to be critical. Consider the situation in Fig. 6 where flexural hinges
wih ductility capacity u = 6 form at all except one beam end at a given level (column 3)
where a flexurelshear failure is predicted, in accordance with Eq. (15) at u = 3. A
9onservative approach would limit the ductility of the mechanism to that of its weakest link,
k.1 = 3. Provided it is assessed hat the shear strengh will not degrade below the gravity
s4shear foree (say VnL < Vie from Eq.(14b)), die trame may &: able wo defori o ik
fwl4wcdi6,albei'a edcea iengm, widoui colspse. For säse ot argumenu assume
w he fiesural srengh of each hiige, including hiige 32 (see Fig. (6)ipnor o shear
= =4. +s+&äi ä+WGü4is6isi iisY..='s -i.
3swaeneaswc mebanismsoengo wiiepoporonaiwo e poaiGGäie aueniiy
+m=oeäääiäisiäiG+iiää,iZ.
y%5 four 230-kV lines, Binga 1 and 2 and Bauang 1 and 2. The substation
double-bus and circuit-breaker-and-half configuration. There are three
s between the buses, each with two taps and three circuit breakers for a
pf six taps: one tap for each of the four transmission lines and one for each
$ transformer feeds. The design of the substation is similar to many in the
4 generally incorporates good seismic-design practice. Some features
p be an improvement to U.S. practice. Within the substation, almost all bus
tions use flexible bus that is generally supplied with adequate slack be-
;uipment. Disconnect switches are supported from ground-mounted
$9es constructed from steel lattice. Equipment and bus-support structures
%olted with castin-place bolts in concrete ioundations. For esample, a bus-
t structure foundation was reported to be about 2.4 meters deep, and the
ure was secured by fve 2,5-cm cast bolts at each of the corners (see Figure
Allequipment had irames boled directlv to foundation slabs, and no iicon
rere used. Except as noted below, there were no signs of foundation settle-
t or motion.
natuan lost two major buildings: a college library and a multistory classroom
ding. A muricipal building collapsed in Agoo.
After several days, organized search-and-rescue efforts shifted to the moun-
ous regions north of Manila, where landslides buried sections of vital high-
ys, trapped motorists, and damaged small villages, Other ad hoc
iich-and-rescue attempts took place throughout damaged urban and rural areas
ree victims trapped in small-building collapses or buried by landslides.
Predictably, he bulk of rrescues were made by fellow building occupants and
al rescue personnel 4. 6). In Baguio, for instance. locally based Philippine
itary Academy cadets, along with gold and copper miners from the Benguet
poration and surrounding mining communities, arrived to search, shore up.
tunnel through collapsed structures. For example, at the Nevada Hotel,
al shored, tunnel-like entrances were built by miners in rescue attempts
9wre93).
A lack of search-and-rescue training and adequate equipment hampered local
uers. Most early rescues were accomplished with hand implements. Philip-
$And U.S. military personnel (from Subic Bay and Clark AFB) later arrived in
anatuan with generators, torches, and heavy equipment to break up concrete,
.pinforcing bars, and assist with debris removal at the collapsed Christian
iege of he Phlippines
$Foreign heavy-rescue specialists began arriving at the disaster scenes on [uly
1990, with he U.S. Disaster Assistance and Rescue Team (DART). The British
mal International Rescue Corps, the Japanese International Rescue Group, the
nch nternational Medical Mission, and the Singaporean Civil Defense Team
fved shortly thereaier.
Chances for survival decreased as rescue time elapsed (Table 9.2). Eighty-four
Rrent of the survivors surveyed by the Department of Health Field Epidemiol-
9Training Trogram staff were rescued within the first hour, and 99% within the
st24 hours (16).
The Philippine earthqquake of [uly 16, 1990, underscores the need to examine
Atility and future ole other than symbolic) of foreign rescue teams. Here, as
disrupts the regular subduction process during which elsewhere the Cocos plate
slips beneath the Caribbean plate (see thrust mechanism tl of the M = 7.0 event of
03/25/1983). The overall convergence rate between the Cocos and Caribbean plates
isabout 9 cm/y, in a northeasterly direction.
Theridge collision seems to have recently disrupted the former, active volcanism of
the arc in the Cordillera de Talamanca. Farther northwest, active volcanos still
puncture the overriding Caribbean plate,forming segmentsof linear volcanicchains
extending from central Costa Rica into Nicaragua. The volcanos are markers for
where the subducting Cocos plate has plunged into the earth's upper mantle to a
depth of about 100 km or more.
Atpresent, the Cordillera de Talamanca appears to be an uplifted,remnant volcanic
arc that was volcanically active quite recently. Now, the Cordillera seems to be
undergoingcrustal shortening and thickening (maximum elevation of nearly 4km),
apparently caused partly by imbricate (northeastward dipping) frontal under-
thrusting of the Cocos Ridge beneath the Peninsula de Osa (PdO) and, in turn, by
thrusting of inlying crustal slivers beneath the Cordillera. In addition, there is clear
evidence for back-arc (southwestward dipping) thrusting of Caribbean plate ocean
crust beneath the Cordillera de Talamanca.
This south-west-directed back-arc thrusting is geologically prevalent in the
coastal area both offshore and on land, between about Puerto Limon (PLY) in the
northand the Panama border in the south (see Figure 2-1, thrust focal mechanisms
85,6and 7).
The subjectM = 7.5earthquake of04/22/91 and itsM =60aftershock of 05/04/
91 are both part of this crustal shortening process (source modeling is shown in
Fgure2-2).The coastal uplift of 0.2to 2m that has been reported for this major event
Sequence, from near Puerto Limon in the north to Panama in the south, is a clear
manifestation of thecrustal shorteningprocessin the back-arcregion, yetapparently
resulting from the distant impact of the Cocos Ridge in the fore-arc region.
The back-arc thrust, as inferred from the teleseismic focal mechanisms alone
0?igure 2-2), seems to dip at a gentle angle, less than about 30 degrees (to the
southwest); it implies that the ocean crust is slipping southwestward beneath the
northeastward overridingcoastal-plain sediments that make up the hanging wall of
the thrust. The thrust plane of this earthquake may either have reached the ocean
bottom surface some distance offshore, or may have ended in a ''blind thrust''
without reaching the surface.
Based on distribution of landslides, some early reports after the April 22
mainshock suggested that a steeply eastward dipping fault in the eastern foothills
(VValle de la Estrella) of the Cordillera de Talamanca was the causative fault. We do
not believe that this interpretation is correct. Rather, a sub-horizontal rupture plane
Wasforming beneath much of the coastal plain at depths shallower than 20 km. The
point of rupture nucleation (hypocenter) was at a depth of about 21 km, located at
the southwestern down-dip end of the rupture. Hence, much of the shaking in the
coastal plain and nearby eastern flanks of the Cordillera de Talamanca was gener-
ated by fault slip only 20 km away from, and almost directly below, the respective
points of impact at the surface. The sub-horizontal net static slip (before minus after)
lowerturbidities, treatment was very difficult. The rainy season was only two weeks
old, and would be expected to last into November. There was hope that the
extremely high turbidity was a transient condition, and that the loose soil would be
flushed away with further rain. Normal maximum turbidity prior to the earthquake
was 1,600 mg/l. Construction of a pre-sedimentation basin was being considered,
but will not be possible for three to six months. On April 26, when turbidities
increased, Cat-Floc polymer addition to the raw water was initiated to enhance
treatment. With the polymer,adequate treatment up to about 12,000 mg/lturbidity
was provided.
Atthetreatmentplant,anunrestrained 68 kg chlorinecylindertoppled,breaking
connecting piping, and releasing chlorine. One worker was injured by the chlorine.
One unanchored dry chemical feeder toppled, breaking connecting piping and
puncturing the bottom. It was repaired and reinstalled. Asbestos cement baffles in
the flocculators and asbestos cement Lamella clarifier plates were broken (Figure
7-8). One concrete channel cracked. On April 30, the plant had been restored to 135
lps flow,asa result of the damaged baffles and clarifier plates. Glassware in the lab
and the portable radio, siting on the shelf, were undamaged.
The 300 mm cast iron pipe had 20 to 30 failures, depending on the information
source. Damage rates range from 1.1 to 1.7 failures per km. Most were joint
separations on the order of 10-15 cm, with some joint offsets of about 20 cm. There
wereonlya few failures in the pipe barrel. The pipeline was functioning on 29 April,
butprovidingonly minimal water pressure to some sections of Limon.Of the 135lps
flow into the line, 100 lps was coming out the other end, so there were some
remaining leaks. Small diameter hoses were connected to the main in the city to
allow residentstoget water. A boil water order was in effect. It wasnoted thatduring
repair of pipelines within the city, trench dewatering was not being practiced, and
open pipe was being submerged in muddy water.
The pier strengthened frame was subjected to static cyclic loading (with
reversal) to a maximum interstory drift of approximately 0.5%, The strengthened
columns exhibited monolithic behavior, and shifted the governing failure mode to
flexural hinging of the spandrel beams. Flexural cracking at the ends of the spandrels
can be seen in Fig, 6. The maximum lateral load applied to the frame model was
1.75 times the predicted design nominal capacity, and more than 2.5 times the 1982
UBC design load (adjusted for scale). The available excess strength would result in a
reduced ductility demand for the frame. The test was stopped just prior to reaching
the frame's capacity (the frame was to be re-used for examining the second
strengthening scheme, described later), but the maximum applied load was judged to
be within 2-5% of that capacity. Using a plastic analysis based on formation of
hinges at the ends of the spandrels and accounting for strain hardening of
reinforcement, the calculated capacity was within 5% of the capacity observed in the
test. The observed initial stiffness of the strengthened frame was also 3 times that of
the original frame.
Excellent bond was exhibited between the old and new concrete, which
enhanced monolithic behavior. Due to the lug action which occurred within the
window segments, most measured dowel stresses were quite low, suggesting that
selected dowels could have been eliminated. However, dowels play an important role
in connecting new concrete to old concrete. The difficulty in predicting loads carried
by the dowels suggests that a conservative approach should be used in design. Test
observations indicated that the window segment dowels embedded in the column side
faces behaved as shear reinforcement, rather than as shear friction reinforcement.
These dowels restrained pier shear cracks from widening. Cracks also developed at
the top of the window segment (beneath the beam soffit) along the new/old concrete
interface. The cracks appeared to have been caused by inadequate compaction
rather than high stresses (the construction joint was just above this interface).
Presence of these cracks emphasized the need for good consolidation, which might be
achieved by drypacking the concrete beneath the beam soffits.
As shown in Fig. 3, the steel bracing system consisted of concentric X-bracing
(continuous over two stories) with horizontal collector members attached at the slab
level and vertical chords attached to the exterior column side faces. The two story
bracing configuration was chosen to avoid the problems associated with chevron
braces and to limit the unbraced lengths of the diagonals. The bracing system was
designed for the entire lateral load, which was the same as for the pier strengthened
frame, but with a 25% increase according to 1982 UBC provisions for bracing
systems. It was also necessary for the bracing system to limit drifts in order to
prevent excessive damage to the non-ductile columns. Selection of the number of
earthquake. By referring to Fig. 4, it can be observed that the fundamental period of the unit
in both directions is very close to the predominant period of the record No, 4 causing a quasi-
resonance situation.
In the 0'' configuration this high frequency response was characterized by an uplift of
panel FG and a rocking of the book shelf. In particular the rocking of the shelf increased
significantly after all the books have fell. Therefore the introduction of a ledge on the book
shelf is important to prevent the books from falling and cause potential injuries, Figure 6 shows
a photograph of the unit at the completion of test 4D where 100% of the books fell off the book
shelf. The final position of the unit at the completion of the 0'' configuration test series is shown
in Fig. 7 after 17 repetitions, the lateral shift of the unit was about 100 mm while the
longitudinal shift was about 25 mm. These motions are quite small and should not result in
injuries.
In the 45'' degree configuration the high frequency response was characterized mainly
by vibrations of both pedestals and very small motions of the complete unit.
After completing the initial series of tests (tests 1 to 5 and 6 to 10), the amplitude of
record No. 4 was magnified in an attempt to detect a possible mode of failure of the unit. No
structural damage was detected in both configurations even under a very large peak acceleration
of 1009 of gravity (tests 4E and 9D).
In an attempt to further identify a potential mode of structural failure of the unit, high
level resonant tests were performed for both orientations.
The energy balance in a building can be expressed as follows:
extended the entire story height in Festival (Fig. 10(a))and the floor slab was used to link
the sections of wall. In Torres del Sol (Fig. 10(b)) lintel beams were located over the
openings and web reinforcement was provided in the beams. The locations of the
openings were staggered in Torres de Miramar (Fig. 10(c)), however, the reinforcement
details were similar. The wall reinforcement was oriented at 45' to vertical in a few
buildings in regions of high shear stress (Fig. 10(b)), but in most cases the web
reinforcing bars ran horizontally and vertically.
Engineers tend to use the standard reinforcement details described in a publication
by the domestic rebar producer (Barras). The recommended details for development
length, lap splices, hooks, and bar spacing are the same as ACI318 (1983)requirements
for nonseismic zones.
Strong-motion records from 31 stations were obtained during the 1985 earthquake
(Fig. 2)(Saragoniet al. 1985). The epicenter was located approximately 80 km (50 miles)
southwest of Vihia del Mar. A strong-motion instrument was located in the basement of
a 10-story building in downtown Vihia del Mar. Acceleration records from the site are
plotted in Fig. 11. Peak accelerations of 0.36g in the S20W direction and 0.23g in the
N70W direction were recorded. Both componentsincluded approximately 40 seconds of
motion in which the horizontal accelerations exceeded 0.lg.
The influence of exposure conditions on the optical properties of the
cover materials studied is summarized in Tables 3, 4 and 5. The solar trans-
mittance of cover materials E, G, L and M was affected by exposure to high
humidity at both 70C and 90fC. Materials J and N showed some whitening,
a corresponding loss in transmittance and an increase in brittleness when
exposed to high humidity at 90C and material H became very brittle and
broke into little flakes after 2000 hours of exposure at this temperature.
Pronounced changes in transmittance were measured for materials G and
M at 90C and 125C. The transmittance of materials E. G and M changed
considerably after exposure to xenon-arc radiation in a chamber maintained
at 90C. Outdoor real-time exposure of cover material G on both full-size
8tagnating collectors and cover mini-boxes at all four outdoor sites resulted
tained for about one month and followed by a further strong increase.
Finally, a slow decrease is observed, towards a S/Ca value intermediate be-
tween the maximum and the first plateau. The latter phenomenon is likely
to be a consequence of the exposure of specimens to rain, given that the
solubility of CaSO, 2H4O is nearly two hundred times higher than that of
CaCOg. It has been estimated that, in urban sites, 50-100 um per year of
material is washed out (Grasserbauer, 1983). The relationship of the S/Ca
ratio with time suggests that the formation of gypsum at the surface of
marble is a multistep process, each step having a different activation energy.
The clarification of the reaction kinetics of marble sulphation is beyond
the scope of the present work. We established, however, that SO4 attack of
the marble surface is a rapid, straightforward process under usual conditions
of atmospheric exposure. Moreover, we demonstrated that the XPS surface
analysis technique constitutes a powerful method for investigating degrada-
tion phenomena of building and monument materials.
This work is part of a study on the state of conservation of Colonna Anto-
nina in Rome, programmed and financed by Istituto Centrale del Restauro
(Rome, Italy). I.C.R. kindly gave permission for the present results to be
published.
According to the explanations stated above, it becomes obvious that in
principle there are at least three ways to record infrared radiation:
The classical infrared method is based on the first possibility mentioned
(Fig. 1). The thermal image of the body being investigated shows the pattern
o the surface temperatures. To transform radiation readings into tempera-
ture data, the emissivity of the surface must be taken into account. Also,
when applying this method one should keep in mind that the heat radiated
is always measured. This implies that possibly a transient heat flux must be
considered, and that local damage, for instance, can only be detected if a
significant temperature differential exists between the sound and the un-
sound area. Under an ambient air temperature of 20'C the resolution of an
infrared detector is about 0.2'C,
The second way of recording infrared radiation is independent of either
the body's surface temperature or temperature differentials. In this case a
radiator is used with appropriate wavelength, for instance, about 3 um (Fig.
2). The radiation of this tool most not heat the surface of the body. It is
only necessary that the positions of camera and radiator are chosen in such a
manner that the radiation from the radiator is reflected on the surface of the
body or area to be investigated and then arrives at the camera. The proce-
aure is called infrared reflectography.
Another property of infrared radiation has not been mentioned so far. In
Special cases electromagnetic waves are able to penetrate a thin layer - the
outer layers of a painting, for instance -- and reflect from the second or
other layers underneath (Fig. 3). This third method of infrared thermo-
graphy also requires a suitable radiator. Remarkable results have already
been achieved using the technique for investigating old paintings (van As-
peren de Boer, 1970), because so-called underdrawings can be made visible.
exposed GRP showed similar patterns of degradation and it is notable that
not only do changes take place faster at Dubai but also, with the exception
of gloss, to a greater extent. The stronger and longer radiation and the
less persistent presence of moisture account for the difference. Some mate-
rials performed well enough to be acceptable for roof- and wall-lighting
purposes.
The effect of pigmentation on the weathering of GRP is to increase its
susceptibility to the action of moisture on the surface, though not necessarily
to an unacceptable level. Sheeting made with pigmented resins was found
to change appearance on weathering, and there was significant loss of weight
with some samples. However, gel-coats designed to have better weat.. er
resistance were used in a new set of trial panels, and the results so far are
encouraging. This performance is confirmed by the evidence of white gel-
coated GRP panels performing satisfactorily in exposed positions on a
number of prominent buildings in the Middle East.
The use of glass-fibre reinforced cement (GRC) has increased quite rapidly
in the Middle East in the past few years, for a variety of applications. To
obtain first-hand information about its weathering behaviour, sheets of
GRC of different composition were prepared in which spraying and pre-
mixing techniques were used to incorporate Cem-Fil 2 AR glass fibre;
these sheets were then exposed at the Dubai site at angles of 90' and 15' ;o
the horizontal, facing south.
Sets of these sheets were recalled after l and 3 years exposure, and
appropriate-sized test specimens cut from them. Flexural properties were
determined by subjecting the specimens to uniform bending with four-
point loading. From the results, modulus of rupture (MOR), limit of propor-
tionality (LOP) and Young's modulus in bending were determined. Izod
tests were used to determine impact strength. The results were studied and
compared with the results from similar sheets exposed in the U.K. at Garston,
Hertfordshire, and Lathom, Lancashire.
After three years exposure at Dubai, significant reductions were ob-
served in MOR and impact strength (from their 28-day laboratory-cured
values) for all types of GRC. The specimens made by spray-up without
dewatering retained 75% and 40% of initial MOR and impact strength.
respectively, whereas the retentions for spray-dewatered material we'e
65% and 35%, respectively. For the same period, samples exposed und r
temperate conditions in the U.K. showed smaller reductions, retaining
85-90% and 50-60%, respectively, of their initial MOR and impact-strength
values.
There was considerable variability in the LOP values obtained for Dubai:
exposed samples. It appears that the LOP in bending of some types of GRC
showed a reduction of 15-20% from its 28-day values of 7-10 MN/m'
A'though the rate of deterioration of monuments has greatly increased,
Bpecially in recent decades, it is still too slow to provide abundant data
oncerning deterioration mechanisms or to evaluate the efficiency of con-
aervation treatments. This fact, together with the randomness and variety
of environmental factors, always acting simultaneously, makes it difficult
to establish the influence that each has in the deterioration.
The accelerated weathering tests were conducted with the aim of over-
toming these problems by increasing manifold the aggressivity of the simu-
l4ted environment to which the samples are exposed, and by studying each
riable both separately and in combination. Then, a compromise is made
sformance impact tests may be defined as those involving whole or
pieces of end products, the method of test being designed to simulate
isrd likely to occur in service. The procedure is usually a drop-weight or
specimen test. They provide a measure of impact resistance involving
crack initiation and growth. Data are usually variable and require
itical treatment from a large population of tests. As constant energy
f they are becoming widely used for defining 'pass' or 'fail' criteria in
ications. Such a test has been adopted in the Australian Standards
2376 Parts 1 and 2 as a quality control test, with no pretention to
iite performance in service. It is known as the 'Gardner' test and has
id favour in such applications because of its simplicity (Abolino, 1973).
iunrestrained specimens are supported on a base with a cylindrical hole,
re struck via a falling mass hitting a hammer positioned over the hole
esting on the specimen. The impact causes plastics specimens either to
t (dent), fracture, hole, or shatter.
itest that is more simulative of practice is that described in British
aird 4203-1967, 'Extruded Rigid PVC Corrugated Sheeting'. This test
ires the full-size sheet, loosely supported at 914 mm centres, to sustain
out fracture a ball of mass ).89 kg and diameter 60 mm falling on it
i;a height of 1829 mm. It mav be noted that the lack of fixings is not in
rdance with conditions of sheet use in practice, and that the radius of
ball limits the impact area to the crests only, for most sheet profiles.
other variant of this test is that applied to rigid plastics pipes and
ibed in Australian Standard AS 1254-1973, 'Unplasticized PVC (UPVC)
and Fittings for Storm or Surface Water Applications'. A falling tup is
ias a striker which enables projectiles of variable mass and constant head
ieter to be used. A long guide tube also permits more accurate aiming of
irojectile. When applied to rigid plastics sheets, it is possible to impact at
t the crest or trough of the sheet profile. It has therefore been adapted
amine currently available sheets fixed to timber supports and simulating
roof lights.
fliminary testing showed that the fising and side support of the plastics
made a marked difference to their behaviour under impact. Unre-
ed sheets, as in BS 4203, grossly deformed under loads that caused
gto shatter under the same impact when restrained. Sheets were there-
$estrained by fixing at each trough to purlins spaced at 1.07 m, and by
buously supporting sheet edges with angle iron to simulate the side
ort of metal roofing. Sheets were struck at both crests and troughs of
es, and at centre and quarter points between 'purlins'. Where no
ing was induced, up to 4 impacts were applied to different locations on
9ecimen. Results are given in Tables 5 and 6.
e tables give data for transitions in specimen behaviour and do not
t all the data obtained. The data generally represent results from about
plicates. Owing to the small population of specimens available, statistical
$nent such as that reported later for 'Gardner' impacts was not possible.
specimen, the unbonded seam interface factor was investigated primarily
with the ultrasonic pulse echo method (Table 1).
Without any interfacial layer or with the talc present in the interface,
the ultrasonic pulse echo method confirmed that the specimen contained
an unbonded seam (Table 1). As expected, with the film of water between
the unadhered sheets, the response of this method was the same as if the
sheets were bonded together. Again, this was due to the presence of
the water which provided a coupling medium for ultrasonic pulse echo
measurements. In addition, the infrared method did not indicate the presence
of any defect between the unbonded sheets having the film of water between
them. This observation was not attributed to the presence of the water
but to the uniformity of the test specimen, and the uniform heat flow
across its entire surface.
Single-ply membraes are used over a variety of substrates in service.
Thus, the response of the seam specimens with voids on different supporting
panels was investigatad. The results indicated that the voids were detected
by both NDE metho-is when the specimen was placed over insulation, wood,
and metal panels.
This laboratory study investigated the use of the ultrasonic pulse echo
pethod and infrared thermography method to detect voids and delamin-
tions nondestructively in seams of single-ply roofing membranes. Voids
ere intentionally incorporated in seam specimens prepared using EPDM
$heet membrane material. A number of factors affecting the response of
the N1DE methods was examined including: pressure applied during seam
fabrication; void size; water in the void; temperature; unbonded seam
nterfaces; and supporting panel under the seam. The results indicated that,
Bnder certain laboratory conditions, both NDE methods were successful
ih locating hidden voids and delaminations in seam specimens. Both the
ltrasonic pulse echo and the infrared thermography methods need further
fesearch before their reliability in locating defects is ascertained and their
tesults can be more fully interpreted.
In particular, in the case of the ultrasonic pulse echo method, a technique
or applying constant pressure to the transducer during measurements is
mheeded. Reasons for obtaining echo patterns with false indicators and
haultiple responses should be understood so that such echo patterns may
be eliminated or reliably interpreted. A practical limitation of the ultrasonic
pulse echo method is that water provides a coupling medium between
mnbonded sheets and voids. Thus, on a roof, if water were present, voids or
graph of degree d. Here, we will compare
the average number of relays (average dis-
tance) and the maximum number of relays
(diameter) for the proposed SR network of
degree 2, and 8 = 1 with those of other net-
works of degree 2.
In a regular graph of degree 22, one can
reach at best two nodes by a single relay
and at best four nodes by one more relay.
If every node is connected with any other
node by at best r relays, this graph is said
to have the minimum average distance and the
minimum diameter. Known as the optimal
graph, it is useful in the discussion of dis-
tance property of a network, although it
does not generally exist.
In this comparison, we use the Manhat-
tan Street Network (MSN) proposed for MAN
[5, 6] as an example of real networks of de-
gree 2. This network has properties similar
to the proposed SR network, while the struc-
ture of MSN is a torus and that of the SR
network is a ring as described in the follow-
ing:
Figures 7 and 8 depict calculated aver-
age distances and diameters for the proposed
SR, MAN, and the optimal graph that corre-
sponds to the lower bounds for networks of
this kind. These figures show that the SR
network is superior to the MSN network both
in distance and diameter.
In this section we will obtain the aver-
age delay that is needed for a packet that
has occurred at a source node to be received
at its destination node. The following
assumptions were made for the derivation:
Taking into consideration that the pro-
posed network is uniform, the average delay
is given by
where W, is the average waiting time at a
source buffer; W,, is the average waiting time
at an insertion buffer; R is average number
of relays; E[T,,] is the average packet
is increased (and hence the relative band-
width is increased).
In comparison to other characteristics,
the measured values for the sintered ferrite
are not always within the range of the model
,, 8 - 16). This is because the normal-
ized center frequency is affected relatively
strongly by the material parameter p.
A new method was proposed for determin-
ing the matching point of a single-layered
electromagnetic absorber. By this method,
the characteristics of the ferrite electro-
magne tic absorber wre discussed systemati-
cally.
In the conventional methods, a two-
variable complex transcendental equation
must be solved from the viewpoint of imped-
ance ma tching. On the other hand, in the
present paper, the reflection coefficient is
used. It is shown that the matching condi-
tion can be separated into the determining
equation for the matching frequency and that
for the matching thickness. Hence, only one
variable real equation has to be solved.
Further, the normalized frequency is defined
in this method so that the material parame-
ters can be reduced for convenience of sys-
tematic study.
A model was introduced for the material
dispersion of ferrite. Nomographs are given
for the matching frequency, matching thick-
ness, relative bandwidth and center frequen-
cy. It is shown that the conventional empir-
ical findings can be derived systematically
from the relaxation-type dispersion charac-
teristics of the permeability. The relation-
ship between the permittivity and the
absorber characteristics neglected to date
has now been given. A possibility of fur-
ther increase of the bandwidth is shown by
the use of a material with a lower permittiv-
ity.
This possibility is of practical im-
portance and is presently being explored
experimentally. Future topics of study in-
clude the discussions on the matching for an
oblique incidence and the problem of multi-
layered structures.
With the diversified development of com-
unica tion services, several cases are pro-
duced wtere terminals with different trans-
mission rates and transmission schemes are
installed in premises, and the distribution
networks are provided for each of those
kinds of terminals. With this as the back-
gEound, a distribution system is considered
[1, 2] which integrates a number of differ-
ent distribution networks, aiming at dissolv-
ing wiring congestion. Utilizing an elabor-
ate core-switching function, the flexibility
of the distribution network is enhanced,
and the installation and wiring management
is facilitated.
Such a distribution system is composed
primarily of metallic cable, and is not
suited to the high-speed wideband premises
transmission. Another problem is that the
design of the distribution network must be
made manually according to the placement of
terminal devices, as in the traditional sys-
tem. Neither is it always true that the de-
signed wiring configuration is the optimal
one. Especially when the wiring is of a
large scale, the design is improved aiming
at the optimization of the design, 4. e., the
minimization of the installation cost.
Still the improvement remains to be local,
and the global optimization is difficult.
Because of such a situation, if one can
know beforehand the configuration of the
ideal network which will result from the
global optimization, the design of the dis-
tribution network will drastically be made
easy with a large economical advan tage.
Such a knowledge will not only be applied to
the existing distribution systems, but also
to new installations. The design condition
for the distribution system, such as the
scale of nodes and core series of cable, can
be optimized. This will establish eventually
the design method for the distribution sys-
tem itself.
TIn contrast to the premises wiring net-
work, the distribution network for the sub-
scriber is constructed so that the differ-
ence between the predicted demand and the
installation is minimized by applying proba-
bility theory, as in the cases of free wir-
ing 3] and feeder cable wiring [4]. The
method is effective in the design for inde-
terminate demand. On the other hand, in the
cases of the premises wiring network, the
demand usually is defined from the beginning,
and the accuracy of the demand prediction
is not a problem. Then the optimal
The complexity of the generation of the
W set is proportional to mln. The complexity
of the generation of the LS sequence is pro-
portional to nm)E, which is the number of
branches of the LS sequence. The complexity
of the generation of the LT sequence is pro-
portional to rmwu, which is the number of
branches of the LT sequence. Since the upper
bound of u is (m-1), the total complexity of
the generation using the SW method is the
larger of m3 or m2n.
We examined the following four points
for evaluation.
The applicability of the generation
method is good, as long as the conditions
for the generation are easy.
This point is for the complexity of the
test sequence generation, but the complexity
of the transfer sequence generation is ex-
cluded because the complexity of the transfer
sequence is nearly equal in every method.
This is the upper bound on the valida-
tion and transfer sequence length.
Since the even.t ''Case 2'' is the comple-
mentary event of Case l, we obtain
Finally, the average numbber of slots required
for the transmission of an arbitrary packet
is given by
In case @.; l. according to Eq. U5.1),
E[ 8] is evaluated as follows:
Since packet transmission is always success-
ful at the downlink, the system performance
is determined only by the uplink parameter.
Similarly, in case of p = 1, we obtain
Since packet transmission is always success-
ful at the uplink, the system performance is
determined only by the downlink parameter.
In case of P.) P,, 1. obviously, it holds
that E[81 = 1 according to Eq. (15.2).
Figure 6 shows the throughput perform--
ances of the tandem Go-back-Rl and standard
Go-back-N schemes for f,, ' ; and round-trip
propagation delay D = 64. The performance
of the tandem scheme is always better than
that of the standard scheme. Approximations
by Mase et al. [4] are also shown.
Figure 7 shows the throughput perform-
ance of the tandem and standard Go-back-
schemes with successful packet transmission
probability p,, in the uplink as a parameter.
As $, decreases, the throughput also decreas-
es. This is because retransmission occurs
frequently at the uplink.
Figure 8 shows the throughput perform-
ance with respect to the round-trip
propagation delay D, where an essential char-
acteristic that higher throughput is obtained
with shorter propagation delay is observed.
Thus, the throughput performance of the
tandem Go-back-ll scheme, where a satellite
is equipped with a retransmission buffer, is
always better than that of the standard Go-
back-RA scheme, where a satellite is used
only as a repeater.
The throughput performance of a tandem
type Go-back-A ARO which can effectively
shorten the round-trip propagation delay is
studied. With this scheme, a retransmission
buffer is introduced into a satellite. Con-
sidering the effect of this buffer, an exact
formula for the throughput is derived, the
throughput performance is evaluated, and it
is shown that the tandem Go-back-ll ARO scheme
is always better than the standard Go-back-
ARO scheme in throughput performance.
The tandem Go-back-l ARO scheme applied
to a satellite channel is studied in this
paper. The scheme can also be applied to
terrestrial link-by-link error control. The
transmission delay characteristic of the
tandem ARO scheme is now under study.
In the foregoing equations. ggg the
throughput of adaptive GBN.
Summarizing the forementioned equations
and setting ?a<s 7, , the following equa-
tions finally are obtained:
The foregoing equations, which do not depend
on 4ican be solved for %A-
Letting () = L for all i in the fore-
going equations, the throughput in GBN ggs
is given by
where J[(t, 2t,)C/(L-+H)]+ 1, and d is a
constant, corresponding to J(i). The fore-
mentioned result agrees with that of Sastry
I11.
The following examples assume comuni-
cation speed of C = 4800 bit/s, header
length of B = 6 Byte (B), ACK and NACK
length of 6 Byte (, 10 ms) and propaga-
tion delay of , 2 ms.
The throughput of the adaptive SR
scheme for the observation period, M = 10,
is plotted in Fig. 4. In this figure, K =
4, L(0) = 256 Byte, 9(1) = 128 Byte, L(2) =
64 Byte, and L(3) = 32 Byte. For comparison
the SR scheme throughputs for L = 256 Byte,
128 Byte, 64 Byte, and 32 Byte are also
plotted. The figure shows that the through-
put of the adaptive SR scheme is very close
to that of the conventional SR scheme in
which the optimal data block length is
adopted for a given bit error rate. Conse-
quently, the adaptive SR scheme selects the
data block length effectively according to
the bit error rate.
The effect of M on the throughput is
shown in Fig. 5. This figure shows that the
smaller M is, the higher the throughput for
small F,- The larger M is, the higher the
throughput for large 4,- When F4, is small,
however, there is little difference due to
changes in M. Consequently, M should be set
somewhat large. However, when f4, varies
with time, there is a danger that the system
cannot immediately follow the time variation
of F if M is set too large.
To clarify the forementioned situation,
Fig. b shows the effect of the time varia-
tion of F, on the throughput. Simulation
results for 1000 s for M values of 1, 10 and
20 are plotted in Fig. 6. In the period
from 960 to 990 s, f, is set to 10 ; in the
other period, it is set to l0n. Figure 6
shows that throughput varies greatly when
M = 1. When M = 20, the throughput de-
creases greatly immediately after F, in-
creases. On the other hand, when M = 10,
the system follows changes in F4, well.
seasons of the year at the four presumed levels of solar radiation, as a percent of the
maximum expected clear sky radiation (100%, 50%, 25%, 0%).
The installation of the food/waterlpower complex was carried out in two major phases.
Phase I included the installation of the solar power plant. This phase started on April
15, 1980, and was completed on March 1, 1981. This was followed by one year of power
plant operational tests and component optimization. Areas that needed design optimiza-
tion included pipeline supports and expansion joints, freeze protection for working fluid,
and upgrading of the data acquisition and control system.
Phase II included the utilization of the low temperature waste heat from the power
plant for water desalination, the installation of the reverse osmosis and multistage desali-
nation systems, the construction of the four ponds, the solar greenhouses, the palm tree
plantation, wind break, and related irrigation network. This phase started in April 1982
and was completed in June 1985. Figures 9 and 10 show part of the food/waterlpower
complex.
Although the complex was designed, installed, and operated as planned, the experi-
ence gained in design, installation, and operation suggests potential modifications which
will reduce the cost and improve the performance of future systems.
and the production costs of electricity. At the first stage, the production costs of a con-
dnsing power plant of about 200 MW were calculated (Table 1). A conventional con-
densing power plant was compared to three IGCC concepts: oxygen gasification and cold
S cleanup; air gasification and cold gas cleanup; and an advanced process with air
gsification and hot gas cleanup. Pressurized fluid-bed technique was applied in all alter-
ntives. The first two alternatives resulted in nearly equal production costs of electricity,
which was also almost equal to that of a conventional power plant process (Table 2). The
highest potential cost saving was apparently achievable with the alternative of air gasifi-
cation and hot gas cleanup. However, research and development work for several years is
still required for the industrial application of this alternative. The work will be continued
by comparing different gasification processes and size classes also in combined electrici-
ty and heat generation.
la Finland, A. Ahlstrom Corporation manufactures the Pyroflow circulating fluid-bed
reactor both for combustion and gasification. Four gasifiers of 15-35 MW,,, have been
3l4 to pulp mills, where they produce fuel gas from wood waste for lime kilns. Ahl-
om-Bioneer Corporation manufactures fixed bed gasifiers for sod peat and wood chips.
Seven gasifiers of up to 7 MW,,, have been delivered so far. These companies have not
anufactured pressurized gasification processes thus far.
Kemira Oy, a big Finnish chemical corporation, put a pressurized fluid-bed oxygen
$sifier of about 150 MW,, into operation in Oulu in June 1988. The gasification plant is
lean gas provides an attractive miscible flooding drive because it is relatively cheap and
abundant (Whorton and Keischnich 1950; Defferne 1961; Rutherford 1962; Hutchinson
and Braun 1961; Stone and Crump 1956; Clark, Schultz and Shearin 1956) when com-
pared with other miscible drives (enriched hydrocarbon gas, for example). The mecha-
nism of lean gas displacement is different from other miscible drives. Since it is immisci-
ble to crude oil at normal conditions, the gas is pressurized to achieve dynamic
miscibility.
A lean gas drive is different from an enriched gas in many aspects, one of which is
the nature of mass transfer in the mixing zone between the oil bank and the displacing
bank. In the enriched gas flooding, intermediate hydrocarbons migrate from the rich gas
into the oil zone creating the mixing zone (Stalkup 1983a). In lean gas drives, interme-
diate hydrocarbons migrate from oil to the lean gas, thus enriching it and creating a
Mmixing zone. This dynamic miscibility involving lean gas and oil can only be achieved
ove a minimum miscibility pressure, which limits the feasibility of the process (van der
Burg and Yrma 1983; Stalkup 1983b; Craig 1970).
Refinery flare gas is an everlasting source of lean gas, which is usually burned re-
ling in the production of several combustion products that are ecologically harmful. In
fineries that have sulfur recovery plants, the flare gas may mainly consist of hydrogen
44 light hydrocarbons. However, the composition of the flare gas varies considerably
ith the nature of the crude processed in the refinery. It will also vary with the severity of
of simulation, where solid coal is considered to be made up of blocks or lumps separated
by fractures and cleats. Furthermore, it clearly suggests the dual porosity nature of coal,
where gas first diffuses through the coal matrix toward the fractures and then flows
through these toward the gas-producing wells. All models must consider this type of
physical characteristics. Kozeny Carman or the Marshall type of models, where the flow
ls primarily assumed to be through interconnected pores and capillaries (Carman 1956;
Marshall 1962)-although not used for coal-should never be considered due to the
difference in mechanism of gas flow in coal.
ln order to better understand the mechanism of gas flow in solid coal, more thorough
knowledge of the coal structure is needed. With modern techniques in electron microsco-
Py, a three-dimensional picture of the fractureslcracks network should be obtained to
study the extent and connectivity of these. With an ''auger'' microscope, equipped with
an ion mill within the specimen chamber, a layer can be removed (by ion bombardment)
after observation, and the new layer underlying it can be observed, thus giving informa-
tion on continuity of fractures and pores. Another techniquue that must be tried is the
Application of the ultrasound method to study the surface and discontinuities in coal.
Although the sonogram method is being extensively used for human diagnostic purposes,
S application to study properties of rocks and coals has not been tried. If this can be used
uccessfully, useful information about the microstructure of larger coal samples can be
se, the formation of a quasi-crystalline structure is favored. Regarding the formation of
totropic gel, which represents gelling on standing, his is favored by aging, probably
ause the packets of lamellae are dispersed, due to swelling effect, but only to a certain
it because in the concentrated suspension, the structure is so rigid that it is unaffected
aging. But with an increase in concentration, thixotropy always increases, due to the
mation of a card house structure that is more favored.
The general trend of curves of apparent viscosity and yield value is
milar in nature, with a sharp maximum for low concentration of NaCl and a gradual low
aximum with increasing concentration of NaCl added, as shown in Figures 5 and 6 and
ble 2.
The stabilizing action of electrolytes on clay suspensions is attributed to the forma-
$ of electric double layers on the surface of clay particles. Such potential forming
IIt is convenient to express the new unknown functions and
variables as
The boundary-value problem (1) is described by the following
dimensionless parameters P. * @?.. - continuous-phase Peclet
number , WL * ,^, * Nusselt number 4, is the effective diam-
eter of the catalyst grain corrected for porWsitY. an h,, 4 the
thermal con4uctivity of the gaw). L - a'R/) is ehe Bot number
epresenting the heat brought to the bed by the air; and Ri, *
- o,7/k,; 4= the same for the soli4 phase.
The solution of boundary-value problem (1) is written as the
series
The eigenvalue oy 4s the solution of the transcendental equation
F, is defined by the expression
a4 = m<wi < 6 5; ', *6 ; .. awa F's/4. . wr===
tn+ vw1es t 4, ama e;'ara aseseia teo ene boun4ars eona-
tions of problem (1).
According to experimental data plotted in Figs. 3 to 5, the
temperature of the phases in the ordered bed are different and
should hence have a marked effect on the product yield under high
heat flux density conditions. If we drop all but the first term
of series (2), which falls off least with increasing axial coordi-
nate, then the corresponding coefficients can be calculated after
determining the two phase temperatures in a given point in space.
Those values for different Re are given in the second and third
columns of Table 2. According to this table, the difference be-
tween the phase temperatures increases with Re, tending to a cer-
tain ratio. This situation is the mathematical result of the
existence of the first integral
ot problem 0), rwlatie te coettieients a;'' asa e;''. Te saiin
advantage of Eq. (3) is that it allows us to calculate the phase
temperature difference (which controls the heat and mass transfer
replacing f Ey 1A'
Equation (5) is the extension of the familiar Prandtl equation to
the case of flow along the generatrix of a cylinder.
In integral methods one must specify the velocity distribu-
tion. We consider the wall layer of the jet (D $ 4 6). Con-
verting in the Prandtl-Boussinesq formula (the overbar designating
averaging is henceforth dropped)
to variables qg = ulk, with n = yV./v. and transforming, we obtain
We assume for IY;; 4n4 Wg/i the distributions
The thickness n, of the laminar sublayer is determined from the
condition that the value of i,, 4t the boundary of the sublayer
(n n,) will not exceed 0.001u. Solving Eq. (9) for n, and assum-
ing that tanh 1 = 0.7616, we obtain
On the surface of the cylinder n = 0, which case we have from
Eqs. (8) and (9)
23EE3EE2E3S9EE.2E.8LEESEE.ESESEE- Proces+w oeewrrins t
the course of rapid motion of bodies through a spray stream have
long been of interest [1]. We shall consider the flow generated
when a spray stream flows over a flat plate. The velocity of the
gas phase induced by the motion of droplets will be neglected. The
momentum of a droplet stream acting on a plate that makes an angle
o to the velocity vector V of the stream is
where L and h are the span and width (i.e., dimension along the
flow) of the plate. The forces exerted by the spray on the plate
vary and depend on the model of interaction between the droplets
and the plate. Three cases of combination of these forces are also
The main purpose of the conceptual design activi-
ties of the International Thermonuclear Experimental
Reactor (ITER), jointly conducted under the auspices
of the IAEA by four Parties, EURATOM, Japan, the
Soviet Union and the United States, is to develop an
experimental fusion reactor to demonstrate scientific
and technological feasibility of fusion power. The ITER
''Terms of Reference'' agreed upon by the four Parties
constitutes the basis of the cooperative activities [l,2].
Accordingly, the device is designed to achieve three
major objectives: to demonstrate controlled ignition
and extended burn of D-T plasma with steady state as
an ultimate goal, to demonstrate technologies essential
to a reactor in an integrated system and to perform
integrated testing of the high-heat-flux and nuclear
components required to utilize fusion power.
The cooperative activities of the Conceptual Design
Activities of ITER that began in April 1988 and were
completed at the end of 1990 consisted of:
Considerable design and analysis support was pro-
vided by staff at the Parties' home sites; these efforts
involve about 80-100 man years each during the three
years' period of the Conceptual Design Activities.
The present paper presents a summary of the ITER
Conceptual Design Activities.
The design is based upon the scientific knowledge
resulting from the operation of dozens of tokamaks
around the world over the past two decades and upon
the technical know-how flowing from the extensive
technology R & D programmes of the four ITER Par-
ties. In fact, fusion performance of plasma was in-
creased by two orders of magnitude in the last five
years, achieving plasma conditions close to those
needed in ITER. The fusion performance of ITER is
now less than an order of magnitude away from the
performance already demonstrated.
The large body of knowledge available to ITER has
made it possible to specify a concept which meets the
technical objectives specified in the Terms of Refer-
ence, and provides a starting point for the Engineering
Design of the device. It is natural that some uncertain-
ties remain, and complete resolutions should await the
operation of ITER itself, as benefits its purpose. De-
sign policy of aggressive and multi-faceted approach to
be optimized via dynamic analysis of the fuel cycle
operation, and careful attention to system interfaces to
minimize tritiated gas hold-up.
With the exception of components connected di-
rectly to the torus (Fuelling, Primary Exhaust, and the
main loop of Blanket Tritium Recovery), FC compo-
nents are located away from the machine, in an area
dedicated to components containing tritium (fig. 8).
This area has separate access and ventilation system to
prevent the spread of contamination, and simplify
emissions control under normal and upset conditions.
While the CDA demonstrated that feasible design
options exist for all essential FC elements, only limited
optimization of the FC design was possible during the
CDA Phase. An ''optimized'' design for the Fuel Cycle
is tied to the overall process of ITER design integra-
tion and can only emerge towards the end of the EDA
Phase. Furthermore, it will depend on component de-
velopment priorities and schedules followed by the
ITER Parties. Detailed long-range R & D plans were
double skin made of thin plates of Inconel 625. The
first-wall armour, in form of tiles, is fastened to the wal]
near the cooling channels to increase the thermal con-
duction from the tiles to the coolant gas. The double
walled vessel is mechanically strong against disruption
forces, but not very well suited for heat flow across the
wall due to the low thermal conductivity between the
two skins. Therefore the divertor plate was provided
with cooling channels so as to protect the vessel wall
from high heat fluxes emanating from the divertor
armour. The armour is fitted to the cooled base plate by
a backing plate. The design is shown in fig. 4. The
divertor plate has a simple structure but the cooling
tubes between the plates are much more complicated
because a fraction of the eddy currents, induced in the
vessel skin by the disruptive plasma motion, circulates
in the tubes. Therefore the tube has many U bends
which are carefully attached to the vessel so as to
minimize the amount of eddy current circulating in the
tube. The divertor plates are also carefully aligned so
that the height difference between neighbouring plates
is kept below 0.5 mm. The distance between the vacuum
vessel and the armour was kept below 6 to 8 cm for the
divertor and to 4 to 5 cm for the other areas. These
distances were minimized in order to maximize the
plasma cross-section which then enables JT-60U to
sustain discharges with plasma currents above 6 MA.
The maximum heat flux to the divertor was esti-
mated to be 20 MW m '' assuming that
considering the cyclicity. Seven armor tiles 1-7 are
attached to the cooling tube with a 0.2 mm gap between
neighboring tiles. A heat flux of 10 MW /rmf is applied
to the surface of the armor tile in the area shown in fig.
7. The cooling tube with a swirl tape is water cooled
with coolant conditions of v = 10 m/s and p = 3.5 MPa
being assumed.
The temperature and stress distributions are calcu-
lated for the anisotropic thermal conductivity and
mechanical properties of CC tiles. The thermal conduc-
tivity of the CC tile in the tube axial direction is about
30 smaller than that in the other two directions. The
maximum temperatures in the armor tile and cooling
tube are about 1150*C and 350C, respectively, as
shown in fig. 8a. The detailed temperature distribution
in armor tile number 4 and the sliding support is shown
in fig. 8b.
The onset of wrinkling in a membrane is associated with the appearance of compressive
stresses. In the so-called tension field theory, the direction of a wrifkle is one of the principal
directions. Therefore, in a wrinkled state, the principal stress resultant along a wrinkle is
tensile, while the transverse principal stress resultant is zero.
Mansfield [1,2] analyzed a wrinkled membrane based on a principle of maximum strain
energy. In his modelling the lines of tension were found in the direction for which the strain
energy of the membrane is at a maximum. Wu and Canfield [3] gave another approach
describing the wrinkling of membranes based on a finite-stress theory. They introduced a
strain-like kinematic variable to describe the physical wrinkliness, which was determined by
the condition that one of the principal stresses in a wrinkled state is zero. Pipkin [4] showed
that the tension field theory can be incorporated into the ordinary membrane theory by
substituting a suitable relaxed energy density for the strain energy density function.
There have been several numerical approaches to grasp the wrinkling phenomenon.
Panagiotopoulos [5] dealt with the inelastic wrinkling analysis of cable structures. He formu-
lated the wrinkling problem as a variational inequality introducing slack variables for the
unknown initial strain, and solutions are obtained using optimization methods. Miller et al. [6]
introduced the variable-Poisson's-ratio concept. Similarly, a technique with variable stiffness
was presented by Nishimura et al. [7] and Ikemoto et al. [8]. On the other hand, Contri and
Schrefler [9] introduced a two-step procedure. They obtained the final equilibrium position
iteratively by the total elimination of the principal compressive stresses. However, in these
numerical analyses, there was no attempt to describe the physical wrinkliness of the surface.
Recently, Roddeman et al. [10,11] presented a wrinkling model of plane anisotropic mem-
branes by introducing a modified deformation gradient tensor with an extra variable for the
physical wrinkliness.
The true incremental strain ,,e' is obtained as
From eqns. (22) and (26), the true incremental strain can be split into the incremental strain
of the membrane part on II, and the incremental wrinkle strain:
The 2nd PK stress increment is related to the true incremental strain:
where ,;CeM is the elasticity tensor at the configuration 'T2. For isotropic material, the
elasticity tensor, as given in [14], is
where E, v and h are Young's modulus, Poisson's ratio and the membrane thickness,
respectively. The 2nd PK stress at time t + 2t is expressed as
The local equilibrium equations for a thin membrane shell are expressed in terms of the
Cauchy stress tensor at the configuration ''/1 as follows [13,14]:
where l denotes a covariant differentiation with respect to the metric tensor at the configura-
tion '?'/1, b,, the second fundamental form on the mean deformed middle surface, and f
external forces per unit area of the surface.
For the wrinkled state, the above equilibrium equations are also satisfied on the smooth
surface II, [3]. So, the variations of displacements are taken in the surface Il,, Applying the
principle of virtual displacements and considering eqn. (28), the total Lagrangian equation for
thin wrinkled membrane shells is given by:
where dt4 is the surface element in '(1 and '*MI is the virtual work of external loads.
If the above equation (30) is expressed with respect to the principal stress axes at time
t + 2t, it is rewritten as follows:
where left subscript p denotes the principal direction of '',S. If a principal stress is about
to be compressive, then the membrane starts to wrinkle and the principal stress becomes zero.
If both principal stresses are tensile, the membrane is taut. These physical conditions can be
described in the following complementarity relations:
where the summation convention is not applied with respect to a, and the values of a run
through l and 2. Equation (34a) and (34b) mean that if the principal stress is not compressive,
Test Example 4. Simply stupported two-layered square spherical panel under external pressure
The following geometrical and material properties are considered: R, = R, = 1000 cm,
a = b = 50 cm, h = 1.0 cm, E, = 25 x 10% N/cm', E;= 10 N/cm', G;; = G;, = 0.5 x
10 N /cm', G4;= 0.2x 10 N /cmf, <2 = 025, p = 10 N s%cm'.
The layups considered are (- 458/45%) and (0*/90). The variation of central deflection
with time for two values of the externally applied pressure is plotted in Figs. 6 and 7 and
compared with the solution of Ref. [l1], which employed a nine-noded quadratic shear
flexible element along with Newmark's integration scheme. In Ref. [10], for transient response
analysis of plates, the same element as that Ref. [l1] was used with reduced integration
scheme for eliminating shear locking. Hence the difference in results may be due to
The virtual displacement of the lumped-mass centre in eqn. (23) is:
small. Therefore, as pointed out in the last example, the approximation iü = s made in Kane's
model leads to much smaller difference in the extension and lead-lag deflection. Conse-
quently, the result agree better with the present model. The response of the manipulator's tip
in extension (u) is also presented in Fig. 20.
where 9 means 'is interleavable with,'' a temporal relation explained in Section
9.7. For simplicity, let tasks B and C be composed of sequential steps, as
shown here in the UAN;
The STD for this simplest possible example of interleaving, shown in
Figure 1, would contain complicated transition conditions that depend on the
real current state in each separate sequence. To include that real state
information in the STTDD of Figure 1, one would have to replicate every
possible subsequence of task B in conjunction with every subsequence of task
C, producing a combinatorial explosion of states and transitions. [ust as one
example, if task B is really at subtask E and task C is at l, then transitions are
not legal from E to G or H nor from l to D. For real tasks, such as the use of
spreadsheets and text editors, the result is overwhelmingly large and complex.
It is equally important to note here that the original intention was to
represent the asynchronous relationship between tasks B and C. The se-
quencing of B is independent of the sequencing of C, but the diagram
obscures that relationship by interconnecting them.
It is clear that possible transitions between subtasks of B and subtasks of C
in the just-cited example must be represented implicitly, an approach taken,
for example, by Jacob (1986) and by Wellner (1989). Because direct
manipulation interfaces are composed of many individual simple dialogues
that interact like coroutines, [acob divided an interface into interaction
temporal fossibilities for extensional instantiations of the tasks within a specific
performance of the containing task. We use the term instance as needed for
clarity. However, the terms task and action and related terms are often
sufficient to refer to their instances, unless it is important to make the
distinction.
Instances of user actions are events in time, and thus we wish to apply the
relation H to them, where o: is an instance of a user action and t is a point in
time:
Again, it is postulated that the right-hand side can be evaluated for any user
action, o, Equation 10 is fundamental in that it relates user actions to time.
An instance of a user action, o:, has a Lifetime, denoted L(c:), which is the
interval spanning just those times that satisfy H:
where the Beginning of the Lifetime, B(L(c)), is the least point in time such
that the action instance is happening at that time:
and the End of the Lifetime, E(L(o:)), is the greatest point in time such that
the action instance is happening at that time:
Given these definitions, a user action (primarily a task in this context) need
not be happening at all times during its lifetime; there may be times of
inactivity as well as times of activity. The graph of activity versus time, which
we call an activity waveforim diagram, is a boustrophedon (alternating rectangu-
he or she is prompted to turn Basil to the desired heading by tapping on him.
The default implicit constraint (when no touch governs an action) is that the
position is set by the user (Figure 3a). Should the user disagree, he or she may
select 'always here,'' which means constant absolute position; 'this far from
last point,'' which means constant relative position; or 'relative to an object,''
which means that the point should have been constructed. In the latter case,
Basil asks him or her to draw the construction and adjust the original object's
position afterward if necessary. (The construction steps are inserted into the
procedure ahead of the original action.)
electron microscope (SEM). Phase analyses were
conducted with a Cameca SX50 electron microprobe,
using an accelerating voltage of 15 YV. Results were
treated with the application of a PAP model of the ZAF
correction program (atomic number Z, Absorption and
Fluorescence).
A crystallographic identification of the phases was
done by X-ray analysis performed with a@- 28
Siemens diffractometer apparatus.
For each type of alloy we present the results concerning
the phases, their composition and their morphology. All
the phases that we identified had already been observed
and crystallographically characterized in one of the
binary systems. The crystallographic features of all the
phases have been reported in Table 2.
The respective compositions indicated are presented in
Tables 3, 4, 5 and 6. When the size of the particles is
too small to be analyzed accurately, an approximate
composition is indicated by a star.
The phase fields of oxicarbides have been determined at
about 1500*C by Alyamovskii et al. !3I and Zainulin et
al, 14, respectively; they are presented in Fig. 6. For
Alloy B, whose composition is in the Nb2C phase field,
we found both carbides NbC and Nb2C. This may be
explained by the formation of primary NbC at a higher
temperature (2000'C) than the one used in previous
investigations. The micrograph on Fig. 3 shows primary
niobium carbide dendrites. At lower temperature, during
the cooling process, NbC undergoes two
decompositions: first into Nb2C and then into Nb2C
and Nb, which appear as white striations. Considering
that solidification of Alloy B occurs at 2000'C, at this
temperature the NbC phase occupies a large phase field
overlapping the Nb2C phase field. This observation is
consistent with the binary phase diagram.
Both authors /3,4I have determined the lattice
parameters at 1500?C for 14 compositions of NbC.
Ouensanga et al. /5I have also determined the lattice
parameters at 1017'C; but most of their alloys contain
two phases, and, consequently, the value of the
parameter cannot be attributed to a particular
composition. Wjth the 14 values we calculated a
polynomial first-order representation after smoothing the
experimental values by a least square treatment.
2- and Kg being the respective atomic percent contents.
The crystallographic structure of NbC is formed from
two cubic NaCl type sublattices: one sublattice being
occupied by the metallic atom and the other one by C or
O atoms, The occurrence of vacancies on the niobium
nodules embrittle and may form small cracks, which act
as sharp notches for the substrate.
Frequency has also an influence on hie flow stress
and the extent of crack tip plasticity /16/. Results in /16/
at 600%C showed that the crack tip plastic zones (under
same values of SK) increased in size by a factor of 4
when the test frequency was reduced from 10 Hz to 0.1
Hz. Our results /15/ also showed that at the higher
frequency, all these alloys have a higher Ky than at he
lower frequency (Ko is an approximation of Kc
calculated from the applied loads and the critical crack
lengths and the crack shape at failure). For Rene 142
Alloy XB, Ko is 85.9 MPa Vii at 10 Hz o 77.8 MPa
Viä at 0.1 Hz. Considering the average loading rate at
the high frequency to be 100 times higher than that at
the lower frequency, we can expect an increase in flow
suess at high frequency as compared to low frequency of
the order of
(We assumed a creep exponent of 20 in the Norton's law:
Under load controlled testing this means that the total
strain range and the plastic strain range will be greater at
low frequency. It was concluded that the combination of
high temperature and low frequency suggest a time-
dependent plasticity effect on fatigue performance in ad-
dition to the oxidation attack.
The notch fatigue performance of three advanced cast
blade nickel base superalloys was measured at 760PC
(1400%F) at 10 Hz and 0.1 Hz in tension-tension under
load control. The conventionally directionally solidified
alloys MAR-M 246 and the DS Rene 142 processed with
a low thermal gradient are comparable in their fatigue
performance. The DS Rene 142 produced by an ad-
vanced withdrawal casting process, with a smaller DS
grain size and a refined structure resulting from a rapid
solidification rate, has a much better fatigue performance
due to smaller microporosity.
The fatigue crack initiation and fatigue crack
propagation lives were assessed experimentally by the
DCPD technique and theoretically by integration of the
crack growth data. Both procedures showed that in most
cases when the materials are free of large defects (2100
microns), at least 70% of the fatigue life is spent to
initiate a crack of 100 microns. But for single crystal
alloy HTC 254, the crack initiation life is greatly reduced
due to the presence of large, irregular micropores with
sizes of 2100 microns. Extensive fractographic analysis
showed that most of the cracks are surface initiated and
grow as semi-elliptical thumbnail cracks keeping a very
uniform crack front geometry. The crack initiation sites
are generally associated with coarse microstructure
features, such as eutectic nodules and inclusions, surface
and subsurface defects (micropores, shrinkage).
The marked decrease in fatigue life at 0.1 Hz as
compared to 10 Hz is due to acceleration of crack
initiation by the substantial oxidation attack.
The pollution of ground and groundwater has
come to play an important part in the assessment
of the environmental quality of many areas. It has
become recognized that, although the processes in
the soils occur slowly and often without imme-
diate and dramatic consequences, the long-term
effects of contamination will be serious and poss-
ibly irreversible.
Several major pollution sources have been
identified in recent years: agricultural sources,
waste disposal sites (controlled and uncontrolled),
urban areas, especially old cities, river and off-
shore sediments, acid rain and atmospheric depo-
sition in general, and industrial facilities.
Industrial sites fall into three divisions. In the
first category there are proposed sites, where the
initial soil quality is good and the problem is one
of soil protection. The second category is of aban-
doned waste sites, where industrial activity has
been permanently discontinued and perhaps the
site has been levelled. Here the technical bound-
ary conditions for the site clean-up are a function
of the future site use. The third category, which is
the subject of this Paper, concerns those sites that
are currently in operation but pose a significant
environmental risk to the surrounding geosphere.
Soil contamination can lead to contamination
of the groundwater and water wells, contami-
nation of drinking water through permeation of
pipes and the formation of explosive gas. In the
last case there are special and often stringent con-
straints on the solutions available for dealing
with the problem.
This Paper describes the approach to the
problem based on recent developments of reme-
dial techniques. First the soil contamination must
be described in detail, based on field data and
knowledge of the behaviour of the contaminants
in the soil. This should include information
regarding the flow pattern of the groundwater,
approximately 65 m high facing north-west and
smaller refuse slopes on the other faces, with a
large flat area on top.
The landfill was constructed by stripping vege-
tation and topsoil, and then sealing the surface of
the tipping area using PVC sheets on cut plat-
forms, soil plaster (chunam) with bitumen coating
on cut slopes, and stabilized soil fill with bitumen
coating on natural slopes (Fig. 4). A system of
subsurface drains to collect leachate was laid
along the line of the old valley and led to existing
Although the mechanical behaviour of
assemblies of disks reflects general features of gra-
nular materials, magnitudes of parameters such
as peak friction angle and peak dilation rate are
much lower than those of real sands. This is
either due to the shape of the particles or to the
planar nature of simulated systems.
This Paper addresses one aspect of this uncer-
tainty by studying the effect of particle shape on
the behaviour of planar granular assemblies. It
presents the results of numerical experiments on
assemblies of elliptically shaped particles for a
range of particle eccentricities.
Planar assemblies of oval particles were studied
in physical experiments by Konishi, Oda &
Nemat-Nasser, (1983) and Oda, Konishi &
Nemat-Nasser (1983). The emphasis in these
investigations was on effects of depositional
(inherent) anisotropy on macroscopic behaviour.
The present Paper describes general micro-
mechanical aspects of assemblies of elliptical par-
ticles, and emphasizes new qualitative features in
the development of stress-induced anisotropy. It
will be shown that the stress-force-fabric
relationship verified previously for planar
assemblies of disks (Rothenburg & Bathurst,
1989) is also valid for assemblies of elliptical par-
ticles.
Simulation of assemblies of elliptical particles
follows the general methodology of the DEM (ie.
solving Newton's equations of motion for individ-
ual particles using an explicit time-finite differ-
ence scheme as outlined by Cundall & Strack,
1979L The specific implementation of the DEM
method within the computer code used in the
current study is different from that described by
Cundall & Strack but these differences are not
essential as far as the objectives of this Paper are
concerned. A typical isotropic assembly of ellip-
tical particles under initial hydrostatic stress con-
ditions is illustrated in Fig. l(a). The total
number of particles in these simulations including
the circular boundary disks is 1001. The assembly
was created from an initial assembly of com-
pacted disks by slowly increasing the eccentricity
of each particle to a target value over many cal-
culation cycles while keeping the mean radii of all
the particles constant. During the assembly's cre-
ation the interparticle friction angle was set to
zero in order to achieve maximum density. In the
course of particle growth the assemblies were
near static equilibrium under constant hydro-
static stress. The assemblies were then subjected
to biaxial compression under constant horizontal
stress and with interparticle friction set to a non-
zero value. An example of an assembly after sig-
nificant dilation had taken place is illustrated in
Fig. 1h).
The main challenge that had to be met in the
simulation of elliptical particle interactions was
the development of a reliable contact detection
algorithm.
When the preprocessing has been done, the
orientation analysis itself is carried out.
To obtain the orientation of a vein in an image,
the code numbers R, of a chain code should be
transformed into angles o(i), where (i) =
R, 45%. The probability p(i) of the angle @(i) can
be obtained from the histogram of a chain code,
and so equation (8) can be used directly to calcu-
late the orientation @, of the vein k, representing
particle k
This orientation is the mathematical expecta-
tion of the vector angles of a vein. In practice it is
correct for veins consisting of a few segments with
acute angles between their directions, but if there
are obtuse angles between their directions the
orientation calculated may be quite different from
what would be estimated intuitively. Fig. 5 shows
an instance of this problem. Clearly it is necessary
to rotate the orientation counter-clockwise. The
following definition solves this problem.
Suppose o(i) and o(j) (i w j) are the angles of
two adjacent segments i and j respectively, joined
at point p. Suppose , > @@j. Then (compare
equation (9)) the angle between segments i and j is
defined as
If @;; is greater than or equal to 180', then @j;
should be rotated by 180' clockwise, ie.
To apply this definition when calculating the
orientation of a polygon, an iterative method
must be used. The n edges of the polygon are first
taken in adjacent pairs and an orientation is cal-
culated for each pair. In effect, this step replaces
the n original edge directions by m = n/2 direc-
tions. In the next step these m directions are simi-
larly taken in adjacent pairs and replaced by m/2
directions, and the process is repeated until only
one direction remains. This direction is taken as
the orientation of the polygon. If at any stage
there is an odd number of directions, the last one
is not processed but is carried forward as it
stands to the next stage. The theory of this
method is summarized in Appendix 2.
Convex hulls form the basis of important
methods in image processing and pattern recogni-
tion. These methods have a wide range of appli-
cation including tests for linear separability,
cluster admissibility and concavity, describing the
shape of a set of points and image processing.
The convex hull of a polygon P is the minimum
area convex set of the vertices of P; an example is
shown in Fig. 6. Usually, a convex hull problem
may be classified into one of two categories,
depending on the input data. One is finding the
convex hull of a set of points (Graham, 1972;
Eddy, 1977; Preparata & Hong, 1977) and the
other is finding the convex hull of a polygon P
(Shin & Woo, 1986; Orlowski, 1985). The present
work concerns the case where the point set S is
planar.
The convex hull is found by the method of Luo
& Macleod (1991)L This is a simple and fast
method when, as in this application, the bound-
ary of a particle is represented by the co-
ordinates of points on a grid. Briefly, it entails
Professor P. R. Vaughan, Imperial College of
Science, Technology and Medicine
The Author is to be congratulated on his con-
tribution to the study of residual strength.
Observing this strength continuously as the salt
content of the pore water is changed by leaching
isolates the effect of this variable, and shows its
influence. This seems to be comparable with the
variation between test results which is sometimes
obtained from different samples of the same clay
strata. Uncontrolled chemical effects may explain
part of the scatter in residual strength measure-
ments that is often experienced.
Perhaps two points shculd be stressed. First,
chemical effects should apply only to sliding
shear, when clay content is high and platey, low-
friction clay particles slide over one another, and
when, as demonstrated by Lupini, Skinner &
Vaughan (1981), residual shear strength depends
on interparticle friction. In turbulent shear, domi-
nated by rotund sand and silt particles, shear
involves rotational particle movement, and the
average resistance is not affected by interparticle
friction (Skinner, 1969). Second, it is important to
test very slowly, as was done by the Author, so
that the effects of displacement rate do not com-
plicate the situation. The displacement rate
adopted in the Bromhead ring shear apparatus,
with its thin sample, is often fast enough for rate
effects to be significant.
The range of chemical change studied by the
Author is probably greater than is common in
UK clays. Table 3 indicates that the salt content
of the Gault Clay is rather low and similar to that
of rain-water. However, Fig. 4 indicates that the
biggest changes in residual strength result from
small changes in salt concentration when this is
low. What might the typical error be in preparing
typical UK clays by mixing them with distilled
water? The total salt content should remain the
same, although the salt concentration in the pore
water would change.
If distilled water is used in the water bath, how
long would it take for the natural salt concentra-
tion in the sample to change by diffusion, and
what might the change in strength be'?
Could changes in strength due to diffusion and
chemical change be minimized by a simple expe-
dient such as using tap-water in the water bath'?
These studies may be relevant to slip stabiliza-
tion. Lupini et al. (1981) note that the influence of
chemistry on residual strength may account for
part of the effect of the old techniques of drop-
ping lime down tension cracks, or cement grout-
ing. The conditions where these techniques might
be worthy of revival could be determined by the
Author's techniques.
A. Sridharan and S. M. Rao, Indian Institute of
Science, Bangalore
The data presented in the Paper, in particular
that for the montmorillonites, raise certain
queries.
Sodium and calcium montmorillonites both
exhibit an increase in residual strength param-
eters r, and o,' with increasing permeating solu-
tion concentrations (0-2-1-0 molar) (Table 4). An
increase in the salt concentration of sodium chlo-
ride and calcium chloride respectively is also
observed to result in an increase in consistency
limits of the sodium montmorillonite and calcium
montmorillonite specimens. Hence, based on the
variations of residual strengths and consistency
limits of sodium and calcium montmorillonites
with increasing salt concentrations, it appears
that r, and o,' increase with consistency limits of
the montmorillonite specimens. However, com-
paring the data presented for sodium and calcium
montmorillonites at a common salinity (0-2
molarl, sodium montmorillonite with a higher
liquid limit (148-8%) exhibits notably lower r,
and o,' (3209 kPa and 7-3* respectively) values
than calcium montmorillonite (which at 0-2
molar has a liquid limit of 60%, t, = 4827 YPa
and o, = 109', Table 4). Hence the experimental
results presented for the montmorillonite speci-
mens do not seem to follow a consistent pattern.
We have been working on the role of physico-
chemical factors in the engineering behaviour of
clays (Sridharan & Rao, 1973, 1979; Sridharan &
Jayadeva 1982, Sridharan, Rao & Murthy, 1986a,
1986b, 1988). The work has shown that for mont-
morillonites an increase in salt concentration
leads to a decrease in consistency limits as
demonstrated by the liquid limit data of a mont-
morillonite clay from Kolar District, Karnataka
State, with increasing sodium chloride concentra-
tion (Fig. 5). The data in Fig. 5 is contrary to that
Each of the novices answered three pairs of questions, one with Gbook,
one with Mbook, and one with paper. The order in which the subjects
used the different media was different for each user and was recorded.
Hbook was not included due to the unreliability of our copy of the
software.
Fig. 2, a structure node could be created with a link to each of the 8
documents. The user could author this structure node in whatever way
the user wished, for example, as a table of contents:
The italicised text represents a hypertext link to the corresponding
document.
The structure node could also be used to annotate documents, for
example, to provide a better explanation or additional information on a
piece of text.
In Hyperbrowser, the text retrieval sub-system provides facilities for
adding, deleting and retrieving documents from the document database.
Each document is stored with the following attributes:
Each document is identified by its identification and version number.
When a document is updated, its version number is incremented. The
security code restricts access to users with an equal or higher security
code. The set of keywords is a sequence of words and phrases describing
the document's content, i.e. the text. A full index is stored on every non-
common word in every document of the document database, and a
sophisticated query language is supported which uses this index to
provide fast access to its documents.
The hyperfile environment provides facilities for browsing through all
the hyperfiles in the system, and for adding, deleting, reading, authoring
and maintaining individual hyperfiles. Each hyperfile is stored with the
following attributes:
The purpose of this paper is to reinterpret computer systems as media in
which we can tell stories about human feelings, desires, aspirations, and
conflicts in new ways. The paper contains both practical and theoretical
issues. Section 2 searches for basic elements of expression that are as
characteristic of the computer medium as the montage techniques are of
film, Section 3 describes a concrete system exploiting a subset of these
ideas, and Section 4 widens the theoretical scope by placing the ideas of
the paper in the setting of catastrophe theory.
Most of the work was done as a part of the VENUS-project at the
Department of Information and Media Science, University of Aarhus.
The purpose of VENUS is to develop narrative techniques for interactive
systems. It is characterized by consciously working both with theory and
practice; most projects aim at eventually producing a useful product, but
take their time, and if interesting theoretical issues grow out of the
practical work, the former gets priority over the latter.
The practical basis of the theory presented here are two half finished
and one finished system. The half-finished systems consist of small
pieces of interactive fiction designed by Berit Holmqvist and me and a
system for French teaching designed by Seren Kolstrup and Bjarn
Laursen. The finished system is a museum system built by Helle Juel
Andersson, Bjern Laursen, and me. It was tested at the local museum and
is described in Section 3.
In the example, window 2 is attracted to window 1, not physically on
the screen, but within the property space. So when window 1 moves to
card A, window 2 will do the same relative to its cards. Note also that the
cards need not be identical: window 2 is just required to move so close to
its destination as possible, i.e. to find a 'best match'.
This section describes a museum system exploiting a subset of the ideas
in the preceding section. It is implemented in Supercard 1.5 and presents
information about Nordic bronze age and employs a selection of the
techniques described above. The design is finished, but at present the
system only contains information about 70 items. The system was tested
at the local museum for a week. Two observers surveyed the users from
an adjoining room; in addition the museum guests were asked to fill out a
questionnaire, and tape-recorded interviews were made at the museum.
The system is in 8-bit colour, so the black/white screen dumps in this
section give only a partial impression of the real system.
The main system is for browsing and consists of four windows:
In addition, the system contains two smaller subsystems. The Mystery
of the Razor lets the user participate in solving the mystery of a bronze
age razor, the idea being to demonstrate archaeological methods of
interpretation.
in System C. (We will show that f.s is defined whenever f.s, is defined.) We will prove that, for
predicates p, 4 Over system state, s:
Note. The close correspondence between A, C, as given by the above relationships, does not directly
imply that B is a faithful implementation of A. A more direct proof should show that any property of
system A also holds in any process of system B, i,e, for any i (where p.s, denotes the predicate p in
which s is replaced by s,)
A safety property relating s, s,
We show that at every point in the computation of C, s may be obtained by applying the functions in
the incoming channels of i to s, in a certain order, Specifically, the following is an invariant for C.
invariant For process i there is an interleaving, y, of chj, i), for all j, j = i, such that
Proof. Initially, the invariant holds with y as the empty sequence. To prove that every change to s or s,
preserves the invariant, we observe that these variables may be changed by: applying an action in process
i (which may change both s, s,), applying an action in process j, j = i (which may change s and ch( j, i),
but does not change s,) or, process i receiving a message, g (thus changing s, but leaving s unchanged).
Action in process i: Executing
preserves the invariant, trivially, if f.s, is undefined. Otherwise, prior to the execution
Thus assigning f.s to s is legal whenever f.s, is defined. Next, observe that
Hence, the invariant holds with f.s,, f.s in place of s,, s, No channel chj, i), j = i, is changed by the
action in process i; thus, y remains an interleaving of the incoming channels of i.
Action in process j, j = i:
has the effect that s may be changed; also chj, i) is then extended by appending 'g' to it.
In the SAM, a number of supporting opera-
tions can be issued at each cvcle while the SEU is
issuing an ALUU operation. These operations may
include:
The following are the node types which can
occur in the DFG:s.
Therefore, the Function Cell must be able to
recognize and distinguish the types of the data
objects which are communicated in a DFG. More
specifically it must be able to recognize the struc-
ture of a data object received on an input, as well
as if the whole object has been received.
When data structures are represented by word
An important property of the Function Proces-
sor is that the length of one clock cycle can vary
between the Function Cells. Thus, the communi-
cation between Function Cells will be asyn-
chronous. This makes the Function Processor
containing p the maximal group g,, of H
which satisfies the following conditions:
Under these conditions and the restrictions on
the regime for solving read conflicts (see Sec-
tion 2.5) all processors in g,, evaluate expr)
to the same value. Therefore all these proces-
sors choose the same branch of the if state-
ment and hence are at the same program
point.
Note that a processor outside of g,, runs asyn-
chronously with the processor p even if it
evaluates expression erpr) to the same value.
Example. Assume that during program execu-
tion we have obtained the following group
hierarchy:
and that all processors execute synchronously
the if statement
where an instance of is a shared variable
relative to G ,, and another instance of x is a
shared variable relative to G,, Then the pro-
cessors of group G , work synchronously with
the processors of group G; and the same holds
for groups G, and G,, respectively. The pro-
cessors of group G, work asynchronously with
the processors of group G, even if the two
instances of variable x contain the same value.
When a processor of G has finished the exe-
cution of its branch, it waits until the other
processors of G have finished their statement
sequences. When all processors of G have
arrived at the end of the if statement, G
becomes maximally synchronous again. This
means that even if two processors of G work
asynchronously inside the if statement they
become synchronous again after the if state-
ment.
2. The expression depends on private variables,
constants, or functions. In this case we cannot
even be certain that all processors inside the
same leaf group evaluate expr) to the same
value. In this case both the group hierarchy H
and the synchronism are changed as follows.
Each leaf group of G generates two new leaf
groups: The first one contains all processors of
the leaf group that evaluate erpr) to true,
and the second one contains the rest. All new
leaf groups obtain the group number of their
father group. The new leaf groups become
maximally synchronous. Thus, the processors
inside the same new subgroup work syn-
chronously, while the processors of different
subgroups work asynchronously.
Again, when a processor reaches the end of
the if statement, it waits until the other pro-
cessors reach this point. When all the newly
generated leaf groups have terminated the ex-
ecution of the if statement, ie, when all pro-
cessors have reached endif, the leaf groups are
removed. The original group hierarchy H is
reestablished and G is again maximally syn-
chronous.
is semantically equivalent to
Thus, its semantics is determined by the seman-
tics of if-then-else and recursion. The semantics
of other kinds of loops, c,g. for or repeat loops,
invariants. The fourth componcnt (and also an
invariant), Pt ;, can be interpreted as synchroni-
sation. It is currently introduced only in its im-
plicit form, e.g. as a silent action in CCS [18]. The
fifth component. Z.s. 1S not to our knowledge a
part of any of the existing models. It captures
disabling of one event by another event and was
discussed in [9] from where we took its intuitive
meaning. As one now may see, the five compo-
nents described quite precisely the semantics of
the nets of Fig. 2, namely a ll),b, a w ,b. a
*A,, b - ,,4. 4 P* ,;,b and a.n,b. The ap-
proach to concurrency which is based entirely on
the concept of causality relation requires that for
every concurrent history the following holds: if
two event occurrences can be observed simulta-
neously, then they can also be observed in both
orders, and vice versa. This means that every
concurrent history besides being invariant-closed
must also satisfy the following: (So e 3. a v , b)
= (3o e 3. a - ,,b)z. (So e 3. a r- , b). Note
that this formula is built from simple traits. In
general, any formula built in this way will be
called a paradigm, and will characterise the inter-
nal structure of of concurrent histories.
Formally, the paradigms, w e Par, are given by
the following syntax.
The evaluation of the formulas o e Par follows
the standard rules [19]. Note that in this grammar
we need all three basic terms ,, . and ,..
A history 2 e Hist satisfies a paradigm u e Par
if for all a, b e dom(2), a = b == ou(a, b, 2). We
denote this by 2 e Par(as). Two paradigms, w and
w,,, are equialent, o - w,,, if Par(ai)= Par(o,,).
Let w, i = 1,...,5) be the following paradigms:
Proposition 7.
Proposition 8 (equality up to -). Par = [w,,
e.. =,; lk s 5 n i, s 5}.
From the last proposition it follows that we have
2= 32 different paradigms. However, the nature
of problems considered in Computer Science is
such that two of the ou,'s may be safely rejected.
The first ws, that we reject is a, since it rules out
causality and hence invalidates the sequential
composition construct. For a similar reason we
reject or, since it excludes systems consisting of
completely independent components. Thus we
consider 2= 8 paradigms:
Proposition 9 (relationship between components
and paradigms).
Your task in this experiment is to find and
replace the faulty component in each net-
work. Each network contains one and only
one faulty component. In order to solve each
problem, you may move the cursor around
the network. When the cursor is at a compo-
nent, you may do one of two things:
Each action you take has a cost: $1.00 to
display the output state of a component, and
$5.00 to replace a component. In addition, the
time you spend costs $05 per second ($3.00
per minute). You should try to solve each
problem for the lowest possible cost (just like
a technician in the real world). Remember
that if you solve the problem most efficiently,
you will be replacing only one component per
problem. You must keep working on each
problem until the faulty component has been
replaced.
To show you how to solve these problems,
the computer will guide you through each
problem during this practice session. A gray
dot will indicate which component to test
next. You should move the cursor to the com-
ponent with the dot, then press the left key to
test the component. The computer will then
show you which component to test next.
When you have gathered enough informa-
tion to determine which component is faulty,
the computer will place a gray square on the
faulty component. You should then move the
cursor to that component and press the right
button to replace the component and solve
the problem.
It is important that you pay close attention
during these problems because the second
part of the experiment will test your ability to
solve problems without guidance from the
computer.
[After the first ten problems:]
For the rest of the practice phase of the ex-
periment, the computer will provide guid-
ance only if you make an error. You should
choose which component to test or replace,
move the cursor to that component, and press
the appropriate button. If you are at the
wrong component or press the wrong button,
the computer will then display a gray dot or
Square to show you what to do next. Each
mistake you make will be charged to your to-
tal cost for solving the problem, so you should
try to make as few errors as possible.
Remember that the second part of the ex-
periment will test your ability to solve prob-
lems without guidance from the computer.
To show you how to solve these problems,
the computer will guide you through each
problem during this practice session. Gray
dots will indicate which components to test
next. You should move the cursor to one of
the components with the dots, then press the
left key to test the component. The computer
will then show you which components to test
next. At each point, you may test any of the
components marked with dots.
When you have gathered enough informa-
tion to determine which component is faulty,
the computer will place a gray square on the
faulty component. You should then move the
cursor to that component and press the right
Computerized storage and transmission of
pictures requires their transformation into a
matrix of picture elements (pixels). A stan-
dard television picture is conventionally
composed of 512 x 512 pixels. Furthermore,
256 levels of gray are required in order to
avoid contouring effects. It follows that the
representation of a black-and-white picture
requires about 2 megabits of information. In
addition, a real-time (25 frames/s) transmis-
sion of a picture would require a transmis-
sion rate of 50 megabitsls. Thus reducing the
number of bits in a picture is very useful in
increasing the efficiency of storage and trans-
mission. Reducing the number of bits re-
quired to represent pictures is known as pic-
ture compressiorn.
Picture compression has been at the center
of research in electrical engineering since the
1970s (eg., Ahmed and Rao, 1974; Chen and
Pratt, 1984; Netravali and Robbins, 1979;
Stuller, Netravali, and Robbins, 1980), but
only recently has the idea of picture compres-
sion been applied to stereo pictures. Dinstein,
Guy, Rabani, Tzelgov, and Henik (1989) dis-
cussed two possible approaches to stereo pic-
ture compression. Dinstein, Kim, Henik, and
Tzelgov (1991) suggested and evaluated an-
other approach to the same problem. Tzel-
gov, Henik, Dinstein, and Rabani (1990) have
shown that one can compress stereo pictures
and still preserve the information necessary
for depth perception (see also Dinstein et al.,
1989, 1991). The results of the experiments
from one category of vehicle to corresponding
locations in the other category. However, one
can attempt to find functionally equivalent
locations. The choice of locations depends on
one's theoretical explanations for the
CHMSL's effectiveness-the point of fixation,
triangulation of lights, or separation of light
sources. To allow for all possibilities,
CHMSLs were mounted in two alternative
positions on each vehicle: one position form-
ing a triangle with the standard brake lights
(triangle) and the other placing the CHMSL
at the natural open-road point of fixation (fix-
ation).
Both locations provided separation of light
sources. Given the different locations of the
standard brake lights in each of the four ve-
hicles, the resulting specific configurations
differed among the vehicle types. They are il-
lustrated in Figure 1. On the straight truck,
the triangle and normal fixation locations co-
incided. The locations designated as fixation
positions in Figure 1 represent areas where
following drivers may tend to look when fol-
lowing a vehicle with rear surfaces that are
entirely opaque.
In all cases the CHMSL's shape approxi-
mated that of the standard brake lights for
the specific vehicle type, and all were certi-
fied by their manufacturers as meeting Fed-
eral Motor Vehicles Safety Standard 108.
When mounted on the vehicles, the CHMSLs
It has been evident for a long time that the tradi-
tional modes of engineering education were going to
have to be modified and/or supplemented if engineer-
ing was going to continue to be done-and I don't
think anyone can imagine a world without engineer-
ing. The current, nominally four-year, academic cur-
riculum leading to the bachelor's degree is adequate
only as an introduction to what engineering is about
and the acquisition of a few simple tools necessary,
but not sufficient, for the doing of engineering. No
one in the last 40 years, at least, has pretended that
the skills thus acquired would equip the graduate with
anything more than a year or two of grace during
which he or she would begin the real task of becom-
ing an engineer by doing engineering. Even that mod-
est goal has been compromised by an increasing em-
phasis upon coursework oriented toward graduate
study and a research career, as evidenced by the
changing curricular requirements for ABET accredi-
tation of the program and by the interests and experi-
ence of the faculty (and the criteria for their appoint-
ment, tenure, and promotion).
Upon graduation (with whatever degree), the
young engineer presumptive could expect to work
some years under the direction of an experienced en-
gineer (who would continuously critique the perfor-
mance and output) and in the presence of other engi-
neers of varying experience and knowledge. Together
with the neophyte's own learning from the literature
and feedback from the plant itself (I mean real hard-
ware), this mentoring process could produce (under
ideal conditions) outstanding engineers and identify
the best career paths for those engineers to pursue.
Regrettably, this professional education and devel-
opment path (which I may have described too roman-
tically) seems to be rarely available in today's world,
and for many reasons. For one, too few of our really
outstanding students are encouraged to go on to ad-
vanced degrees, which may be highly desirable to
develop their full intellectual capabilities even if they
have no desire to go into research. I don't know ex-
actly how to allocate the responsibility, but too many
students are lured away too soon by high starting sal-
aries.
When the new hire shows up to work, too often he
is handed a computer program and told to design
something without ever finding out what the ''some-
thing'' looks like, or how it operates, or why it is
important in the process, or what's in the computer
program, or how good the output is (certainly not,
under any circumstances, as good as the four to six
significant figures in the usual computer output), or
what questions he should have asked before turning
on the computer. There are too few experienced engi-
neers around, and they are far too rushed to do all the
monitoring that needs to be done. Many companies
have virtually wiped out what had been outstanding
engineering staffs.
And as soon as an engineer gets a really firm grasp
on what the job is all about (i.e., has become an engi-
neering professional), he or she gets promoted into a
management position where the emphasis is mainly
on keeping the files in good order. I have no argu-
ment with finding and promoting good managers, but
too many companies have not learned that a good
engineer is not necessarily-in fact, maybe is only
rarely-a good manager. And few companies pay any
more than lip service to rewarding good engineers in
perks and salary commensurately with good man-
agers. So, many good engineers are turned into medi-
ocre managers.
The only positive response to these developments
has been the rapid growth of continuing education
programs. Originally conceived as a means of updat-
ing skills or providing highly specialized training to
mature engineers, these courses are increasingly di-
rected toward filling the gap between what the uni-
versities teach and what the young engineer needs to
know just to get started. Continuing education has
many drawbacks: the courses are generally not inte-
grated into degree-granting programs, nor do they
usually grant any kind of academic credit. (There are
exceptions such as National Technical University, a
consortium of universities operating through Colo-
rado State University via satellite.) Most courses in-
volve no homework or examinations; some do have
in-class problem solving, but the emphasis is defi-
nitely on communicating philosophy and facts rather
than developing specific operational skills. Courses
that do extend over a period of time frequently get
upset by students being called away on business.
There are no accrediting standards; the student is
largely dependent upon the reputation of the sponsor-
ing organization (university, professional society,
profit-making company, or individual entrepreneur)
for a clue as to the likely quality.
On the other hand, the workings of the free market
pretty well cull out the incompetent, the superficial,
the misdirected, and the overpriced. And, particu-
larly, through the professional societies, these courses
are frequently taught by outstanding engineers from
industry, who are not otherwise free to share their
knowledge. Being sent to a carefully selected set of
these courses should be a part of the early profes-
Robots are used for a variety of applications such
as robotic-aided manufacturing, assembly processes,
and automation [1-3]. In many cases, the tip posi-
tional accuracies of the robotic hand play an essential
role in successful robotic operation. In some indus-
trial processes involving nonhomogeneous tempera-
ture fields, the knowledge of the temperature distri-
bution in the multiaxis robotic arm equipped with a
multifigured robotic hand is required to determine the
time-dependent expansion and contraction of the ro-
botic arm and hand for the purpose of maintaining
robotic tip positional accuracies. Protection of the ro-
botic hand and arm from excessive temperatures may
also be an important issue. For example, in die cast-
ing, press work, and molding, the robotic arms have
proved that they can thrust their grippers into a die, a
furnace, or the like with fairly good precision, if the
expansion and the contraction cycles of the robotic
structure are fully understood. Furthermore, robots
are proving to be extremely handy for cryogenic ap-
plications involving extremely cold temperatures.
They can also be used in fire-mitigation operations.
In all these operations, an understanding of the ther-
mal interactions of a robotic assembly moving peri-
odically in a nonhomogeneous temperature field is
1mportant.
Robotic-aided processes conducted in relatively
high-temperature environments may require the use
of insulation to protect the robotic arm and hand
against heat transfer both by radiation and convection
It was recognized in the early stages of boiling
research that the heat transfer coefficient in flow boil-
ing is an interaction of nucleate and convective boil-
ing. The first attempt to relate the effects of heat flux
and mass velocity in the flow boiling process is due
to Davidson [1], who in 1943 introduced a dimen-
sionless ratio called the boiling number,
which was later used in numerous correlations. It
may be interpreted as a measure of the nucleate boil-
ing contribution: As heat flux increases, nucleation is
increased, and so is the nucleate heat transfer coeffi-
cient o,a,, increased mass velocity results in higher
convective coefficient o,,, lower wall temperature,
and hence decreased activation of nucleation cavities.
Large influx of flow boiling data from the nuclear
energy research in the 1950s and 1960s permitted de-
velopment of more sophisticated models.
The first true correlational model for flow boiling
coefficient was proposed by Rohsenow [2] in 1952 as
a simple addition of the nucleate and convective coef-
ficients (based on the then very limited available
data):
This model was used in principle by Chen [3], who
in 1963 formulated the first cohesive flow boiling
method, which became very popular. However, Chen
found it necessary to introduce a nucleate boiling
suppression'' factor to the term o,,,, in order to ac-
count for diminished contribution of nucleate boiling,
as convective boiling effects were increasing with
Solving the A matrix for the eigenvector
corresponding to the maximum eigenvalue
&. 6 and applying equations (7) and (8), the
scalar factors for the criteria are
Substituting the scale rating factor a in equation
(3) to the equally-important fuzzy-based criteria
C; to C, that were used in Table 4 results in
weighted fuzzy criteria. The decision set R is
determined according to equation (3). The fuzzy
sets C; to C,, and R are summarized in Table 5
in a similar format to Table 4. The maximum
Operation on the set R was performed to rank
the components according to equation (4). The
priority ranking for the components is shown in
column 16 of Table 5.
A component ranking comparison based on
equally and unequally-important criteria is
shown in Figure 3. It is evident from the figure
that selecting the appropriate model is essential
for obtaining a credible ranking of the
components of a system.
In this study, a risk-based methodology with
uncertainty evaluation and propagation was
developed for the purpose of creating an
inspection strategy. The ranking priority list for
inspection purposes was based on the assess-
ments of the probabilities of failure, resulting
consequences, expected human and economic
risks and the uncertainties associated with their
assessments. The consequences included pro-
perty damages, injuries and fatalities.
In order to utilize the impact of the
even in unstable situations, restabilizing so fast,
that in most cases the instability was not visible
to the observer and could only be visualized by
review of the cruise data later. Only one
problem remained: One was still able to set up a
race course which was so hard to the car, that it
lost its orientation on the track. This is mainly
due to the fact that the only means of vision for
the car are the three distances measured by the
ultrasonic sensors. From that, the car has to
deduce its position and its orientation in the
track. Therefore, in some situations and in
combination with sliding, the car loses its
orientation.
To cope with even those situations, a new
control strategy is currently under development.
It will base on 'memory functions' which will
store the values of intermediate variables, so the
definition of rules like
If the car is shifting over the front
wheels and wasn' t shifting over the front
wheels a little while ago, then . . . '
is made possible. These 'memory functions' are
implemented as conventional code blocks and
integrated in the fuzzy controller by
fwzzyTECH.
The fuzzy model car was implemented as a
teaching example and research vehicle, since
most successful applications of fuzzy logic
control in complex processes may not be shown
for competitive reasons. It was shown that for
such a complex problem, where no mathematical
model may be built in reasonable time, a first
prototype was made in 2% hours and a good
controller was made in two weeks.
From the theoretical and methodological view
it was shown that for more complex control
applications the simple 'fuzzy if-then' calculus,
employing MIN/MAX operaors, is insufficient and
it was shown which advanced methods can be
An operational propositon over f9' which is
Then D = {e4, e.), and
the test operations for p(D) are F, and F,.
(e4) = D, and [e,) = Dg,
In addition to the viewpoints of f-set theory
and operational statistics we now establish a
third root for the redesign considerations in the
preceding sections by critically surveying some
of the existing concepts in the area of traditional
fuzzy probability theory. According to the
approaches they take, these concepts can be
roughly classified into three different groups:
This group of papers mainly comprises the
works of Nahmias [18, 19], Puri and Ralescu
[21-23], Kwakernaak [16, 17], Kruse and Meyer
[13-15], and Coral and Gil [1], who all develop
notions of fuzzy random variables and assume
fuzzy reals as outcomes of experiments. We will
not further dwell on their results at this early
stage of our operational investigations, since we
believe that the technical levels of these two
approaches are not yet comparable - unlike their
intuitive background.
There is however a crucial question one has to
ask: What is to be observed as the outcome of an
experiment in order that classifications like
'approximately 3.123' do not become meaning-
less? There are only two (extreme) possibilities
which come to my mind: Either we use a
technical (i.e. non-natural, artificial) measure-
ment device: the result of such a measurement
will then usually be regarded to be an exact one.
Or we work with a measurement unit which can
be split so far still yielding something perceive-
able and/or reasonable, like tons, kilometers, or
miles: but in the latter case such a precise fuzzy
numbers seems inadequate to the conditions of
observations.
But as soon as semantical principles will have
been established for assigning linguistic labels
for fuzzy reals to observations, and to assign
functions r: R -- [0, 1] modeling fuzzy reals to
such linguistic labels, we will at once be able to
use these formal calculi for various evaluations
of data samples.
Thus P(A) gives the expectation E[A] of the
valuation A over U, whatever A is interpreted to
represent (e.g. costs, utility, compatibility or
'possibility', etc.), and cannot properly be
regarded as a probability. Furthermore, some
additional algebraic properties for V are
(activated) rules are available in some control
scenarios (in cases of no matching between
conditions of rules and available facts) then one
can set a certain fixed control value (usually a
mean value taken over the range of accessible
control values is a reasonable choice).
On the basis of obtained results we can easily
analyze the characteristics of the developed
controller resulting from the control rules, and
the implementation of the inference mechan-
isms. Should any changes be necessary one can
iterate through the previous part of the design.
The main menu and related pull-down menus
are selected using a keyboard. When selected
the relevant item is highlighted. Furthermore,
what is really a highly desirable feature, the
system forces an appropriate design sequence
causing that after the completion of a certain
design step only some appropriate items of the
menu (highlighted in the screen) could be
selected. (For instance without building or
loading control rules, the simulation option
cannot be accessed.)
During the design, all files (with relevant
extensions) are created and stored so that they
could be utilized in the future.
The manual which comes with the system is
structured into two separate parts. The first one
describes the software component and navigates
the user through a logical series of design steps.
The second part is hardware-oriented and
includes all necessary details (wiring, testing,
etc.) pertaining to the design of the hardware
version of the fuzzy controller.
Overall, the completeness of the design
process supported here, the user-friendliness of
the system itself along with its unified software-
hardware resources make the MYCOM system
useful and well-suited for engineering tasks of
fuzzy control.
HyperlLogic Corp.
2411 East Valley Parkway, Suite 294
Escondido, CA 92029, USA
Tel: 619-746-2765
Fax: 619-746-40S9
System Requirements: IBM/AT or compatible
with 286, 386, or 486 CPU and at least 1 Mb of
RAM. Microsoft Windows (version 3.0 or later).
Mouse (however, many activities in CubiCalc
can be performed without it).
Medium: floppy disks (both 5.25' and 3.5'
formats are provided).
The software CubiCalc ver. 1.0 constitutes a
platform for formulating and implementing
control and decision-making problems where the
relevant protocols involve if-then statements
with fuzzy predicates. CubiCalc handles prob-
lems with an arbitrary number of input variables
(subconditions), up to the performance limits of
the host machine. Due to existing visualization
capabilities the system can be found particularly
useful in solving problems of two-dimensional
tracking.
This type of problem constitutes a general and
important control task found in many areas of
engineering applications. One can consider, e.8.
a relatively easy scenario of chasing a cat by a
dog (this detailed example has been worked out
as an illustrative application of the software).
One can think of more advanced applications in
robot navigation, missile interception, tracking
aircrafts, etc.
Traditionally, there have been two general approaches
to performance measurement of computer networks:
modeling [1] and monitoring [2-4]. The modeling ap-
proach is appropriate when the network is not yet
operational. Two modeling techniques, analytical and
simulation, apply a workload to a model of the network
in order to derive performance parameters. An analytical
model is a static, mathematical approximation, whereas a
simulation is a computer program that models a network
and provides data about its dynamic behavior. There are
inherent problems in any modeling technique, primarily
because the model is a simplification of the real system.
Simplifying assumptions may compromise the extent to
which the model represents the actual behavior of the
network under real operating conditions. Another
problem is verifying the accuracy of the model and the
results. For the emerging distributed systems, modeling
techniques may provide only limited results.
The alternative to modeling is the monitoring ap-
proach, which provides real-time surveillance and con-
trol of an operational network. A monitor observes the
network, collects data, and presents results in a usable
form. Monitors can provide traces or profiles of network
transactions. Although useful for debugging purposes,
traces, or complete records of network traffic, impose
excessive storage and processing requirements on a
monitor. Thus, a profile made up of certain statistics
about network traffic is often more practical.
There are three basic types of statistical network
monitoring [5]: performance analysis, performance
verification, and network management. Through perfor-
mance analysis, the actual network performance is
measured and can be compared with that predicted by
analytical or simulation studies. Performance verification
ensures that a network meets its design specifications.
Finally, network management involves determining
whether a network is operating properly and efficiently.
These functions of network monitoring are outside of
the scope of the modeling techniques. Hence, monitors
become a vital part of the network in order to effectively
characterize its performance. A network monitor serves
as the agent for making measurements of appropriate
network parameters and summarizing the results.
This article discusses a network monitoring measure-
ment technique and tool in the context of the CSMA/CD
protocol for local area networks. The second section
briefly reviews this protocol and its performance im-
plications. The next section describes existing monitor
measurement techniqucs. In the fourth section, a mas-
ter/slave monitor measurement technique and the func-
tions and measurements that the monitor supports are
presented. Finally, in the last section, an experimental
implementation of the monitoring technique is described.
Carrier Sense Multiple Access with Collision Detec-
tion (CSMA/CD) is a medium access contention method
that defines the means by which two or more nodes
share a common bus medium. The CSMA/CD protocol is
described fully by the IEEE Standard 802.5. A brief
overview of its operation is provided in this section.
A node wishing to transmit listens to the medium and
acts according to the following rules [5].
Selection and measurement of the network parameters
are often dictated by the protocol of the operating
network. Certain characteristics of the CSMA/CD protocol
are significant, in particular, the slot time and the
retransmission scheduling algorithm. The transceiver
cable interface defined by the Ethernet specification is
also important for the measurement technique proposed
in the fourth Section.
The CSMA/CD standard requires a certain minimum
packet length in order to ensure reliable collision detec-
tion by all the nodes. The minimum packet length is the
number of bytes that can be transmitted during the
round trip propagation delay between the two furthest
points in the network. This delay is called the slot time.
Transmitting a packet for a slot time ensures that a
collision signal has time to propagate back to all nodes
involed in the collision while the nodes are still transmit-
ting
The retransmission scheduling algorithm defined by
the IEEE Standard 802.3 is binary exponential backoff.
When a collision occurs during transmission, the node
backs off and schedules a retransmission attempt. If
there are successive collisions, the node continues to
back off and reschedules transmissions until either the
transmission is successful or a maximum number of
attempts have been made. Thus, CSMA/CD specifies a
non-deterministic delay for accessing the channell
Finally, the transceiver cable interface is an important
interface in the network specification. When used in a
configuration, it ensures compatibility between physical
interfaces and the Ethernet. The transceiver cable, also
referred to as the Attachment Unit Interface (AUI),
interconnects the transceiver (which taps directly into
the coaxial cable) and the Ethernet controller of the
network node. The transceiver cable consists of five
twisted pair wires, each of which carries one of five
signals: transmit, receive, collision presence, control, and
power. In the measurement technique described in the
fifth section, the transceiver cable serves as the interface
between a monitor and a network node. Certain local
parameters at each node can be derived by monitoring
these signals.
Various monitor measurement techniques have been
proposed and implemented [2,5]. The following issues
should be considered when selecting an appropriate
technique for a given network.
Artifact-is the interference in a target system caused
by the introduction of a monitoring device. Besides
minimizing artifact, its magnitude should be known, so
that it can be removed from the measurements. This
ensures an unbiased indication of performance.
Artfical tmfficgenemton.-Traffic generators produce
traffic loads with known characteristics, thus facilitating
network testing and debugging. Artificial traffic genera-
tion can be used. to emulate high load. conditions,
produce repeatable and variable traffic patterns, and
extract timing information. For example, a traffic
generator can issue packets with periodic or Poisson
arrival rates, which facilitates comparison investigations.
Also, time-stamping of packets provides measures of
network traffic speed and node response time. If
generators communicate with a monitoring system, traf-
fic generation and data collection can be synchronized.
AAealdme analyss-The data collected by the monitor
can be either analyzed immediately (inn real time) or
stored for analysis later. When packets are transmitted at
a rapid rate, there is limited time to collect and analyze
data, which makes real-time analysis difficult.
Paarzmeters of interest-In order to decide which
monitoring approach to adopt (centralized, distributed,
or hybrid, which are described below), the parameters
to be measured must be defined. For a given network,
the best approach may change depending on the
parameters of interest.
The monitor measurement techniques may be divided
into three categories, depending on the location of
measurement: centralized, distributed, and hybrid.
The centralized approach is natural for broadcast net-
works. Typically, a modified interface taps onto the
channel to support the functions of the central monitor.
Two major types of centralized measurement techniques
have been proposed: the probe monitor and the spy
monitor [5]. These are depicted in Fig. 1.
The probe monitor injects packets into the network at
specified intervals and can record network parameters
for each injected packet, such as the channel acquisition
delay, transmission delay, and number of collisions. It is
an active monitor and thus introduces monitoring ar-
tifact. The monitoring resolution is dependent on the
frequency at which the traffic is injected and observed. If
the resolution is increased, the artifact increases as a
result.
The spy monitor is a special node dedicated to moni-
toring the network passively. It receives and analyzes all
of the packets on the network, and introduces no artifact.
For complete measurement, the spy monitor must be
capable of processing packets as they arrive. Hence, the
monitor must be equipped with sufficient processing
power and data storage space.
Although probe and spy monitors tend to be simpler
and less costly to implement than other types of monitors,
they cannot provide all of the desired information. Infor-
mation pertaining to a particular node interface cannot
be obtained. Also, central measurement biases some
timings, such as the arrival time of a packet onto the
network. An arrival recorded bv a central monitor does
not represent when the packet entered the channel. It is
offset by the propagation delay of a signal over the
channel. The monitor needs to account for this bias to
provide accurate measurements.
In the distributed approach, complete information
about network traffic at all nodes is available. Each node
captures data and periodically transmits its information
to a central analyzer. The central analyzer only analyzes
data received from the distributed monitors, since it
does not monitor the network. Two broad approaches
are adopted for distributed measurement: hardware
monitoring and software tmonitoring.
Hardware monitoring requires that each node monitors
traffic. This approach is more suitable for future network
designs, since it is expensive to modify the existing node
interfaces. Software monitoring, on the other hand, is
more flexible, because the software can be modified.
However, since nodes are typically of different types,
different software may be needed for each node. Also,
monitoring software may increase the processing load
and memory requirements of the nodes, and some types
of nodes may not be capable of meeting these needs.
Distributed monitoring, in general, has certain draw-
backs. Communication overhead results when transmit-
ting lrge amounts of data to a central location. Unless a
dedicated channel is used, each node must send data
over the network to the central analyzer, which intro-
duces artifact. Besides introducing artifact, using the
target network as an integral part of the monitoring
system could cause loss of the monitoring functions if
the network fails. Finally, synchronization of real-time
clocks at each node interface to coordinate timing
measurements poses a classical problem. The distributed
approach is depicted in Fig. 2.
The hybrid approach combines the essential features
of the centralized and distributed approaches, as shown
in Fig. 5. It allows accurate and comprehensive measure-
ment, while reducing modifications to node interfaces
and the amount of data sent to the central monitor. The
central monitor directly observes the network to collect
its own data and also receives data from each of the
node interfaces. It analyzes all data to calculate the
parameters of interest. Although artifact is introduced
onto the network if a dedicated channel is not es-
tablished, the artifact should be less than in the dis-
tributed approach since the central monitor locally col-
lects some of the necessary data.
Tables I and II summarize the features, advantages, and
disadvantages of the monitor measurement techniques
described above. The remaining sections of this article
describe a variation of the hybrid monitor measurement
technique that has certain novel features.
The previous section pointed out that the hybrid
technique is the most advantageous measurement tech-
nique in the sense that most of the measurements of
interest can be obtained without introducing severe
traffic overhead. However, in spite of its advantages, the
hybrid technique is impractical to apply in an operating
network, primarily because substantial changcs are re-
quired in both hardware and software for each node on
the network. To counter the drawbacks of existing hybrid
measurement techniques, this paper proposes a mas-
ter/slave measurement technique. Compared to existing
techniques, the master/slave approach differs primarily
in the following three areas: the location of the interface
between monitor and node, the use of an effective and
economical monitor bus, and the relationship between
master and slave monitors.
A major difficulty of applying an existing monitoring
technique in an operating network is establishing an
interface between the node and the monitor. Figure 4
depicts possible locations for this interface. Current
techniques typically place the monitor at locations
denoted by a, b, or c. However, in the master/slave
measurement techniquc, a monitor resides at the loca-
tion denoted by a'. As a result, this approach does not
introduce changes in the network nodes or in the
software running on the nodes.
The master/slave measurement system consists of four
basic components, which are illustrated in Fig. 5: 1)
master monitor, 2) slave monitor, 5) tapping cable, and
4) monitor communication bus. In this system, slave
monitors are connected to the transceiver cable (Attach-
ment Unit Interface, or AUI) through the tapping cable.
The slave monitors function passively (that is, they do
not interfere with the normal operation of each node ),
and they only receive data from the tranceiver cable. The
data include collision signal, received data, and transmit-
ted data. All received data are processed and reduced by
the slave monitor to minimize the monitor's memory
storage requirements and also the load on the monitor
bus. The processed statistics of each slave monitor are
collected by the master monitor via the monitor bus. No
statistics are transferred to the master monitor over the
medium. The monitor bus is a serial, asynchronous bus
whhich interconnects the master and slave monitors.
A master monitor collects statistical information from
the slave monitors via the dedicated communication bus.
A network user or manager can interact with the master
monitor to obtain network information. The information
available from the master monitor includes channel
utilization, total offered traffic, network delays, and in-
dividual node information. Analysis of the network delays
using artificial traffic generation and statistical informa-
tion is an essential function of the master monitor. The
functional block diagram of the master monitor is given
in Fg. 6.
A slave monitor passively monitors signals in the AUI
of an Ethernet node. The signals available for inspection
by a slave monitor are transmit data, receive data, and
collision detect. From these signals, performance infor-
mation is collected, processed, and transferred to the
master monitor by the slave monitor. The types of infor-
mation include the following:
A network user can then access this information via the
user interface in the master monitor. Figure 7 shows the
functional block diagram of a slave monitor.
Two different configurations can be established for
the master/slave measurement technique depending on
the network size and purpose of the measurement: 1)
fully distributed monitor system, and 2) locally dis-
tributed monitor system. Figures 8 and 9 depict the two
different architectures, respectively.
If the network size is small or the purpose of the
measurement is for system set-up or exhaustive testing,
then the fully distributed monitor system is the best
approach. In this configuration, every node is associated
with a slave monitor, and only one master monitor
exists.
When the network is lArge (for example, hundreds of
nodes), the fully distributed monitor system may not be
practical to implement. In this case, the locally dis-
tributed monitor system can be configured so that the
monitoring of the nodes is partitioned into smaller, local
areas as shown in Fig. 9. Only certain nodes are allocated
slave monitors. Since the slave monitors are designed as
plug-in modules, they can be easily moved from one area
to another. This configuration can provide a representa-
tive view of overall network performance if the nodes
with monitors are selected appropriately. Monitors should
be placed at nodes which are known to possess ypical
or desired types of traffic.
The hierarchical of structure of this system permits it
to be used in a multiple network environment. Multiple
master monitors can be set up, one per network, each
with its own set of slave monitors. Figure 10 shows a
configuration with three master monitors: MM1, MM2,
and MM3. MM3 is the main master monitor and receives
network information from MM1 and MM2.
An implementation of the master/slave measurement
technique is currently under development for use with
an operating AT&T 3BNet network [6] at Iowa State
University. The 3BNet interconnects certain AT&T Unix-
based computers and network-compatible peripheral
devices to form a local area computer network. The ISU
3BNet currently consists of one 3B20, two 3B5, and
twelve 5B2 computers. All are located in the same
building and connected over a 50-ohm coaxial cable
operating at ten megabits per second. The topology of
the network is shown in Fig. 11. A 3BNet communica-
tion protocol is implemented on top of the Ethernet
protocol.
The 3BNet network provides an experimental environ-
ment in which to implement and evaluate the mas-
ter/slave measurement technique. However, the tech-
nique itself is independent of any particular Ethernet
configuration. The distinctive components are the slave
and master monitors.
The slave monitor is a custom-built hardware device
consisting of the functional blocks shown in Fig. 12. It
passively taps onto the network at a node interface. As
shown in the figure, a simple plugin ''tee'' connector
attaches the slave monitor to the Medium Access Unit
(MAU) in the node interface. This tap is compatible with
the IEEE Standard 802.3 Attachment Unit Interface (AUI)
specifications. The slave monitor can be connected and
disconnected without interfering with the network opera-
tion.
The slave monitor has three primary functional com-
ponents: 1) an isolator unit, 2) a phase-locked loop
circuit, and 5) a processing unit. A standard, low-cost
processing unit which provides a small memory and
Supports serial communications was selected, namely
the MCS-8051 processor [7]. Since the storage require-
ments of the monitor are not very extensive, no addi-
tional memory unit is needed, although memory can be
added with minimal cost. All monitor software can be
stored in the four Kbytes of program memory on the
MCS-8051. An RS-422 driver interfaces the processing
unit with the serial inter-monitor bus. The serial port is
used to transfer data between the slave monitor and the
master monitor. RS-422 was selected for several reasons.
RS-422 differential line drivers provide good distance
and data rate characteristics. A multiple drop topology
for the inter-monitor bus is supported by the ''wired-
OR'' logical connection capability of the RS-422 inter-
face. Finally, communication over twisted pair calbles is
unexpensrve.
The isolator unit performs two functions: 1) provides
isolation between the slave monitor and MAU, and 2)
improves the fanout of the monitored signals. The phase-
locked loop offers timing-related information to the
processing unit. It can detect the start and end of a
packet, which defines the packet length. The state of the
collision presence signal can be used to derive a count
of the number of collisions. The processing unit calcu-
lates and records the following local parameters: transmit-
ted load, packet size, maximum and minimum packet
lengths, inter-packet transmission time, number of pack-
ets transmitted, number of packets received, packet
reception rate, and number of collisions. Statistics are
transmitted to the master monitor when the slave
receives a request from the master.
Control information provided by the master monitor
assists the processing unit in its calculations. Control
information includes network data rate, the required
format of the data packets sent by the slave monitors to
the master monitor, and tuning parameters. The tunitng
parameters correct for measurement discrepancies found
by the master monitor when the statistics provided by
the slave monitors do not match the statistics kept by
the master monitor.
The master monitor is an off-the-shelf hardware device
based on a single-board Ethernet controller. It serves
four essential functions: 1) monitoring data on the net-
work, 2) generating traffic on the network, 5) setting up
and polling the slave monitors over the serial inter-
monitor bus, and 4) presenting network parameters to
the user.
Figure 12 depicts the modules comprising the master
monitor. The main components are an Intel 82586 LAN
Coprocessor [8], an Intel 82501 Ethernet Communica-
tions Interface [8], a standard Ethernet transceiver. a
processing unit, a memory unit, and two serial interfaces.
The shared memory stores any data transmitted or
received by the master over the network or the inter-
monitor bus. An RS-422 serial port serves as the interface
between the processing unit and the inter-monitor bus
while an RS-232 serial port sets up communication with
the user. The master monitor can be attached to a
display terminal or a personal computer, depending on
the user's needs.
An 82501 Ethernet Communications chip provides the
interface between an 82586 LAN Coprocessor and the
network. The 82586 Coprocessor manages the medium
aCcess mechanism for the master monitor. This scheme
relieves the processing unit from packet reception and
transmission overhead. However, the processing unit
handles the communication with the slave monitors over
the inter-monitor bus.
The processing unit is responsible for configuring the
slave monitors and resolving any traffic-mismatch measure-
ment problems. That is, when statistics kept by the
master are not consistent with statistics received from
the slaves, any slave monitor can be dynamically recon-
figured so that the necessary requirements are met. For
example, each slave monitor reports the number of
collisions at its station to the master monitor. The mas-
ter, however, observes the network and maintains its
own count of the total number of collisions. If the
master detects a discrepancy between its count and the
sum of the received slave counts, then it can invoke
diagnostics to check the operation of the slaves. Other
discrepancies might occur in the measurements of colli-
sion time or transmission time recorded by the master
and slaves, which can be corrected by fine-tuning the
slaves to improve the accuracy of local parameter
measurement.
The master monitor can calculate approximate net-
work delays by injecting its own packets onto the net-
work bus and asking receiving nodes to send back the
same packets. It time-stamps injected packets within a
resolution of ten microseconds. The delay is calculated
based on the time stamp of a packet (the time at which
the packet was sent) and the time at which the packet is
received back. Delay measured in this way is representa-
tive of actual delays only if a large user population is
asSumed. If the number of users is small, the traffic
generated by the delay monitoring packets becomes
significant and introduces an error in the delay calcula-
tions. However, when there is a large number of active
users, delay packets have little effect on total network
traffic, and any error is small enough to be neglected.
To handle the computational load of the master
monitor, an Intel 8086-based single board computer can
be used with the Ethernet controller board. An Intel
Multibus (IEEE Standard 796 bus) interface, included on
the boards, supports multiprocessing and DMA transfers.
The extra processing unit can perform statistical com-
putations, data analysis, system control, and user in-
put/output for the master monitor.
In section three, several existing monitoring tech-
niques were examined and contrasted. This examination
led to the development of a master/slave monitoring
system. This monitoring system has several advantages
over the existing systems. By using a passive tap, the
slave monitors provide distributed measurement without
introducing changes in the network nodes or in the
software running on the nodes.
The slave monitors communicate with a master
monitor via an economical multidrop twisted pair net-
work. This network provides sufficient bandwidth without
the overhead and cost associated with a standard net-
work. The slave monitors consist of a small number of
offthe-shelf components. These monitors process the
data locally and send only the processed results to the
master monitor. The slave monitors can be reconfigured
by the master monitor to provide maximum flexibility.
The master monitor provides all the functions of a
central monitoring system. The monitor can generate
traffic with time stamps and can use the information
obtained from the slave monitors to provide detailed
node traffic information.
This system is under development at Iowa State Un-
iversity and will be used for both teaching and research.
The monitor system will assist in the development of a
distributed file system based on a large collection of
UNID machines connected via Ethernet.
Doug Jacobson is an Assistant Professor of Electrical En-
gineering and Computer Engineering at Iowa State University,
Ames. He received his B.S. in Computer Engineering in 1980,
his M.S. in Electrical Engineering in 1982, and his P.D. in
Computer Engineering in 1985 fom ISU. From 1981 to 1985,
he was a Senior Design Engineer at the Iowa State University
Computation Center. Hc is currently teaching in the area of
networking and data communication, including courses with
NTU. Areas of research include network performance and
protocol verification and spccification.
Sunll S. Gahomde is a Pn.D. student in Electrical Engineer-
ing and Computer Engineering at Iowa State University, Ames.
He recetved his BTech. in Electrical Eagineering from the
ndian Institute of Technology, Ksragpur, India, in 1985, and
his M.S. in Computer Engineering from ISU iin 1985. His re-
search interests are in the area of voice/data integration on
computer networks.
Jon-Nyun Klm is a Phh.D. candidate in Electrical Engineer-
ing and Computer Engineering at lowa State University, Ames.
He received his B.S. in Electronic Engineering from Seoul Na-
tional University, Seoul, Korea, in 1978, and his M.S. in Com-
puter Engineering from ISU in 1986. He is currently invotved in
the performance analysis of local area networks and protocol
specification and verification.
Jai Yong Lee is a P.D. candidate in Electrical Engineering
and Computer Engineering at Iowa State University, Ames. He
received his B.S. in Electronics Engineering from Yon-Sei Uni-
versity, Seoul, Korea, in 1977, and his M.S. in Electrical En-
gineering from ISU in 1984. From 1977 to 1982, he was a
research engineer at the Agency for Defense Development of
Korea. He is now a temporary Assistant Professor of EE/CprE at
ISU. His current research interests include local atea networks,
integrated voice/data communication systems, and protocol
design and analysis.
Dlane Thlede Rover is a Ph.D. student in Electrical Engi-
neering and Computer Engineering at Iowa State University,
Ames. She received her B.S. in Computer Science from ISU in
1984, and her M.S. in Computer Engineering from ISU in 1986.
From 1984 to 1986, she was an Ames Lb Associate at the
Microelectronics Research Center, and since 1985, she has
been an IBM Graduate Fellow. Her research interests include
parallel computer architectures and performance of computer
systems.
Mansoor Sarwar is a Ph.D. student in Electrical Engineering
and Computer Engineering at Iowa State University, Ames. He
received his B.Sc. in Electrical Engineering from the University
of Engineering and Technology, Lbore, Pakistan, in 1981, and
his M.S. in Computer Engineering from ISU in 1985. His inter-
ests include functional programming l4guages, parallel pro-
cessing, computer architecture, and local area networks.
Muhammad Shafiqq is a Ph.D. student in Electrical Engineer-
Ing and Computer Engineering at lowa State University, Ames.
He received his B.E. in Electrical Engineering from N.E.D.
University of Engineering and Technology, Karachi, Pakistan, in
1982, and his M.S. in Computer Engineering from ISU in 1986.
His interests include local area networks, protocol develop-
ment, and embedded computer systems.
s aduck paddles along the edge of
a pond, it nips at the tops of under-
water vegetation. When one nip catches
a shoot of Chara, a relative of the green
algae, it sends a spectacular system into
action. The force of the duck's bite trig-
gers an electrical mechanism in the
plant, and ionic current rushes across
the membrane of the nibbled cell. Then
the fluid inside the cell, the protoplasm,
stops its normal flow around the pe-
riphery. The protoplasm quickly jells,
preventing any leakage that could arise
from the duck's attack.
Chara is hardly the only plant that re-
sponds to external stimuli, All plants re-
spond to gravity as they grow, and
plants can have various responses to
light. Some follow a 24-hour cycle, ad-
justing the orientation of their leaves for
the maximum absorption of light dur-
ing the day. Some plants respond with
movement when they are touched by
predators.
What may be less obvious is how
plants respond to stimuli, Although
most people know that electrical signals
mediate the responses of an animal's
nervous system, it is less widely known
that plant behavior, too, is governed by
complex electrical mechanisms. Plant
cells, in fact, are hotbeds of electrical ac-
tivity, and plant studies have provided
much of the foundation of what is
known generally about electrical activi-
ty in cells. Chara has been important in
those studies and continues to be.
Physiological studies of electrical ac-
tivity began in the 19th century, and
since then animal and plant physiolo-
gists have worked in parallel. In order
to study the activity where it happens,
at the cellular level, investigators had to
find organisms in which the activity
could be studied in isolation from the
whole plant or animal. They also need-
ed to find cells large enough that they
could be probed with electrodes. In ani-
mal studies, the search led to the long
nerve cells of squids, in which axons,
the fibers carrying messages from the
cell body, are so large that they were
originally thought to be blood vessels.
Plant physiologists, on the other hand,
selected species of algae that have large
cells, such as the characean algae Chara
and Nitella.
In 1898 Georg Hörmann, a German
physiologist, observed that big differ-
ences in voltage measurements could
develop across cell membranes of
Nitella. When such differences are re-
generative they are called action poten-
tials, because the regeneration implies
action-the passing of an impulse. By
the 1930s, characean algal cells were so
well known that many investigators
studied them. For example, K. S. Cole
and Howard Curtis of the National In-
stitutes of Health, who later became
known as pioneers in the electrical ex-
citability of squid neurons, began
studying excitability in Nitellaa. These in-
vestigations showed that an action po-
tential in Nitella is accompanied by a
200-fold increase in the cell membrane's
conductance, as measured by the num-
ber of ions crossing the membrane.
They concluded that ions carry the cur-
rents that create the action potential.
Although plants are no longer the
leading organisms used in research on
the basis of electrical excitability, a num-
ber of investigators have significantly
advanced our knowledge of both the
mechanisms and the effects of electricity
in plants. Modern techniques common
to neurophysiology have been applied
to a variety of plants, and the results
show that electrical physiology in plants
is as complex as the systems found in
animals. Moreover, a variety of plants
use electricity to initiate action; exam-
ples are the closing of the leaves of a
Venus flytrap and the touch-driven
drooping of the leaves of some Mimosa
species. Nevertheless, the most detailed
information exists for characean algal
cells, which I shall examine here. The
electrical activity in these algae is worth
examining not only for its importance in
plant biology, but also because studies
of plant excitability may help us under-
stand the evolution of the human ner-
vous system.
Characean algae have been used in
much of the work on plant excitability
They are stoneworts, with a fossil
record stretching back to the Devonian
period, which began about 400 million
years ago, and they are the ancestors of
all higher plants. Extant stoneworts be-
long to a single family, Characeae,
which is composed of six genera in-
cluding Chara and Nitella. The majority
of the extant species inhabit the bottom
of clear freshwater ponds, where they
live entirely submerged.
As I have noted, the primary attrac-
tion of characean algae as an object of
study is the size of their cells. In Chara,
the plant body is composed of long in-
ternodal cells separated by smaller
nodal cells. A single internode may be
six centimeters long and half a millime-
ter wide, or about half as long as a
toothpick and half as wide. The inter-
nal structure of an internodal cell is un-
like that of an animal cell. Like all plant
cells, the external border is a cell wall,
which is composed of cellulose fibers
that provide rigidity to the cell but are
permeable to the extracellular fluid. Just
beneath the cell wall is a semipermeable
plasma membrane, which is composed
of two layers of lipids that are inter-
spersed with proteins. Beneath the plas-
ma membrane there is a layer of chloro-
plasts, the sites of photosynthetic
processes. Most of the interior of the cell
is a vacuole,a sac filled largely with wa-
ter and bounded by another membrane.
The area between the vacuolar mem-
brane and the plasma membrane is
filled with protoplasm; here are found
the cell nucleus and the cytoplasm, a
viscous fluid that contains the cell's or-
ganelles such as mitochondria and ri-
bosomes.
The protoplasm of characean cells
moves constantly around the periphery
of the cell, just beneath the chloroplasts.
The rotating belt of protoplasm travels
at a speed of about 100 microns per sec-
ond. Its movement, visible through a
microscope, is called protoplasmic
streaming or cyclosis. The streaming
process is driven by the same interac-
tions between actin and myosin that
create contraction in muscles. The
movement of the protoplasm mixes and
transports molecules through the cell,
which would take too long in such large
cells if diffusion were the only mecha-
nism available.
A fundamental concept in electrical
physiology is defined by the term po-
tential. A potential is a voltage across a
membrane, which is created by the sep-
aration of positive charges from nega-
tive charges. In biology, charges are car-
ried by ions. Positive charges are carried
by cations such as sodium, and nega-
tive charges are carried by anions such
as chloride. If one side of a membrane
has more positively charged ions and
the other side has more negatively
charged ions, then there is a potential.
or voltage, across the membrane.
Here I shall discuss four potentials:
membrane potential, resting potential.
receptor potential and action potential.
A membrane potential is the voltage
across a membrane, or a measurement
of the distribution of ions. The resting
potential is the membrane potential
when the cell is not being stimulated.
Both a receptor potential and an action
potential change the membrane poten-
tial. A receptor potential arises when a
receptor in a membrane, such as a mol-
ecular mechanoreceptor, is stimulated.
The stimulation generates an ionic cur-
rent that changes the membrane po-
tential, but the receptor potential de-
creases in magnitude with distance
from the stimulated receptor. An action
potential is a large, transient change in
the membrane potential that is self-
perpetuating, or regenerative, and it
can travel the length of the cell without
decreasing in magnitude.
Characean algae generate action po-
tentials when subjected to a variety of
stimuli, including a sudden change in
temperature, ultraviolet radiation, odor-
ants and mechanical action. These stim-
uli first cause the plant to produce a re-
ceptor potential. For example, a small
mechanical stimulus is converted by a
receptor into electrical energy that is
proportional to the magnitude of the
stimulus. In a resting characean cell,
there is a negative voltage inside the
plasma membrane relative to the out-
side of the cell. n other words, there are
more negatively charged ions inside the
membrane and more positively charged
ions outside the membrane. The recep-
tor potential generates depolarization,
a decrease in the voltage difference be-
tween the inside and the outside of the
cell. This potential generally lasts as
long as the stimulus is present, and it is
essentially an electrical replica of the
stimulus. If the stimulus depolarizes the
cell to a specific threshold level, an ac-
tion potential is generated.
An action potential in one area of a
characean cell causes protoplasmic
streaming to stop throughout the cell.
As I shall explain below, the action po-
tential causes external calcium to move
into the protoplasm. The increased cal-
cium concentration activates a protein
kinase that adds a phosphorus group to
myosin and thereby inhibits its interac-
tion with actin, which stops the driving
force behind protoplasmic streaming.
membrane and the vacuolar mem-
brane) and the vacuolar fluid (the fluid
inside the vacuole). The concentrations
of ions can be given in the ratio of extra-
cellular concentration to protoplasmic
concentration to vacuolar concentra-
tion, because only the relative values are
significant to this discussion. The aver-
age 1Onic-concentration ratios are
100:1:12,000 for calcium. 1:55:405 for
chloride, 1:50:340 for sodium and
1:1,100:1,030 for potassium. In other
words, the concentration of calcium is
low in the protoplasm and high in the
vacuole; the chloride concentration is
higher in the protoplasm than in the ex-
tracellular fluid, and higher still in the
vacuole; the distribution of sodium is
similar to that of chloride; and potas-
sium has a higher concentration in both
the protoplasm and the vacuole.
The location of an ion is determined
by a chemical force and an electrical
force (Figure 8). bn response to the chem-
ical force, an ion tends to go from an
area of higher concentration to an area
of lower concentration. The electrical
force pulls an ion toward an area of op-
posite charge, so that a cation, or posi-
tively charged ion, is drawn toward a
negative area. Consider a potassium ion
in the protoplasm. Potassium is more
concentrated in the protoplasm than in
the extracellular fluid, and thus the
chemical force tends to pull potassium
out of the cell. The resting potential of
the plasma membrane, however, is neg-
ative in the protoplasm relative to the
extracellular fluid. This creates an elec-
trical force that pulls potassium, a
cation, from the extracellular fluid into
the protoplasm. At equilibrium, the
chemical and electrical forces balance,
and there is no net movement of ions,
or charge. Therefore, an uneven distri-
bution of ions can create a stable mem-
brane potential.
The membrane acts like a capacitor, a
component that separates electrical
charge. By knowing the difference in an
ion's concentration across a membrane,
itis possible to calculate the voltage dif-
ference, or potential, at which the chem-
ical and electrical forces will be bal-
anced for that ion. This potential is
called the equilibrium potential or the
Nernst potential, after Walter Nernst,
the German physical chemist who de-
rived it, and the equation follows:
In this equation, E is the Nernst poten-
tial, R is the universal gas constant (8.31
joules per mole per degree in Kelvin), T
is temperature in degrees Kelvin, z is
the ion's valence, F is the Faradav con-
stant (9.65 s 10Coulombs per mole), C,
is the ion's concentration on the outside
of the membrane and C, is the ion's con-
centration on the inside of the mem-
brane. By assuming a temperature of 20
degrees Celsius or 293 degrees Kelvin,
which is approximately room tempera-
ture, the equation can be simplified to:
This equation gives E in millivolts. Con-
sidering the Nernst potential for sodi-
um across the plasma membrane, the
equation would be:
(For sodium, z = 1.) This means that
sodium would be in equilibrium across
the plasma membrane at a potential of
-98.5 millivolts.
Each ion has a Nernst potential. In
characean cells, the average Nernst po-
tentials for the major ions across the
plasma membrane are 59 millivolts for
calcium, 103 millivolts for chloride,-100
millivolts for sodium and -180 milli-
volts for potassium.
The resting membrane potential aris-
es from the combined equilibrium po-
tentials of all of the ions. You may have
noticed, however, that both the resting
potential of the characean plasma mem-
brane and the Nernst potential for
potassium are -180 millivolts. This is
not merely a coincidence. It has been
shown that in resting characean cells, as
well as in most resting animal nerves,
the membrane is largely impermeable
to calcium, chloride and sodium, but it
is readily permeable to potassium. This
means that the resting potential is large-
ly determined by the passive diffusion
of potassium. During an action poten-
tial, the membrane's permeability to
specific ions changes.
Ionic movement generates action po-
tentials in animal, plant and fungal
cells. In 1949 Alan Hodgkin and
Bernard Katz, both then at Cambridge
University, showed that external sodi-
um is necessary for an action potential
in a squid nerve. Through a series of ex-
periments, they developed the sodium
hypothesis, which states that the mas-
sive depolarization of an action poten-
tial results from sodium rushing into a
cell. It was later shown that tetrodotoxin
(the deadly poison found in the [apan-
ese puffer fish and removed before the
fish is eaten as sashimi) prevents an ac-
providing a secure trap. Then nearby
secretory cells exude enzymes, forming
a little stomach that digests the insect.
One of the best-known examples of
plant behavior comes from Mimosa p1u-
dica, often called the sensitive plant.
When the leaves of the plant are
touched, they bend over and appear
dead. The drooping arises from a me-
chanically driven action potential. More-
over, an action potential propagates
from the stimulated region throughout
the plant. This causes drooping in the
rest of the plant, a defense mechanism
apparently designed to make the whole
plant look unappealing.
Not all plant action potentials, how-
ever, cause obvious responses. In
Luffa-the plant whose gourd or fruit is
used for ''loofah'' sponges-action po-
tentials cause a transient inhibition of
growth. And in a variety of flowers,
pollen landing on the stigma generates
an action potential, which may be in-
volved in subsequent pollination or the
maturation process. In tomato seed-
lings, a mechanical wound induces
electrical activity that causes the accu-
mulation of proteins that limit further
damage to the plant.
Electrical phenomena control many
responses in plants. In a characean alga,
we understand many of the details of
the mechanism that leads from a duck's
nip on the plant to the cessation of pro-
toplasmic streaming. But we are ust be-
ginning to address the similarities be-
tween the electrical excitability in
characean algae and higher plants, let
alone animals. bn any case, it is apparent
that plants can perform long-distance
communication through electrical sig-
nals, such as the passing of information
from a mechanical stimulus from one
Mimosa stem to another. Many biolo-
gists continue to describe electrical ex-
citability as part of the animal world. In
the future, we should think of plants as
excitable too.
Thanks to Drs. Atsushi Furuno, Owen
Hamill, Roger Spanswick, MarkStaves,
Robert Turgeon and Scott Wayne for
their comments on this manuscript.
stant and one component, e.g. water vapour m', can be determined diagnost-
ically using
for given values of m', m; m?. We study the warm rain process with the
following transitions between water vapour, cloud and rain drops: activation
of cloud drops ACT, condensation on cloud drops CON, evaporation of cloud
drops E VC, autoconversion of cloud drops AUT, accretion ACC and evapo-
ration of raindrops E VR (cf. Fig. 1). The parameterization of each transition
as a function of mass fractions, temperature, pressure, and the two additional
parameters cloud droplet concentration at cloud base N,; and spectral width
CC, is based on Höller ( 1986). It is described in the appendix. Diffusional
growth of raindrops is ignored and cloud drops are assumed to be suspended
in the flow with zero terminal velocity. Both assumptions are explicit parts of
the physical distinction between cloud drops and raindrops. Note that satu-
ration is not assumed. The three ordinary differential equations describing
the system are
In the following, these equations are referred to as Cloud Microphysical Sys-
tem CMS. Transitions between water vapour and liquid water involve tem-
perature changes which are described by the latent heat of condensation l;(T),
a given function of temperature (cf. Eq. 34). The transformation rates are
continuous functions of their respective variables within the range of physi-
D;; are zero. The ratio of rain production to rain fall out is given by the coa-
lescence-Damköhler number D,. For large values of Dg, D; and D,- the fluxes
can be ignored and the system is approximately closed. Values near l describe
critical situations, while for small values the fluxes dominate the transforma-
tion terms.
To compare with the closed system, trajectories are shown in the same 3-D
space spanned by the coordinates (m ',T,m) which now, however, is a sub-
space of the four-dimensional phase space of Eq. 19. Thus two trajectories
might go through one and the same point in the (m ',T.m*)-subspace.
Similar to the closed system two groups of initial values with the same
equivalent temperature are studied. In the first set of experiments T.0 is
assumed so that the compensating flow of dry air does not change the temper-
ature of the system; consequently the heat exchange term vanishes and D;; = oo.
Three cases have to be distinguished according to the magnitude of D,
(il ]D;] I; fall out eevaporation or condensation. For this case, closed and open
system solutions agree in so far as first a strong motion towards the saturation
curve is observed, approximately in the plane of a constant equivalent tem-
perature. Then slow and isothermal motion occurs in the saturated state (cf.
Fig. 5). The equivalent temperature plane, however, is left now with devia-
tions of the order of 0.01 K. For t--oo trajectories approach fixed points char-
acterized by:
Cloud and rain drops vanish completely with t-- oo. All trajectories reaching
saturation are very close with slightly but significantly different temperature
values. The temperature difference, however, is much smaller than the initial
temperature difference. The final values m} and T, depend in a nontrivial
way on the initial values. Analytical relations were not obtained, except for
saturation:
but for undersaturated final states m} < mf' holds, Compared with saturated
states, undersaturated ones show higher final temperatures. The fixed points
appear to be stable. Asymptotic stability may be possible for limited regions
of the phase space but a more detailed stability analysis was not performed.
(iil ]D] = I; fall out=evaporation or condensation. If fall out of raindrops
approximately equals evaporation, trajectories first are heading the plane
m'=0 and then approaching the fixed point. Saturation may be reached for
m'a0. Again, changes do not longer occur in the constant equivalent tem-
perature plane.
(iii) ]D ] I; fall outs evaporation or condensation. If falling of raindrops
dominates evaporation, within a second or less all raindrops have left the sys-
tem and trajectories are found in the plane m 'a 0. Again, if saturation could
be achieved, the fixed points are significantly different but very close, as com-
pared with the initial distance between the respective trajectories.
The motion in the saturation state needs further explanation. The main
cept of constant equivalent temperature, the saturation state and in the case
of an open system by three Damköhler-numbers. Cloud microphysical pa-
rameters do not affect the stability behaviour of the solutions. As the results
are governed by the basic characteristics of the parameterization scheme and
even do only slightly depend on the involved parameters, they might be trans-
ferred to similar parameterization schemes. Thus it is likely that the complex
structure of the presented scheme can be significantly simplified without loss
of accuracy. This, however, was not intended in this study. The inclusion of
ice processes will complicate the stability characteristics. Modelers observe a
strong sensitivity of the precipitation development on timing, location and
formulation of the ice initiation process (WMO, 1986). This paper, like the
one of Chaumerliac et al. ( 1987), demonstrates how the constraint of the
saturation assumption can be avoided and suggests a convenient numerical
method. Studies of this type are strongly recommended for any cloud micro-
physical scheme. Understanding of the underlying basic mathematics of cloud
microphysical transformations is helpful for understanding cloud develop-
ment and in interaction with the dynamics.
H6ller (1986) derived parameterization schemes for cloud microphysical transformations
for use in a mesoscale model. One of these schemes and which is based on a saturation assump-
tion has been used successfully in numerical cloud studies (Schumann et al., 1987). In this
paper the scheme is extended to account for non-saturated states. For discussion, the reader is
referred to Hller (1982, 1983, 1984). Chaumerliac et al. (1987) developed a similar parame-
terization scheme with explicitly calculated supersaturation. In contrast to this scheme, the in-
tegration method was modified in order to increase the time constant of the condensation resp.
evaporation process. This was necessary to avoid numerical problems when the cloud micro-
physical equations were solved together with the dynamical equations. Transformation rates
are given as functions of partial densities p', cloud droplet concentration at cloud base N,4a, and
spectral width C,, If mass fractions m' are required,p' may simply be replaced by p'=spm' with
p determined by
Cloud drops are assumed to follow a non-normalized log-normal density distribution:
with
m - drop mass
Ng; - total number concentration of cloud drops in m?
a; - variance of f;(ln m)
Based on detailed spectral calculations Höller ( 1984) derived a parameterized approxima-
tion of the form
with
and 44= 1 kgm%g' is a reference conversion rate for the autoconversion rate. The collection
kernel of Berry and Reinhardt ( 1974) was used for the detailed calculations which are de-
scribed in Höller ( 1982 ). The autoconversion formula is valid within the range (PG,ia. P..)
where pGa. =gm? is the maximum value and where pG,;, is determined as the minimum
value from either N,,,(p')> 500 cm? or
Below the minimum value autoconversion is a linear function in pf, whereas above the maxi-
mum value G,, autoconversion is constant. The autoconversion vanishes for vanishing cloud
drops only:p'=+0.
Following Manton and Cotton ( 1977) raindrops grow at a rate ACC given by
where p,a= 1.28 kg m' is a reference density value and a,=4.71 m' kg' 4'. The accretion
vanishes if either cloud drops or rain drops vanish, that is for pf=0 or p'a0.
Rainfall in subtropical southern Africa is strongly seasonal, with a well-defined summer (December-March)
maximum over most of the subcontinental interior (Nicholson et al., 1988; Lindesay, 1993). Relatively small
areas along the eastern and southern coasts receive year-round rainfall, and the south-western tip of the
subcontinent has a winter (June-September) rainfall maximum. Most of the interior is semi-arid to arid, and
a marked rainfall gradient exists from the wetter east coast to the hyper-arid west coast (Tyson, 1986;
Lindesay, 1993). Important features affecting atmospheric moisture over the region are the high potential
evapotranspiration, exceeding 2000 mm year'' over the west-central interior in summer due to generally
clear skies and high insolation, and low levels of available surface moisture from the arid, sparsely vegetated
continental surface (Henning, 1989; Lindesay, 1993). Most of the moisture that contributes to precipitation
over southern Africa therefore must be imported over the subcontinent from source regions elsewhere.
Despite the importance of water vapour transport to rainfall over southern Africa, relatively few analyses
of the availability of atmospheric moisture have been undertaken for any part of the region. On a hemi-
spheric scale James and Anderson (1984) have shown that tropical-mid-latitude transport of water vapour
increases the growth rate and vigour of mid-latitude baroclinic systems. In the southern Africa region, mean
atmospheric water vapour content and vapour fluxes have been investigated over South Africa (McGee,
1971, 1972, 1975, 1986), as has the interannual variability in water vapour (McGee, 1978). The relationship
between precipitable water and rainfall has also received attention (Harrison, 1988). Whereas regional
studies of atmospheric moisture have been undertaken for North Africa (Flohn et al., 1965), West Africa
(Adedokun, 1978; Anyadike, 1979), South America (Rathor et al., 1989), North America (Benton and
Estoque, 1954; Hastenrath, 1966; Rasmusson, 1967) and Australasia (Hutchings, 1961), the content, intra-
and interannual variability, and transport of atmospheric moisture over the southern African region as
winds and dew-point temperatures at 850, 700, 600, 500, 400, and 300 hPa, have been obtained from Monthly
Climatic Data for the World (US Department of Commerce, various dates) for those stations in the southern
African region with the best data availability for the period (Figure 1). Stations with the shortest data series
are Douala, Kinshasa, and Grootfontein; the location of these stations, coupled with the lack of other
adequate radiosonde observations over tropical Africa, makes the interpretation of flow fields north of 10'S
difficult. Wind vectors have been resolved into zonal and meridional components, with westerly zonal
components positive and northerly (poleward ) meridional components positive. Monthly means of outgoing
longwave radiation (OLR) on a 5 s 5' grid, from the NOAA AVHRR, were available for the 10-year period
1975-1984 and have been used to identify areas of convective activity in the mean and for anomalously
wet/dry months within this period. Although the circulation and rainfall data have been analysed over
a 20-year period, it is possible to make qualitative comparisons between the longer term meteorological
results and the shorter term OLR changes between wet and dry months.
Water vapour transports in the zonal and meridional planes are calculated from the equations
(after Hutchings, 1961), where g is gravitational acceleration, is specific humidity, and u and v the zonal and
meridional wind components respectively. Specific humidity has been obtained using the method described
in McGee (1971) to derive saturation vapour pressure (E,) from dew-point temperature (T, ).
and specific humidity from E,:
Although the relationship between j and T,, is non-linear the underestimation of ä by this method is
sufficiently small, particularly in lower latitudes, to make the estimation acceptable (Gaffen et al., 1991), The
vertically integrated zonal (W,,) and meridional (W,.) vapour fluxes have been calculated by integrating
equations (1) and (2) between 850 hPa and 300 hPa using the trapezoidal rule:
where the total flux is uq = 4j + u (after Rathor et al., 1989).
The vertically integrated vapour flux has been calculated from the equation
(after Hastenrath, 1966). The use of monthly mean data makes it impossible to calculate eddy and hence also
total vapour fluxes. The term 'vapour fluxes' as used here, therefore, refers to circulation or advective fluxes of
vapour. The vector mean wind V has been obtained from the equations (after Giles, 1963):
for magnitude, and
for direction, where V,, is the zonal component and V, the meridional component of the wind.
The total precipitable water in the atmosphere across southern Africa in October and January is illustrated in
Figure 2 to show the changes in the actual distribution of moisture between the wet and dry early and late
summer months. An increase in moisture content with the progress from early to late summer is evident, with
maximum wet-month precipitable water reaching only 24 mm in October (Figure 2(a)) and exceeding 30 mm
in January (Figure 2(c)). During dry months of January the air contains more moisture than in wet months of
October. In all cases maxima are found over the central subcontinent at 3 15'S, and in the dry months of
October and January there are additional maxima over Madagascar (Figure 2(b and d)). This coincides with
the eastward shift in the preferred locations of tropical-temperature troughs and cloud bands between wet
and dry Januaries (Harrison, 1986a, b). Minimum values of precipitable water (less than 10 mm) are found
over the south-western parts of the subcontinent, particularly, and as is expected, in October (Figure 2(a and
b)). Although these moisture fields allow speculation on some aspects of the circulation associated with
moisture changes over southern Africa, it is not possible to formulate a precise picture of the sources of
moisture over the subcontinent without analysing the fluxes of vapour over the region. The presentation and
analysis of vapour fluxes across southern Africa forms the focus of this paper, with a view to identifying
important source regions, circulation controls, and interseasonal and interannual variability as these
contribute to rainfall changes.
Analysis of vertically integrated water vapour fluxes for mean October conditions (Figure 3(a)) reveals
maximum fluxes of 1400 g cm '' 4'' over the western Zaire Basin in tropical Africa, and secondary maxima
in excess of 1000 g cm'' 4T' over Madagascar to the east, and over central South Africa at about 30'S,
Minimum moisture fluxes occur in the subtropics across Namibia, Botswana, and southern Zimbabwe at
approximately 20'S. Fluxes over Madagascar are north-easterly, but further north over the tropical Indian
Ocean, where October atmospheric moisture levels are relatively low (Hastenrath and Lamb, 1979),
southerly fluxes prevail. Across the east coast of the subcontinent north of about 20'S vapour fluxes are
easterly to south-easterly, becoming north-easterly further to the west. North-easterly fluxes extend over
northern Namibia, but south of 25%S all fluxes are westerly to west-north-westerly (Figure 3(a)).
fluxes in wet Januaries. Vapour fluxes further north are largely unchanged from the wet-January pattern,
with north-easterly fluxes in both cases (Figure 5(b and c))
Vapour fluxes on the zonal component of airflow are weakly westerly in the tropical regions in wet months
of January (Figure b(a)); easterly fluxes extend over the subtropics, with a maximum across the subcontinent
at 20*S, and there are westerlies further south over South Africa. Westerly fluxes also occur between the
Equator and about 15'S in dry Januaries (Figure 6(b)), but are stronger and more extensive than in the wet
months. Fluxes over the subtropics are easterly, with a maximum concentrated over Zimbabwe at 20'S,
Although maximum easterly fluxes over the subtropical regions are not as strong in wet as in dry Januaries,
the area in which fluxes exceed 600 g cm ' ' 4'' is larger in the wet months, as is the overall area of easterly
fluxes. Comparison of zonal vapour fluxes for anomalously wet and dry months of January thus indicates
a northward displacement of the mid-latitude westerly and subtropical easterly fluxes between wet and dry
months (Figure 6(a and b)), and a simultaneous northward movement and strengthening of the tropical
westerly fluxes which extend across the subcontinent from the Atlantic to the Indian Ocean in dry Januaries
summer, when rainfall-producing systems are more baroclinic in structure, vapour fluxes on the zonal
westerly airflow are dominant. This is particularly true in wet months of October, when vapour flux
convergence occurs over the west-central subcontinent and divergence further east. An important influx of
vapour in these months comes from the north-west, from the southern tropical Atlantic Ocean. This flux is
absent in dry Octobers, when south-easterly fluxes dominate the vertically integrated vapour fluxes.
Convergence over the subcontinent is reduced in these months, and only continentally modified drier air is
advected southward over the summer rainfall region.
During January the pattern of vapour fluxes across southern Africa is quite different, reflecting the more
important role of the tropical circulation and tropical -subtropical interactions in late summer rainfall-
producing systems. Easterly zonal fluxes across subtropical southern Africa are stronger in January, but it is
the changes in the northerly meridional flux over the central subcontinent that are important for rainfall
variations. Wet months of January are characterized by a strong north-easterly flux over the eastern
subcontinent, advecting moist tropical Indian Ocean air into a convergence area along the ITC and ZAB.
There moist north-westerly fluxes from the tropical Atlantic Ocean provide further input to the strong
convective activity over central southern Africa, verification of which is provided by the negative OLR
anomalies in these months. Poleward fluxes of moisture continue southward from the convection area across
South Africa, raising moisture levels above those critical for effective rainfall. In dry Januaries the importance
of the tropical circulation declines; fluxes are less meridional, with stronger westerly fluxes across South
Africa, and south-easterly fluxes across the east coast indicating a strengthened anticyclone over the
south-west Indian Ocean.
Adjustments in the circulation and vapour fluxes over southern Africa between months and years are
clearly important in influencing rainfall over the subcontinent, with moist tropical air from both the Atlantic
and Indian oceans contributing to rainfall-producing systems. Changes in the areas of vapour flux conver-
gence and divergence are equally important, however, and the January situation with convergence and
convection in a disturbance over central southern Africa at about 20''S, coupled with poleward vapour fluxes
across South Africa, seems to be the most favourable for rainfall in the summer rainfall area. Although it has
been possible to identify some of the main features of the vapour fluxes and their interannual variability over
southern Africa, more detailed work is needed to confirm these findings for the data-sparse regions of central
southern Africa. Additional analysis using more spatially comprehensive data would also contribute to the
identification of vapour source regions and the changes that take place in those regions over time. Only with
improved understanding of factors such as vapour flux variations will more complete explanations of rainfall
variability in southern Africa become possible.
Helpful comments by Professor P. D. Tyson and an anonymous referee on previous versions of this paper are
gratefully acknowledged. Mrs W. Job and Mr P. Stickler drew the diagrams. The South African Weather
Bureau, Pretoria, are thanked for supplying the rainfall data, and Dr P. Aceituno for providing the the OLR
data. The research forms part of the Special Programme on Climatic Change: Analysis, Interpretation and
Modelling (SACCAIM), funded by the Foundation for Research Development.
The first recipe for synthetic nutrient medium for
Tetrahymena thermophila is more than 40 years old
(Kidder and Dewey 1951). This type of medium has
been used for gaining insight into nutrient requirements
(Holz 1973), metabolic pathways (Kidder 1967), uptake
mechanisms (Dunham and Kropp 1973) and reactions
between inorganic and organic molecules in the medium
(Hutner 1972). In spite of the fact that it was possible
to maintain doubling times and final cell densities
comparable to those obtained in the best complex
medium, the synthetic medium are deficient in at least
one respect; single cells in l ml cultures die shortly after
being transferred to such a medium (Christensen et al.
1992).
We have recently shown that T thermophila grown
under nutritional stress situations need various com-
pounds not required at initial densities of more than
1,000 cells per ml. The stress conditions consist of
growth at low population density (Schousboe et al. 1992)
or growth at low ambient nutrient concentrations (Chris-
tensen et al. 1992). These results agree with the notion
that the cells produce and release compounds with
effects like those of growth factors (Ghiladi et al. 1992).
Here we present further detailed evidence for these
Views.
Cells: The following cell lines of Tetrahymena thermophila were
used: wildtype inbred strain B 1868-III (Orias and Bruns 1976); the
mutant strains MS-1, secreting lysosomal enzymes at low rates
(Hiünseler et al. 1987); II8G, defective in food vacuole formation
(Tiedtke et al. 1988); and SB 281, releasing no mucocysts (Orias et
al. 1983).
Nutrient medium: Cells were grown either in a complex
medium, PP}S, or in a standard synthetic nutrient medium, SSM,
PPYS is a solution of 0.75% proteose peptone (Difco Laboratories,
Detroit, Michigan, USA) enriched with 0.75% yeast extract (Difco)
and-salts. SSM consists of 19 amino acids, 4 nucleosides, glucose, 7
vitamins, salts, and citrate (Szablewski et al. 1991). In both cloning
and mass culture experiments SSM was diluted in the ratio of 1:1 with
TRIS/HCI buffer (pH 7.5). When testing the presence of stimulatory
compounds in extracellular fluid, we replaced the buffer with ex-
tracellular fluid, keeping the concentrations of the nutrients in the
medium constant. Cells requiring special conditions for growth and
multiplication were grown in media modified to fulfill these condi-
tions: mutant strain II8G was grown in medium enriched with iron
and copper salts and folic acid as prescribed for this mutant (Tiedtke
et al. 1988). Chemicals were from Sigma Chemical Co., St Louis,
Missouri.
Cultures: Stock cultures were grown in 2 ml portions of standard
synthetic medium, SSM. Experimental cells were grown in 10 ml
portions of SSM in conical flasks for 20 h, transferred to 10 mM
TRIS/HCI buffer (pH 7.5). centrifuged for 3 min at 800xg and
resuspended in the buffer. This procedure was repeated three times
and resulted in more than a 10'-fold dilution of the extracellular fluid.
The cells were then divided into three batches and used as follows: i)
for preparation of extracellular fluid, ii) for multiplication analysis in
10 ml portions of SSM where effects of initial cell density and
presence of hemin, phospholipids, proteins or extracellular fluid were
studied, and iii) for cloning analysis in various volumes of either
PPYS or SSM or SSM enriched with either hemin, phospholipids or
proteins. The cells were grown at 37%C
Preparation of cell-free extracellular fluid: Cells were starved
in 10 ml TRIS/HCI buffer (pH 7.5) in conical flasks for 0-5 min and
5 h at a density of 50.000 cells per ml. After that, a sample of a culture
was transferred to a centrifuge tube and placed on top of 2 ml of a
solution of 10% Ficoll and precipitated for 10 min at 3000xg. The
Ficoll solution was sterilized by filtration and used at 4%( The
cell-free supernatant, containing no cells, was removed and used in
the experiments. The Ficoll has a high specific weight and high
viscosity and prevents the cells from swimming back up. To certify
that no cells were left in the extracellular fluid a sample was placed
in a small Petri dish and checked under a stereo microscope.
Cloning procedures: After centrifugation and resuspension in
the buffer single cells were transferred to two different sizes of
cultures, 1 ml or 10'' ml (lul). One ml cultures: single cells were
transferred to either PPS, SSM or SSM enriched with either hemin.
phospholipids or proteins with a fine pipette (see Schousboe et al.
1992). One ul cultures: single cells were suspended in either SSM or
PP}S in test tubes at a density of 1,000 cells per ml. By means of a
10 ul Hamilton pipette droplets of lul from the test tube were placed
on the top of paraffin oil placed in a glass Petridish. The droplets sank
to the bottom of the dish and the paraffin oil prevented evaporation
of the medium. The dish had a diameter of 7 cm, the paraffin oil was
autoclaved at 121%C for 45 min and reached a height of 2 mm in it.
Only droplets containing a single initial cell were kept under observa-
tion.
Cell densities in test tubes and droplets were recorded every day.
In most cases many cells (1,000) were present already after the first
24h. In all cases droplets and tubes were kept under observation until
day 4.
Compounds added to SSM: Hemin and asolectin, a crude
preparation of phospholipids, were used at final concentrations of
7.5 uM and 50 ug per ml, respectively (see Schousboe et al. 1992 and
Christensen et al. 1992). Bovine serum albumin,egg albumin, trypsin
and soy bean trypsin inhibitor were used as sources of proteins. They
were dissolved in redistilled water, sterilized by filtration and used at
final concentrations of 50 ug per ml.
Cell counting: Culture samples were enumerated in an electronic
particle counter (see Christensen et al. 1992).
All experiments were repeated more than 10 times with similar
results.
Initial cell density is an important parameter in cell
multiplication of Tetrahymena in synthetic medium.
Figure 1 shows the number of cell doubling of T ther-
mophila, wildtype, as a function of time and number of
cells transferred into SSM. In cultures having 750 or
more initial cells per ml the cells thrive, whereas 250
and 500 cells per ml die within 20 h. One thousand or
more cells per ml have a short lag phase (about 1 h);
750 cells per ml have a lag phase lasting about 10 h.
Extracellular fluid is also an important parameter for
cell proliferation in synthetic medium. Figure 2 shows
the number of cell doubling of T thermophila, wildtype,
as a function of time and concentration of added
extracellular medium where cells were starved for 5 h.
In all cases the initial cell densities were 250 cells per
ml. Without any addition of extracellular fluid the cells
do not multiply. If extracellular fluid is added in the ratio
of 1 portion of SSM to 0.02 portion of extracellular fluid
(to achieve 2% of extracellular fluid in the medium) the
cells multiply with a lag phase of about 15 h. If the
extracellular fluid amounts to 10% of the volume, the
cells have a lag phase lasting about 10 h before multi-
plying, and where extracellular fluid is 50% of the
media's volume the cells have a short lag phase, repeat-
ing the growth curve representing 2,500 cells per ml.
Hemin or phospholipids or proteins also initiate cell
proliferation. Figure 3 shows the effects of these com-
pounds on cell multiplication in the assay system used
for testing effects of extracellular fluid. Cells in SSM
enriched with these compounds multiply after a short
lag phase (0-1 h), repeating the growth curve repre-
senting 2,500 cells per ml.
It should be noted, that if the cells survive and start
multiplication then they multiply at the same doubling
times (about 2 h) and reach the same final cell densities
irrespective of the duration of the lag phase.
Results of cloning experiments in various volumes of
PPYS, SSM and SSM enriched with either hemin, phos-
pholipids or proteins, are listed in Table 1. T ther-
mophila, wildtype, in l ml portions do not form clones
in SSM unless it is supplemented with hemin, phos-
pholipids or the proteins bovine serum albumin, egg
albumin, soy bean trypsin inhibitor or trypsin or grown
in PPYS. In 1 ul portions of SSM, however, the cells
have a high probability to form clones. Thus the cloning
efficiencies of T thermophila in SSM are culture volume
dependent. The presence of paraffin oil in test tubes with
one ml of either SSM or PP)S does not affect the cloning
efficiencies (data not shown).
Many workers have shown that a wide variety of
compounds are released from Tetrahymena. These in-
clude hydrolytic enzymes (Tiedtke et al. 1992), mating
pheromones (Adair et al, 1978), transcription inhibiting
factors (Andersen et al, 1980), mucocysts and accom-
panying compounds (Maihle and Satir 1986) and
proteins without ascribed functions (Suhr-Jessen 1987).
Our results suggest that Tetrahymena also release un-
known compounds with effects on cell proliferation.
Also known groups of compounds (tetrapyrroles, phos-
pholipids and proteins) have similar effects on cell
proliferation (Schousboe et al. 1992 and Christensen et
al. 1992). The cell-produced factors appear to be dif-
ferent from those added by us in that they are active at
lower concentrations.
Our experiments show that cell-free media samples
of a TRIS/HCI buffer, in which cells have been starved,
improve cell proliferation and reduce lag phases in a
concentration dependent fashion, Figure 2. As controls,
we have also made experiments with cell-free media
samples, where cells have been exposed to starvation
conditions for a period of 0 to 5 min. In these cases we
did not see any stimulatory effect on cell multiplication.
Furthermore, we have shown that single cells in 1 ul of
standard synthetic medium (corresponding to an initial
cell density of 1,000 cells per ml) form clones with high
probability. These observations indicate that com-
ponents from dead cells - or physical contacts between
the cells - are not responsible for the stimulation.
Therefore, it seems that a small volume of growth
medium allows the single cell to build up sufficient
concentrations of one or more molecules stimulating its
own growth and multiplication. In larger volumes these
molecules may become too diluted to be effective.
The ideas presented here may explain why initial cell
densities are decisive for the fate of T thermophila in
standard synthetic medium. The amounts of growth
stimulating factors released at low initial cell densities
may be insufficient for stimulation of the cells. However,
at higher cell densities the amounts of these molecules
may be sufficient to support cell proliferation. This idea
may also explain why cells exposed to starvation con-
ditions for 4 h can multiply when inoculated at 250 cells
per ml in synthetic medium as previously reported
(Christensen et al. 1992): the inoculation volume needed
to transfer cells to the growth medium may contain
sufficient amounts of growth stimulating compounds
which had been released during the period of starvation.
We have previously shown that certain porphyrins,
including hemin, as well as phospholipids, improve
proliferation under nutritional stress and conditions of
expected low cloning efficiencies in standard synthetic
medium (Schousboe et al. 1992 and Christensen et al
1992). Here we show that these compounds, in addition
to certain other proteins, stimulate cell growth and
multiplication in mass culture experiments where the
cells are inoculated immediately after the extracellular
medium had been diluted more than 10'-fold by
centrifugation and resuspension in a TRIS/HC1 buffer.
These data, combined with results obtained with ex-
tracellular fluid and ''micro-cloning'', seem to indicate:
i) that Tetrahymena thermophila produce and release
growth stimulating compounds into the growth medium;
ii) that the compounds are responsible for cell prolifera-
tion in standard synthetic nutrient medium; iii) at low
initial cell densities we can see that exogenous com-
pounds as hemin, phospholipids and certain proteins can
substitute for cell-produced stimulatory compounds
which may be present, but at ineffective concentrations;
iv) stimulation is expressed neither in changes of dou-
bling times, nor in attainment of higher final cell
densities, but in the rate with which the cells leave the
lag phase and start growth and multiplication. We choose
to name these stimulatory compounds growth factors,
and point out that they appear to represent a group of
compounds different from the well-known nutritionally
required components.
PPYS supports cell proliferation at low initial cell
densities. This may mean that the complex medium
contains growth factors or growth factor-substituting
molecules, e.g. proteins. In SSM these compounds are
missing and the cells are forced to adapt or ''condition''
the medium by releasing their own growth factors. Thus,
survival, growth and multiplication are dependent on
cell-produced growth factors in synthetic medium.
There are several possible mechanisms behind
release of compounds from Tetrahymena. Molecules
leave the cells either via lysosomes (Tiedtke et al. 1992),
mucocysts (Maihle and Satir 1986), or egestion of food
vacuoles to mention a few. To investigate possible
connections between some of these mechanisms and the
release of growth factors, we have made experiments
with the mutant cell strains II8G (lacking food vacuoles),
MS-1 (releasing lysosomal enzymes at low rates) and
SB 281 (releasing no mucocysts). The observed patterns
of growth and multiplication in both cloning and mass
culture experiments were similar to those presented in
Figures 1, 2 and 3 and Table 1. This seems to exclude
the possibility that growth factors are released together
with lysosomal enzymes and compounds from
mucocysts. Furthermore, food vacuoles appear to play
no part in either utilization or release of the growth
factors.
Tanabe et al. (1990) have reported on a growth factor
from a mutant Paramecium tetraurelia, When this factor
is added to the mutant, it will recover the multiplication
rates characteristic of the wildtype. In our case the
growth factor is so far unknown, but it is apparently
secreted by wildtype cells. Cells in synthetic medium
seem to require the presence of a growth factor.
Previously, Kidder and Dewey (1951) reported that
it was necessary to inoculate Tetrahymena cultures in
synthetic medium with not less than 1% of the new
culture volume. This concurs with the idea that the cells
somehow change their medium making it fit for cell
survival and multiplication. Lilly (1967) and Lilly and
Stillwell (1965) have reported on the effects of growth
factors on multiplication of TI pyriformis in conditioned
medium. Their results indicate that such compounds
may be present, but the reported difference between
control and experimental cultures was small. In view of
our results it is obvious, that their initial cell densities
(20,000 cells per ml) were too high to reveal any
significant effect.
The experiments presented here indicate that a variety
of compounds, including hemin, phospholipids etc., can
initiate cell multiplication in synthetic medium. These
compounds may act as signals for cell multiplication.
They may do so by acting on the cell surface, in the
cytoplasm or inside the cell nucleus. Furthermore, the
cells themselves appear to release these or other factors,
by what looks like an autocrin process, to stimulate their
own cell division.
Acknowledgements. We thank the NOVO Foundation, Copen-
hagen, Denmark, for support; Peter Schousboe for stimulating dis-
cussions; Helle Eis and Esther J. Laursen for the results obtained with
the strains II8G, SB 281 and MS-1; and Lene lergensen and Ruth
lLenstrup for technical assistance. Flemming Bryde Hansen advised
us in establishing micro-cloning procedures.
case of those who oppose the prac-
tice and creates conflicts among
potential allies. Are the long-term
interests of the scientific commu-
nity best served by AAU, for ex-
ample, taking the side of the ex-
ecutive branch versus that of
Congress, the side of authorizers
versus that of appropriators, and
condoning earmarkers on the agri-
culture appropriations subcom-
mittee? Only the clearest and most
inclusive definition of earmarking
truly supports the goal of achiev-
ing the highest-caliber science, not
only within the academy but
throughout the federal govern-
ment. This definition should be
adopted by organizations such as
the National Academy of Sci-
ences, the American Association
for the Advancement of Science,
and AAU to help them put a lid
on the academic pork barrel.
If you're out of town and need
cash, an automatic teller machine
can instantly determine if you have
money in your account and let you
withdraw what you need. If you
apply for a loan, the lender can
quickly get access to your credit
history to help determine your re-
liability. If the Internal Revenue Ser-
vice 1s reviewing your tax return
and needs to know how much was
withheld from your salary during
the year, the information is readily
available. But if you need emer-
gency medical care and the physi-
cian needs information on your
medical history, it is highly unlikely
that it can be found and made avail-
able promptly. In fact, much of the
information necessary to understand
the workings of the U.S. health care
system is not to be found no mat-
ter how long one is willing to wait.
And without better access to medi-
cal data, the health care system will
be hampered in its efforts to pro-
vide better care to individuals and to
enhance the overall effectiveness
of the entire system.
The information-management
challenge experienced by health
care professionals and institutions
is growing daily. At least three fac-
tors contribute to this growth. First,
health care practitioners must mas-
ter and track an ever-increasing
base of medical knowledge. MED-
LINE, the computer data base of
biomedical literature, grows by ap-
proximately 360.000 new articles
per year. Second, patient records
include more data as patients live
longer, experience more chronic
disease, undergo a greater variety
of tests, and have more encounters
with health care providers. Third,
the demand for patient data is in-
creasing. In addition to supporting
the diagnostic and therapeutic
work of clinicians, patient data are
used to document patient risk fac-
tors, expectations, and satisfaction
with treatment; to perform quality
assurance, risk management, cost
monitoring, and utilization review:
to identify emerging public health
problems; to track adverse reactions
to pharmaceuticals; to document
services provided for billing and
legal purposes; and to assess the
effectiveness of new technologies
and procedures.
The users of patient data in-
clude not only physicians, nurses,
and other health care practitioners
but also virtually everyone associ-
ated with the health care delivery
system. Patients themselves are in-
creasingly likely to be interested
in their records as they become
more informed consumers of
health care services. Administra-
tors of health care institutions seek
data to manage the quality and
costs of services provided as well
as to project staff, budget, and fa-
cility needs and identify opportu-
nities for new programs. Insurance
companies, other third party pay-
ers, and employers who pay for
health benefits seek patient data to
monitor the frequency, cost, and
quality of health care services pro-
vided to their subscribers or em-
ployees. Health services researchers
seek access to aggregated patient
data to study patient outcomes,
variations n practice patterns, or
appropriateness of alternative treat-
ments for a particular condition.
Policymakers seek data to monitor
the performance of health care in-
stitutions, to evaluate coverage de-
cisions for federal and state insur-
ance programs, and to evaluate the
availability of health resources to
meet current and future needs.
Patient records are a linchpin
of information management in
health care, but traditional medi-
cal records have not kept pace with
the changes in health care and can-
not satisfy many of the new de-
mands placed on them. Despite the
broad diffusion of computer tech-
nology, most patient records today
exist only on paper and are often
inaccessible, inaccurate, incom-
plete, illegible, disorganized, not
Secure, and not integrated into the
various settings of care. Comput-
erzzing current paper records
would help, but it would not meet
all current and future user needs.
Existing patient files do not have
a standard form, do not integrate
data from multiple care settings,
and do not include all the types of
data needed to enhance patient care
and better manage the system.
Given the broad array of users and
uses of patient records and the new
technologies available to support
them, a new concept of the patient
record is needed.
In 1989. the Institute of Medicine
(IOM) convened a Committee on
Improving the Patient Record that
articulated a vision for ''an elec-
tronic patient record specifically de-
signed to support users by providing
access to complete and accurate
data, alerts, reminders, clinical de-
cision support systems, and other
aids.'' As a first step, the commit-
tee described what an ideal com-
puter-based patient record (CPR)
system would entail: ease of opera-
tion, convenient locations of work-
stations in the patient care setting,
24-hour availability, rapid response
time, and simultaneous use of a
given record by multiple users.
Security is a critical require-
ment of CPR systems and depends
on technology and user behavior.
Systems must track when users log
on and off the system, lock out at-
tempted lug ons after failed at-
tempts, requtre uusets to update
their passwords on a regular basis,
and be able to generate secondary
records that exclude patient iden-
tifiers and contain only those data
needed by nonclinical data users.
The ability to connect the com-
puter systems within and beyond
an institution is another essential
component of CPR systems. For
example, physicians would be able
to request laboratory tests, order
prescriptions, refer patients for con-
sultation, or admit patients to the
local hospital from the CPR work-
stations in their offices. Informa-
tion would also flow into the CPR
svStem from other sources. Labo-
ratory test results, consultation
notes, and discharge summaries
would be sent electronically to the
physician's office and filed auto-
matically in the patient's record.
Similarly, bills could be generated
automatically at the end of each pa-
tient visit and sent electronically
each day to third-party payers. Rel-
evant data could be automatically
reported to the Centers for Disease
Control, the Food and Drug Ad-
mnistration, or tumor registries,
rather than requiring practitioners
to complete forms manually.
CPRs should offer users assis-
tance with routine tasks, thereby in-
creasing the time physicians and
other health professionals can spend
with patients. For example, users
would be able to generate with the
stroke of a key routine forms such
as school or insurance examinations
and patient instructions for a range
of illnesses or treatments. Perhaps
the most significant feature of the
CPR environment would be the
availability of clinical decision sup-
ports. Repeated laboratory test re-
sults could easily be transformed
into a graph, thus facilitating recog-
nition of a pattern. Decision algo-
rithms and clinical practice guide-
lines would be available to assist
in diagnostic and treatment deci-
sions. Access to current medical
knowledge would be facilitated by
linkage with MEDLINE and other
literature and bibliographic data
bases. On-line, clinical reminders
would support preventive medicine
by informing practitioners or pa-
tients of needed vaccinations or
tests. Clinical alerts, identified by
subroutines embedded in the com-
puter's program, would prompt
practitioners if a patient's lab results
revealed a dangerous trend or if in-
compatible drugs were prescribed.
In addition to improving the
quality of care by providing bet-
ter information to physicians,
CPRs should also contribute to the
moderation of health care costs in
several ways. Direct entry of lab-
oratory test results should reduce
the frequency of redundant test-
ing that occurs when previous test
results cannot be found. Produc-
tivity is likely to be enhanced as
time need not be spent tracking
down missing records or missing
data or waiting for records that are
in use elsewhere. Since data need
be recorded only once in the com-
puter record, redundant data entry
can be eliminated.
Finally, CPR systems will sup-
port the advancement of medical
knowledge by making improved
patient care data available for clin-
ical and health services research.
Data that are maintained in CPR
systems are likely to be more easily
and less expensively collected and
aggregated since data will no longer
need to be manually abstracted
from records and entered into re-
search data bases. And CPRs offer a
means of bringing research results
directly to practitioners.
Although health care lags behind
other industries in applying com-
puter technology for data storage
and retrieval, some activity in this
arena has begun. Automated pa-
tient records can be found in vari-
ous stages of development in some
health maintenance organizations,
outpatient clinics, hospitals, and
multihospital systems. In addition,
some physicians are using clinical
decision support systems that pro-
vide guidance in areas such as gen-
eral medical diagnosis, drug ther-
apy decisions, and the management
of chemotherapy for patients par-
ticipating in formal clinical trials.
But nothing currently in use pos-
sesses the scope and scale of the
envisioned CPR, How can we
move from the present inconsis-
tent and frequently archaic infor-
mation-management practices and
technology toward widespread and
compatible CPR systems'?
Developing a comprehensive
CPR system represents a signifi-
cant, but not insurmountable, tech-
nological challenge. Progress is
needed in four major areas: Facile
user interfaces must be developed
so that practitioners will not find
it cumbersome to use CPRs; sys-
tem security technology and pro-
tocols must be enhanced to protect
the accuracy and confidentiality of
patient data; local, regional, and
national networking capabilities
must be built so that linkages
among CPR systems can be set
and data standards must be estab-
lished so that data can be shared
between CPR systems and used for
various purposes.
Equally important, though per-
haps more difficult to overcome,
are the nontechnological impedi-
ments to CPR development: the
lack of a clearly articulated and
widely agreed-upon definition of
what a CPR is and what the per-
formance expectations of its users
are for vendors; high research and
development costs and an uncer-
tain market; an inadequate num-
ber of experts trained in medical
informatics; the public's concern
about protecting confidentiality of
patient data; the issue of patient
data ownership; and ambiguity in
and inconsistencies among state
laws related to patient records.
Organized or overt resistance
to CPRs is unlikely, but subtle re-
sistance is likely on several fronts.
Individuals who believe that their
jobs are threatened by the change
and health care workers who are
reluctant to learn new skills may
be unwilling participants. Among
those who stand to benefit from
CPR implementation, competing
and sometimes conflicting inter-
ests must be addressed. Vendors
who must play a key role in the
success of CPR development must
strike a balance among cooperat-
ing to facilitate development,
avoiding antitrust violations, and
pursuing profits. Finally, individ-
ual institutions may be hesitant to
invest in a CPR system due to high
costs and as yet unquantified ben-
efits. Overcoming these barriers
will require coordination among
the many organizations and indi-
viduals interested in CPRs and a
decisionmaking process that will
be accepted throughout the health
care system. For this reason, the
IOM patient record committee's
major recommendation was the es-
tablishment of a Computer-based
Patient Record Institute (CPRI) to
promote and facilitate develop-
ment, implementation, and dis-
semination of the CPR,
In the spring of 1991. the
A merican Health Information
Management Association (for-
merly the American Medical
Record Association), American
Hospital Association, American
Medical Association, American
Nursing Association, and U.S.
Chamber of Commerce formed a
coalition for establishment of the
CPRI The CPRI was incorporated
in January 1992 and held its first
annual meeting in July 1992. The
CPRI currently has 22 organiza-
tional members representing the
health care professions, insurers,
payers (for example, employers),
information systems and service
vendors, and government, as well
as a data base of interested groups
that includes over 7(00 organiza-
tions. Aware of the major barriers
to implementation, the CPRI has
established four workgroups: CPR
demonstration projects: confiden-
tiality, privacy, and legislation:
codes and structure; and education.
The CPRI is not alone in its
efforts to advance CPRs. The fed-
eral government is demonstrating
increasing commitment to im-
proving information management
in health care. In early 1991, the
General Accounting Office (GAO)
issued a report on the benefits of
automating medical report systems.
Since the GAO and IOM reports
were released, at least two bills in-
troduced in Congress have explic-
itly addressed the automation of
patient data systems by requiring
hospitals participating in Medicare
to be able to submit their claims
electronically and by authorizing
funds to develop model systems
''to facilitate gathering of health
care cost, quality, and outcome
data.' Several federal agencies-
particularly the Agency for Health
Care Policy Research, the Health
Care Financing Administration, the
National Library of Medicine, and
the Department of Veterans Af-
fairs-were actively involved in
the IOM patient record study and
continue to support improved in-
formation management and CPR
development through involvement
of their staff in CPRI workgroups,
bv funding research related to or
directlv associated with CPRs. and
by disseminating information about
the value of and ways to accom-
plish information management in
health care settings.
In addition, in November 1991,
Secretary of Health and Human
Services (HHS) Louis Sullivan con-
vened national health care leaders
to discuss the challenges of reduc-
ing administrative costs in the U.S.
health care system. At the forum,
three health care industrv-led work-
groups were created--the Work-
group for Electronic Data Inter-
change (WED), the Task Force on
Patient Information. and the Work-
group on Administrative Costs and
Benefits. In its July 1992 report,
WEDI presented a vision and rec-
ommendations that are consistent
with the efforts of CPRI, WEDI
will continue in existence as a col-
laborative effort among health care
industry participants and will report
to the secretary of HHS each year
on industry progress. The other two
workgroups are still conducting
their deliberations. The potentially
complementary efforts of these
three workgoups and of the CPRI
must be coordinated by their re-
spective leadership to avoid re-
dundancy and possible conflict.
Despite the extensive attention
that CPRs have been receiving, we
will not have them in place and
ready for use fast enough. Health
care could benefit from the use of
CPRs today, and certainly any re-
formed health care system will rely
heavily on the information-man-
agement capabilities that CPR
offer.
President Clinton's new tech-
nology initiative includes increased
investment in high-performance
computing and networking appli-
cations to improve the provision
of health care by ''furnishing health
care providers and their patients
with better, more accurate, and
more timely information,'' This
initiative may provide a signifi-
cant boost to CPR development
efforts by acknowledging the im-
portance of building an informa-
tion-management infrastructure to
support health care and by pro-
viding the level of funding that is
needed to support large-scale CPR
demonstration projects.
But just as technology alone
cannot overcome the challenges
involved in improving patient
records, money alone will not get
us to the CPR. The many federal
agencies and private-sector orga-
nizations that are involved in CP[
issues-particularly open discus-
sions regarding standards-must
coordinate their efforts. In the short
term, the newly appointed presi-
dent of the CPRI may be well ad-
vised to convene a ''CPR summit.'
In the long term, if CPRI is to ful-
fill this coordinating role, it must
increase its visibility and credibil-
ity by strengthening its financial
base and producing tangible results
that move us toward the ultimate
goal of CPRs.
BEGIN THIS PAPER with an immedi-
ate confession of bias,Ithink entomol-
ogy is a superior discipline-from a
child's point of view. Entomology offers the
chance to run about outside, splash around
in creeks, holler, shriek, put things in jars,
and see wonderous adaptations and bizarre
creatures,In short, it'sjust plain fun-if one
isnot fearfulof insects, if one is not discour-
aged from the interest by socialized gender
roles, or if one's interaction with insects is
not limited to inner-city roaches.
A complex interaction of gender,
ethnicity,race,and socioeconomicclass dis-
advantages historically has prevented full
participation of some groups in
entomology-specifically women and
minority groups: African-Americans,
Hispanics, Native-Americans, Asian-
Americans, and others, Of the 7,506
Entomological Society of America (ESA)
members in 1990, only 3% were not white,
and 6% werewomen (ESA National Office).
While significant improvements have been
made, much more must be done to achieve
full integration of women and minorities
into entomology in the coming decades.
The problem of recruiting students from
these underrepresented groups is one
component of the larger task of educating
the American public about entomology.
Nearly 95% of the American population is
classified as scientifically illiterate
(Goodstein 1990). An informed electorate
is essential in determining how best to
respond to challenging issues such as
biotechnology,sustainable agriculture, and
environmental degradation. In short, the
population mustmove beyond the mentality
of 'the only good bug is a dead bug.'
In the past three years I have taught in
a university outreach program in elemen-
tary schools and in a 'gifted' student
program. My time with these children has
demonstrated clearly that young students
view science as a boring field that is limited
to 'geniuses,' Additionally,I observed that
the willingness of children to handle insects
varies greatly by age, gender, and race.
These observations inspired me to investi-
gate what could be done to increase the
numbers of students from underrepresented
groups entering (and understanding) ento-
mology.
I therefore conducted a survey to iden-
tify how current graduate students became
interested in entomology, with the purpose
offinding commonalities that could be used
to design outreach programs to interest
these underrepresented groups in entomol-
ogy. A previous survey (Wrensch 1986) of
seventy professional entomologists found
that teachers were the greatest influence on
choice of entomology as a career.I wished
to quantify what these teachers or other
individuals were doing that successfully
persuaded students to become entomolo-
gists, Additional questions on the graduate
experiencewere included; responsesto these
questions are reported in Pearson (1992)
(see pp. 103-114).
The questionnaire was prepared, tested,
andmailed under the same procedures listed
in Pearson (1992). In total, 470 surveys
were returned for a 44.5% response rate.
The demographic profile of the respondent
population is shown in Pearson (1992).
Several cautions are made about the inter-
pretation of data from these surveys in
Pearson (1992); these warnings also apply
to the data reported here.
Responsestoclosed questionswerecoded
and examined with F-tests and chi-square
analyses (SAS Institute 1985). Occasional
fragments of students' responses will be
quoted, but this is done only when the
response is representative of a number of
responses and cannot be identified to any
individual student.
Students were asked to name the age at
which they became interested in science and
entomology (Table 1). White, or majority,
Americans appear to develop an interest in
science an average ot two to four years
earlier than international and minority stu-
dents, respectively. Significant differences
existed between genders in the average age
of choosing entomology as a career, While
men chose entomology at age twenty-one,
women did notuntilagetwenty-three (Table
1). American minority students also ap-
peared to choose entomology later in their
careers at age twenty-three. The ranges of
agesnamed for these groups also show large
differences.
The next question asked students to
describe their childhood attitudes toward
insects(Table2).Significant differenceswere
seen between men and women (' 18.7, df
4, P 0.005), majority and minority
Americans ('= 15.7, df4,P0.01),apd
Americans and international students ('=
17.8, df=4,P 0.007).
Respondents were asked who was re-
sponsible for their interest in science and
entomology. High school and junior high
school teachers and parents were named as
most influential on students' developing
interest in science (Table 3). Some other
influencesmentioned werecountyextension
agents, Marlin Perkins, Mr. Wizard, and
Mother Nature.
University professorswere cited by 60%
of respondents as influential in making the
decision to become an entomologist (Table
3). These results are quite similar to
Wrensch's(1986), which showed thatteach-
ersinfluenced 50% of her sampleto become
entomologists. Other influences mentioned
were beekeepers, a National Science Foun-
dation (NSF) Female Mentor Program,and
J.H. Comstock (presumably not in person).
Toquantifyhow these teachers and other
individuals influenced students toward en-
tomology careers, survey takers were given
a list of choices of possible actions of the
person (Table 4). 'Enthusiasm and excite-
ment ofthe individualfor entomology'' was
ranked first by 35% of the respondents.
While neither rankings or first choice sig-
nificantly differed by gender, race, or na-
tionality, the comments of the respondents
were quite revealing. Ofthose students com-
menting on this question, the majority said
some person had taken a personal interest,
orhadencouraged them in some way. Eighty
percent ofthe studentsmaking this observa-
tion were female.Some representative com-
ments were:
Inever seriously thought about a career in
science-this individual broke through the
mental block I had about math, etc. The
realization Iwas capable changed my outlook
and gave me more self-confidence.
[He] was very encouraging and demand-
ing, This feeling of acceptance was lacking in
other scientific areas and prompted me to
change majors.
This person felt I had the ability to be
successful at the graduate level and encour-
aged me to pursue these goals. [Hej gave me
a great deal of positive feedback for the work
I did, enhancing my self-esteem.
Several studieshavereported thatwomen
in nontraditional fields may suffer from a
lack of self-confidence about their abilities
(Ehrhart & Sandler 1987, Berg & Ferber
1983). For the students in this survey, an
advisor's, employer's, or teacher's show of
beliefin their abilities apparently motivated
them to set and achieve higher goals.
Students were asked to rank a list of
elements of entomology that they found
most appealing (Table 4). Their responses
may perhaps indicate the best 'selling
points'' to emphasize when dealing with
nonentomologists. 'Wonderous diversity
of insect life'' was the definitive first choice
(46%). Of twenty-six comments on this
question, nearly all were some sort of dis-
paraging comment written in next to the
choice of 'Job opportunities and pay,'
Perhaps the best comment was 'What?
Where??2)y*
The survey results illustrate that some
differences do exist between the paths to
entomology traveled by majority,minority,
male, and female students. A glance at
sociologicaland educationalliteratureshows
that this is not surprising. Harty & Beall
(1984) and Simpson & Oliver (1985) both
reported more positive attitudes toward
science in boys than girls. Young girls and
minorities also have less exposure to science
extracurricular activities and thus less
opportunityto develop an interest in science
(Anderson 1989;Kahleet al. 1985, Jones &
Wheatley 1988).
Perhaps the most dramatic report on the
way in which children view science and, by
association, entomology, is described in
Chambers (1983). He used the 'Draw-A-
Scientist-Test'' to chart the development of
children's science imagery. Of 4,708
drawings made by children, only twenty-
eight women scientists were drawn, even
though halfof Chambers's research subjects
were female. Seven children drew images of
scientists as naturalists; the rest portrayed
scientists indoors in laboratories. Children
from low-income families were slow to
develop an image of science; in fact, while
upper-andmiddle-class children had a well-
developed image of scientists in the first and
second grades, low-income children did not
until the fifth grade, and even then their
drawings were less detailed. Laboratory
equipment frequently dwarfed scientists in
these drawings, suggesting feelings of
overpowerment and disconnectednessfrom
the scientific process. While Chambers did
not analyze his data by race, it is an
unfortunate fact that minorities make up a
large portion of low-income households in
the United States. Studies on older students
(Anderson 1989) found attitudes of
alienation from science common among
African-American students.
Discussion of youth programs has begun
in the ESA, and programs for youth should
be a priority in the effort to involve under-
represented groups in entomology. Girls
are so strongly socialized to be pretty in-
stead of smart, and there is such a lack of
positive scientificcareer images for African-
Americans and other minorities, that the
majority of these future entomologists are
probably lost by the fourth grade.
The survey revealed that women and
people of color are more likely than other
groups to say they were frightened or re-
pulsed by insects as children. It is important
to reach these students at an early age before
societal concepts of what is and is not
appropriate for a group have been formed.
My own experiences teaching children show
that while at age four most girls and Afri-
can-Americans will readily handle insects
and are quite curious, by age six they begin
to hang back and watch, and by age eight
there is squealing and giggling, and some-
times fear, White boys remain equally inter-
ested in handling insects at allages, and will
sometimes push other groups out of the
way,dominatingclassroom demonstrations.
These behavioralobservations parallelthose
made in several general science education
studies ([ones ß Wheatley 1988, Kahle et
al.1985). My own observations on children
are echoed in the comments of an African-
American survey respondent:
Ibelieve thatan interest in insects (and life
in general) is something that has to be culti-
vated at an early age. By exposing minority
children, especially city youth, to nature ...
many more ofthese individuals will not grow
up thinking that 'black people don't do that
kind of thing.''
It is also particularly important to reach
the children of those groups for whom
agriculture is not a positive career image. If
people from one's own group are most
likely to be laboring in the fields harvesting
crops,one isnot likely to view agriculture as
an enticing profession.
Studies have shown that African-Ameri-
can high school students are interested in
science but feel that it lacks applied use
(Anderson 1989). Research has also dem-
onstrated that women are more likely to
show interest in social and ethical applica-
tions of science (Rosser 1990). By stressing
the potential benefits to agriculture and
global ecology, we may successfully recruit
these and other students into entomology.
The effort to begin youth programs in
the ESA and elsewhereis admirable, and we
should make sure that outreach to under-
represented groups is an integral part of
each program. Women and minority role
models, portrayal of girls and children of
color in visual aids, and inclusive language
are all essential parts of this process.
I have occasionally given a 'Draw-An-
Entomologist-Test'' to children. Of sixteen
drawings I unearthed in a search through
my files, from twelve girls and four boys,
there are twelve female entomologists and
four male entomologists, For me, this is a
clear example of the power of a visit by a
same-sex role model. My use of women in
visual aids resulted in an excited young
entomology convertnamed Anna, who was
fascinated by a picture of Anna Comstock.
In envisioning themselves as entomolo-
gists,it is easier for children to feel they will
be one of many and not a statistical oddity.
For children to develop a concept of science
as a process and insects as something more
than unpleasant things in their home, they
must find something connected to their
experiences in the world.Iwould argue that
only then can they see themselves as partici-
pants in the process.
High school and undergraduate mem-
bers of underrepresented populations might
be effectively reached by visits or research
partnerships with traditionally African-
American and women's colleges and reser-
vation schools.Land-grantuniversities often
exist near colleges with predominantly
African-American or female student bod-
ies, but this resource of potential students
remains largely untapped. Guest lectures,
arranging for independent-study students,
participation in career days, and seeking
student technicians to participate in field
work are all examples of ways in which
these studentscould be exposed to entomol-
ogy. These same techniques could also be
effective at a department's own university.
Istrongly recommend the Association of
American Colleges publication Looking For
More Than A Feu Good Wowen In Tradi-
tionally Male Fields (Ehrhart & Sandler
1987) as an aid in developing departmental
outreach programs. It contains many useful
recommendations for graduate recruitment
and retention strategies that are also easily
adapted to minority students, Hannah
(1990) and Stikes (1990) also discuss mi-
nority retention and recruitment.
The importance of teachers' influence
on students' interest in science and in
entomology should provide increased moti-
vation for the development of excellence in
teaching. The way in which students are
taught science is an area in which increased
ESA and member participation could
provide benefits. Congress, NSF, American
Association forthe Advancement ofScience,
and many other organizations have already
recognized the need for reform. Attitudes
toward science (and entomology, as shown
in this survey) are formed early in life. By
third grade, 50% of all elementary students
dislike science-and by eighth grade, 80%
dislike it, Nobel laureate Leon Lederman
described this phenomenon as the way in
which schoolstake 'naturally curious,natu-
ral scientists and manage to beat that curi-
osity right out of them.' As long as science
is still taught by lecture and memorization,
this trend will presumably continue.
'Cover less, uncover more' is the theme
of many educational reforms that stress
observation, experimentation, and discus-
sion. Some examples of improvements in
general education through entomologist
participation are: elementary and second-
ary school visits, involvement in teacher
training programs, and providing lesson
plans in entomology which emphasize stu-
dent discovery.
Undergraduate teaching is as much in
need of reform as that of secondary schools.
Astudyexamining scienceenrollment found
most science students change their majors
after their first-year science course (Tobias
1990). Even those who did not change
majors had negative comments about the
classroom climate, describing their fellow
students as unfriendly and highly competi-
tive. With a few exceptions, lecturing to
classes of more than one hundred students
followed by memorization of the facts pre-
sented is the predominant method of teach-
ing introductory biology. Is this the best
manner in which to challenge students?
Doesn't it weed out not only those less able,
but also those with a low tolerance for
boredom, those who wish for more interac-
tion with their instructor and peers, and
those who find a passive dictation process
intellectually stagnatingi Are students in-
terested in what they memorize for an exam,
or are they interested in memorizing the
exam? Evidence supports a strong correla-
tion between process and product and sug-
gestswomen and minoritystudentsare likely
to be those most affected by teaching style
NSECTS HAVE LONG BEEN DEPICTED IN
a variety of art forms (Hogue, C. L. 1987.
Cultural entomology. Annu. Rev. Entomol.
32: 181-199 ). Especially interesting are art
forms in which the insects themselves are the medium for
the creation of art,The phrase Victorian scientificart is used
by many antique dealers to describe art displays using
natural objects such as insects,flowers, or seed pods created
during the Victorian era (1837-1901).
Smaller displays of the era, such as framed pressed
flowers and table trays showing colorful butterfly wings,
continue to be popular and are readily collectible. Larger,
framed arrangements of insects as art that originated in this
period, however, are rare (Richard Schachner, personal
communication).
Entomologists are aware of the fragile nature of dried
insects and the need to protect them from breaking and
from destructive pests, Unfortunately, lack of awareness on
the part of most owners of Victorian scientific art over the
years has left few examples of this highly esoteric art.
One surviving example of Victorian scientific art is
Joseph A. Kaplan's ''fly case'' (left). The large,framed insect
display, which dates from 1891, previously was featured in
an antique shop in Berkeley Springs, West Virginia, and
now resides in the living room of its new owners from
Washington, D.C.
thought by Edna Engel to have been
created in the 1920s, it does not actu-
ally qualify as Victorian scientific art.
Any connection between [oseph
Kaplan and Henry Engel is specula-
tive. Engel was born in Germany and
moved to Pittsburgh in 1888 at the
age of 15.Engel first bought a farm at
Library, Pennsylvania, and later the
farm at Finleyville, both near Pitts-
burgh. He operated Engel's Flower
Farm and later Engel's Farm Market,
growing many varieties of flowers,
fruits, and vegetables for sale and for
family use.
Engel expanded his earlier interest
in Lepidoptera by planting wildflower
gardens and other flowers especially
attractive to butterflies and by light-
ing for moths on a hillside overlook-
ing his farm. He corresponded with
other lepidopterists and exchanged specimens. Engel collected with local hobby
lepidopterists including his brother-in-law Fred Friday and his wife, among others.
A photograph of this ''butterfly club'' remains at the Finleyville farm. Engel kept his
extensive Lepidoptera collection in his workroom in the dahlia cellar on the flower
farm. After his death in 1943 at the age of seventy, Engel's collection was sold to
the Carnegie Museum in PPittsburgh where it remains.
When Joseph Kaplan created his fly case in 1891, Henry Engel was 18 years old
and had been in the United States only three years, We have no evidence that Engel
was collecting insects or making displays that early in his life. Thus, Engel probably
was not an influence in Kaplan's artistic creation.
Another example of Victorian scientific art resides in the shop of a notary and book
collector at Allison Park, Pennsylvania, north of Pittsburgh. This display, which
measures 71 cm by 101.6 cm, is a mixture of pinned native, western U.S., and exotic
insects, Included are representatives of many families of Lepidoptera and Coleoptera,
aswell assome Odonata, Homoptera,Orthoptera,Hemiptera,Hymenoptera, Diptera,
Chilopoda, and Araneae. Several pinned salamanders are also present.
Don Johnston, the shop owner, was given the display by an unidentified man who
explained that he was cleaning the attic; he said the display could either be sold or
thrown away.The man told [ohnston that his great-grandfather made the display.
The Allison Park display is smaller and less well preserved than the Kaplan
display, It definitely qualifies as Victorian scientific art because of its obvious age
and the nearly symmetrical arrangement of insects on either side of the central line
of luna and cecropia moths. A symmetrical arrangementappears to be typical of the
large Victorian scientific art displays.
We may wonder how many other examples of this art form are in family attics
waiting to be discovered or discarded. Because of its age, its degree of preservation,
and its known history, the Joseph A. Kaplan fly case remains one of the best
examples of this type of Victorian scientific art.
For assistance and cooperation,lacknowledge F. L. Kaplan, David and Irene Addlestone,
Jeffrey Eling, Edna Engel, Cheryl Engel, Don Johnston, C. E, Mecca, John Rawlins, Richard
Schachner, and the staff of Youngblood's Antique Shop. U
N 1975, several
children from Lyme,
Connecticut, were
diagnosed as having
juvenile rheumatoid
arthritis, However, the
rural setting and presence
of a unique rash led several
nvestigators to suspect a
different etiology. Subse-
quently, Willy Burgdorfer
and coworkers at the
Rocky Mountain Laborato-
ries in Hamilton, Montana,
isolated a spirochete, which
was eventually named
Borrelia bwrgdorferi, from
Ixodes dammini ticks, In
1983, this spirochete was
isolated from the blood of
patients with what is now
called Lyme disease. Since
that time, Lyme disease has
become the most frequently
reported tick-associated
illness in the United States,
may be initiated when a mature
female releases a pheromone that
the sexually mature male senses
via receptors on his palps,
During copulation, the male
positions himself on the ventral
surface of the female so that he
can insert his hypostome and
chelicerae into the female genital
aperture (Fig.1). It has been
hypothesized that this allows the
male to detect specific
aphrodisiac-type pheromones
that, when emitted from the
conspecific partner, signal the
male to form a spermatophore.
The spermatophore forms
outside the male gonopore, Since
male I, dammini do not have
copulatory organs, they use
subtle body contortions to push
the spermatophore to the female
genital aperture. The male then
uses its palps and chelicerae to
insert the tip of the
spermatophore into the genital
aperture, where the enclosed
sperm are released into the
female genital tract.
After copulation, the female
I, dammini tick rapidly ingests a
large amount of blood. The
epicuticle has been observed to
expand up to nine times during
this bloodfeeding phase (Fig. 2).
The female tick disengages from
the host after full engorgement,
which has been shown to take
eight to eleven days under some
laboratory conditions, and the
blood is digested. The digestion
products are then utilized for
vitellogenesis and egg
maturation, Investigators have
observed a direct correlation
between the volume of the final
blood meal and the number of
eggs produced.
In our laboratory, eggs were
deposited seven to fourteen days
after engorged adult female ticks
were physically removed or had
dropped from the host (Fig. 3).
fggs were deposited via an
ovipositor (Fig. 4) which forms
when the vestibular vagina
prolapses through the genital
aperture. During egg deposition,
the female's Gene's organ and
porous areas excreted an
aqueous solution which may be
a waterproof, fungicidal agent to
coat and protect the eggs from
deterioration (Fig. 5). The female
I, dammini ticks died shortly
after the egg masses, each
containing approximately 2,500
eggs, were deposited.
I, dammini eggs hatched after
twenty-nine to thirty-six days of
incubation at conditions described
above. When hatching, the egg
split mediolaterally and the larva
emerged mouthparts first (Fig. 6).
The recently emerged six-legged,
teneral larvae were opaque (Fig. 7).
during the final blood meal
before oogenesis (Figs. 15 and 16).
I, dammini are highly
developed parasites that
complete their life cycle by
obtaining blood meals from mice
and deer, Mice and other small
animals are efficient B.
burgdorferi reservoirs, and
immature I, dammini often
become infected while obtaining
a blood meal, The abundance of
these hosts and their role as a
reservoir for B. burgdorferi has
allowed B. burgdorferi-infected
I, dammini to disperse
throughout many regions of the
United States and increase the
threat of Lyme disease.
Oil palms (Elaeis guineensis) are now one
of the world's most important plantation
crops figure 1) and in Malaysia palm oil is
one ot the country's major economic prod-
ucts, In parts of South East Asia, oil palms
are estimated to outnumber the population
by almost two to one. Whereas plantation
management and the growing of oil palm,
particularly of high-yielding material, has
reached a high standard of efficiency, har-
vesting of the ripe fruit and its conveyance
to the factory still involves losses that lead
to unacceptable reductions in overall yield.
The numbers of fruit produced by a palm
(the promise of oil on the palm) is not sat-
isfactorily reflected in the amount of oil re-
covered from the harvested bunches.
The cause of the shortfall is the non-
synchronous ripening of the fruit on the
difterent spikelets on any one bunch and
the shedding and loss of the ripest fruit of
the bunch before it reaches the factory. A
recent and new understanding of the
processes of ripening and shedding in the
oil palm fruit offers a biotechnological ap-
proach to overcoming these losses. It com-
bines the advances of clonal propagation
programmes with the understanding of the
unique physiological and biochemical con-
trols that determine ripening and the tim-
ing of fruit-fall from the palm.
After fertilisation the fruit enlarges, reach-
ing maximum size and fresh weight of
both mesocarp and kernel by 120 days or
so. Then, at a signal not yet understood in
chemical terms, the cells of the mesocarp
initiate a new range of gene expressions
which include the synthesis of carotene
and lipid and the production of at least two
enzymes, a lipase and a cellulase. This is
the start of ripening and for the next 30 or
40 days the intensity of the orange colour
of the mesocarp increases as the carotene
level rises: the commerciallv valuable
palmitic, oleic and linoleic triglycerides in
the flesh accumulate and the activitv of
the lipase reaches a maximum [1]. These
are all concurrent events and they continue
until another signal initiates a further set of
gene expressions that herald the onset of
fruit shedding. From the point of view of
high quality lipid, the fruit should reach
the processing plant just before, or at the
time, that the shedding processes are initi-
ated, for subsequently the lipase progres-
sively functions in a hydrolytic role and
the lipid levels start to fall. This results in
the release from the triglycerides of the
free fatty acids (again, mainly palmitic,
oleic and linoleic) which, as they accumu-
late, increasingly spoil the quality of th:
oil extractable from the fruits. For the
highest oil yield, though, harvesting should
be delayed until a considerable number of
the fruit have been shed, since lipid syn-
thesis continues in the fruit that are still at-
tached. At present, some loss of oil quality
is accepted to maximise the yield, and
where fallen fruit are collected, there is
also a considerable additional cost. An al-
ternative solution to the problem, there-
tore, is the elimination of the shedding
process altogether, Current research
Suggests that suitable genetic manipu-
lation of the oil palm could offer this pos-
sibility.
The anatomy of abscission of the oil palm
differs from that of most commercial
fruits, Instead of a synchronous series of
cell separations across a plane of cells be-
tween the fruit and the stalk, resulting in
immediate shedding of the fruit above, the
oil palm undergoes abscission in two dis-
tinct stages with a time lag of 1-2 days be-
tween the two [2].
The flower of the oil palm is hermaph-
rodite, but normally only one of the sexes
will complete development in any one
bunch so that sequential bunches will carry
only male or female spikelets. In the fe-
male, the staminal ring aborts before an-
thesis and is left as a circlet of tissue, the
rudimentary androecium, surrounding the
base of the ovary and immediately adja-
cent to the inner whorl of papery tepals
(figure 2a, b). (Tepals are elements in the
perianth of a flower lacking differentiated
sepals and petals.)
As the fertilised fruit enlarges, the cells
of this rudimentary androecium and the
tepal bases continue to divide, keeping
pace with the increase in diameter of the
base of the fruit. This pattern of differenti-
ation results in the fruit being attached to
the spikelet at a junction with three distinct
tissue types. The first (position 1) is the
base of the fruit itself (figure 3a, c) and
here, even before anthesis, the sites of cell
separation are clearly defined by a line of
small cells with dense cytoplasmic con-
tents. This is the destined site of the first
stage of the abscission process and separa-
tion at this position occurs only when the
fruit is fully ripe. Even when cell separa-
tion at position 1 is complete, the fruit is
not shed, for the cells of the circlet of the
rudimentary androecium and the bases of
the tepals still adhere closely to one an-
other (positions 2 and 3, figure 3b and c).
The fruit is, however, loosened and at this
stage can, in the wild, be readily plucked
from the bunch by primates, parrots or
other fruit feeders.
The second stage of abscission, which
results in the fall of the fruit to the ground.
occurs only after the first stage is complete
and involves a special role for the cells of
the rudimentary androecium. Usually, the
cells of the circlet immediately adjacent to
the fruit base (position 2), though fre-
quently some of those adjacent to the tepal
bases (position 3), will undergo separation
(figure 3c) so that the fruit falls free from
the enclosing tepals. The upper parts of the
tepals are by then brown and dry. The shed
fruits are therefore either naked of any
basal attachments (figure 4a) or may have
fragments of the rudimentary androecium
still adhering.
A naked fruit leaves a complete circlet
of its rudimentary androecium still at-
tached to the tepal bases and partly to the
pad of stalk tissue at position 1 (figure 4b).
but where some ot the androecial ring
clings to the fruit, the remaining tissue of
the circlet stays with the tepals on the
spike.
in exceptional circumstances, when
spikes of fruit have been removed from a
bunch before they are fully ripe, fruit shed-
ding will eventually occur across the bases
of the tepals (positions 4 and 5). Even in
these conditions, separation always occurs
first at position 1, to be followed by the
second stage at either position 4 or 5. Such
fruit are therefore shed still enclosed in the
ring of tepals (figure c) and normal sepa-
ration at positions 2 and 3 at the margins
ot the rudimentary androecium is by-
passed.
On the plantations, palms are checked
every few days tor bunches at the appro-
priate stage ot ripeness for cutting.
Conventionally, the talling ot a few ripe
fruit to the ground is taken as the harvest
signal, and for the taller palms this affords
proof of ripeness of a bunch that is other-
wise difficult to see among the bases of the
fronds. But before the bunch reaches the
factory many more fruit are shed and so a
yield loss is established
LLong ago, occasional abnormal fruit devel-
opment was reported to occur in certain oil
palms [3]. In these, the rudimentary an-
droecium did not remain as a circlet of tis-
sue but instead enlarged and developed
into six (usually) additional seedless lobes
ot female mesocarp (supple mentary
carpels) surrounding the central fruit (fig-
ure 5a, b). These parthenocarpic lobes syn-
thesised carotene and lipid and ripened in
concert with the kernel-containing fertile
ovary. This additional lipid-rich mesocarp
oftcred a potential for high yiclds, and cer-
tain seedlings and at ieast one genetic line
o oil palm was found which routinely pro-
duced such truit. The promise ot high
s ields from these so-called 'mantled' fruit
was not fultillcd, however, perhaps he-
cuuse, although the fruit ripened, it was not
shed. In the absence of the usual signal of
the first iew ripe fruits that tall to the
ground, bunches on the mantled palms
were left unheeded and the fruit were
uuick to rot on their spikelets.
In the 1980s, great efforts were made to
upgrade the yields ot lipid by the introduc-
tion of clonal plant material raised by
tiSSue culture trom root or shoot fragments
taken from elite, high quality, high lipid-
producing palms. Mlany thousands ot these
clonally propagated individuals are now
bearing truit in plantation trials around the
world and improved yields have resulted
trom these plantings, Certain of the tissue
culture procedures, involving the use of
plant hormones in the media have, how-
ever, also led to a proportion of the palms
showing sexual abnormalities that resem-
ble the naturally occurring mantled fruit
[4]. While the rudimentary androecium
may form very well-developed lobes of
supplementary carpels that extend the
whole circlet ot the androecial ring, some-
times, only one or two small lobes may
arise while the remainder of the ring may
be normal. In such fruit. abscission occurs
normally at position l. hut at positions 2
and 3. cell separation takes place only
where the rudimentary androecial ring has
re mained as aborted staminal tissue.
Where the ring has differentiated into
mesocarp tissue, the fruit remains attached
to the bases of the tepals (figure a, b).
Control of this second stage of fruit ab-
scission, and hence of fruit shedding, cun
theretore be manipulated by altering the
developmental programme of the cells of
the rudimentary androecium early in dif-
terentiation and before anthesis. Evidence
from clonal propagation biotechnology
now indicates that the levels ot hormones
used in tissue culture can determine the de-
gree ot mantling expressed by a palm sev-
eral vears later when it starts to flower.
Because the condition does not show con-
stant expression it may have an epigenetic
origin within these highly specialised cells,
but RFLP (restriction fragment length
polymorphism) analysis for certain of the
clones expressing mantling indicate that
DDNA genomic changes may also have oc-
curred between the parent palm and the tis-
sue culture progeny [5]. Although the
mechanism by which mantling is directed
remains unresolved, the techniques by
which it can be induced are now known,
atfording new possibilities for manipulat-
ng abscission.
In those crops where fruit shedding has
been closely studied, the signal for abscis-
sion can be directly linked to critical levels
of ethylene produced by the ripening fruit
and perceived by the ethyiene-responsive
target cells of the abscission zone. In ap-
ples, oranges or tomatoes, for example, the
synthesis of ethylene increases as ripening
proceeds, and at full ripeness the levels
reach a threshold that initiates in the zone
cells an expression of new cell wall modi-
fying enzymes (glucanhydrolases) that
loosen adhesion at the zone cell interface
with neighbour cells. Because the line ot
separation is so precise (despite the fact
that the secreted enzymes may migrate
considerable distances through the cell
walls of adjoining tissues), it is evident
that substrate specificity exists between the
secreted enzvmes and the walls of a lim-
ited number of cells which are restricted to
the immediate vicinity of the zone [6]. In
this way, only certain cells become sepa-
rated from their neighbours by the en-
zymes that are induced in the zone. In
these dicotyledonous fruits, once the ab-
scission cascade of enzymes is produced
(diagnostically, this usually includes a spe-
cific 9.5 pl isozyme of 5-1, -glucanhydro-
lase) separation is initiated across all the
cells of the fruit stalk and the fruit is shed.
In the oil palm, the situation is different.
Throughout ripening, during the rise in
lipid and carotene, the levels of ethylene
produced remain insignificant. It is not
until the fruit is deep orange in colour and
near maximum lipid content that ethylene
synthesis starts to rise, This occurs first at
the apical end of the fruit and quickly pro-
gresses towards the base. In spikelets that
are removed from the bunches, ethylene
production by a fruit reaches a maximum
within 24 hr of the initial rise and there-
after declines. Ethylene production and the
onset of cell separation at position 1 indi-
cate that these events are very closely
linked. Time-course experiments have
shown that onlv those fruit that exhibit the
rise in ethylene synthesis initiate separa-
tion at the fruit-pedicel junction (position
1) figure 7.
It is not mesocarp tissue, however, that
abuts directly onto zone cells. Instead, sev-
eral layers of cells that synthesise neither
carotene nor storage lipid provide a narrow
barrier between the mesocarp and the cells
of the zone. It is at the junction of these
'barrier' cells and the zone cells that the
first wall separations take place. The
second stage of separation at positions 2 or
3 appears to be dependent upon the
achievement of the first stage of separation
at position 1, and not directly upon the sig-
nal of ethylene. Another signal, generated
by separating cells at position l (and possi-
bly an oligosaccharide) would appear to
pass to, and be perceived by, the cells that
constitute positions 2 and 3. During this
second stage of abscission, cells of the
rudimentary androecium dissociate from
each other and from other cells adjacent to
them. Since these other cells include those
at the outer edge of the base of the fruit,
the pedicel and the bases of the tepals, sev-
eral distinct cell types are intimately in-
volved. When the rudimentary androecium
develops as mesocarp, as in the formation
of the mantled condition, cells are differ-
entiated that no longer have the compe-
tence to participate in cell separation
responses to abscission-inducing signals,
and the intimate complex of inter-tissue
signalling fails.
In dicotyledonous fruits and leaves, abscis-
sion is linked tu the induced expression of
the 9.5 pI isozyme of 5-1, 4glucanhydro-
lase (cellulase) by the separating cells of
the zone. This is not so in the oil palm
fruit. Although ripening mesocarp tissue
produces a cellulase, the zone does not.
Furthermore, the cells of the mesocarp do
not separate despite the production of cel-
lulase isozymes, Instead, at position 1 of
the oil palm fruit, an active polygalactur-
onase is produced at abscission, indicating
break-down of uronide linkages in the
zone region, while at positions 2 and 3
high levels of a -1, 3-glucanhydrolase are
induced. Although these enzyme activities
may in part reflect the overall differences
between the cell wall composition of di-
cotyledons and monocotyledons, they also
serve to illustrate the differences between
the major enzymes induced in the two dis-
tinct stages of abscission in the oil palm
fruit [7].
A number of ways are now open for the
genetic manipulation of the oil palm: these
should reduce the losses that result from
unwanted shedding. Each could be engi-
neered during the clonal stages of propaga-
tion in tissue culture.
Firstly, the production of ethylene could
be blocked in the fully ripe fruit by the in-
corporation of antisense genes to the im-
portant enzyme ACC-synthase. This
enzyme controls the synthesis of the ethyl-
ene precursor ACC. Antisense mRNA pro-
duction has already been achieved for the
tomato, thereby preventing translation of
the normally produced sense mRNA and
inhibiting the rise in ethylene formation by
mature fruit, so delaying ripening [8]. It
would be important, however, that the anti-
sense gene be expressed only in the ripe
fruit and not in all other cells of the palm,
for enough is not yet known about ethyl-
ene control of other aspects of palm devel-
opment. This control may be achieved
through the use of gene promoters specific
to fruit tissue. Alternatively anti-sensing
ethylene-response genes may give greater
specificity. In this case a promoter specific
to abscission zone cells would be required.
Secondly antisense could be introduced
for the polygalacturonase gene of position
1 or for the B-1, 3-glucanhydrolase gene of
positions 2 and 3, again under the control
of the appropriate promoters. Since it is
likely that specific isozymes of the en-
zymes are synthesised only at these separa-
tion positions, such options offer a precise
developmental control of gene expression
that is restricted to the abscission zone
alone.
Thirdly, biotechnology could be further
advanced so that the tissue cultures from
which elite palms are propagated could be
so precisely manipulated hormonally that
the rudimentary androecium would consis-
tently develop on the female inflorescences
as a narrow circlet of mesocarp cells. This
differentiation would then permit fruit loos-
ening at position 1, but preclude the possi-
bility of the second stage of the fruit
abscission process at positions 2 or 3.
Whenever any one of these options be-
comes a reality - and there is no doubt that
each would increase the total harvestable
yield of fruit - they will raise a serious
practical problem for the men who work
on the plantation. If the fruit are no longer
shed, so that none falls upon the ground,
how will the cutters know exactly when
each bunch has fruit that are fully ripe and
so be able to judge exactly when each
bunch should be harvested'? To resolve
this, new practices for monitoring ripeness
in the field must now be developed.
Overall, the importance of these re-
Searches is the promise they now offer for
operating more regulated fruit shedding
and harvesting regimes not only for the oil
palm, but for other major palm plantation
crops of high value including dates and co-
conuts.
The Bunger Hills are located near the coastofEastAntarctica
at about 100'E longitude (Fig. 1). They cover an area of
about 300 km'and consist of low rocky hills and glacially
deepened valleys, with many lakes (Fig. 2). Together with
adjacent islands and the Obruchev Hills, to the south-west,
the Bunger Hills form a well-exposed section of the Pre-
cambrian EastAntarctic Shield in a region of generally poor
outcrop. Thefirstgeologicalstudies ofthe Bunger Hillswere
made in 1956-57 by Soviet Antarctic Expedition scientists,
a detailed, mainly petrological, account being given by
Ravich et al. (1968).
Thispaper,based on fieldworkcarried outduring the 1986
summerseason,summarizes the geology ofthe Bunger Hills
areaandpresentsresults ofthe firstdetailed geochemical and
geochronological study of the area. It also attempts to
correlate the geological history with that of once contiguous
parts of Gondwana. A detailed account of the geology will
be published in a forthcoming AGSO Bulletin (Sheraton
et al. in press). The results of specialized structural and
petrological studies of the main Bunger Hills outcrops are
given by St6we & Powell (1989), Stüwe & Wilson (1990)
and Ding & James (1991).
The Bunger Hills area consists of granulite-facies
metamorphic rocks with a variety of compositions (of both
igneous and sedimentary origin), intruded by voluminous
plutonicrocks ranging from gabbro to granite, and a variety
ofmaficdykes (Fig. 1). The field relations,petrography,and
geochemistry of these rocks are described in the following
sections.
Felsic orthogneiss is widespread in the Bunger Hills and
adjacent islands, and makes up most of the Obruchev Hills
(Fig. 1). It also occurs as a relatively minor component of
layered metasedimentary rocks. Major constituents are
orthopyroxene(upto 15%),quartz(mostly 15-40%),feldspar
(mostly 50-65%) and biotite (up to 3%); minor amounts of
clinopyroxene, hornblende, or garnet may also be present.
Opaqueminerals,apatite,andzircon areubiquitousaccessory
phases, whereas monazite is less common and allanite and
rutile are rare. Textures are commonly granoblastic
inequigranular,andmostgneisses have onlyaweakfoliation
defined by elongated mineral aggregates, biotite grains, or
lenticular quartz agggates. In many gneisses the only
feldspar is plagioclase (commonly antiperthitic andesine,
rarely calcic oligoclase or sodic labradorite), and such
tonalitic (using the nomenclature of Streckeisen 1976)
orthogneiss predominates in the south-eastern Bunger Hills
andObruchevHills. Elsewhere,morepotassic(granodioritic
to granitic) orthogneiss occurs interlayered with tonalitic
orthogneiss and metasedimentary rocks.
Chemical data (Sheraton et al. in press) and field
relationships indicate an igneous origin for the vastmajority
ofthese gneisses. Massive,poorly layered orthogneiss,such
as that in the Obruchev Hills, is probably ofintrusive origin,
whereas at least some of that interlayered with supracrustal
rocks may well be metamorphosed extrusive rocks. Most
orthogneisses aremetaluminous oronlyslightlyperaluminous
and are thus equivalent to the I-type (derived by partial
melting of igneous precursors) granitoids of Chappell &
White (1974). Like similar orthogneisses elsewhere in the
East Antarctic Shield (Sheraton et al. 1987b, Sheraton &
Collerson 1984),mostare depleted in Y and heavy rare-earth
elements. Such Y-depleted orthogneisses are thought to
representnew continental crust derived by partialmelting of
a hornblende 8 garnet-bearing, but feldspar-poor, mafic
source(possiblysubductedhydratedoceaniccrust)(Sheraton
& Black 1983). Only a few orthogneisses belong to the
Y-undepleted suite of Sheraton & Black (1983), which are
thought to represent partial melts at relatively low PR,o of
predominantly felsic crustal rocks, their relatively low Sr
being due to partial meltingwith major residual plagioclase.
An Y-depleted granodioritic orthogneiss from south-
western Bunger Hills has given an ion-microprobe U-Pb
zircon age of 1700 fMa, whereas a tonalitic orthogneiss
fromthe ObruchevHills yieldeda lateArchaean conventional
U-Pb zircon age of 2641; Ma (Sheraton et al. 1992); both
are interpreted as emplacement ages. Granodioritic
orthogneissfrom Thomas Islandwas emplaced 1521E 29Ma
ago and underwent high-grade metamorphism at 1190 x
15 Ma (ion-microprobe data: Sheraton et al. 1992).
Mafic granulite is commonly interlayered with both felsic
orthogneiss and paragneiss, although individual layers are
rarelymore than afew metres thickand some are boudinaged.
Mostprobably represents metamorphosed mafic intrusives,
buttexturesare granoblastic. Typical granulite layers contain
plagioclase (?4- 50-60%), clinopyroxene (10-20%),
orthopyroxene (10-20%), opaque minerals (up to 5%), and
minor apatite. Greenish-brown hornblende (up to 30%),
biotite (up to 5%), or minor quartz may also be present.
Ultramaficlayersand,morecommonly, podsarewidespread,
butvolumetricallyinsignificant. The mostcommon varieties
are hornblende pyroxenite and pyroxene hornblendite,
containingvarious amountsof clinopyroxene,orthopyroxene
and pale brown hornblende. Other bodies contain abundant
biotite or phlogopite, and olivine or plagioclase may also be
present. Minor phases include opaque minerals and dark
green spinel.
Layered, garnet-bearing felsic gneiss occurs throughout the
Bunger Hills area, but is rare in the Obruchev Hills.
Almandine-pyrope garnet(2-15%),biotite(up to 6%),quartz
(30-50%), perthite (up to 50%), plagioclase (commonly
4 p to 50%) and minor opaque minerals and zircon
occur in most rocks, and small amounts of apatite, rutile,
monazite, corundum, spinel, and sillimanite may also be
present; orthopyroxene occurs in some layers. Much garnet-
bearing gneiss is of relatively potassic composition, with
K-feldspar being the dominant feldspar. It is commonly
associated with aluminous metasedimentary rocks, and is
probably mostly of sedimentary origin. However, some
massive leucogneiss of granitic composition, with minor
garnet, is clearly intrusive.
Aluminous metasedimentary rocks (metapelites) crop out
with garnet-quartz-feldspar gneiss in much of the Bunger
Hills and nearby islands. Garnet (commonly 5-25%),
sillimanite (up to 10%), cordicrite (up to25%),biotite (up to
10%). quartz (25-50%), perthite (up to 45%). commonly
antiperthitic oligoclase-andesine (up to 25%), opaque
minerals (magnetite + ilmenite; up to 3%), and minor dark
greenspinel are prominentconstituents. Accessory minerals
comprise zircon,rutile, corundum,monazite, and rare apatite
and ?chevkinite. Orthopyroxene (generally rather altered)
occurs in some layers, and cordierite is partly replaced by
pinite and/or biotite. Corundum occurs in association with
opaque oxides and spinel.
Typical assemblages are:
The assemblage
The predominantmesoscopic structures in the Bunger Hills
area are open to tight or isoclinal, commonly asymmetric
folds (F,, Fig. 4b) which probably formed during a major
shortening event (Stüwe & Wilson 1990), under granulite-
facies conditions (M,). The small separations of D,boudins
and minor flattening component suggest a much smaller
finite strain than during D,. F, folds have highly variable
orientations, largely due to the effects of D,; they commonly
plunge to the WSW or to the E in southern Bunger Hills and
in more northerly or southerly directions near the Fishtail
Gulf pluton. D, axial planar foliation (S,) and mineral
elongation lineation (L,) are only weakly developed.
The regional strike in most of the Bunger Hills area is the
result of a thitd major ductile deformation (D,), still under
granulite-facies conditions (M,), which is thought to have
been accompanied by emplacement of the Lake Figure and
Fishtail Gulf plutons. F, folds are major upright, commonly
asymmetric structures with shallow E- or W-plunging axes
(Fig.4c). They clearly refold F, and F,structures, which are
thensteeplyplunging(Fig.4d). F, foldsin south-westernand
south-eastern Bunger Hills have axes plunging at low angles
(0-30') to the WSW (Ding & James 1991), whereas major
F, folds adjacent to the Fishtail Gulf pluton trend N-S to
NNW-SSE (Fig. 1). This variable orientation of F, folds is
thought to be due to deformation having been
contemporaneouswithemplacementofthe two Bunger Hills
plutons. Exceptfor those adjacent to the Fishtail Gulf body
(which has the strongestmarginalfoliation),plots offoliation
orientations define girdles consistent with folding about
WSW-trending axes having been the main factor in
determining theregional strike. Stüwe & Wilson(1990)also
explained the variable orientation of F,(theit F,)folding by
theeffectsofthe Fishtail Gulfpluton,althoughtheyconsidered
thisbody tohave been emplaced during D,. In contrast, Ding
& James (1991) interpreted the N-S folds as a separate
generation (F,), on the basis of superimposed folding
relationships with the WSW-trending F, folds. However,
such relationships could be explained if the plutons were
emplaced late in D,, after the WSW folds had started to form.
Much of the Charnockite Peninsula pluton (the unfoliated
quartz monzodioritic and probably the granitic rocks) is
about 20 Ma younger than the Bunger Hills plutons and
clearly cross-cuts D, structures (e.g, at eastern Thomas
Island). Metamorphosed dykes within the pluton include
probable syn-plutonic dykes which have some chemical
features (e.g. high Nb) in common with the quartz
monzodioritic rocks. Group 1 dolerites may be of similar
age. However, subconcordant, folded and metamorphosed
mafic dykes which cut the country rocks are clearly of pre-
D,(and possibly pre-D,) age.
A complex history of more localized brittle and brittle-
low to medium grade; alternatively, the two areas may not
have been juxtaposed at that time (Black et al. 1992).
Althoughgranulite-faciesmetamorphism apparently occurred
in the Archaean, the time of the most extensive new zircon
growth in the tonalite, T[';model ages of other tocks in the
area (intermediate to felsic intrusives and a paragneiss) are
much younger (1600-2280 Ma), indicating considerable
continental crust formation (and at least one period of high-
grade metamorphism) in post-Archaean times.
A similarrange ofrocktypes is presentnear the Sovietbase
at Mirny,350 km west of the Bunger Hills (Fig. 6) (Ravich
et al. 1968). Granulite-facies country rocks consist mainly
of interlayered tonalitic orthogneiss and mafic granulite,
withminorgarnet-bearingparagneiss. These are intruded, in
turn,bydyke-like dolerites and gabbronorites,''charnockites''
(orthopyroxene monzodiorite, quartz monzodiorite and
granodiorite; fayalite quartz monzonite and granite), and
several generations of aplite and pegmatite. The fayalite
granitoids have given a Rb-Sr whole-rock isochron age of
50224Ma(McOueen etal. 1972),andigneous activitywas
accompanied by resetting of the K-Ar system in the country
rocks (Ravich et al. 1968). However, in the absence of
definitive geochronological data, the ages of high-grade
metamorphism and mafic intrusives at Mirny, as in the
Denman Glacier area, are unclear and regional correlations
(Table II) can only be tentative.
The nearestmajor outcrops to the east of the Bunger Hills
arethe Windmill Islands, about 400 km distant(Fig. 6). The
Garnet compositions are commonly a basis for ther-
mobarometric calculations, and thus modifications to
prograde garnet zoning have been the subject of many
previous studies. Volume diffusion can modify garnet
compositions at temperatures greater than ca. 550-650
*C (Loomis et al., 1985; Florence and Spear, 1991). The
prograde composition of garnet around primary mineral
inclusions, trapped during garnet growth, can be modified
by exchange reactions (Tracy, 1982) and by net transfer
reactions (Whitney, 1991). Yardley (1977) suggested that
modifications to prograde garnet zoning could be region-
ally mapped and used to define an isograd comparable to
the breakdown of muscovite to form sillimanite and feld-
spar. Zones of abrupt change in prograde garnet zoning
patterns, such as those described in the present study, can
be formed by sequential episodes of garnet growth with-
out volume diffusion (Rumble and Finnerty, 1974), by
discontinuous reactions (Thompson et al., 1977), and by
resorption with volume diffusion (de Bethune et al., 1975;
Tracy, 1982; Karabinos, 1984).
This study focuses on modifications to prograde garnet
zzoning patterns in two locations, representing two differ-
ent metamorphic episodes. These garnets abruptly change
composition in a zone 50-150 um wide at their rims,
around a variety of mineral inclusions, and along some
cracks in the garnet interiors. The zones are decorated by
small mineral and fluid inclusions 1-2 um in diameter.
These zones are interpreted to have formed by metaso-
matic dissolution and reprecipitation of preexisting garnet.
Rocks in this study were sampled from two locations
in the Acadian zone of western New England (Fig. 1).
Peak metamorphic minerals west of the Proterozoic mas-
sifs were produced mainly during Middle Ordovician
(Taconian) metamorphism and during Early to Middle
Devonian (Acadian) metamorphism east of the massifs
(Sutter et al,, 1985). Both metamorphisms produced Bar-
rovian facies series mineral assemblages. The Acadian
metamorphic front (Fig. 1) represents a zone of transition
from rocks in which the mineral assemblages, textures,
and structures are dominantly the result of Taconian
metamorphism to those in which they are due to Acadian
metamorphism (Hames et al., 1991). Pre-Silurian rocks
east of the Acadian front, as at the locality described be-
low, locally contain polymetamorphic mineral assem-
blages that reflect both events.
Rocks within the Sharon Quadrangle and vicinity,
northwestern Connecticut (Fig. 1), were initially meta-
morphosed at middle amphibolite-facies conditions (Zen,
1981; Wang and Spear, 1991) during the Taconian orog-
eny and were again metamorphosed at varying grades
during the Acadian orogeny (Hames et al., 1991). Sample
WH141 is a metamorphosed shale, part of the Walloom-
sac Formation, which in New York, Massachusetts, and
Vermont contains Middle Ordovician faunas (Ratcliffe,
1974; Finney, 1986).
The structure and stratigraphy of the Strafford Quad-
rangle and vicinity in east-central Vermont (Fig. 1) were
described by Doll (1943), White and Jahns (1950), How-
ard (1969), and Ralph (1982). and the metamorphism of
the area was described by Menard (1991). Devonian rocks
exposed in the quadrangle were deformed and metamor-
phosed during the Acadian Orogeny. The major struc-
tural feature in the Strafford Ouadrangle is the Strafford
Dome (Fig. 1), which exposes a sequence of rocks increas-
ing inward from chlorite grade to kyanite grade. The sam-
ple described here is from a region that Ferry (1990) pro-
posed to have experienced high synmetamorphic fluid
flow.
Mineral compositions for the sample from Connecticut
were determined at the Virginia Polytechnic Institute and
State University on a Cameca SX50 electron microprobe.
Areal maps of Fe, Mg, Mn, and Ca concentration were
based on X-ray intensity data accumulated from wave-
length-dispersive spectrometers. Similarly, mineral com-
positions for samples from Vermont were determined us-
ing the Jeol 733 Superprobe at Rensselaer Polytechnic
Institute. Analytical conditions for spot analyses of all
samples were 15 kV and 15-20 nA for a maximum of 40
s per element analyzed. Natural and synthetic silicates
and oxides were used as standards. FTIR and Raman
spectroscopic study was conducted at Virginia Polytech-
nic Institute and State University.
Sample WH141 (Fig. 1; UTM coordinate B33234370)
is a schist in which Taconian metamorphism produced
the assemblage kyanite = staurolite garnet + biotite
quartz -+ muscovite -+ plagioclase -- ilmenite with
minor apatite, graphite, and pyrite (Hames, 1990). Com-
positional layering in the exposure defines a crenulated,
composite surface that is overgrown by the garnet, kya-
nite, and staurolite (T, porphyroblasts of Sutter et al.,
1985; Hames et al., 1991). This composite surface and
the porphyroblasts are deformed by both northwest- and
northeast-trending Acadian folds (the D, and D, folds of
Ratcliffe and Harwood, 1975).
Porphyroblastic garnet, kyanite, and staurolite have ir-
regular, rounded shapes and are surrounded by coarse,
randomly oriented muscovite, suggesting resorption of
early-formed, less hydrous porphyroblasts and formation
of a more hydrous assemblage. Traces of acicular silli-
manite are present on the resorbed margins of garnet por-
phyroblasts in sample WH 141. Plagioclase porphyro-
blasts are texturally zoned, with fractured, inclusion-rich
cores and clear subhedral rims; however, they have a uni-
form composition of about An ,,. The porphyroblast re-
sorption, coarse muscovite, and acicular sillimanite are
interpreted as having formed during Acadian metamor-
phism (Hames et al., 1991). The rim compositions of
porphyroblasts in WH141 define P-T conditions of 510
E 25 C and 4 8 0.5 kbar; from this sample locality,
Acadian metamorphic P-T conditions increased mono-
tonically to greater than 650 'C and 7 kbar southeastward
along the present erosion surface (Hames et al., 1991).
Textures of garnet representative of this locality are
shown in Figure 2. The garnet interior is characterized by
concentric changes in major cation concentrations that
are consistent with growth zoning. In contrast, there is an
abrupt decrease in A,,,, and accompanying increases in
A.. Ag,, and k-; within -100 um of the garnet rim.
Identical compositional changes in garnet occur along
some cracks and around some primary apatite, musco-
vite, biotite, and ilmenite inclusions. These composition-
al changes are uniform and do not vary with respect to
which mineral is in contact with garnet along the rim or
inclusion interface.
The regions of sharp compositional change are marked
by abundant secondary fluid and mineral inclusions, up
to --2 um in diameter, that impart a clouded appearance
in thin section (Fig. 2A, 2C). The clouded regions along
cracks in the garnet cross the concentric growth zoning
pattern, and thus these small inclusions are secondary
with respect to prograde garnet growth. Mineral inclu-
sions in these regions are highly birefringent and appear
to be muscovite and sphene. The fluid inclusions are gen-
erally equant, and negative crystal shapes (reflecting the
host isometric structure) are common. Spacing between
these inclusions ranges from 2 to 10 um. Some inclusions
are arranged in parallel, linear arrays: these arrays are
generally perpendicular to the garnet surface. The largest
fluid inclusions (-- 1-2 um in diameter) have distinct va-
por bubbles that adhere to the inclusion wall, suggesting
that the fluids are H,O-rich. FTIR and laser Raman spec-
troscopic data from garnets in sample WH 141 (and oth-
ers in the vicinity) detect both CO, an H,O in the fluid
inclusions.
Sample 8516g is a calcic pelitic schist from the Devo-
nian Gile Mountain Formation collected from a roadcut
on the northbound entrance ramp of I-89 in Sharon, Ver-
mont (Fig. 1). The matrix assemblage is kyanite + stau-
rolite + garnet + biotite + plagioclase (An,,) + quartz
-+ muscovite + graphite, with minor rutile + ilmenite f
tourmaline + zircon, Ilmenite and the first generation of
muscovite and biotite define a crenulated S, schistosity.
Orientation of a second generation of biotite defines the
S, schistosity. The garnet core has straight inclusion trails,
composed of epidote and graphite, that define an includ-
ed S, surface at a high angle to the matrix S, foliation
(0Fig. 3).
Garnet in sample 8516g is intensely resorbed (Fig. 3)
and replaced along its rim and interior by coarse, unori-
ented muscovite that crosscuts the internal S1 fabric in
the garnet (Fig. 3). The garnet rim and surfaces of the
garnet adjacent to the late muscovite have a zone 30 um
wide with abundant micrometer-scale fluid and mineral
inclusions. Garnet compositions (Fig. 3C) are similar to
those in sample WH141. 3,,, in the garnet core is high
and fairly uniform. However, in the outer 50 um of the
garnet rim, k,4,, decreases sharply as 2.., and , in-
crease; similar sharp zoning patterns are evident adjacent
to some muscovite, ilmenite, and plagioclase inclusions.
Garnet in this sample grew during Acadian metamor-
phism by the reaction (Menard, 1991) Chl + Ep + Ms
-+ Qtz + Gr = Grt - Bt + P1 + H,O + CO;, Garnet
was interpreted to have grown along a computed P.T'
path of heating during a 1-kbar pressure increase up to
conditions of 550 8 25 C and 7.5 8 0.6 kbar (Menard,
1991). The subsequent development of sharp composi-
tional zoning at the rim of the garnet probably formed
without a large change in P and T: the distribution coef-
ficient of Ca between garnet and plagioclase inclusions
does not change substantially across the sharp composi-
tional zoning in the garnet. The large change of garnet
and plagioclase compositions under seemingly isother-
mal, isobaric conditions indicates open-system behavior
and metasomatic loss of Ca from the rock.
We propose that this single-crystal style of metaso-
matism formed the textural and compositional disconti-
nuities along garnet surfaces in these samples. The garnet
resorption textures, presence of fluid inclusions, and ac-
companying compositional changes suggest that the outer
surface of the garnet was dissolved in a metamorphic
fluid. The garnet being resorbed probably had relict, dis-
equilibrium compositions and the equilibrium garnet
compositions changed as a result of the resorption. This
mineral-surface scale metasomatism modified the origi-
nal prograde garnet zoning pattern along every pathway
accessed by the metamorphic fluid, even along cracks and
some inclusion boundaries inside the garnet. Reprecipi-
tation of garnet in equilibrium with the fluid, concomi-
tant with the dissolution, trapped both fluid and mineral
inclusions in pits near the garnet surface.
We cannot prove that the new garnet was in equilib-
rium with the fluid, yet fluid-controlled equilibrium is
strongly suggested because (1) garnet composition is sim-
ilar along every surface that contains the mineral + fluid
zone, irrespective of which mineral is in contact with gar-
net across the zone; (2) we obtain consistent pressure-
temperature estimates (i.e., consistent mineral K,,, and
thus P-T estimates) from the garnet rims and matrix min-
erals in these samples; and (3) the P-T estimates based
upon garnet rim and matrix mineral compositions are
compatible with the sample matrix assemblages and the
regional metamorphic setting.
These samples provide evidence of compositional
modification interpreted to result from fluid-assisted
metasomatism in metapelitic rocks at lower to middle
amphibolite facies. The extent to which processes more
rapid than volume diffusion can modify composition in
minerals places limits on the applicability of models based
on volume diffusion. Thus the validity of a volume dif-
fusion model to establish guidelines for which natural
minerals are safe to analyze in polymetamorphic rocks
(Jiang and Lasaga, 1990) relies upon the extent to which
volume diffusion is the fastest process for effecting a
change in a particular garnet sample. Tracy (1982) sug-
gested that hydration reactions can modify garnet rim
compositions more effectively than volume diffusion. This
study documents that these reactions occurred at am-
phibolite facies conditions and were metasomatic in the
sense of Lindgren (1928). This study also emphasizes and
expands a conclusion of Loomis (1983) and Whitney
(1991): postentrapment net-transfer reactions can occur
among garnet, primary mineral inclusions, and the ma-
trix if there are cracks or inclusion boundaries that pro-
vide a link with the matrix assemblage.
The data of this study are from two areas in the New
England Appalachians. In addition, previous studies in a
variety of metamorphic settings have documented zones
10-300 um wide enriched in fluid and mineral inclusions
that precede a reaction front. Zhou and Fyfe (1989) noted
that an inclusion zone precedes reaction fronts in altered
volcanic glass. Features analogous to the present study
have been reported in a separate part of Vermont by Ka-
rabinos (1984), in the Italian and Swiss Alps by de Be-
thune et al. (1975), and in the Dalradian of Scotland by
Yardley (1977), indicating that this style of garnet com-
positional modification is more extensive than previously
recognized.
Garnets from a prograde metamorphic zone in Ver-
mont and a polymetamorphic zone in Connecticut
changed composition by a similar, crystal-scale metaso-
matic process. This metasomatism dissolved garnet with
disequilibrium compositions and reprecipitated garnet
with a composition in equilibrium with the metamorphic
fluid and the evolving matrix assemblage. Fluid and min-
eral inclusions trapped within garnet and associated abrupt
compositional changes are the principal indications of this
process. The metasomatism also changed the composi-
tion of garnet along cracks that linked the garnet interior
with the matrix.
The research represented in this study was completed during the dis-
sertation work of the authors, and we gratefully acknowledge the advice,
guidance, and support of R.J. Tracy and F.S. Spear in our respective work.
W.E,H. wishes to thank R.J. Bodnar for advice in this work and assistance
with the Raman and FTIR analyses. Comments and reviews from Donna
Whitney, Doug Smith, R.J. Tracy, Frank Florence, and K.V. Hodges pro-
moted considerable improvements in the final manuscript. Financial sup-
port was provided for W,E.H. through Connecticut and New York Geo-
logical Survey grants, and NSF grants EAR-88-16382 and EAR-86-96064
(Tracy). Financial support was provided for T.M. through NSF grant EAR-
89-16417 (Spear).
gramme (WCRP) with four components: data,
applications, impacts and research. The World
Climate Research Programme was initially
established by the World Meteorological
Organization and the lnternational Council of
Scientific Unions (lCSU). n 1992, the intergov
ernmental Oceanographic Commission (lOC) of
UNESCO became an additional sponsor. The
World Climate Research Programme was given
specific objectives of determining:
Climate research then become a major
focus of international scientific activity. t was
recognized from the beginning that, to achieve
these objectives, it would be necessary:
The mandate of the WCRP is to study
climate variations and changes, both natural
and those induced by human activities, on time-
scales ranging from seasons to a century.
The scientific strategy of the WCRP has been
to build physicalmathematical models of the
coupled global climate system. There is a
need for process studies to improve under-
standing of key elements of the climate
system so that they can be adequately repre-
sented in climate models. There is a need for
global observations against which climate
simulations can be verified. The WCRP does
not attempt to organize all climate research
(let alone all atmospheric or oceanographic
research); instead the emphasis is placed on
those activities which require international co-
operation, either because of their global or
large tegional scale or because of the human
and financial resource requirements. t is
recognized that much essential climate
research is being done-and is best done-in
small research groups at universities or
governmental institutes around the world.
The concept of time- and space-scales is
important for scientific analysis of the climate
system. There is a wide range of scales in the
climate system (Figure 2). We cannot hope to
represent the small scales explicitly in a
climate model, but it is necessary to include
their impacts through parameterizations. It is
useful to think of the climate system as being
divided into two components: the fast and the
slow climate systems. The fast climate system
is the atmosphere, the upper ocean (that part
subject to an annual cycle of vertical mixing
due to wind) and the transient processes at
the land surface. The fast climate system is
active, driven by the atmospheric engine, and
comes to statistical equilibrium in a few years.
The slow climate system consists mainly of
the deeper ocean and the perennial land ice,
with a response time which may be decades
to centuries. The major interactions between
the fast and slow climate systems take place
in a limited number of areas where heat is
transferred by the up- or downwelling of ocean
water and at high latitudes, where cold, dense
water sinks to great depths. To a first approxi
mation, the magnitude of climate response to
a change in forcing, such as increasing green-
house gas concentrations or changes in solar
radiation, will be determined by the fast
climate system; the decadal rate of change
will be determined by the slow climate system.
The sponsors established the Joint
Scientific Committee (lSC) for the WCRP with
the mandate of providing scientific guidance,
determining the main research objectives,
reviewing and assessing the development of
the Programme, and giving overall co-ordina
tion of efforts at the international level. The
members of the JSC are selected by mutual
agreement of the sponsors, on the basis of
their scientific knowledge, capability and
breadth of vision. The Director and the Joint
Planning Staff for the WCRP are responsible
for developing detailed plans for the WCRP,
within guidelines established by the JSC, and
following up with the implementation. Prior to
'Southern Africa experiences its worst drought
this century.'(BBC news broadcast, 22 April
1992:
'Darkness falls on Colombia as Ihydrolelectric-
ity plants run dry.'(The lndependent, London,
20 Aprl 1992)
'Britain faces worst drought for centuries.
Prince Charles warns country is dying' from
water shortage.' (The Sunday Times, London,
24May 1992)
These are but three climate anomalies noted
recently in the British news media. Earlier, there
were reports that Australia had experienced
severe drought in the latter part of 1991, while
the Gulf Coast of the USA was wetter than nor-
mal. Some people will recall something similar
hapening in 1986/1987 and those with a good
memory will remember the large-scale climate
disruption of 1982/1983.
The climate always seems to be anoma-
lOus somewhere, but there are times when it
seems to be more anomalous than others, and
the last several months is one such time, as
were the two episodes mentioned above. Part
of such behaviour is undoubtedly random and
not predictable. Flip a coin 100 times and you
might find a run of six heads. t is not instruc-
tive to ask why and-more to the point-
observing a run of six heads does not help you
to predict the next flip; it could just as likely be
a head as a tail. On the other hand, part of the
climate signal is not random, but results from
coherent interaction between the atmosphere
and the ocean. The largest such signal is
ENS0-El Nfio/Southern Oscillation, here EN
refers to the oceanic component and SO to the
atmospheric component, TOGA was established
to observe the tropical regions, to investigate
why there are strong low frequency (three- to
fiveyear) climate signals there, and to try to
predict the coherent part of the signal.
Since space does not permit a thorough
review, the reader is referred to Glantz et al.
(1991) for a review of ENSO observations; to
released globally by deliberate or wild
biomass burning;
ln the aftermath of the fires, WMO re-estab-
lished the radiosonde and related meteorologi-
cal observations facilities and telecom-
rmunications system in Kuwait and also estab-
lished three new GAW regional background air-
pollution monitoring stations. These are
situated in the downwind regions of the Gulf and
are located in lran, Pakistan (see photo below)
and on the Red Sea coast of Egypt. They moni-
tor atmospheric turbidity, precipitation cherm-
istry, aerosol composition, black carbon,
surface ozone, UV-& radiation and radiation bal-
ance components, all of which, combined with
automatic station observations, are recorded
on computers for ready access.
Recognlzing that the atmospheric mea-
surements collected in the Gulf region during
the Kuwait oil well fires constitute a valuable
resource for environmental and atmospheric
researchers, the Kuwait Data Archive (KuDA)
project to be located at NCAR (Boulder, Col-
orado) has been established. It s planned that
this resource will be easily accessible and will
contain all pertinent data sets with a database-
management system providing software tools
for accessing the database.
The results of these two meetings and
concrete augmentation of both the GAW and
the WWW facilities in the region affected by the
fires demonstrate how an international scien-
tfic effort, co-ordinated by WMO, can be highly
successful. Through this event, much has been
learned that can be applied in any hiture atmo-
spheric accidents.
After the Atmospheric Transport Model Evalua-
tion Study (ATMES) in which model calculations
were compared against the accidental release
of radioactive material, it was realized that fur-
ther testing of models was necessary and a
follow-on project was proposed: the European
tracer experiment (ETEX). The experiment will
consist of three distinct parts: the release of
an inert tracer sometime in early 1994, appli-
cation of realtime models during the release,
and, when all the meteorological and tracer
data are available, a detailed model evaluation
will be performed along the lines of ATMES. t
is sponsored by the European Community, the
lnternational Atomic Energy Agency and the
World Meteorological Organization.
At the consultation of experts held in the
WMO Secretariat (Geneva, 1-4 June 1992).
representatives of the Meteorological Services
which will participate in ETEX were brought
together for the first time. The ground pro-
gramme of tracer release was discussed and
plans were initialized.
The tracer will be the same material that
was used in the longrange experiments con-
ducted in North America in 1983 and 1987. t
comes from a family of man-made perfluoro-
carbons that are inert in the troposphere and
have a very low background concentration.
This allows a release of the tracer at low con-
centrations and detection at thousands of kilo-
metres from the source. Over 200 synoptic
stations were chosen as sampling sites, rang-
ing from France in the west, to Poland and
Romania in the east. Collectors will be pro-
vided, consisting of a pump, a set of ion
exchange columns and a timer. The air is
At the invitation of the Government of Para-
guay, a WMO/North American Plant Protection
Organization symposium/workshop on meteo-
rology and plant protection was held in Asun-
ci6n from 1 to 10 April 1992.
The programme included sessions on:
There were about 40 participants from
Regions ll and N.
ln colllaboration with the Technical Centre for
Agricultural and Rural Cooperation (CTA) and
FAO, WMO organized a workshop on the dis
semination of agrometeorological information
by rural or national radio, television or the writ
ten press. At the invitation of the Government of
electricity production, medical treatment, envi-
ronmental research and other peacelul pur-
poses, Many countries were now applying
nuclear techniques to sohe practical ecological
and social problems, and, through international
cooperation, progress had been made in the
areas of safety and waste management.
He said that countries would have to
assess carefully what mix of energy sources
was compatible with sustainable development
and what was the optimum use of energy. The
burning of fossil fuels, which made up about 90
per cent of the world's commercial energy was
also the cause of some of the world's most seri
ous environmental problems. The use of nuclear
power would have to be part of the attempt to
meet growing energy needs in a safe and envi-
ronmentally sustainable way.
The AEA issued a booklet for UNCED entitled
Nuclear Power, Nuclear Techniques and Sus-
tainable Development. t contains sections on
energy use and the environment (energy SUD-
ply/demand, radioactive waste and nuclear
plant safety) and on applications of nuclear tech-
niques to protect the atmosphere, manage
ecosystems, preserve the quality of water
resources, satisfy basic health needs and pro-
mote sustainable agriculture.
Single copies of this booklet are available upon
request from the AEADivision of Public nfor-
mation, P.0. Bo 100, A-1400 Vienna, Austria.
Tel.: 431) 23601270. Fa: 4431) 234564.
Telex: 1-12645.
Under an AEA project in Poland, electronbeam
(EB)technology is being applied to remove flue
gas emissions, which are linked to acid rain. At
a pilot plant near Warsaw, it has been demon-
strated that approximately 80 per cent of the
sulphur dioxide and 90 per cent of the nitrogen
Oxide can be removed from combustion flue
gases.
The technology consists in using electron
accelerators to irradiate the smokestack
exhausts of coalfired power plants, and to
transform sulphur dioxide and nitrogen oxide, in
the presence of ammonia, into ammonium sul-
phate and ammonium nitrate, which can then be
used as fertilizer.
More information may be obtained from the
Division of Physical and Chemical Sciences,
AEA, P.0. Box 100, A-1400 Vienna, Austria.
Tel.: (431) 23601270. Fa: 431) 234564.
Telex: 1-12645.
'Only when international research networks and
institutions have a welldefined funder whose
interest is international, not national, whose
agenda is dictated by international scientific
concerns, not domestic concerns, will the
global change enterprise flourish.''
So spoke Dr Peter E. de Janosi, Director of
the lnternational lnstitute for Applied Sys-
tems, at the close of the conference held
from 12 to 13 May 1992 to mark ts 20th
anniversary
The conference, which attracted more
than 300 experts from some 25 countries,
including policy-makers and scientists from
many different disciplines, centred on a num-
ber of workshops concerned with energy use
brighter trails. For example, for a 1'.0 increase in apparent
visual brightness, and a factor of -2.5 increase in pixel
responses, the tuning distance can be set at --86,700 km.
This distance corresponds to an angular scan rate with re-
spect to the celestial sphere of -110 urad/s, (i.e., leading
Earth's rotation by --40 urad/s). However, the detection vol-
ume (see Figure 8) decreases severely with decreasing tuning
distance [ates, 1989], and the detection rate per frame is
less by a factor of -4.
The reduced mass estimate is important in assessing the
physical state of the water vapor responsible for the transient
absorption of ultraviolet dayglow observed as atmospheric
holes with Dynamics Explorer 1. Specifically, the mass of
the small comets is originally estimated by Frank et al.
[1986b] with the assumption that the water vapor in the
obscuring cloud is composed of unbound water molecules.
As discussed in section 2, the content of the water vapor
cloud may consist of dimers and molecular clusters, in which
case the ultraviolet absorption cross section is expected to
increase significantly with respect to the corresponding mul-
tiple of the single-molecule cross section. The mass of water
vapor required to produce an atmospheric hole is corre-
spondingly reduced to values in the range of the masses from
the direct telescopic sightings.
The brightness of a small
comet after fragmentation is a subject of considerable interest
because of the possibility of visual detection by a ground
observer positioned at predawn or postdusk local times while
the ice cloud is illuminated by sunlight at higher altitudes.
As a benchmark value, we note that the apparent visual
magnitude of the small comets just prior to breakup at an
altitude of --1000 km is M, 22 8 if we use the telescopic
Where does the water come from, where does it go, and
what happens along its path? These seemingly simple ques-
tions about the oceans' (semi-) permanent circulations are
at the heart of physical oceanography. Answers are sought
from hydrographic data and direct current observations and
tracer measurements, and ideas are dissected with analytical
fervor. The North Atlantic Ocean is the most completely
observed and extensively studied of all the world's oceans,
and yet it still resists thorough description and rationaliza-
tion, More observations and higher-resolution models using
cutting edge supercomputers have helped but have not yielded
satisfaction that an accurate picture is at hand. However, it
isnow possible to present a replacement for the North Atlantic
circulation scheme hypothesized by Worthington [1976]
(hereinafter W76) and resolve many of the transport dilem-
mas he highlighted. Basically, what we do is describe a
general circulation that is compatible with community wis-
dom as opposed to one author's idea or the results of one
approach or method.
The difficulties experienced by W76 and us in producing
a circulation scheme arise to a large extent because there are
only a few circulation components that are quantitatively
tightly constrained. That is, we rarely are able to measure
transports very well. Uncertainties in transport estimates are
typically at least 5-25%, with the smaller error unique to
well-defined channels like the Straits of Florida. Careful
treatments of individual basins or smaller regions may not
be particularly compatible when merged, even though the
individual treatments may each appear ''right.'' Fxr many
locations the intensity of the general circulation is obscured
by the presence of mesoscale eddies and recirculating gyres,
and choice of ''reference level'' is usually arbitrary. There
are, however, a few circulation elements that we feel are well
constrained by the observations. The northward transport of
the Florida Current through the Straits of Florida is close to
30 Sv, and approximately 45% is from the South Atlantic.
The net transformation of warm water to cold water in the
North Atlantic is 13 Sv. The rate of production of dense cold
overflow waters in the Nordic Seas is 6 Sv. Downstream from
the Straits of Florida, the 30 Sv transported as the Florida
Current isincreased toroughly l00 Svby recirculating gyres.
Our goal is to combine these four circulation elements with
what else is known of the distribution of currents and water
masses to produce a circulation scheme for the North
Atlantic.
Schmitz and Richardson [1991] (hereinafter SR91) re-
cafand Stalcup, 1967]. This is clearly true, with only about
1 Sv (comparatively fresh) in the 12-24'C range (the un-
dercurrent temperature (T) interval) moving into the Straits
of Florida from the South Atlantic. However, about 7 Sv of
water in the upper 50-100 m of the water column (T > 24'C)
above the undercurrent depth and T/S rapge, and 5 Sv (7?
ec T 12%C) below the undercurrent TIS range, can move
into the Caribbean from the South Atlantic and out with the
Florida Current [SR91; Schmitz et al., 1993]. Worthington
[1976] closed the Florida Current (30 Sv) within his sub-
tropical gyre, which is confined essentially to the southern
recirculation of the Gulf Stream west of the Mid-Atlantic
Ridge.
Another low-latitude transport dilemma is associated with
the Sverdrup relation [Böning et al., 1991; Leetmaa et al.,
1977; Leetmaa and Bunker, 1978; Roemmich and Wunsch,
1985; Wunsch and Roemmich, 1985]. Sverdrup dynamics,
relating the Sverdrup transport distribution to the curl of the
wind stress, is a good first approximation for the currents in
the interior of oceans and is the cornerstone of widely ac-
cepted ideas about the wind-driven circulation. The reader
interested in pursuing this topic could start with Stommel
[1957, 1965], Wunsch and Roemmich [1985], and Schmitz
et al, [1992] including references. The question typically
posed is whether or not the well-established 30 Sv or so
flowing north through the Straits of Florida [Schmitz and
Richardson, 1968; Richardson et al., 1969] is balanced by
a southward Sverdrup transport in the interior North Atlantic,
and the answer is controversial. A partial resolution [Schmitz
et al., 1992] of this quandary appeals both to SR91 and to
the idea that the Sverdrup transportoontribution to the Florida
Current is coming from east of roughly 55?W along 24N.
Schmitz et al. [1992] suggest that the more eddified ther-
mocline layer flow field west of 55%W is' associated with a
smaller-scale recirculation overlying a strong deep western
boundary current (DWBC) system.
The comparison of the geostrophic flow that would result
from the Sverdrup relation using known mean winds with
the geostrophic flow calculated from observed hydrographic
data and a reference level estimated by inverse techniques
by Roemmich and Wunsch [1985] indicated that at about
55%-60 W along 24'N the theoretical and observed curves
begin to diverge. East of this longitude the calculated and
observed geostrophic transports agree to within a few sver-
drups for a total of 20. West of this line where the observed
geostrophic currents are varying on much smaller zonal
scales than the calculated transports and may be strongly
time dependent, the disagreement may reach 15 Sv east of
55%W the Sverdrup transport is 17 Sv, comprised of a north-
ward Ekman transport of 3 Sv and a southward interior
the layers. Cation substitution in phyllosilicates is a special
form of isomorphous substitution of a higher charged cation
by a lower charged one (e.g., Li' replacing Mg ', Mg't
replacing AP', or AP* replacing Si'*). The negative layer
charge resulting from cation substitution is often times called
structural or permanent charge, the latter because the particle
charge is not dependent on the solution pH.
When the magnitude of the negative layer charge is
relatively low and the chemical potential of water high, the
phyllosilicate expands or swells as inter layer cations hydrate.
Swelling behavior and cation exchange capacity attract the
interest of mineralogists and chemists because they are the
source of many important chemical and physical properties.
Reference to octahedral and tetrahedral sheets (Figures 3
and 5), the basal-oxygen plane; i.e., the oxygens at the base
of the tetrahedral sheet (Figures 3 and 5), cation substitution,
and interlayer cations will appear time and again in the com-
ing sections. Because phyllosilicates are made up of sheets
of coordination polyhedra, clay mineralogists often refer to
all phyllosilicates comprised of one tetrahedral and one octa-
hedral sheet as ''1:1'' phyllosilicates (Figures 3 and 4). Min-
erals like pyrophyllite, the micas, talc, and the smectites
(Figures S and 6) are comprised of one octahedral sheet
between two tetrahedral sheets. These are referred to col-
lectively as ''2:l'' phyllosilicates. With the basics of nomen-
clature and structure defined, we can turn to the first topic:
quantum chemistry.
Quantum chemistry is a broad and often complicated sub-
ject. I will restrict this discussion to those concepts and
methods that are necessary to understand the quantum chem-
ical literature as it relates to phyllosilicates. The most popular
quantum chemical computing techniques are classified as
either ab initio or semiempirical. The former attempt, with
varying degrees of rigor, to explicitly compute the electronic
states of the material, while the latter simplify the consid-
erable computational task by a variety of semiempirical
expressions and the neglect of certain terms. Given the enor-
mous computational demands of ab initio methods and the
complexity of phyllosilicate structures, complete and rigor-
ous quantum chemical calculations of phyllosilicates are and
will remain impractical for some time to come. In the mean
time, most studies will use semiempirical methods.
The basic approach is to represent the electronic states of
a material, molecule or solid, using one-electron orbitals [9,]
that are themselves a linear combination of atomic orbitals
(LCAO):
The atomic orbitals ix,.) in (1) are centered at atomic
positions. The molecular (crystal orbitals) [o4,) extend over
the whole molecule (crystal). Ab initio methods construct
an antisymmetrized product of the (9,). Semiempirical
methods generally represent electronic states as they appear
in (1).
LCAO quantum chemical calculations solve an eigenvalue
problem.
The eigenvalues [e,) are the energies of the orthonormal-
ized one-electron states [g,}. The basis is the set of atom-
centered atomic orbitals [x,.).
Equations (2) of the LCAO quantum chemical eigenvalue
problem are used to generate a series of secular equations,
called Roothaan equations in honor of the scientist who sug-
gested the LCAO method. The power of this approach derives
from the matrix methods used to solve the Roothaan equa-
tions:
Evidently, organic loading into these systems is small in
comparison to inorganic loading. Perhaps this is not sur-
prising, inasmuch as suspended material delivery (with sus-
pended material contributing, on average, half of the organic
loading; Table 2) by northern European rivers to the ocean
is small [Milliman and Meade, 1983].
Nevertheless, we accept the data in Table 5 as our best
estimate of the relationship between primary production and
respiration in the coastal ocean. Clearly, more data are
needed to test this relationship further. With that caveat, (1)
can therefore be used along with the estimated primary pro-
duction rate of estuaries and the continental shelf in order to
calculate net system metabolism for these portions of the
coastal ocean.
Estuaries
Continental shelves
By inspection of Figure 1 we conclude that (P-R) at the
primary production rate estimated for estuaries (300 g C m'-
yr!) is significantly less than zero. Estuaries apparently
respire an average of about 20% more organic carbon than
they produce. If we accept this conclusion that estuaries are
net heterotrophic, then there must be an external source of
organic matter. Within the resolution of this analysis, con-
tinental shelves respire about as much organic matter as they
produce; we assume (P-R) for shelves is indistinguishable
from zero. We cannot rigorously evaluate the error in these
estimates of (P-R), but we suspect that the numbers are
accurate to about 50%.
The data on delivery, burial, and metabolism of organic
carbon can now be synthesized into an estimate of organic
carbon flux in the ocean (Figure 2). First, kt us consider
the coastal ocean.
Primary production of salt marshes, the open water area
of estuaries, and the continental shelf totals about 500 x
10'mol Clyr. About 80% of this primary production occurs
in the water column and 20% occurs on the bottom. Res-
piration exceeds primary production in the coastal ocean
estuaries plus continental shelf) by about 7 x 10'% mol Cl
yr. This number is reasonably close to the estimate, derived
from Ittekkot [1988] and Spitry and Ittekkot [1991], that about
10 x 10% mollyr of terrigenous organic matter reaching the
ocean is labile on a time scale short relative to exchange
with the open ocean. When one uses the primary production
and net metabolism data, respiration in the coastal ocean is
thus estimated to be about 507 x 10' mol C/yr. About 30%
of the respiration occurs on the bottom, and the remainder
occurs in the water column. Respiration in the coastal zone
apparently exceeds primary production by about 1.4%.
Net heterotrophy of the coastal ocean is supported by input
of organic material from land. This input totals about 34 x
10% mol Clyr. From Berner's data, about 9 x 10'% mol Cl
yr are buried in the coastal ocean. If the input, burial, and
net metabolism of the coastal ocean are all accurate, then
about 18 x 10% mol Clyr are not directly accounted for.
This material is assumed to be oxidized on a longer time
scale than the turnover of the labile organic carbon pool of
Ittekkot [1988] and Spitzy and Ittekkot [1991]. This estimate
is consistent with the conclusion that much of the POC de-
livered from land is converted to DOC and slowly oxidized.
The time required for this oxidation is apparently long in
comparison with the water exchange time between the coastal
ocean and the open ocean but short in comparison with the
calculated turnover time of organic carbon in the ocean
(4000-8000 years). Fxr budgetary purposes we assign this
slow respiration to the open ocean.
Open ocean primary production is estimated to be ap-
proximately 3600 x 10'% mol Clyr. Martin et al. [1987] used
sediment trap data to estimate that open ocean new produc-
Evidence from a variety of angiosperm species shows
that sperm cells maintain a close association with the
vegetative nucleus (VN) and each other as they move
down the pollen tube. These observations have led to
the proposal that the sperm and VN move as a Male
Germ Unit or MGU and that the association may be
important for successful transmission of the male ga-
metes (Russell and Cass 1981; Dumas et al. 1985;
McConchie et al. 1985; for review, see Knox et al. 1988;
Russell et al. 1990; Mogensen 1992). Arguments against
the universality of such an association have also been
presented however (for discussion see Heslop-Harrison
et al. 1986).
The mechanisms by which these interactions are es-
tablished and maintained are unclear. In a number of
species with tricellular pollen, the association is clearly
present in the mature grain (e.g. Russell and Cass 1981;
Dumas et al. 1985; McConchie et al. 1987). On the other
hand, connections can also be established after pollina-
tion (Mogensen and Wagner 1987). Many studies have
focused on interactions between the VN and sperm cells
(see Russell et al. 1990), but involvement of the anteced-
ent generative cell (GC) has received attention as well
(Derksen et al. 1985; Mogensen 1986a, b; Kaul et al.
1987; Heslop-Harrison et al. 1988; Hu and Yu 1988;
Wagner and Mogensen 1988; Taylor et al. 1989). Avail-
able information suggests that an association with the
VN is present in the mature pollen grain but may become
more complex after division (Mogensen 1986a, b;
Wagner and Mogensen 1988; Kaul et al. 1987). Further-
more, the relative positions of the GC and VN are vari-
able between species and within a given pollen tube (e.g..
Venema and Koopmans 1962; for review, see Heslop-
Harrison and Heslop-Harrison 1989a), lending addition-
al mystery to their relationship. A higher density of pores
The association is based at least in part on embay-
ments in the VN occupied by extensions at either or
both ends of the usually fusiform-shaped GCs and sperm
(for reviews, see Knox et al. 1988; Russell et al. 1990;
Mogensen 1992; Palevitz and Tiezzi 1992). Angiosperm
GCs and sperm also contain prominent Mt arrays in
the form of highly cross-bridged bundles (Palevitz and
Tiezzi 1992) that continue into the extensions.
concerned, from rudimentary ovaries with no style and stigma to
more developed ovaries bearing a style of different lengths up to
pistils with a long style and stigma, often evoking, in spite of their
sterility, female pistils. All of the flowers of one plant are, however,
strictly of the same type. Occasionally (from about 0.1% to 4%,
according to the cultivar), a few male plants bearing flowers of
the last type can also produce a certain number of berries (andro-
monoecious plants). On the contrary, to our knowledge, female
plants also bearing hermaphroditic flowers (gynomonoecious
plants) have never been reported.
A better knowledge of the genetic and/or environ-
mental factors controlling the development of the male
pistil in male flowers and of the behaviour of the differ-
ent types of male flowers in relation to pollination could
provide a glimpse to the origin of dioecy in Asparagus,
as well as further clarify whether, besides the primary
regulatory genes involved in sex determination, other
genes are involved in the inheritance of sex characters,
as first suggested by Franken (1970), with reference to
the heritability of andromonoecy. The investigation pre-
sented here was therefore mainly aimed at determining:
(1) whether and to what extent the character ''length
of the style'' in male plants is genetically determined;
(2) whether the general lack of fecundation in male ovar-
ies is due just to the absence of normal female gameto-
phyte inside the ovule, or if a mechanism inhibiting pol-
lination is already operating at the level of stigma or
style; (3) if there is always a correlation between the
length of the style, the presence of a stigma and the
extent of development of the megagametophyte in male
ovules.
Asparagus plants were grown at the Horticultural Research Insti-
tute of Montanaso Lombardo (Milan). Anthers for in vitro culture
were collected at the early uninucleate microspore stage from sever-
al male plants exhibiting a good aptitude to androgenesis (cvs 'Ear-
ly of Argenteuil' and 'Lucullus') and cultured in vitro according
to previously described methods. (Falavigna et al. 1990; Qiao and
Falavigna 1990). After 25-40 days, embryos from the anthers were
transferred to T1 medium (Oiao and Falavigna 1990) where they
proliferated into morphogenic calli, The calli, divided into small
pieces and subcultured 2-3 times every 15 days, were induced to
root, and the rooted plantlets were transferred to the greenhouse.
Of these, 58% were diploid. Seven doubled haploid male plants
and seven females were subsequently selected as parents of the
F, all-male hybrids.
AIl YY and XX plants obtained from in vitro anther culture, F,
and backcross (XY males from F, x XX parental female) progenies
were grown in a greenhouse. During flowering the temperature
was maintained at 26 [ 2 C during the daytime under natural
light; at night the temperature was lowered to 18*+ 2 C.
To visualize the growth of the pollen tubes within the styles, female
and male flowers (bearing pistils with a style 0-1.4 mm long) from
plants of a selected backcross, called backcross E (male parent:
doubled haploid from cv 'Lucullus'; recurrent female: doubled
haploid from cv 'Early of Argenteuil)', were hand pollinated and,
after 24 h, the pistils were collected. To test whether self-pollination
occurs spontaneously, pistils were also collected from opened flow-
ers of male plants that had been isolated for 72 h in insect-proof
cages. All of the pistils were fixed in FAA (formalin, 80% ethanol
and acetic acid at a 1:8:1 ratio) for 24 h, washed thoroughly under
running water, transferred to 8 N NaOH for 12 h, washed and
stained with 0.1% aniline blue in 0.1 N K,PO,. The pistils were
gently squashed under a coverslip and scored for pollen tubes with
a fluorescence microscope (excitation filter 350-460 nm, barrier
filter 515 nm).
Female pistils and male pistils of the different types (with a style
0, 0.2, 0.6, 1.4mm long) were fixed in 2.5 M, glutaraldehyde in
0.1 M phosphate buffer, washed, dehydrated and embedded in
epon araldite (Mollenhouer 1964). Serial sections (1 um thick) were
cut with a Reichert ultramicrotome and coloured with 2%4 Azur
In order to visualize the vascular elements, female pistils and
male pistils of different types were cleared and stained in fuchsin
as previously reported (Bracale et al. 1990).
Seven doubled-haploid Y Y male plants were crossed
with different homozygous females also obtained from
anther culture to yield seven different XY, all male, F, s.
Some of these plants were in turn crossed with the female
parent to give a backcross (BC, ) progeny. Two of these
seven crosses (specifically meant for use in a RFLP anal-
ysis in search for restriction markers localized on the
sex chromosomes) were particularly interesting with re-
spect to the present study: these were named cross E
and cross G. The male parent in cross G had pistils
with a long style (average length: 1.4 mm) and showed
the presence of stigma (incidentally, this shows that fac-
tors affecting style length and stigma development are
not localized on X chromosome). The male parent in
cross E exhibited, to the contrary, a reduced ovary, com-
pletely devoid of style and stigma. The male plants of
the F, (39 individuals of cross E and 43 individuals
of cross G) consistently had styles of an intermediate
length (0.5-+-0.07 mm for cross E, 0.65-4- 0.05 mm for
cross G. Apparently in cross E factors for the presence
of the style had been introduced by the female parent.
Interestingly enough, when F, male plants were crossed
with the female parent, the character ''length of the
style'' segregated in the male plants of the backcross,
which exhibited a great variation in lengths of styles,
from 0 to 1.4 mm (Fig. 1). The length of the style was
always correlated with the overall size of the ovary. In-
side each male plant, however, style length and ovary
dimension were strikingly uniform. We focused our at-
tention on cross E, and only these results will be referred
in subsequent experiments and discussions, even though
data from cross G were quite similar. Backcross E gave,
ent tectum formation (Fig. 6). Detailed descriptions of
pollen wall synthesis during normal development are
presented elsewhere (Majewska-Sawka et al. 1992),
hence this report will center on the most important
events and their relation with aberrant wall development
around MS microspores.
Tapetum. During the tetrad period, the mitochondria
show a further reduction in size (Table 1), and the plas-
tids seem to present greater morphological variability.
In addition to the forms observed during meiosis of
MMCs (Fig. 3) another plastid type with more lightly
stained stroma and long, narrow, irregularly arranged
lamella appear. Figures of dividing plastids are seen fre-
quently. Occasionally, concentric arrangements of ER
are noted. In depressions formed between the plasma-
lemma and tapetal cell walls, large numbers of spherical
electron-grey pro-Ubisch bodies accumulate (Fig. 4).
trtp'-S1-2 is a B. napus plant derived from selfing tr;p ', the fe-
male in the cross with pol-1 that gave rise to the first sexual cybrids
(Erickson and Kemble 1990). The tr designation indicates that the
plant carries the B. campestris cytoplasm with the mutant chloro-
plast gene conferring resistance to triazine herbicides (Reith and
Straus 1987). trp''-S1-1 was a B. napus plant closely related to
tr/p-S1-2. Both were derived from a line which had lost the
11.3-kb mt plasmid characteristic of this cytoplasm during in vitro
culture. The plasmid had been restored to the cytoplasm of one
line (p ') following a sexual cross to a male carrying the plasmid,
but not to the other line (p') (Erickson et al. 1989).
Westar-1, Westar-2 and Regent-2 are random plants from the Ca-
nadian B. napus cvs ' Westar ' and ' Regent '; both contain the
normal napus cytoplasm that does not confer triazine resistance.
Westar-1-S1-1 was a plant derived by selfing Westar-1. Similarly,
'Marnoo' and 'Andor' are normal B. napus cultivars of rapeseed.
'Bronowski' is a B. napus variety with B. campestris cytoplasm,
but is not triazine resistant; this particular accession of ' Bronows-
ki' does not carry the mt plasmid.
The data for this report are drawn from three separate pollen
transmission studies: PTS-1, -4, -8. Cross number one in Pollen
Transmission Study One would be designated PTS-1-1 and progeny
plant number 10 from that cross would be designated PTS-1-1-10.
As it is possible to maintain a B. napus plant for several years
because of its indeterminate growth habit, the same plant could
be used in subsequent experiments. Pol-2, for example, was used
as a male in PTS-1-1 (Table 1A) and several months later in PTS-1-
35 (Table iC).
The purpose of crosses in PTS-1-1, -1-2, -1-3, -1-4 (Table 1A) was
to compare the rate of pollen transmission between crosses. The
particular genotypes in PTS-1-1 (tr{p'-S1-2x pol-2)had given rise
to cybrid progeny in previous experiments (Erickson and Kemble
1990), and we wished to determine if transmission could be
achieved with other genotypes by varying the male of the cross.
Cross PTS-1-1 also served as a positive control; the transfer of
paternal mitochondria to the female plant in this cross would elimi-
nate the possibility that a failure to observe pollen transfer in the
other crosses to this female was due to the female genotype and
not the male genotype. For statistical analysis the RFLP data from
the crosses in Table 1 A was organized into a 2 x 2 contingency
table. The two columns were the number of progeny with maternal
and mixed restriction patterns of mtDNA. The two rows consisted
of data from PTS-1-1 and the pooled data from the other three
crosses. A chi-square statistic was used to compare the differences
between the two groups of crosses with respect to cybrid progeny.
Crosses PTS-1-5 and -1-6 were reciprocals of PTS-1-1 designed
to test whether the direction of mitochondrial transfer could be
reversed using the same plants (Table 1 B). The males of these two
crosses are near-isogenic lines differing mainly with respect to the
presence of the mt plasmid, i.e. tr/p' versus tr/p '. In crosses PTS-
1-7, -1-8, -1-9 pol-2 was again used as a female, but with males
other than those carrying the tr cytoplasm (Table 1 B). As above.
a contingency test was used to compare the frequency of cybrid
progeny in PTS-1-1 (Table 1A) with the pooled freuency of all
the crosses in Table 1 B.
The purpose of crosses tested in Table C was to compare the rate
of pollen transmission between a cross where it was first observed
(PTS-1-10) in previous experiments (Erickson and Kemble 1990)
and other crosses by using the same male as in PTS-1-10, but
varying the female genotype. PTS-1-10 served as a positive control
since failure to observe pollen transmission in the other crosses
could not be attributed to the male if cybrid progeny were observed
in the control cross.
Detection of paternal mitochondria
Microspore culture
Role of the male genotype
The results from cross PTS-1-1 (Table 1A) were similar
to those presented in a previous report (Erickson and
staining with 4',6-diamino-2-phenylindole (DAPI) (Coleman and
Goff 1985). The FCR assay as described by Heslop-Harrison et al.
(1984) was used to assess the viability of the sperm cells. The pres-
ence of a cell wall was tested for by calcofluor white and analine
blue staining (Tanaka 1988).
The weight of one anther was 0.4 mg30.03 (SE), and the mean
number of pollen grains/anther was determined to be 5853 +- 214
(SE), which was approximated to 6000. The number of sperm
cells present in the starting material was determined by multiplying
the number of milligrams of anthers by 3 s 10' (2.5 s 6000 s 2
sperm cells/pollen grain). After isolation 95-100% of the sperm
cells observed with phase contrast microscopy were FCR positive.
Therefore, the number of sperm cells counted in a hemocytometer
using phase contrast microscopy was considered to be indicative
of the number of viable sperm cells. The number of isolated sperm
cells from the pollen grains was estimated by dividing the number
of counted sperm cells by the calculated number of sperm cells
in the starting material times 100%.
pH and osmolality were varied to assess the optimal osmotic shock
conditions for sperm cell release from the pollen grain. The pH
of the isolation medium was varied in a range of 5.0 to 8.0 at
780 mosmollkg H4O. The buffer at pH 5.0 and 5.5 was 5 mM
MES (2(N-Morpholino) ethane sulphonic acid). For higher pH
values the buffer was 5 mM MOPS. For sperm cell isolation at
different pH values, the concentration of fetal calf serum in the
storage medium was 0.8%4 (v/v) instead of the optimal concentra-
tion of 6.5% (v/v) as described in the isolation procedure. The
osmolality of the isolation medium was varied with sucrose 10-
20%4 (w/v) from 360 mosmol to 780 mosmollkg H4O at pH 6.0.
The effects of pH and osmolality on sperm cell isolation were
measured by estimating the number of disrupted pollen grains in
relation to the number of intact viable pollen grains x 100% after
osmotic shock, and by determining the yield of viable sperm cells.
exemplified in Fig. 2 is applicable in
this case' and others'': soil moisture
(water-filled pore space) controls both
(1) the specific ratios of NO and N,O
fluxes and (2) the contributions from
nitrifying and denitrifying bacteria.
The findings of Davidson et al.%;
however, disagree with the results
from studies of other tropical soil
systemg'-' jn which the fluxes of
either NO, N4O or both were derived
exclusively from denitrificationn. While
these discrepancies have yet to be
clarified, one possible explanation
for these conflicting results may be
the difference in soil pH. The pH of
the soil studied by Davidson et a1,%1?
is 6.4-7.8, while that of soils in
Amazon rainforest and other ecosys-
tems north of Manaus, Brazi]1-
and in the Guayana Shield, Vene-
zuela'8 are 2.6-4.0 and 4.6-5.2
respectively. The source of N4O in
the acidic tropical soils may be more
clearly understood from results of
inhibition experiments with acety-
lene that are similar to those con-
ducted in Mexico'. On the basis of
the information from laboratory
studies'?, it was pointed out'% that
the production of N4O by chemo-
autotrophic nitrifiers may be insig-
nificant when the soil pH is below 6.
It would be worth examining closely
the lower limit of pH that supports
the activity of chemoautotrophic
nitrification in soil systems.
EcouoGICAL ASSESSMENT - predicting
or determining the effects of human
activities on natural populations - is
assumng increasing importance in
decision-making. Like other applied
sciences, it also has the potential to
contribute to the basic principles it
draws on, Some assessments can
be seen as quasi-experiments',
where ecological theories can be
tested and improved by comparing
predictions to results, with fewer
assumptions about extraneous vari-
ables than for observational studies,
though more than for experiments.
Indeed, assessments and deliberate
manipulations, on spatial scales too
large to allow the replication and
randomized assignment on which
many statistical analyses are based,
may provide information unobtain-
able otherwise'.
The value of the contributions, to
both decision-making and basic sci-
ence, depends on the reliability of
the determination of effects. Un-
fortunately, the most urgent and
dramatic assessment problems
often have the least reliable determi-
nations. They may be too wide-
spread to admit comparison or
'Control' areas (e.g. global warm-
ing), or too unexpected to allow
baseline ('Before') data to be gath-
ered (e.g. oil spills). Determining
what would have happened, had the
human activity not occurred, may
then require the modeling of pro-
cesses driven by large numbers of
variables (rmany of them unob-
served) con-nected by unknown
relationships, often nonlinear and
sometimes sensitive to small errors
in functional form or variable values.
Greater scientific benefits may
arise from assessments of small-
scale planned activities, which can
avoid these problems. The existence
of 'Before' and 'Control' data may
permit the use of simple models
whose errors are small compared to
the effects of concern. Such tasks
arise in the work of public agencies
granting permits or enforcing regu-
lations.
In practice, the scientific value of
these assessments is often low, in
part because the applicants hire the
investigators. But alternative struc-
tures are possible. The California
Coastal Commission (CCC), in allow-
ing Southern California Edison
(SCE) to expand its San Onofre Nu-
It is becoming well established
that there has been a progressive
decline in mean global oceanic tem-
perature over the last 100 million
years'% As continents assumed
their present day positions there
was a fundamental rearrangement
of oceanic circulation systems and
a dramatic steepening of polar-
equatorial temperature gradients.
Latitudinal climatic zones and their
attendant biotic provinces are as
ddistinct today as they have ever
been'. With the progressive reduc-
tion in the area occupied by tropi-
cal biotas, there was a concomitant
increase in their polar counter-
parts. Many marine and terrestrial
clades diversified through the
Cenozoic within these newly devel-
oped climatic zones and it is usual-
ly assumed that they did so in
comparative isolation. Interchange
of taxa between high- and low-
latitude regions would appear to
have been very much more difficult
through the Cenozoic era'.
Nevertheless, the overall tempera-
ture decline through the Cenozoic
was not a simple, smooth one. In a
discussion of climatic changes and
their likely biotic effects in the high
southerm latitudes, Clarke and Crame'
identified a sequence of ten major
cooling and warming trendst* (Fig.
1). In addition to these longer-term
trends, which occurred on a time-
scale of 10%-10' years, we now know
that there were also smaller-scale
ones occurring on a timescale of
10'-10' years. Such individual
climatic cycles can be linked to
orbital (Milankovitch) cyclicity, and
in the last 2.4 million years alone
there may have been as many as 50
complete cycles'''. Evidence from
the deep-sea record suggests that
high-frequency Milankovitch-type
cycles are detectable in the latest
Miocene-earliest Pliocene, middle
Miocene and earliest Oligocene
epochs'. They are represented
within Mesozoic sequences too'',
and curves such as that depicted in
Fig. 1 would almost certainly have a
sawtooth aspect at a finer scale.
Evidence is accumulating that
there may have been times of sig-
nificant biotic interchange between
the high and low latitudes over
approximately the last 100 million
years. The taxonomic composition
of certain Cenozoic and living
marine invertebrate assemblages
rmay hold some important clues
here, with those of the highest lati-
tude regions being particularly rel-
evant. Possible longer-term effects
of repeated latitudinal range ex-
pansions and contractions should
also be assessed.
A widely held view of the living
Southern Ocean benthic fauna is
that it is the product of evolution in
isolation over long periods (often
held to be the greater part of the
Cenozoic eray'M This view is
within the deep-sea record of the
North Atlantic Ocean. Morozovellid
and globigerinid foraminifera
define at least nine acme events
within the Paleogene sub-era, and
both these and other calcareous
nanoplankton taxa Suggest that
there were at least eight further
events in the succeeding Miocene
epoch'. Substantial latitudinal
shifts of both coccolithophorids and
foraminifera have been tracked
over the last 225 000 years'*.
Although the true nature and scale
of biotic interchanges between the
high and low latitudes will only
become apparent when further tax-
onomic and phylogenetic studies
have been completed, it is import-
ant to emphasize how frequent
they may have been. As our knowl-
edge of high-latitude palaeocli-
mates has increased, it has become
possible to estimate meridional
temperature gradients through
time'P8 (Box 3). Unfortunately,
these estimates are necessarily both
incomplete and open to interpreta-
tion; this is especially so after the
establishment of a major East
Antarctic ice sheet iup to 36 million
years ago), when the volume of iso-
topically light water locked up in
the ice cap affects palaeotempera-
ture calculations. Nevertheless.
conservative estimates Suggest
that, for perhaps as much as 96% of
Cenozoic time (65-2.6 million years
ago), polar-equatorial surface water
temperature gradients were less
than they are today; indeed, for
perhaps as much as 75% of the
Cenozoic (65-16 million years ago),
we can regard them as having been
substantially less. The sharply
diefined climatic zOnes we see
today are atypical of the past.
Of course, we would not necess-
arily expect the faunal response to
marine climatic change to be on a
simple latitudinal basis. Because of
the predominant gyral circulation
patterns in the world's oceans, it is
likely that the interchange of many
shallow-water taxa would have
been concentrated along the west
coasts of major north-south trend-
ing continents. In particular, the
extension of eastern boundary cur-
rents and increased upwelling in
these regions during a global cool
phase would have facilitated
breaching of the tropics. Further
controls on gyral circulation by tec-
tonic events at key portals iBering
Strait. isthmus of Panama. etc.)
have been discussed elsewhere'''
The possible effects of repeated
range expansions and contractions
in the shallow marine realm can be
gauged by reference to hypotheti-
cal stenothermal taxa (Fig. 2i. Both
stenothermal-warm (Fig. 2a) and
stenothermal-cool (Fig. 2b) taxa
could undergo significant range
shifts during the course of a full cll-
matic cycle. Such fiuctuations may
not in themselves be of too great a
significance, until it is remembered
that there were at least ten major
cooling and warming trends
through the Cenozoic and super-
imposed on these was a whole
series of smaller-scale events. In
the last 2.4 million years ago there
may have been as many as 50 cli-
matic cycles. In response to these
climatic changes certain taxa may
have undergone concertina-like
range shifts over long periods, and
this in turn could have played a
major role in determining regional
patterns of taxonomic diversity.
Repeated latitudinal range shifts
are the basis of so-called taxonomic
diversity pumps''' If we consider
equatorial regions first, it is likely
that two main types of processes
were in operation here. First,
repeated expansions and contrac-
tions served to stimulate speci-
ation by disruption of distributions
into allopatric fragments, followed
by renewed contact during the next
phase of the cycle'''. Secondly,
and on a somewhat longer time-
scale, certain taxa retracted into
broad-scale centres of endemism
from which they subsequently did
not re-expand. ft is possible to
regard both the Indo-West Pacific
and Eastern Pacific high diversity
foci as major Cenozoic refugia''.
Following similar lines of reason-
ing, it is possible to suggest that
repeated latitudinal range shifts
served to increase taxonomic
diversity in high-latitude regions
too. However, on this occasion the
result is intuitively less satisfying
as it seems to run contrary to the
perceived nature of taxonomic
diversity gradients. Nevertheless.
our perspective on global biodiver-
Sity patterns may now need to be
changed, particularly for the south-
ern hemisphere. In a recent review
of diversity patterns, Platnick'' has
pointed to two major biases, one
boreal and the other megafaunal.
that have persistently coloured our
perceptions. Cursory surveys of ter-
restrial groups such as arthropods
(and especially spiders) and
flowering plants suggest strongly
that there may be no simple
reduction in the number of taxa
between the southern high and low
latitudes, and the same is also trnue of
lLate Cretaceous iapproximately 83-65 million
years ago)' during this phase of global cool-
ing there were probably significant tempera-
ture contrasts between the sea surface tem-
peratures (SSTs) of high and iow latitudes;
estimates of 25-27C obtained for equatorial
Pacific and 12-14'C for Antarctica; palaeonto-
logical data certainly confirm warmer poles
but true nature of tropics less certain.
Eariest Canozoic lesrty Paleocene) (6S-60 million
years ago): continued phase of global cooling
but stable isotope data now suggests greatly
reduced meridional temperature gradients;
tropical SSTe may have been in 12-15?C
range and the difference between the equatorial
regions and poles as iow as 3-5%C; however,
calculated values could have been affected by
anomalous hightropical surface water salinities.
lLate Paleocene-lete Eocene (approxirmately
59-43 million years ago). widespread marine
and terrestrial evidence for sustained period
of global warmth and low polar-equatorial
temperature gradients; balance of evidence
suggests Eocene tropical SSTs some 4-7%C
below present values.
lLste Eocene-early Miocene (43-23 million
years ago): stable isotope evidence for lower
tropical SSTs and reduced meridional tem-
perature gradients is again backed by the
narine and terrestrial fossil records.
Early-middle Miocene (23-17 million years
agO); a conservative estimate for the early-
middle Miocene warming in both tropical and
subpolar regions is in the order of 3'C.
Middie-lste Miocene (approximately 16-10
miliion years ago): a combination of tectonic
and oceanographic events (compietion of
physical isolation of Antarctica, expansion of
the East Antarctic ice cap, constrictionn of the
Indonesian Seaway, etc.) lead to major cli-
matic changes; fundamental shift from pre-
dominantly equatorial to strongly meridional
oceanic circulation patterns; a drop of some
4-5*C in polar SSTs was not matched in the
tropics, and it has been estimated that during
the Miocene the meridionaf SST gradient in
the southern hemisphere doubled from
6-12C; middle and late Miocene marine fau-
nas show a marked increase in provincialism;
nevertheless it should be emphasised that,
even at the end of the Miocene, latitudinal
temperature gradients were only in the region
of three-quarters of their present day values.
Late Miocene-early Pliocene (approximately
10-3.5 million years ago): gradients main-
tained their late Miocene values.
lLate Pliocene-&ecent (from approximately
2.6 million years ago): available evidence sug-
gests gradients did not assume their present
form until the onset of bipolar glaciation at 2.6
million years ago. Data fron Refs 2, 6. 7 and 29.
other adaptations. Hence, there is
no more reason to believe that the
brain is a tabula rasa than to
believe that the stomach is a gener-
al digester designed to track the
foods an organism may encounter.
In its pure form, DA focuses on
differences in LRS between individ-
uals encountering different en-
vironments, and uses the methods
of behavioural ecology to study
these differences. EP, in its purest
form, uses the methods of evol-
utionary biology and experimental
psychology to study the naturally
selected design of psychological
mechanisms. Consider how these
two types of researcher might
approach testing the Trivers-
Wiliard'' hypothesis about the allo-
cation of parental investment to
male and female progeny.
Trivers and Willard argued that if
(1) variance of male LRS exceeded
that of female LRS, (2) the relative
health and dominance of mothers is
passed on to their progeny, and (3)
healthy or dominant males obtain
more matings than males lacking
these attributes, then (4) females
will be selected to allocate invest-
ment in progeny as a function of
their health or dominance. Clutton-
Brock et al.''; in a comprehensive
study of red deer (Cervus elaphus).
found considerable support for the
hypothesis. Sons born to mothers
above median rank were more
reproductively successful than their
daughters, while daughters born to
subordinate mothers were more
reproductively successful than their
sons. Moreover, the ratio of sons to
daughters produced by dominant
mothers was higher than for subor-
dinate mothers. Because the sex
ratio and reproductive success
were key dependent variables in
this study, it is similar to some
studies of sex allocation done by
DAs and described by Sief'*
An evolutionary psychologist
attempting to test the Trivers-
Willard hypothesis would first con-
struct a selection model relating
sexual dimorphism in variance in
reproductive success in males and
females and health or status of
mother to the benefits of differen-
tial investment in sons and daugh-
ters''. Varying the parameters of
the model would provide a des-
cription of how sex allocation might
have been selected for in a particu-
lar species. The model would be
used in conjunction with infor-
mation about the natural history of
the species to explore the param-
eter space of the independent vari-
ables to determine whether a 'win-
dow' of opportunity could have
existed for the evolution of the
putative adaptation. If the results
of the modelling suggested that the
evolution of the adaptation is
plausible, a theory of the nature of
the adaptation, specified in terms
of decision rules assumed to be
instantiated in neural hardware,
would be formulated. The depen-
dent variables would be outputs
from the decision process affecting
nursing time, amount of protection
from predators, etc., given to sons
and daughters, rather than fitness
measures or behaviours assumed
to enhance fitness. Attitudes, val-
ues, intentions and motives would
be measured in human studies. A
decision rule might be something
like: 'If subordinate and physically
weak, be more responsive to the
needs of daughters than of sons:
but if strong and dominant be
more attentive to the needs of
sons than of daughters'. It would be
necessary to formulate a theory of
the relation between ancestral and
Such a theory requires a model
of how the crucial independent
variables, which are measures of
adaptation-relevant external and
internal environmental variables,
are represented to the ancestral
adaptation. Dominance, for exam-
ple, might have been represented
in terms of posture, frequency of
unreciprocated threat displays, or
resources held by different ances-
tral individuals. Once the decision
rules that describe the adaptation
increased with light intensity even as leaves lost turgor
and wilted. Similarly, 8. increased in response to HL
intensity, while C, decreased slightly. Drawdown of C,
with increased light intensity is regularly observed in me-
sophytes growing under benign conditions (e.g., Kiüppers,
1984; Chazdon and Pearcy, 1986; Kirschbaum and Pear-
cy, 1988). Drawdown in C, occurs with increased light
because photosynthetic demand for CO, is increased, but
leaf architecture and stomatal aperture nonetheless pre-
senta partial barrier to CO, diffusion (Farquhar and Shar-
key, 1982). Our hypothesis, which held that stomatal clo-
sure would markedly reduce C;, was not supported.
These findings contrast those found for other forest
herbs growing in HL environments, Knapp, Smith, and
Young (1989) and Young and Smith (1979) observed
reductions in P,,,, and g. with reductions in , in the
subalpine understory herb Arnica latifolia exposed to pe-
riods of HL. Likewise, Schulz (1991) documents steep
reductions in both 9, and g. in Aster macrophyllus under
a similar light regime. These studies differ from the work
described here because they concerned the exposure of
entire plants (or possibly of ramets) to high radiation
loads. In this study, the exterior canopy leaves intercepted
most of the radiation load and subsequently wilted.
Photosynthetic rates declined over several hours at HIL.
However, the magnitude of this decline was small (ca.
10%) relative to the much greater limitation on photo-
synthesis imposed by light in the course of a typical day.
Throughout the day, 8. patterns changed in the same
direction as P,,,,, but were proportionally larger. Altera-
tions in 8. under HL resulted in small changes in C,.
Morning to afternoon comparisons of gas exchange under
shade light intensities showed a comparable pattern. Al-
though P,,,, and g. values were smaller for the late after-
noon than for the morning, C, values differed little. Stable
C, rules out stomatal closure a mechanism for reductions
in P,g,. Afternoon depressions in P,,,, may reflect the in-
fluence of carbohydrate feedback inhibition on photo-
synthesis (e.g., Küppers et al., 1986; Rao and Terry, 1989;
Sharkey and Vanderveer, 1989; Goldschmidt and Huber,
1992), or overriding endogenous rhythms in photosyn-
thetic capacity (e.g., Pallas, Samish, and Wilmer, 1974;
between 2,300 and 3,100 m in parts of two watersheds
that cover about 20 km.
Fargesia robusta- Fargesia robusta stands cover about
40% of the land surface below 2,600 m in the study area.
Culms of F. robusta are 2.5-3.0 m tall, unbranched, and
emerge from a densely packed (pachymorph) rhizome
system (Fig. 2). Shoots (culms -1 yr) are produced each
year between April and May and grow to full height by
mid-June (Fig. 3). Shoots have first order branches each
with a few leaves at the end of each branch at the end of
the firstgrowing season. Second order branches and leaves
are produced in subsequent years and leaf biomass ap-
pears to remain constant after the third year (Taylor and
Oin, 1987). The oldest culm we aged from counts of
branch internodes (which reflects culm age, unpublished
data) was 12 yr old.
Plots were established throughout the elevational range
of Fargesia robusta to include observed variation in stand
density and culm size. Plot 1 (80 m%) was placed in a
sparse stand with tall thick culms. Plot 2 (25 m) was
placed in a stand with four small densely packed clumps
with short thin culms. Finally, 35 2-m plots (plots 3-37)
were established systematically by elevation between 2,350
m and 2,600 m.
Bashania fangiana-- Bashania fangiana forms a nearly
continuous understory in the subalpine conifer forests in
Wolong at elevations between 2,700 and 3,400 m. Culm
density and size vary with forest canopy composition and
density, elevation, and slope aspect (Reid et al., 1991).
Average culm density is about 70 m %, and average culm
height is 1.5-2.0 m.
Bashania fanglana produces shoots annually between
June and August from clumps of culms along a spreading
(amphimorph) rhizome system (Fig. 4). Culms reach their
full height during this 3-mo period. A few leaves and
branches are present on the uppermost nodes of the shoot
by the end of the first growing season. Branches and leaves
are produced on most of the upper nodes in the second
and third growing season, and biomass appears to remain
organs within each of the androecial whorls become equal-
ized; however, size distinction is apparent between the
two whorls of stamens. The floral whorls of Pisum sativum
(Tucker, 1989b) also showed this pattern of growth.
Nectary/stigma-The discoid-type nectary found in
soybeans is closely associated with the stamens (Waddle
and Lersten, 1974; Carlson and Lersten, 1987). From Fig.
53, it is evident that the nectary is fused to the inner wall
of the staminal column. The stigma possess two types of
papillae: short 'main body' papillae and several rows of
elongated 'lower whorl' papillae around the rim (Tilton
et al., 1984).
Carpel position-Carpels originate either as lateral or-
gans on the floral meristem, or in a few species a single
carpel is formed occupying an apparently terminal po-
sition on the meristem (Cutter, 1971). As with Pisum
sativum (Tucker, 1989b), Caesalpinia cassioides, C. pul-
cherrima, C. vesicaria (Tucker, Stein, and Derstine, 1985),
Ateleia herbert-smithii (Tucker, 1990), Neptunia pubes-
cens (Tucker, 1988a), and Acacia baileyana (Derstine and
Tucker, 1991), the carpel in normal soybean flowers is
initiated as a central mound and the cleft forms later;
thus the apical meristem is totally utilized and no apical
residuum persists in the region of the cleft.
Floral aberrancies- Loss of organs-Generally when
floral organs are lost, the missing organs will be either an
entire whorl (often the last initiated) or the last initiated
members within a whorl (Tucker, 1988b). This concept
held true for soybean flowers. When sepals, petals, or
stamens were missing, usually the missing organ was the
last organ to be initiated in that particular whorl. The
only whorl found to be completely lacking in some flowers
was the inner stamen whorl, ie., the last whorl to be
initiated.
Extramerosity-At 18/14 C, extramerous soybean
flowers were much more common than flowers that lacked
organs. The extra organs appeared to originate from with-
in their respective whorls. The only cases of heterotopy
were found in the calyx with the convergence of bracteoles
into the sepal whorl. Extra organs were not necessarily
highly numerous pollen grains per flower (Fig. 2). There
were also strong negative correlations between mean pol-
len production per flower and mean flower diameter among
individuals in both P. scotica and P. stricta (Fig. 3). Genets
with large flowers produced significantly less pollen per
flower than those with small flowers in both species.
In P. stricta, there was a negative correlation between
mean ovule number per flower and mean flower diameter;
individuals with large flowers produced fewer ovules per
flower than individuals with small flowers (Fig. 4). Within
P. scotica, individuals that produced many ovules per
flower produced relatively small pollen grains (Fig. 5). Of
the 90 correlation coefficients estimated, the only signif-
icant positive correlation we detected among genets was
within P. stricta; individuals that produced many ovules
per flower also produced large numbers of pollen grains
(Fig. 6). The fact that five of the six significant phenotypic
correlations within species were negative suggests that we
were not detecting strong environmentally induced cor-
relations between characters (which would tend to be pos-
itive).
The two significant correlations detected among species
means (using the mean of the homostylous morphs for
P. farinosa) were also negative (Fig. 7). Among species
means, ovule number vs. flower diameter, and the modal
pollen grain volume vs. pollen production per flower were
inversely related. Since these correlations also appeared
within P. stricta (ovule number vs. flower diameter) and
P. farinosa (pollen volume vs. pollen number), they sug-
gest that there may be intrinsic genetic constraints on the
independent evolution of these characters.
Section ffymenasplenium is one of the best
defined groups within Asplenium, distinguished by
the following synapomorphies: creeping rhizomes,
dorsiventrally symmetrical steles, swollen petiole
bases, unique rachis-costae structure and chro-
mosome base numbers of x = 38 or 39. All other
Asplenium species have erect or ascending rhi-
zomes, radially symmetrical steles, nonswollen pet-
iole bases, and n = 36 or multiples thereof (rare
exceptions differ in only one of these character-
istics).
Hymenasplenium was first described by Hayata
had cristate spores and would therefore belong to
this second group. Because most species of As-
plenium outside of section ffymenasplenium have
cristate spores (Tryon & Lugardon, 1991), the
spiny and papillate character states should be con-
sidered apomorphic.
The neotropical species of section Hymenas-
plenium are all endemic to the Neotropics. They
range from southern Mexico to Panama, the An-
tilles, and South America from Venezuela to south-
eastern Brazil, forming a wide arc around most of
Amazonian Brasil (Map 1).
The Andes from Venezuela to Bolivia harbor the
most species (8) and this is the only region with
endemics (i,e., Asplenium ortegae, A. repandu-
lum, A. volubile). Costa Rica and Panama are also
species-rich, containing six species. The Antilles
and extreme western Amazonian Brazil both have
two species, and the Guianas, southeastern Brazil,
and Paraguay all have one species. The Serra do
Mar region of southeastern Brazil, which is a center
of species richness and endemism for ferns (Tryon,
1972), has played a minor role in the diversification
of the section, Only one species (A. triguetrum)
occurs there and it is nearly endemic (Map 9).
Asplenium obtusifolium is notable for its nearly
circum-Caribbean distribution (Map 5) and its cor-
relation with geography of the 32- and 64-spored
Our recognition of 10 neotropical species of
Asplenium sect. Hymenasplenium is based pri-
marily upon qualitative characters as shown in the
key. In order to check the validity of our species
circumscriptions, we decided to do a Principal Com-
however, can be distinguished from A. triquetrum
by petiole length relative to the lamina, habitat,
and range (see key, couplet 4), and from A. vol-
ubile by the carinate rachis and rachidial wings
parallel to the plane of the lamina. These char-
acteristics were not included in the measurements
used for the PCA. Another result was that the two
spore-races of A. obtusifolium could not be sep-
arated by the morphological characteristics mea-
sured (Fig. 5).
We are not sure what group of species within
Asplenium is most closely related to section Hfy-
menasplenium. Several species of Asplenium (A.
abscissum Willd., A. argentinum Hieron., A. host-
mannii Hieron., and A, otites Link) greatly resem-
ble certain species in section Hfymenasplenium in
leaf form, Consequently, they are often misiden-
tified as a species in section Hymenasplenium,
especially A. laetum (which see for comparison).
These species, which can be immediately distin-
guished from section Ifymenasplenium by their
erect rhizomes and rachis-costa architecture, may
be the closest group in Asplenium related to section
Hymenasplenium. This suggestion is based only
on similarities in leaf form and is therefore tenta-
tive,
Asplenium sect. Hymenasplenium (Hayata)
Iwatsuki, Acta Phytotax. Geobot, 27: 44.
1975. Basionym: Hymenasplenium Hayata,
Bo. Mag. (Tokyo) 41: 712. 1927. rYrE: As-
plenium unilaterale Lam.
Plants terrestrial, epipetric, or epiphytic; rhi-
zome creeping, green to blackish, with two rows
of alternately arranged petioles on the dorsal sur-
face, scaly near the apex; rhizome stele dorsiven-
tral, composed of two unequal meristeles connected
by lateral strands, the ventral meristele wider and
the dorsal one narrower, bearing roots from either
the ventral, dorsal, or connecting meristeles; petiole
terete, scaly at base, glabrous distally, greenish to
atropurpureous, the base swollen and often per-
sisting after the leaf has fallen and decayed; lamina
usually l-pinnate or (in A. cardiophyllum) simple
and cordate; rachis not or very shallowly grooved,
with or without perpendicular or flat green wings,
lacking buds; costae bordered by a flange of green
sification, and consequently requires a larger input
of information. In the case of Pleurothyrium, suf-
ficient information is available for taxonomic re-
vision, but not enough for a phylogenetic classifi-
cation, Obtaining sufficient information for a proper
phylogenetic analysis is a project unto itself and
making a phylogenetic analysis cannot be merely
tacked on to making a taxonomic revision.
Relationships between Pleuroth yrium species as
expressed in the cladogram and as based on mor-
phological similarities show some congruence. For
instance, I consider P. racemosum, P. tomiwahlii,
and P. pilosum as closely related, a relationship
expressed in all cladograms inspected. Likewise,
the species with erect tepals, usually with inrolled
margin of the tepals, are closely related, as shown
Arbor, 20-25 m alta. Ramuli solidi, teretes vel paullo
angulati, rufo-tomentosi, Gemma terminalis ad T mm cras-
sa, rufo-tomentosa. Folia alterna, subcoriacea, 15-3(0 x
8-12 cm, oblonga vel oblongo-elliptica, basi rotundata,
apice paullo acuta, supra glabra, subtus rufo-tomentosa,
nervis lateralibus 14-20 utroque costae latere, prope
marginem sursum curvantibus, vena marginale in dimidio
distale praesente, venatione supra immersa, subtus costa
nervisque lateralibus elevatis, venatione tertia paullo ele-
vata. Petioli rufo-tomentosi, valde canaliculati, 10-15 mm
longi. Inflorescentiae ex axillis bractearum ortae, rufo-
tomentosae, 10-15 cm longae, paniculatae, ramulis vulgo
duplo cymae more ramosis; bracteis sub anthesi praesen-
tibus, rufo-tomentosis, eis ad ramulorum inferiorum basim
ovatis, 12-15 mm longis, eis ad cymarum terminalium
basim ca. S mm longis. Pedicelli florum apertorum ca. 8
mm longi, Flores cremei, saltem 13 mm diametro. Tepala
O, subaequalia, late ovata, ca. S mm longa, intus tomen-
tella. Stamina 9, glabra, 4-locellata, locellis lateralibus,
glandulis permagnis, coalitis, stamina cingentibus. Ovar-
ium ellipsoideum, ca. 1.3 mm longum, basi glabrum, parte
media tomentellum, parte superiore tomentellum O macu-
lis nudis praeditum; stylum glabrum, ca. 1 mm longum.
Tubus floralis intus tomentellus. Fructus ignotus.
Tree, 20-25 m tall. Twigs solid, terete or slightly
ridged, reddish-brown-tomentose, 5-6 mm diam.
5 cm below the tip. Terminal buds to 7 mm thick,
rufous-tomentose. lLeaves alternate, subcoriaceous,
15-30 x 8-12 cm, oblong to oblong-elliptic, the
base rounded, the tip slightly acute, glabrous above,
rufous-tomentose below, venation immersed on up-
per surface, midrib and lateral veins raised on lower
surface, the tertiary venation slightly raised; lateral
veins 14-20 on each side, curving upward near
the margin and united with the superior vein, form-
ing a margnal vein in the upper half of the lamina.
of stamens greatly enlarged, surrounding the sta-
mens, fused. Ovary and style densely brown-pa-
pillose, floral tube brown-papillose inside; ovary
globose, ca, 0.7 mm long, the style distinct, ca.
O.5 mm long. Gupule of young fruit cup-shaped,
ca. 2 cm wide and 1 cm tall; young fruit ellipsoid,
ca, 1.5 cm long. Fruits: July-October. Flowers:
June-July.
Collections studied. EcUADOR. NAPO: Aguarico, Re-
serva Faunistica Cuyabeno, Palacios 7667 (MO). PERU.
LORETO: Quebrada Sucursari, tributary of Rio Napo, Gen-
try 54300 (MO) Pebas on the Amazon River, illiams
1766 (B). Maynas, iquitos, Asociacion Agraria Paujil,
Fdsguez 10877 (MO) Maynas, Explornapo Camp, Rio
Sucursari, Vdsguez 8119 (MO), Vdsquez 13078 (MO).
Pleurothyrium williamsii is only known from
the type collection, four collections, all from Peru
in the area north of the Rio Napo-Rio Amazonas,
and one collection from Ecuador. Unfortunately,
the holotype, which was requested from the Field
Museum, disappeared, together with other Pleu-
rothyrium types, while being sent to St. Louis Of
the type collection, only a duplicate in B (with
buds) and some inflorescence fragments in G (with
a few flowers) exist. The recent collections are a
good match as far as floral characters and leaf
shape are concerned, but there are some differ-
ences. The B specimen has alternate leaves, while
the recent specimens and the photo of the holotype
(in NY and F) show clustered leaves. The photo-
types have the inflorescences alternate along a
leafless twig, the recent specimens have the inflo-
rescences near the tip of the stem, while the B
specimen has a detached inflorescence. The B spec-
imen is also more tomentulose. However, the sim-
ilarities in leaf shape and flowers outweigh these
differences, and I do not hesitate to assign the two
recent collections to P. williamsii.
A close relative of this species is Pleuroth yrium
panurense (Meissner) Mez, a species with elliptic-
obovate leaves, an obtuse leaf base and similar
flowers. It differs, however, in its smaller leaves,
the whitish indument on the lower leaf surface, its
glabrous ovary, and longer petioles. Pleurothyrium
insigne differs from P. williamsii in its larger
leaves, the leaves not so gradually narrowed toward
the base, erect indument on lower leai surface, and
its larger flowers. All three species occur in Am-
azonian Peru and/or adjacent Brazil. Another close
relative is P. maximum, which see for further
discussion.
Among the collections were found a number of
specimens that do not belong to any of the treated
species and which very likely represent undescribed
species. Because these specimens are incomplete
(sterile or fruiting). they are not formally described,
but only listed below so as to call attention to their
existence. I hope that in the near future material
adequate for their description will become avail-
able.
G. Proctor Cooper 539, Panama, Bocas del Toro,
region of Almirante (F, NY, US).
A fruiting collection with large (to 35 cm), el-
liptic to elliptic-oblong, acuminate leaves. Leaves
are glabrous below and have a strongly developed
marginal vein; the twigs are solid, glabrous or near-
ly so. A distinct species, included in Burger & van
der Wer8 (1990) as Pleurothyrium sp. A.
Gentry 57004, Colombia, Valle, Bajo Calima (MO).
A sterile specimen with gigantic leaves, accord-
ing to the label ca. 1 m long. The specimen has
leaves 60-70 cm long and 30-35 cm wide, densely
rusty-tomentose below.
Monsalve 1651, Colombia, Valle, Bajo Calima
(MO).
A species with clustered leaves, dark ferrugi-
nous-tomentose below, ca, 15 x 6 cm. Young
inflorescences and infructescences are very short,
ca, 1 cm long, and carry only one bud or fruit.
There are five collections of this species, but none
with flowers.
Löpez & H. Triana 24, Colombia, Mntioquia, Par-
que Nacional de las Orquideas (MO).
Characterized by its obovate to obovate-elliptic
leaves, glabrous below, with 15-20 pairs of lateral
veins and an obtuse to rounded leaf base. The
young cupules are covered with many small len-
ticels.
Fasquez 3220, Peru, Loreto, Requena (MO).
Leaves glabrous, whorled, narrowly oblong with
abruptly rounded base. Related to P. williamsii,
but with narrower, oblong leaves and much smaller
inflorescences.
and subgenus Uichanthelium from most of the
other C, subgenera of Panicum. A detailed de-
scription of section Dichanthelium based on the
studied species is presented below.
Outline: expanded, either flat or very broadly
V-shaped; arms of lamina either straight or out-
wardly bowed; two halves of lamina symmetrical
about the median vascular bundle; leaf width vari-
able and leaf blade section includes between 17
and 117 vbs; P. aciculare (Fig. 1) is an exception
with narrow (only ll vbs), inrolled leaf blades. Ribs
and furrows: variable, from flat adaxial surfaces
without ribs or furrows to medium furrows (about
a quarter of the leaf thickness); furrows wide and
open, occurring between all vbs; adaxial ribs, when
present, located over the vbs, with rounded apices,
and all are structurally uniform; abaxial ribs usually
absent but slight ribbing may be developed; in P.
aciculare abaxial ribs are clearly present (Fig. 1B).
Midrib: variable, from undifferentiated median vb
to definite keel; median vbs, structurally indistin-
er palea lanceolate, (1.5-)2.1 mm long, 0.6 mm
wide, membranous, the margins ciliate; lower flow-
er absent or present, male when present, with 3
anthers each l mm long. Upper anthecium ellip-
soid, 1.9-2.5 mm long, 0.9-1.2 mm wide, indu-
rate, smooth, the apex of the lemma apiculate,
scaberulous; stamens 3, the anthers 0.3-1 mm
long. Caryopsis ellipsoid, 1.7 mm long. 1 mm wide;
hilum punctiform, embryo V the length of the
caryopsis.
Distribution and ecology: Brazil, on mountain
slopes from Espirito Santo to Rio Grande do Sul,
900-2,650 m.
MAAdditional specimens examined. BRAzIL, ESPIRITO
SANTO: Serra do Caparao, rocky open campo, 2,650 m,
Mexia 4014 (NY, P, US); Serra do Capara6, 2,280-
2,400 m, Chase 10084 (IAN, RB, US). MINAS GErAS:
Barbacena, Serra Mantiqueira, Chase 8667 (F, US); Ouro
Preto, Villa Rica, 1,100 m, Chase 9350 (F, GH, MO,
NY, US); cerrado on middle slopes of Pico de Itacolumi,
ca. 3 km S of Ouro Preto, 1,750 m, Irwin et al. 29483
(MO, UB); Mun, Itamonte, Parque Nacional de Itatiaia,
camino para las Agulhas Negras, 1,550- 1,800 m, Zu-
loaga et al. 2374' (MO, RB, SI, US). rArANA: 4 km E
of Guarapuava along highway BR.277 to Curitiba, 1,050
m, Davidse et al. 11319(MO); Bocaiuva do Sul, Clayton
4285 (K). RIo DE 1AnEIRO: Alto de Itatiaia, 2,200-2,400
m, Chase 8299 (NY, US, W) Teresöpolis, Posse, morro
das Antenas de Televisao, Sucre 2317 (8I) Tijuca, in
open spot at summit, Chase 12161 (US). RIo GRANDE D0
suL: Cambarä do Sul-Itaimbezinho-Bela Vista, alls et al.
1870, 2903 (CEN); Serra da Rocinha, prope Bom Jesus,
in dumetosis, Rambo 53841 (US). sANTA CATARINA: Mun.
Cagador, 9 km W of Cagador, Smith & Klein 10899
(NY, US) Mun. Bom Jardim da Serra, Alto, 20 km S of
Bom Jardim, Smith & Klein 15809 (HB, NY, P, US).
5 km S of Ponte Alta along highway BR-116 to Lajes,
Davidse et al. 11104 (MO, SP) Ponte Serrado, 94 km
W of Joagaba, 700-900 m, Smith & Klein 14008 (HB,
K, SI); Mun. Urubici, 19 km N of Perico, Smith & Klein
15889 (HB, US): Mun. Lajes, Serra do Ilheos, Smith d:
Klein 15456 (K, US). sAo rAULO: Jardin Botänico e
Farque do Estado, Sendulsky 1063 (SI); Campos do
Jordäo, Serra Mantiqueira, sandy campo, 1,600 m, Chase
9s22 (NY, US).
Related to Panicum sabulorum and P. stig-
mosum, P. superatum can be distinguished by its
contracted panicles and appressed spikelets.
There are two indurate florets in Chase 9403.
Sendulsky 1063 has anthers only 0.3 mm long.
34. Panicum surrectum Chase ex Zuloaga &
Morrone, Novon 1: 111. 1991. TYPE: Brazil.
Minas Gerais: Barbacena, long and tangled in
moist brushy base of higher slope, Chase 8664
(holotype, US: isotypes, F, NY). Figures 24D,
30.
Short-rhizomatous perennial. Culms decumbent
to geniculate, then erect, branching at the upper
nodes, scandent, 45-120 cm tall; internodes com-
pressed or cylindric, 7-13 cm long, glabrous; nodes
compressed, glabrous, brownish. Sheaths 4-10.5
cm long, shorter than the internodes, glabrous to
papillose-pilose toward the base, shiny, one margin
ciliate, the other one ciliate toward the base, oth-
erwise glabrous. Ligules 0.2 mm long, membra-
nous-ciliate; collar pilose. Blades linear-lanceolate,
5-12 cm long, 0.6-1 cm wide, glabrous to short-
hispid, attenuate at the base and apex, the margins
scabrous, long-ciliate toward the base; midnerve
conspicuous. Inflorescences terminal, exserted;
panicles lax, diffuse, 3.5- 15 cm long, 3-10 cm
wide; main axis glandular or eglandular, flexuous,
glabrous, the pulvini glabrous; first-order branches
ascendent, whorled toward the base, then subop-
posite or alternate, the axis of the branches gla-
brous, flexuous, glandular or eglandular; pedicels
triquetrous, glabrous, glandular or eglandular.
Spikelets narrowly ellipsoid, 1.8-2.2 mm long, 0.8
mm wide, glabrous, greenish, nonstipitate, the up-
per glume and lower lemma subequal, the nerves
manifest. Lower glume ovate, 0.9-1.3 mm long,
5-(-'4) the length of the spikelet, not embracing
the upper glume, (1-)3-nerved, the lateral nerves
inconspicuous. Upper glume l.6-2 mm long, not
covering the apex of the upper anthecium, 9-nerved.
Lower lemma glumiform, 1.8-2.1 mm long,
9-nerved. Lower palea lanceolate, 1.5-1.8 mm
long, 0.4 mm wide, shortly pilose near the apex,
otherwise glabrous, hyaline; lower flower male or
sterile, stamens 3, the anthers l mm long. Upper
anthecium ellipsoid, 1.6- 1.8 mm long, 0.7 mm
wide, pale, indurate, papillose, the apex of the
lemma shortly crested and pilose; stamens 3, the
anthers 1.2 mm long, purplish. Caryopsis ovoid,
1.3 mm long, 0.7 mm wide; hilum punctiform,
embryo less than %ä the length of the caryopsis,
Distribution and ecology: Brazil, occasionally
present in Paraguay, found at forest edges, between
600 and 1,800 m.
morphism before or during the earliest stages of
the first foliation-producing deformation event.
Sheets of predominantly megacrystic granitoid
make up to 80% of the Proterozoic cordierite-
bearing, high-grade (low-pressure granulite fa-
cies), LPHT rocks of the SE Anmatjira Range,
Arunta Block, central Australia (Clarke et al.,
1990; Collins et al., 1991). The regional metamor-
phic zones form a broad aureole around these
sheets (Collins and Vernon, 1991), suggesting that
the main cause of metamorphism was intrusion of
granitoids. The fold and S-surface terminology
follows that of Collins et al. (1991), as explained
in the previous section.
Stromatic leucosomes outline the earliest folia-
continental heat flow data sets can yield signifi-
cant additional information beyond these first-
order generalizations. Do regional heat flow data
sets, along with ancillary geophysical and geo-
chemical data, provide sufficient constraints to
address variations on the first-order themes de-
scribed above?
We address that question by focusing on east
and southern Africa because there heat flow
measurements define a common first-order heat
flow pattern but also show clear regional variabil-
ity, The first-order heat flow pattern suggests that
the gross thermal structure of the lithosphere
beneath east and southern Africa is similar (Bal-
lard and Pollack, 1987; Nyblade et al,, 1990);
however, the variations in heat flow superim-
posed on this common pattern suggest that there
may also be some differences in the thermal
structure of the lithosphere between these two
regions. Specifically, we wish to determine if vari-
ations in the heat flow pattern between east and
southern Africa can be easily interpreted to show
in what ways the thermal structure of the litho-
sphere may differ between these two regions. We
first briefly describe the first-order heat flow pat-
tern in east and southern Africa on which the
regional variability is superimposed, and review
our interpretation of this first-order pattern.
The southern African subcontinent is a com-
plex Precambrian terrain that comprises two simi-
lar tectonic regions, east and southern Africa.
zania and Kalahari Cratons or between the
Mozambique Belt and the southern African mo-
bile belts, they provide little information about
how crustal heat production at depth may differ
between east and southern Africa vis-a-vis the
linear heat flow-heat production relationship.
Crustal velocity models for the Namaqua
(Green and Durrheim, 1990) and Damara Belts
(Baier et al., 1983) in southern Africa and for the
Mozambique Belt (KRIsF working party, 1991) in
east Africa can be used to constrain crustal heat
production in the east and southern African mo-
bile belts via an A-Va relationship. Using the
A-V, relationship from Cermak and Rybach
(1989) to convert velocities to heat production
yields a crustal column with a surface heat flow
of about 35 mW m''' for the Namaqua Belt, 37
mW m' for the Damara Belt, and 34 mW m*
for the Mozambique Belt away from the Kenya
Rift Valley. The uncertainties in these estimates
are probably 40% or more; however, in spite of
these large uncertainties, the similarity in seismic
structure, and thus in the estimated total crustal
heat production between these mobile belts sug-
gests that the difference in heat flow between the
Mozambique Belt and the southern African mo-
bile belts may not arise from gross variations in
crustal heat production. There are no seismic
velocity data for the Tanzania Craton and so an
A-V4 relationship cannot be used to estimate
differences in crustal heat production between
the Tanzania and Kalahari Cratons.
Comparing the crustal velocity models for the
Namaqua, Damara, and Mozambique Belts cited
above, also shows that the thickness of intra-
crustal layers in the three mobile belts is roughly
the same, further suggesting that crustal heat
production does not differ considerably between
the mobile belts in east and southern Africa. For
the cratons, crustal thickness estimates in the
Kalahari Craton are between 35 and 40 km (Gane
et al., 1956; Stuart and Zengeni, 1987) but, as
mentioned previously, there are no seismic obser-
vations of crustal structure for the Tanzania Cra-
ton, Given the uniformity of crustal thickness in
Archean cratons worldwide (Durrheim and
Mooney, 1991), it is not unreasonable to assume
that the Tanzania Craton crust does not depart
significantly from the thickness of the Kalahari
Craton crust. However, the lack of data from the
Tanzania Craton precludes any further discussion
of differences in intra-crustal structure between
the Tanzania and Kalahari Cratons.
In regard to constructing local models of crustal
heat production from representative rock sam-
ples, there are no heat production estimates for
xenoliths or sedimentary rocks from east Africa
that could be used to constrain the depth distri-
bution of crustal heat production in the Tanzania
Craton or Mozambique Belt, nor are there any
reported crustal cross-sections.
In summary, similar crustal velocity models for
the Mozambique Belt and two southern African
mobile belts suggest that there is no difference in
the total crustal heat production between the east
and southern African mobile belts. However, be-
cause corroborative evidence from surface heat
production and petrologic information is lacking,
it is not easy to make a robust case that differ-
ences in crustal heat production between the east
and southern African mobile belts do not give
rise to the differences in heat flow. There is
insufficient data of any kind to estimate the depth
distribution of crustal heat production in the
Tanzania Craton, and therefore no conclusion
can be reached about variations in crustal heat
production between the Kalahari and Tanzania
Cratons.
In this section we examine the possibility that
the lower surface heat flow in some areas of east
Africa relative to southern Africa may be due to
lower mantle heat flow into the base of the litho-
sphere. There are at least two possible explana-
recorded at Zuoshan. Hence, it is tentatively con-
cluded that five possible large earthquakes, in-
cluding the 1668 one, have occurred in recent
geologic time on the active Yishu fault from
Zuoshan to Hezhuang.
In addition to examining the stream channels
for multifaulting events, an attempt was made to
determine the profile evidence at many localities
along the fault. It turns out that an outcrop at
Mazhuang is very suitable. The fault outcrop
there is shown in Photo 1. On the eastern side is
Cretaceous sandstone and siltstone, whereas on
the western side are alluvial deposits. The fault
zone is composed of gouge and clastic sandstone.
The exposed gouge is 0.9 m wide at the top and
2.5 m wide at the bottom. This high-angle zone
dips toward the east, showing a small amount of
vertical movement due to transverse compression.
Within the gouge zone, there are many sandstone
lenticulars of various sizes. The alluvial deposits
on the west consist of five different layers. From
bottom to top they are designated as layers 1, 2,
3. 4 and 5 (Fig. 10). Layer 1 is Fe- and Mn-con-
cretion-bearing clay; layer 2 is comprised of sand-
stone fragments and sandy gravels; layer 3 con-
tains well rounded to subrounded coarse sand;
layer 4 is fine to silty sand; and layer 5 is residual
soil, with modern vegetation. Note that these
layers have different degrees of deformation. The
bottom layers folded more strongly than the top
ones, This is because the older layers have under-
gone more faulting events than the younger ones,
as interpreted in Figure 11. As mentioned above,
the active Yishu fault is dominated by strike-slip
with some amount of a thrusting component.
Therefore, on the cross-section it looks like a
high-angle thrust fault. Consider a layer l,, which
is deposited before faulting (Fig. 1la). When a
faulting event occurs, it is deformed as in Figure
11b. A fault scarp on the east is formed after
faulting. The scarp is unstable, and easily eroded.
Hence, another layer, lg, is soon deposited over
layer 1 (Fig. 11c). Then, another faulting event
takes place, the crushed zone becomes thicker
and l, is deformed further. However, lg just
begins to deform (Fig. 11d). During the quiescent
period of faulting, another layer, l4, is deposited
over l,, Finally, the fault outcrop looks like the
rectangle in Figure 1le. Thus, the number of
layers in Figure l1 may indicate the number of
faulting events. According to this process of de-
duction, the outcrop in Figure 10 may record five
possible events. Considering that layers 3 and 5
are very thin, they were probably associated with
the same events as layers 2 and 4, respectively. In
that case, at least three events are recorded, the
latest of which is the 1668 earthquake. Unfortu-
nately, the author did not find any charcoal for
carbon 14 dating in those layers. Thus the abso-
lute ages of the faulting events are not available
at present.
One of the typical geomorphic indicators of
faulted channels on the active Yishu fault is the
lous (Stevenson, 1990) because of the usu-
ally scanty nature of the secondary xylem
compared to the well developed parenchy-
matous pith and cortex.
There is also considerable diversity in leaf
and leaflet morphology. The general aspects
of leaf and leaflet diversity in Zamia has
been discussed elsewhere (Stevenson, 1991)
so that the following is limited to that found
in the Panamanian species of Zamia. Some
species (e.g., zZ. dressleri) have only 4-8 pairs
of very wide leaflets in contrast to the up to
70 pairs of narrow leaflets found in Z. chi-
g4a. zZamia acuminata often has diminu-
tive prickles on the petiole in contrast to the
heavy, often branched, prickles found on
petioles of Z. chigua. Most Panamanian
species of Zamia have flat, smooth leaflets
but three species (Z. dressleri, Z. neuro-
phyllidia D. Stevenson, and Z. skinneri
Warsz, ex A. Dietrich) have distinctive leaf-
lets that are deeply grooved adaxially be-
tween the veins so as to appear plicate. In
general Zamia leaflets are sessile on the ra-
chis but one Panamanian species, Z. mani-
cata Linden ex Regel, has distinct petio-
lules. In Z. manicata, there is also an abaxial,
semicircular, collar or gland-like structure,
of unknown function, at the junction of the
lamina and the petiolule (Stevenson, 1990).
Reproductive morphology, on the other
hand, is more constant, but here too the
variability is greater than for other neo-
tropical areas with strobili varying in color
from light yellow to deep red-brown. Three
species of Zamia found in Panama, Z. cu-
naria Dressler & D. Stevenson, Z. ipetiensis
D. Stevenson, and Z. obliqua, have very
distinctive microsporophylls in that micro-
sporangia are found on both the abaxial and
adaxial surfaces (Fig. 1) in contrast to all
other cycads which have microsporangia
only on the abaxial surface. The yellow seeds
of the endemic Z. pseudoparasitica are
unique in the genus.
Because the cycads are strictly dioecious
and reproductive structures are infrequently
encountered in the field, herbarium, or cul-
tivation, the emphasis in the following key
to species is on vegetative characters. Cer-
tainly, to use yellow seeds as the only key
character for Z. pseudoparasitica would not
be useful for a microsporangiate plant. In
addition, the vegetative characters in Pan-
amanian Zamias are so distinct, as outlined
above, that their use in this case is war-
ranted.
As in previous works on cycads (e.g., Ste-
venson et al., 1986), specific localities of the
species of Zamia are not given in this paper
because of their endangered status and com-
mercial value which could lead to their
eradication either intentionally or uninten-
tionally by collectors. Some of the species
discussed and described in this work are
locally endemic and known from only one
or a few small populations and are thus par-
ticularly susceptible to over-exploitation.
All lectotypes and neotypes given in the
present work are from Stevenson and Sa-
bato(1986). Chromosome numbers are from
the works of Norstog (1980, 1981) and Mo-
retti(1990). Information on individual spe-
cies are given in alphabetical order.
pedicellate (to 1 mm), inserted parallel to
perpendicular, in diads distally; sepals con-
nate to middle, glabrous, membranous, two
ca. 2.5 x 1.5 mm, one larger ca. 3.5 x 1.5
mm, much smaller than the petals; petals
valvate, connate to middle, glabrous, mem-
branous, ca. 6 x 2 mm; stamens 15, ca. 6
mm long, the filaments not columnar, ca.
1.5 mm long, the anthers ca. 4 mm long,
sagittate at both ends; pistillode trifid. Pis-
tillate flowers inserted on proximal 5-12 cm
of rachis, fibrous; sepals free, triangular, 3-
5 x 3 mm, contorted to left, glabrous, ir-
regular to dentate on margin; petals free,
triangular, 3-5 x 3 mm, imbricate to right,
glabrous; staminodial ring adnate to petals;
pistil conical, 3-5 mm tall; stigma capitate,
shorter than 1 mm, with three short branch-
es, wrinkled, glabrous. Fruit ovate to tur-
binate, 1.5-2 cm long, ca. 1.3 cm diam., the
stigmatic remains truncate with stigmas ses-
sile and minute, the persistent perianth
shorter than half of fruit; endosperm ho-
mogeneous; seed 1.
Common names: ''Cachand6.''
Additional specimens: BRAZIL. Bahia: Municipio
Salvador, Lagoa do Abiete, 10-20 m, 23 Sep 1976,
Davis 61032 (F); ca. 35 km NE from Salvador and 3
km NE from Itapoä, 30 Aug 1978, Morawetz d Mora-
wetz 21-30878(BH); Salvador, dunas de Itapoä. Lagoa
de Urubu, 12*56'S, 38*21'W, 12 Dec 1985, Noblick d
Briuo 4473 (F-n,v., LPB); Itapoä, Bondar 23 (F); Pi-
d6be, 1835, Blanchet s.n. (G), Esplanada, 1 Dec 1988,
Noblick d Soeiro 702(CEPEC-n.v., F-n,v., HRB-
n,v., LPB); Esplanada, Lago apos entrada para Conde,
11M47'S, 37*55'W, 15 Feb 1978, Orlandi 140 (RB);
central pat of State, Oct 1942, Krukoff' 12631 (NY).
Distribution: Restricted to xeromorphic
vegetation of littoral sandy dunes and cer-
rado in northeastern region of Bahia, Brazil.
Populations occur to 40 km from coast from
O to 20 m elevation in restinga vegetation
(Entre Rios, Salvador) and from 100 to 150
m elevation in cerrado vegetation (Esplana-
da).
Allagoptera brevicalyx is named for its
perianth that is short in fruit, rather than
enlarged as in other species of Allagoptera.
It has been confused frequently with A. are-
Nnaria, which grows in similar restinga veg-
etation but closer to the beach, and also with
A, campestris from the cerrado vegetation.
This new species differs from other spe-
Siphocampylus Pohl (Campanulaceae:
Lobelioideae) comprises over 200 species
of herbs, shrubs, and lianas. Its center of
diversity is the Andes of South America,
from Venezuela to Bolivia; species also oc-
cur in the Caribbean, Central America north
to Costa Rica, and extra-Andean South
America (Brazil, Paraguay, Uruguay, and
Argentina). The most recent monograph of
the genus isthat prepared by Wimmer (1953,
1968) for Das Pflanzenreich. In this paper,
Idescribe a new species from northern Peru,
which was detected during routine identi-
fication of South American lobelioids de-
posited at F.
Lammers, sp.
nov. (Figs. 1, 2)
TYPE: PERU. La Libertad. PRov. OTUzco:
ca, 20 km E of Agallpampa enroute to Qui-
ruvilca, ca. 3390 m, 6 Jan 1983, M. Dillon,
U. Molau d P. Matekaitis 2792 (HOLOTYPE:
F; sorYPEs: MO, USM-n,v.).
A speciebus ceteris Siphocampyli subsect. Megas-
tomis pedicellis bibracteolatis, hypanthio 12-16 mm
diametro, calycis lobis 4-6 mm latis, corollae tubo sine
constrictione, diametro a basi (12-13 mm) ad orem
(14-16 mm) accrescenti, corollae lobis dorsalibus 32-
37 mm ventralibus 23-30 mm longis, et antheris 16-
17 mm longis differt.
Suffruticose, malodorous shrub; stems 2-
2.5 m tall, several from base, puberulent or
subglabrous; latex milky. Leaves alternate,
densely crowded toward the apex, marces-
cent; lamina 11-18 cm long, 1.5-3.8 cm
wide, narrowly elliptic or lanceolate, cori-
aceous; upper surface dull, dark green, gla-
brous or puberulent, the veins pale; lower
surface dull, green, puberulent on the veins,
the veins darker; margin denticulate, the
teeth ca. 1 mm apart; apex acuminate; base
narrowly cuneate; petiole 0.5-1 cm long,
narrowly winged, shortly decurrent, puber-
ulent or subglabrous. Flowers solitary in the
axils of upper foliage leaves, forming a 12-
17-flowered terminal, simple corymb; ped-
icels 10-19.5 cm long (the lowermost the
longest), bibracteolate 3-15 mm above their
base, glabrous or minutely puberulent to-
ward the apex; bracteoles 5-12 mm long,
O.5-1.2 mm wide, linear, ciliate. Hypanthi-
um 7-10 mm long, 12-16 mm diam., hemi-
spheric or broadly obconic, faintly 10-
nerved, glabrous or minutely puberulent.
Calyx lobes 23-35 mm long, 4-6 mm wide,
narrowly triangular, erect; margin entire,
ciliate; apex acuminate. Corolla 52-60 mm
long, cream or yellow with a tinge of purple,
glabrous or nearly so; tube erect, 20-25 mm
long, 12-13 mm diam. at base, gradually
The Guianas; Venezuela (Bolivar), wide-
spread in tropical and subtropical areas of
the New World; in savanna and old fields.
Guyana (Rupununi Savannas), in riparian
scrub.
Guyana, Surinam; Amazon Basin; in rain
forest.
Venezuela (Amazonas); NW Amazon Ba-
sin; in rain forest.
Venezuela (Amazonas, Bolivar); Brazil
(Amazonas); in scrub-savanna and dwarf
forest on tepui tops.
Venezuela (Amazonas); Colombia (Vau-
pes); in scrub-savanna on tepuis.
Venezuela (Bolivar: Carrao-tepui); in dwarf
and montane forest.
Venezuela (Amazonas: Cerro Sipapo); in
scrub and dwarf forest.
Venezuela (Bolivar: Cerro Venamo), in
scrub and dwarf forest.
influence the model even if they are valid after the nominal analysishour;as the model
integration proceeds, fewer observations are left for assimilation and the system
gradually turns to forecast mode. For example, in the current operational system
observationsare assimilated up tofour hours after the nominal analysishour, providing
greater accuracy in the earlier periods of the forecast.
The primitive equations, on which numerical models are based, generally admit
high-frequency gravity-wave solutions as well as the slower moving Rossby-wave
modes. Both types of wave are found in the real atmosphere, but gravity waves,
being readily dissipated, are not of major meteorological importance and the
atmosphere is close to geostrophic balance. If the analyses produced by schemes
based on statistical interpolation are used directly as initial conditions for a forecast,
imbalances between the mass and wind fields will cause the forecast to be
contaminated by spurious high-frequency oscillations of much larger amplitude than
those observed in the atmosphere. Aldtough the damping terms, which are part of a
numerical model, will tend to dissipate these oscillations, they make the short-
period forecasts noisy and may be detrimental to the quality control and the
assimilation cycle. For this reason, an initialization step is performed after the
analysis with the object of eliminating these spurious oscillations. Analyses produced
by schemes based on repeated insertion do not require a separate initialization step
since balance is generally achieved during the assimilation process.
World Area Forecast Centres are required to distribute grid-point data of wind and
temperature at various levels, as well as information on the tropopause and the
maximum wind. Suitably dispayed in graphical or chart form, these data alone are of
great value to aviation forecasters. However, a wide range of ancillary fields and
derived data may be generated from an NWP system which provide a more complete
picture of the numerical forecast. H+24 forecasts from the UK. operational models
serve to illustrate the range of products which can be generated from a numerical
system. Most are used regularly at Bracknell in its role as a Regional Area Forecast
Centre. Some are derived from fields issued routinely on the GTS, but many require
data at present only available at the centre. A mixture of charts from the UK global
and regional models is shown here, but all products could equally well have been
derived from a high-resolution global model. In all cases, the model products have
been projected onto a uniform grid (about 100 km grid spacing) for ourput purposes,
which is considerably finer than the resolution of data available on the GTS in
1990. The output resolution, both in the horizontal and vertical, of course greath
affects the detail that can be identified.
The products presented here relate to a case of explosive cyclogenesis in the
North Atlantic, and all are from the same data time of 1200 UTC 7 January 1990.
Fog prcdictions are also vcry difficult to assess because of the local nature of fog
occurrence. However, comparisons with routine synoptic observations have been
prepared on a monthly basis for the UKMO model. For November 1989, these show
that forecasts from 0000 UTC dara were far better than those from 12 U'TC data,
the latter producing too much fog by thc end of the night. For afternoon forecasts
not been corrected automatically, this can be done at forecast time. A
knowledge of the level of variance in the error will give a general idea of how
much confidence to put in the forecast. hiigher error variances suggest a less
reliable product in day-to-day use. If there is any reason to assume that the
errors are serially correlated, a knowledge of recent performance of the
product will help in the interpretation. For example, if the fact that a
technique forecast the ceiling two categories too high six hours previously
means it is likely that it will do it again now, this information can be used to
adjust the current forecast in the right direction.
To interpret verification statistics, the most important characteristics to
know are: sample size, sample stratification and type of verification
measure used. For sample size, the larger the better; smaller samples may
give rise to misleading verification statistics. Sample stratification is
important to know to be sure that the results are represetative of the
conditions which apply in the current situation. Seasonal stratification is
most common; verification statistics from winter forecasts will not
necessarily say anything useful about performance in summer. Of more
interest are sample stratifications based on the values of the weather
element. For instance, verification informarion could be computed for a
subsarmple consisting of all cases where ceilings less than 200 feet were
forecast. The verification statistics then could say, for example, that
forecasts of ceilings less than 20O feet tend to be one category too high
75% of the time and two categories too high 20% of the time. Such
verifications are of course valid only for forecasts that match the
subsample definition, but they convey specific and clear information
about the forecast. A word of caution, however: subdividing the sample
reduces the sample size - the benefits of stratified sample verification
may be lost if the subsample is too small to give stable results.
7The characteristics of the different types of verification measures in general
use are briefly described in Appendix A, along with the measures that were
used to produce the results reported in section 3.5.
Current statistical interpretation products have some general practical limitations
with respect to aviation forecasting. First of all, they are tied to the production
schedules of the models that drive them. NWP models normally run every 12 hours,
and the output is available three to four hours after initial data time. This means
forecast products cannot be expected to reach the forecaster's desk until at least four
hours after the data on which they are based. For many aviation applications this is
a long time, and model-dependent statistical guidance is not very responsive to short
range aviation needs. When products have been received, it is always possible for the
forecaster to add more recent information in a subjective manner. Objective
updating methods are under development (e.g. Glahn and Unger, 1986) but none is
in widespread use yet.
Second, NWP models tend to be subject to an adjustment period in the
shortest ranges, mainly due to the incompleteness of the initialization. The nature
and extent of the adjustment varies considerably, but it is common to see verification
statistics that show, for example, that the moisture or precipitation forecasts are more
accurate at 24 hours and beyond than they are at 12 hours. To the extent that these
problems are systematic, they can be accounted for automatically in a MOS system
and the quality of statistical forecasts need not suffer, but it is a factor in determining
the responsiveness of statistical guidance in the shortest ranges.
Third, forecasts for some aviation elements such as ceiling and visibility are
required in considerably more detail than is required for other elements or for public
forecasts. The requirements generally exceed the capabilities of the resolution of
current models, and as a result, ceiling and visibility are widely regarded as the
hardest elements to show skill in forecasting. (Technique developers like to do them
last as a result - one cannot sell the value of statistical techniques on the basis of
ceiling and visibility forecasts). It is here that the greatest potential for improvement
exists resulting from improvement in NWP models.
A contingency table is a tabulation of the number of occurrences of all possible
combinations of observed and forecast categories in a verification sample of a cate-
gorical variable. Although they can be used for verification of probabiliry forecasts,
contingency tables are more commonly used for evaluation of forecasts where a
specific category of a weather element has been forecast to occur. The entries of the
table are simply the counts of the number of times each particular forecast-observed
category combination occurred in the sample.
The sample table is for a three-category element. The following scores are
computed from the entries of a contingency table.
where: a, e, iare the number of correct forecasts in each category and T is the total
number of forecasts. The percent correct is the most commonly quoted score from
the contingency table. Events are all weighted equally, which means that the score
may be dominated by common categories. The percent correct can be quoted sepa-
rately by category to get around this problem.
The false alarm ratio is the number incorrectly forecast divided by the total forecast
for each caegory
Th+ FAR is most often used for two category situations where one of the
categories is rare. One way of ensuring a high percent correct for a rare event is to
relax the criteria for forecasting it until all the occurrences are caught. The cost of
this isa high FAR, a tendency to forecast the event much more often than called for.
The FAR thus gives an indication of the tendency towards ''crying wolf'. It is desir-
able that the FAR be as low as possible.
This is the number correct divided by the number observed in each category. It is a
measure of the ability to correctly forecast a certain category, and is sometimes
referred to as the ''hit rate'', especially when applied to severe weather verification.
TH+ POD has a range from 0 to 1, with high values most desirable. It should
be used in connection with a measure of false alarms such as the FAR. because it is
always possible to increase the hit rate for a particular category by forecasting it more
often.
The right oviduct is always longer than the left; this is associated with the
more forward position of the right ovary, Both oviducts are present in
henophidians. In Polemon notatus the left oviduct is missing.
In most henophidians a narrow, laterally compressed, dorsal process of the
premaxilla intrudes between the narrow anterior processes of the nasal bones. In
the study group the anterior margin of the compressed dorsal process may be
reduced so that it is hidden by the nasals in anterior view, there may be a broad
based process tapering to a wedge between the nasals or a broad process on
which the nasals rest. The low process is inferred to be the most nearly primitive,
with the wedge and the broad process derived either separately or one from the
other. We have coded the low dorsal process as the all zero state. To allow the
derivation of one broad state from the other we add a third binary character for
which they both get a one score, Whichever of the three binary characters has
the lowest level of compatibility is deleted and the remaining two indicate the
preferred route of derivation.
In most henophidians the premaxillae have simple lateral processes, which do
not meet the maxillae. In lizards and in Anilius, Gylindrophis and Uropeltidae the
lateral processes do, however, meet the maxillae. On the basis that we root the
Caenophidia near to Loxocemus, we regard a gap between premaxilla and maxilla
as primitive. In the study group we may find this primitive condition, the lateral
process may be turned backwards, a simple process may meet the maxilla, or the
lateral process may be bilobed. In Loxocemus and pythons paired ventral
processes of the premaxilla, separated by a deep cleft, meet the vomers, In the
study group there is a median process which may bear two lobes which are more
or less prominent, or may taper to a wedge between the vomers (characters 33,
34, 35).
there is no obvious basis for partition and we code only for presence or absence of
teeth on the pterygoid.
In Loxocemus the palatine bone has a mesial process arching over the choanal
passage and approaching the vomer. This choanal process has a broad base
which extends back as a triangular process overlapping the anterior end of the
pterygoid. There is a lateral, maxillary, process, In many henophidians this is
perforated by a foramen for the maxillary nerve with its posterior opening on the
dorsal side of the process. In the members of the study group the choanal process
may be long, short or absent. The maxillary process is always present but may be
imperforate. The palatine may overlap the anterior end of the pterygoid with a
broad process (Fig. 7) or a small finger-like process. The two bones may meet in
full width end-to-end contact. The ends of the two bones may taper, with either
point-to-point contact or separation by a gap (characters 58, 59, 60, 61; Fig. 6).
persuade us that there is a particular relationship between the American and
African genera. This may be revised if presence of a lateral rictal gland is
confirmed.
We therefore deleted the South American genera and pursued the analysis of
the 12 African taxa, Some binary characters were deleted as unvarying or
duplicating the scores of another binary component of the same variable. This
left 103 binary characters, We found a maximum clique of 24 characters
marking 10 nodes of the dendrogram and giving complete resolution. Amongst
the 30 highest weight characters we found three further cliques marking nine
nodes in common with the first. We consider the alternatives,
The unique condition of the rictal gland (80) associates Aparallactus capensis,
Polemon and Chilorhinophis. This character is incompatible with character 48.1
which has some missing scores and is therefore discarded as of uncertain value. It
is also incompatible with character 58.1: complete versus reduced choanal
process. Reduction of this process is paralleled in many other snake lineages; we
therefore prefer character 80. These characters give us complete resolution
(Fig. 19). It is an anomaly that four stems of the dendrogram bear primitive
states of a total of six characters, which have to be interpreted as reversals to a
pseudoprimitive state. Insertion of the temporal ligament on a temporal scale
(73.1) is uniquely derived from the absent condition. Insertion of the ligament
on the postorbital (73.2) is homoplasiously derived from absence.
This study shows that, at least in M. jurtina, the male and female genitalia do
not form a precise lock-and-key. The portion of the genitalia used to classify
races of M. jurtina in Europe (Thomson, 1973; Shreeve, 1989), namely the dorsal
process, has no obvious function during copulation. This suggests that variation
may be confined to non-functional parts of the valve, and is in accordance with
Lorkovic's view that male genital morphology is not tightly constrained: a
degree of variation may not be subject to direct selection, This is supported by
the absence of any apparent relationship between valve shape and either mating
success or strength of the male-female bond.
If variation in the shape of a large portion of the valve is not constrained by
selection, then differences between races or subspecies in the valve shape are
unlikely to act as barriers to cross-fertilization, Clearly there is some doubt as to
the accuracy of the lock-and-key analogy. This is of some concern to taxonomic
studies in which the genitalia are assumed to provide one of the most reliable
characters by which species may be distinguished. The use of genitalia in such
studies must be reconsidered: if intraspecific variation in valve shape is not
tightly constrained by selection then how reliable is its use in describing species?
The consistent use of valves in taxonomy over many years may be reason enough
to accept their use as valid, for if it regularly produced anomalies and
inconsistencies they would long ago have been discarded as an important tool.
Why then does the valve shape of the Lepidoptera remain a valuable taxonomic
characterf I would argue that evolution of male genital morphology is best
considered in two parts, those which contact the female during copulation, and
those which do not. The sexual selection of female choice model and the lock-
and-key hypothesis are both only of relevance to those portions of the genitalia in
contact with the female. The rest of the genitalia may be neutral to selection and
subject only to random processes, or to pleiotropic effects as suggested by Mayr
(1963).
This study may explain why genitalia are so valuable in taxonomic studies, for
neutrality to selection is one of the key features used by the traditional school of
evolutionary taxonomy to select characters for use in taxonomic studies, for
analogics (as opposed to homologies) are less likely to evolve when selection is
absent (Ridley, 1986).
I would like to thank Dr Denis Owen, Dr Andrew Lack and Dr Tim Shreeve
for discussion and advice while carrying out this work, and for constructive
criticism of my writing. I would also like to thank Oxford Polytechnic for
providing funding.
occupied by pallial organs. The kidney lies dorsally and is wide, enveloping
other organs from the posterior end of the pallial cavity to the anterior stomach.
The digestive gland is dark brown, occupying half the length of the body. It
consists of two separate parts, The posterior digestive gland is massive and
occupies most of the body situated just a short distance behind the stomach. The
anterior digestive gland is a thin, irregularly and deeply lobed mass, appressed to
the right side of the posterior stomach, although its upper end arches frontally
and reaches the anterior stomach. The gonad has a pale yellowish colour, in
sharp contrast with the underlying digestive gland.
The ctenidium (Fig. 5) has a slightly arched outline, blunt anteriorly but
slender posteriorly. It is almost as long as the pallial cavity, and over a third as
wide. It is placed to the left of the mid-dorsal line. The gill leaflets are thick, low,
and tightly packed into a battery of 28 in the male and 27 in the female.
The osphradium (Fig. 5) is annular, with an elongate, irregularly
subtriangular outline. It is short, spanning over less than one-fifth of the
ctenidial length. It is situated almost parallel to the mid-length of the ctenidium,
and closer to the neck,
The radula (Fig. 6) is typically hydrobioid. The central tooth of each row
bears a long, dagger-like middle cusp flanked by six smaller cusps on either side,
which decrease in size towards the outer edge and are slightly inclined to the
Computer vision is a subset of the
machine intelligence systems area. The
goal of a computer vision system is to
interpret the given ''visual'' data and
to use the interpretation to complete a
task. Typical tasks include (1) the navi-
gation of autonomous vehicles on the
land, in the air, or under the sea, (2)
the assembly or inspection of manufac-
tured parts, and (3) the analysis of
microscopic images and medical x-rays.
In a number of applications, the goal of
the vision system is to identify and locate
a specified object in the scene. In such
APPENDD A. DIFFERENTIAIL GEOMETRY OF
SURFACES
APPENDID B. SURFACE PROPERTY
MEASUREMENTS
ACKNOWLEDGMENTS
REFERENCES
cases, a vision system must have full
knowledge of the shape of the desired
object. Such a priori knowledge of
the object is provided through a model
of the object, and in most cases it con-
tains information regarding the geome-
try of the objects; some models may
contain additional information such as
thermal and stress properties of the
objects. A vision system which makes use
of an object model is referred to as a
model-based vision system, and the gen-
eral problem of identifying the desired
object is referred to as object recognition.
While there is no single definition of the
object recognition problem, the objective
is to identify a desired object in the scene
and to determine its exact location and
orientation.
An ideal model-based vision system
should be able to locate objects in a scene,
assuming that any of the following are
true: (1) objects may have arbitrary and
complicated shapes or forms; (2) objects
may be viewed from any direction; and
(3) objects may be partially occluded by
other objects. Specifically, such a system
may be used in determining the location
of grasp sites for a robot arm to mani-
pulate an object, in the navigation of a
robot or an autonomous vehicle, and in
the assembly and inspection of parts
in a manufacturing environment. For
instance, robots with such vision capabil-
ities may carry out instructions regard-
ing the handling of objects with fewer
specifications than are currently required
and with more tolerance for minor
disturbances.
To design such systems, system
designers must resolve the following
issues: (1) the type of sensor for data
collection, (2) the methods of construct-
ing the necessary object model, (3) the
means of describing the collected data
and the model, and (4) the methods of
matching the object descriptions obtained
from the input data to that of the model.
The sensor determines the resolution (the
total number and frequency of sample
points) and precision (the accuracy of
each sampled point). More importantly,
it determines whether the data provides
2D or 3D information of the scene. Mod-
els provide the a priori knowledge of the
vision system. Representations are used
to describe the collected data and the
object model, a key issue in the field of
computer vision. The representations dic-
tate the matching strategy, its robust-
ness, and the system's efficiency. Also,
the descriptions are used in the calcula-
tions of various properties of objects in
the scene needed during the matching
stage. Matching strategies are performed
at run-time and must resolve many
ambiguities that exist between the data
and the model descriptions. Once the cor-
rect match has been determined, the ori-
entation and translation of the located
object, with respect to the model, can be
calculated, completing the task of object
recognition.
This paper surveys recently published
papers' addressing the problem of the
model-based recognition of objects in 3D
dense-range images.' Section l discusses
in detail the issues outlined above to
introduce the specific problems of model-
based vision. Next, Section 2 reviews var-
ious sensing modalities, beginning from
the data collection step, giving emphasis
to sensors providing 3D information.
Section 3 reviews various low-level pro-
cessing procedures necessary as a prestep
to the description and recognition of
objects. Section 4 reviews various repre-
sentations used to describe the objects.
Section 5 discusses modeling schemes,
giving emphasis to the computer-aided
design (CAD) systems used in the exist-
ing model-based vision systems. Section
6 presents a study of the matching
strategies. Finally, Section 7 presents the
summary and concluding remarks. A
brief overview of differential geometry of
surfaces is presented in Appendix A, and
in Appendix B various computational
methods to calculate surface properties
are reviewed.
The introduction outlined the issues
involved in the design of a model-based
vision system. This section analyzes fur-
ther these issues: data collection, repre-
sentation, model construction, and the
matching strategies (see Figure 1).
The first issue addressed in a com-
puter vision system is data collection,
which may be performed using one or
more of the many existing modalities.
The intensity camera is perhaps the most
commonly used sensing module, measur-
ing visible light. The output of the cam-
era from a scene is digitized to provide a
2D array of numbers; each number corre-
sponds to the average intensity sensed in
a sampled, typically square area. Exam-
ples of other sensory modules include
thermal cameras, which measure the
emitted thermal radiation rather than
the emitted visible light, and laser range
scanners and sonar sensors, which are
used to calculate the distance to the
objects in the scene. These sensors each
provide information on a different aspect
of their environment; the choice of the
sensor is largely application dependent.
For the task of 3D object recognition,
various object dimensions and surface
shape information are essential. Two
general approaches exist to collect the
necessary data. In the first approach,
sensing modalities, such as intensity
cameras, are used, and many depth cues
are analyzed to recover the necessary 3D
information. In the second approach,
external energy sources, such as lasers,
are projected onto the scene. While the
first approach is preferred (since no
external sources are required), the recov-
ered data lacks the necessary resolution
and precision for many common tasks;
therefore, the second approach is used
most frequently in 3D object recognition.
Section 2 studies the schemes for collect-
ing 3D information from the scene.
The second issue in a computer vision
system is to represent the collected data
and the modeled object. The 2D array of
numbers provided by the sensor is not
of much use in its ''raw'' form. A suitable
representation scheme must, therefore, be
used to describe the data and the model.
A representation is desirable if it is (1)
unambiguous (no two objects have the
same representation), (2) unique (there is
a single description for each object using
the representation scheme), (3) not sensi-
tive (with respect to missing data points,
such as in the cases of partial occlusion),
and (4) convenient to use, in the match-
ing stage, and to store. Representation
is a key issue in computer vision. Vari-
ous schemes, such as surface-based and
volumetric-based representations, will
be discussed, with emphasis given to
recently published representation
schemes.
The construction of object models is
the third issue which must be addressed
in a model-based computer vision sys-
tem. There are two main approaches for
model construction. In the first approach,
the actual objects are used to generate a
model; i.e., data points obtained from
several viewpoints of the object are inte-
grated in a coherent fashion to provide
information from all the viewing angles.
In the second approach, a CAD system is
used, and a set of predefined primitives
allows the user to construct interactively
the model of an object. Much of the ear-
lier work in object recognition used the
first approach; however, most recent
research efforts use a CAD or similar
system. Both approaches are reviewed,
and the advantages and the disadvan-
tages of each are discussed.
Once the appropriate descriptions are
derived from the data and the models,
the vision system is able to match the
two descriptions. This is performed
in two steps. In the first step, a corre-
spondence is established between the two
sets of descriptions. Since in most cases
data is collected from a single view and
there may be partial occlusions present,
the matching strategy must establish
correspondence between the partial
description of the object and its full model
description. The correct match of the
collected data to the representation of
the given model ''establishes an interpre-
tation'' of the input data [Ballard and
Brown 1982]. The exact choice of the
matching strategy is dependent on
the representation scheme, the applica-
tion, and the system designer's expertise.
In the second step, using the established
correspondences, a geometrical transfor-
mation (usually a rotation matrix and a
translation vector) is derived such that
the model may be transformed to the
orientation of the object in the scene.
An important aspect of any computer
vision system is its data acquisition mod-
ule, This section reviews the schemes in
which 3D information is acquired from
the scene. The task is performed in one
of two approaches: passive or active. In
the passive approach, 3D information is
inferred from the scene using existing
energy in the environment, such as
reflected light. In the active approach,
the 3D information is derived by project-
ing external energy waves, such as sonar
waves and laser light. As mentioned in
the introduction, 3D data recovered from
current passive approaches lack the nec-
essary precision and resolution for 3D
object recognition; in this section, the
active methods are reviewed [Besl 1989;
Nitzan 1988; Freeman 1988; Fu et al.
1987; Kanade 1987; and Ballard and
Brown 1982].
Active-range sensing can be divided into
two main classes. In the first class, the
principle of triangulation is used. Each
point in a scene is highlighted, using a
sheet of light, and observed by the sen-
sor. Then, using the known geometry of
the imaging system, the distance of each
highlighted point to the sensor is calcu-
lated. In the second class of active-range
sensors, known as time-of-flight sensors,
a signal is emitted, and its return time is
measured and used in calculating the
distance.
This method uses a laser source which
projects a sheet of light onto the scene,
casting a line on the objects'' (see Figure
2). A camera is positioned so that the
laser line is visible. Using the known
imaging geometry, i.e., the distance
between the laser source and the camera
(referred to as the baseline), their angles
a, and a,) with the z-axis (the axis
along which depth is measured) and by
applying the principles of triangulation,
the distance of the illuminated points,
along the cast laser line, to the baseline
is calculated (see Figure 2). Sweeping the
sheet of light across a scene results in a
range map. The sweeping of the sheet is
performed in one of two ways: a rotating
reflector can be used to project the sheet
of light, or the objects can be placed on a
stage which slides by or rotates in front
of the sheet of light. In all cases, the
laser may not illuminate some areas, or
the sensor may not be able to detect the
illumination (i.e., the object concavities
may occlude some areas of the object);
and there are no data points at those
locations of the scene. These areas of
missing points are referred to as shad-
ows.' An advantage of using the stage is
that more uniform spatial sampling
can be achieved, satisfying a common
assumption for surface property mea-
surement approaches (see Figure 3 and
Appendices B and A). The disadvantage
of using the stage is that it limits the size
and number of objects in the scene, and
it may not always be feasible to place the
objects on the stage. Also, the accuracy of
the motion control mechanism of the
stage or the reflector must also be taken
into consideration in determining the
approach used.
In most of today's available triangula-
tion systems, the time required to scan a
typical 256 x 256 scene with 8 bits of
accuracy is on the order of minutes.
However, some triangulation-based sys-
tems exist which are capable of scanning
scenes in seconds. For example, Rioux
[1984], and Rioux et al. [1989] developed
a system able to scan a 256 x 256 pixel
scene with a precision of 0.5 mm in less
than one second. Kanade et al. [1989]
and Gruss et al. [1990] introduced a small
prototype of a VLSI-based system, which
performs the triangulation on a 4 x 4
array of specialized sensors on a single
CMOS chip. Once fully developed, this
system could significantly reduce the
acquisition time in triangulation-based
range sensors.
To reduce the data acquisition time,
several sheets of light in parallel may be
projected onto the scene (first introduced
by Will and Pennington [1972[). The dis-
advantage of this approach is that,
depending on the scene's geometry, a
stripe position may be shifted more than
the existing spacing between the stripes
[Mundy and Porter 1987], causing ambi-
guities. Therefore, it is necessary to
determine which sheet of light is pro-
jected at each pixel in the scene. There
are several ways to resolve this ambigu-
ity. Mundy and Porter [1987] and
Inokuchi et al. [1984] use a set of gray-
coded stripes, assigning to each stripe a
unique code value. Boyer and Kak [1987]
and Vuylsteke and Oosterlinck [1990] use
only one set of simultaneous projections,
instead of a series over time, to obtain
the range information from the scene.
The advantages of using a single set of
patterns are that less time is spent pro-
jecting light patterns, and more impor-
tantly, nonstatic scenes may be scanned,
allowing a broader application area such
as robot navigation, motion analysis, and
moving-object recognition. Boyer and Kak
use a few colors to project single-colored
stripes. Vuylsteke and Oosterlinck use
a binary pattern to illuminate the scene.
Tajim a and Iwakawa (of NEC)
[1990] have reported on a triangulation-
based sensor using collimated white light
diffracted by a grating to form a rainbow
pattern on the scene capable of producing
30 3D frames per second.
One of the disadvantages of a triangula-
tion-based laser scanner is the shadow
effect, where a region of the scene is not
visible to either the laser or the sensor.
In time-of-flight range finders, a laser
beam is emitted and received along the
same path, eliminating the shadow prob-
lem. However, since the system depends
on the return laser light to measure the
distance, high-energy laser sources, pos-
sibly harmful to the human eye, are
required. Also, most time-of-flight range
finders require complex electronics, rais-
ing the cost of such sensors. Two classes
of laser sources are used in time-of-flight
scanners: pulsed and continuous-beam
lasers.
In this class of range finders the laser
light is reflected, and its return time is
measured [Lewis and Johnston 1977];
since the speed of the laser light is known,
the distance can easily be determined.
Such devices have ranges of from l to 4
meters with a precision of 5 0.25 inches;
however, such precision requires sensi-
tive electronic instrumentation capable
of resolving 30-50 psec time intervals
Jarvis 1983a; 1983b].
The second class of time-of-flight range
finders uses a continuous-beam laser
rather than a pulsed one [Jarvis 1983b;
Fu et al. 1987]. In this method, the delay
6 in the returned signal (i.e., the 0 to 2m
phase shift with respect to the transmit-
ting signal) is used to measure the dis-
tance D to the object:
where A is the wavelength of the signal.
This measurement is performed one point
at a time, and the laser beam is scanned
horizontally and vertically across the
scene. An inherent problem in this
approach is that all distances corre-
sponding to 2m multiples of the phase
shifts will be measured as the same dis-
tance by the system (referred to as the
ambiguity interval); one may, however,
assume that 6 is less than 2m. Addition-
ally, since A is usually on the order of
nm, D has a very small range which
is not feasible for most object recogni-
tion tasks. The amplitude of the laser
light may be modulated, however, using
long-wavelength waveforms, effectively
increasing A.' However, the ''ambiguity
interval'' of the scanner still exists and
may be resolved by transmitting at sev-
eral frequencies and checking all fre-
quencies at the ambiguity intervals. To
decrease the effects of photon shot noise,
the distance at each point is measured
several times, and the average is used.
A problem common to all laser range
scanners is the specularity problem,
causing erroneous measurements in both
the triangular-based and the time-of-
flight methods. In laboratory setup it is
possible to decrease this problem by
painting the surfaces appropriately; how-
ever, in outdoor scenes this problem per-
sists. In addition, when laser lights are
used, other sources of error, such as
HE motivation for this paper is the observation that a
scene containing more than one object most of the time
cannot be segmented only by vision or in general by any
noncontact sensing method. Visual information may be suf-
ficient to accurately segment simple objects and nonoverlap-
ping scenes. However, in general, it is not sufficient for
random heaps of unknown objects.
If no a priori knowledge is available, the vision system
cannot reliably distinguish between overlaps caused by two
different objects in the scene and overlaps caused by a single
self-occluding object. A flat rigid object supported by and
totally occluding another smaller object may be recognized as
a large box-shaped object. Similarly, a flat nonrigid object
supported in the middle by a smaller object may be recog-
nized as convex, while if it is supported at the edges by more
than one object, it may be recognized as concave.
Therefore, machine vision alone (or any noncontact sens-
ing method) is not sufficient for segmentation and recogni-
tion. An exception to this may be the case when the objects
are physically separated so that the noncontact sensor can
measure this separation or one knows a priori a great deal
about the objects (their geometry, material, etc.).
The traditional approach is to segment the noncontact
sensory information (range, intensity, etc.) regardless of
scene complexity. Then, based on the outcome of segmenta-
tion, to interpret the scene and recognize the objects. The
problem with this approach is that reliability decreases when
scenes become more complex and when a priori assumptions
are removed.
Our approach is different. Instead of trying to deal with an
ever increasing visual scene complexity, we use the manipu-
lator to make the scene simpler for the vision system. Our
paradigm is analogous to having the hand help the eye when
interpretation of visual information is ambiguous, or when
the scene is visually complex.
Our system is iterative because random arrangements of
objects form layers. Due to our noncontact sensor arrange-
ment, only the top layer of the heap is visible at any given
time and the objects are removed from the scene one at a
time. In general, the system must sense and manipulate more
than once for every random scene.
The system is interactive because the vision system may
request a manipulatory action to resolve an interpretation
ambiguity, to reduce visual complexity, or to grasp and
remove an object from the scene. The manipulatory action
must be monitored by the noncontact sensor (vision system)
as well as the contact sensors (force/torque) in a closed loop.
Our assumptions are:
The domain is the class of irregular parcels and pieces
(IPP) found in a post office environment. The class consists
of rigid and nonrigid flats, boxes, tubes, and rolls. The
objects have different weights, sizes, colors, visual surface
textures (address labels, stamps, and other markings), vary-
ing porocity, coefficients of friction, and rigidity. Because
many of these objects are not rigid, their true geometric
shape cannot be measured; it is rather a function of where the
object is in a random heap, how it is supported by its
neighboring objects and other objects that it supports. The
heaps are formed by emptying a sack of an unknown mixture
of IPP's on a conveyor.
Our immediate goal is to physically segment and sort a
random heap of IPP's into several output streams of single
similar-shape objects. In other words, our first goal is to
disassemble random heaps of separable objects (held together
by gravity and friction) into three output streams. The first
stream contains two-sided planar objects (flats). The second
stream contains three-sided nonplanar objects (tubes /rolls).
The third stream contains six-sided planar objects (boxes).
We view this physical segmentation of disassembly prob-
lem as a subclass of the more general disassembly problem,
which we will address in the future. We believe that the
solution to the general disassembly problem is active sensing
[3], as opposed to the traditional static analysis of passively
sampled data. The problem of active sensing can be stated as
a problem of intelligent control strategies applied to the data
acquisition process that will depend on the current state of the
data interpretation including recognition. This approach is
gaining more and more recognition in the literature, [2], [5],
[10], [21]. In this paper we shall describe our model of
segmentation via interaction between vision and manipula-
tion. We will generate several segmentation strategies. We
will describe the experimental system and the experiments.
The model of segmentation has the following components:
models of sensors, models of actions, a task/utility model, a
world model, and a control model. The segmentation process
is formulated in terms of graph-theoretic operations that are
mapped into corresponding manipulatory actions.
Sensor models include the characterization of the noncon-
tact sensor such as the spatial resolution, signal-to-noise
ratio, and their like; the physical parameters of the different
end effectors, such as a vacuum suction cup; the size of a
spatula for pushing objects; the span of a gripper; and the
maximum allowable forces and torques. Models of objects
are specified in terms of their geometry, size, and substance.
Our world consists of random arrangements of objects
called heaps. Object models are boxes, flats, and tubes/rolls.
A heap is represented by a directed graph. Objects in the
heap are represented by vertices, and the on-top-of relations
among objects are represented by directed edges. A scene is
a partial view of a heap as sensed by the noncontact sensor.
A scene is represented by a directed graph, where surface
segments are represented by vertices and the on-top-of rela-
tions among the surface segments are represented by directed
edges.
It is important to emphasize that, in general, the diagraph
representing the heap is different from the graph representing
a scene. This is because the scene diagraph represents spatial
relations of only the visible surface segments, i.e., as they
appear through the visual sensor, which may not always be
the same as the physical objects. The true physical arrange-
ments of objects in the heap (i.e., the heap diagraph) is not
known, unless given a priori. Only the scene diagraph is
measurable and constructable from noncontact sensory infor-
mation.
In this paper, our task is to measure and construct the
scene diagraph and to use the manipulator to decompose it. In
future work we will show how to use manipulatory and
exploratory actions to recover the true part-whole relation-
ships (i.e., to compose an object diagraph from its scene
diagraph).
Task models include the final goal of the process. An
example of a final goal may be the empty scene. Intermediate
goals may be those scenes that are simply measured by a
cost/benefit function. This cost/benefit function entails the
cost of performing the particular manipulation, and the bene-
fit is measured via the estimate of the outcome of the
manipulation with respect to the final goal, i.e., emptying the
scene.
There are two types of actions: sensing actions (i.e., data
acquisition) and interpretation actions (such as: look, and /or
feel), and manipulatory actions, such as: pick, push, pull,
and shake. The purpose of the manipulatory actions is to
exert physical disturbances into the scene, being either global
(shake) or (push, pull). In view of our formulation of the
segmentation problem as a graph generation /decomposition
problem, we classify the manipulatory action in relation to
the operation that applies on the diagraph. There are two
such operations: the vertex removal, which means, in terms
of manipulation, removal of an object from the scene, and
edge removal, which in turn translates into object displace-
ment in the scene so that the on-top-of relationship does not
hold any more between the two objects. An isomorphism
exits between the manipulation actions and graph decomposi-
tion operations [23].
Our approach is to close the loop between sensing and
manipulation. The manipulator is used to simplify the scene
by decomposing the scene into visually simpler scenes. The
manipulator carries the contact sensors and the manipulation
tools to the region of interest and performs the necessary
manipulatory movements that will result in a visually simpler
scene. The control model deserves special attention and is
described next.
The control model is a nondeterministic finite-state Turing
machine (NDTM) and is shown in Fig. 1. The physical world
(scene) is the tape of the machine, the read actions are the
sensing actions, and the write actions are the manipulatory
actions. The model is a Turing machine because the manipu-
lation actions constantly change the physical environment
(tape) and hence its own input. The model is nondeterministic
because of the nonpredictable state of the scene after each
manipulatory step. From this of course follows also the
nondeterministic control of actions. In addition to the nonde-
terminism of the control strategies, the machine has finite
states, which are determined by the finite numbers of recog-
nizable scenes and the finite number of available actions.
This model is quite general, providing that one can quantize
the scene descriptions and the sensory outputs into unique
and mutually exclusive states, and of course one has only a
finite number of manipulatory actions.
As is well known, the nondeterministic finite-state automa-
ton (NDFSA) that controls the Turing machine is defined as a
quadruple (I, O, S,T) where:
Fig. 1 describes the sensing and manipulation interaction
for segmentation. Relating this diagram to the NDFSA, we
shall describe in subsequent subsections the inputs, outputs,
states, and the transition function, i.e., the control, respec-
tively. There are several advantages to the formalism of the
nondeterministic finite-state Turing machine.
I) Inputs: As indicated above, the inputs come from
sensors. In our current implementation, the sensor is a laser
range imaging system (noncontact sensor). The scene is
segmented into spatially connected surface regions. For each
region, we compute the position of the center of gravity, the
orientation of the surface normal at the center of gravity, an
estimate of the size of the smallest parallelepiped bounding
the region, and an estimate of the maximum curvature. From
these measurements, the objects are initially classified into
one of three generic shapes such as: flat, box, and tube/roll.
These are four object models.
The on-top-of relation between all pairs of visible regions
in the scene is computed and the directed graph representing
this relation is constructed. Vertices represent visible, con-
nected, surface regions. Directed edges represent the spatial
relations between the vertices. See Figs. 2-5.
Top-most surface segments are important in physical scene
segmentation because they may belong to top-most objects in
the scene. Top-most objects are important because they usu-
ally have more surfaces exposed (more ways to be grasped).
The forces required to extract them from the scene are less,
and therefore the chances of loosing positional information
after the object is being grasped are minimized. Furthermore,
manipulating the top-most object keeps scene disturbances to
a minimum.
A partially dispersed scene corresponds to a disconnected
diagraph. An efficient algorithm based on ''fusion'' of adja-
cent vertices is given in [8]. A totally dispersed scene (as
well as a singulated scene) corresponds to a null graph (a
graph with vertices and no edges). Efficient graph theoretic
algorithms exist (testing the diagraph's adjacency matrix for
all zero entires) for singulation verification. Finding the
top-most objects in the scene corresponds to topological
sorting of the diagraph.
2) Outputs: There are two types of outputs. These are
sensing actions, (look, feel) and manipulatory actions (pick,
push, pull, shake, and stop). In this implementation, the look
and feel actions are only commands to take data. In our
future work, these actions will be more complex, i.e., the
system will choose its view point, sampling rate, resolution,
and other data-acquisition parameters. In addition, the cost of
the sensing actions will be included in the overall control
schema.
The manipulation actions are composed hierarchically from
simpler actions. Shake is the simplest action; it provides
global disturbance and displacement to the work place. On
the other hand, push and pick exert local disturbance and
cause local displacement of an object. In fact in our imple-
mentation, both the push and pick actions have two forms:
''push with spatula,'' ''push with suction tool,'' ''pick with
gripper,'' ''pick with suction tool.'' See Fig. 12 below for an
example of a ''pick with suction tool'' action. In addition,
each of these manipulatory actions is associated with an
''error recovery'' action.
The hierarchy of actions is in terms of composition of
complex actions from simpler actions and does not apply to
the execution of these actions. The hierarchy of action com-
position is given in [23]. An example of such hierarchy is
shown for the action: ''pick with gripper'' in Fig. 6. Each
node in the graph in Fig. 6 is a manipulatory action. Some of
these actions are modeled as deterministic finite state au-
tomata (FSA), while others are modeled as nondeterministic
finite-state automata (NDFSA). The lowest level in the hier-
archy of actions consists of very simple actions, such as:
''robot move to'' (RMT), ''robot move to while sensing''
(RMTS), ''gripper move to'' (GMT), ''gripper move to
while sensing'' (GMTS), and their like.
The advantages of hierarchical construction are modular-
ity, testability, and incremental growth. These actions (as
expected) use additional information from contact sensors.
Some of the contact sensors are as follows. Two force /torque
sensors (mounted on the gripper jaws) are used in closed loop
feedback during manipulation. Force feedback is used to
provide force servoing to the gripper, to sense collisions, to
measure the weight of objects, and to determine if an object
or tool is properly grasped. A finger position sensor is used
in a closed-loop feedback manner during manipulation. Posi-
tion feedback is used to provide basic position servoing to a
gripper and to refine size estimates of objects (computed
from vision). A vacuum sensor is used to verify proper grasp
and to differentiate small-size nonpenetrating cavities from
holes that penetrate an object. Note that all the contact
sensory feedback is carried out in a local reflexive mode
rather than in a planned mode with one exception, that is,
when a pathological state is detected.
3) States: This is a finite set of states describing the
environment of the Turing machine as perceived by the
sensors. If new sensors are added, the set of states is parti-
tioned to describe the scene as perceived by the additional
sensors. For example, if a sensor capable of determining the
''touch'' relations of objects in the scene is added, then the
set of five states, can be partitioned (a finer partition) to
describe both the ''touch'' and ''on-top-of'' relations. The
states of the machine are:
The goal of scene segmentation is the empty state. This
state must be not only reachable but also measurable with the
current sensors. In other words, for the machine to halt, the
system must have sensors to sense that the goal state has been
entered. In this work, the empty state is both reachable (see
section on strategies) and easily measurable (all range values
in the scene are zero, which means that no surface segments
and thereby no objects exist in the scene).
A specific place must be given to error states. They are
prioritized in order of severity (most severe first). For more
details, see [23]. The pathological states are: ''sensor dam-
aged,'' ''unable to get tool,'' ''lost tool,'' ''lost object and
tool,'' ''lost object in the scene, '' ''lost object away from
the scene,'' ''unable to reach object,'' ''unable to pick,'' and
''unable to push.'' As more sensors and actions are added
into the system, more, yet finite, pathological states must be
defined. When the machine enters one of these states, error
recovery actions are evoked.
4) State Transition Function: The control problem is
transformed into the problem of topological sorting of object
arrangements. The manipulation actions of object acquisition
(pick) and local displacement (push) are defined as decompo-
sition operations on diagraphs representing the on-top-of
relation of objects in the arrangement. The pick action is used
to break the vertex connectivity of the diagraph by removing
vertices. Several tools may be used to implement this action.
An object may be picked and removed from the scene using
the gripper, or it may be picked by selecting a tool (i.e., a
suction tool). The push action is used to break the edge
connectivity of the diagraph representing the on-top-of rela-
tion, Several tools may be used to implement this action. An
object may be pushed using the gripper, or it may be pushed
by selecting a push tool (such as a spatula or the suction
tool). Complete planning of the push actions is very compli-
cated [14]-[17] and requires knowledge of the friction coef-
ficients of all objects in the scene as well as knowledge of the
spatial relations of all objects in the scene to decide where
and how far to push.
In Section II, we described a nondeterministic finite-state
Turing machine as the control model for sensing and manipu-
lation for scene segmentation. This very general model is
sufficient to describe every strategy for the following reasons:
Let us recall that the ''read from tape'' are the sensing
actions, ''write to tape'' are the manipulatory and error
recovery actions, and the states are scene descriptions. Even
with the restriction that one can categorize every scene into
distinct classes (discrete states) we had to add the following
rules:
With the above rules and the theory described in Sections
II and IIII, we can compose several different strategies to
examine the validity and generality of our theory for scene
segmentation.
Strategy 1 is a noninteractive loop: (look, pick, look,
.. .). The control structure is shown in Fig. 7. The strategy
does not use local displacement (push). The general idea is to
look, pick the top-most object, and look again. If the scene is
ambiguous or unstable, it shakes the heap. If shaking fails, it
continues with the pick action. This strategy is simple and
very effective in dealing with scenes where all objects are
graspable with the set of acquisition tools. The strategy
eliminates ambiguities via the shake and pick actions. If the
shake action fails to remove the ambiguity, then nontopmost
objects are picked up. This causes objects to be lost during
acquisition. For the strategy to succeed, the sensor thresholds
must be raised to enable the system to tolerate higher torques
caused by picking objects off the center of gravity. When the
threshold is raised, the probability of tool losses increases as
well as the probability of damaging the sensors. Therefore,
the probability of entering the fatal error state is increased. If
the weight of the objects is low, the probability of damaging
the sensors (even if the system picks objects supporting other
objects) is low, and the strategy converges; see [23].
Strategy 2 is a noninteractive loop: (look, push until
dispersed, pick, look, . . . ). The control structure is shown in
Fig. 8. This strategy allows no interaction between the pick
and push actions. The only interaction allowed is when the
push action cannot reduce the number of edges in the associ-
ated graph any further. The strategy enforces a rather strong
partition on the manipulation actions. This shows up as a
serial plan where a single action is triggered from one state
and the automaton iterates until the ''look'' action brings the
automaton to another state. This strategy is very effective in
dealing with heaps of few, small-sized objects relative to the
workspace. As object size and number increases, so does the
number of objects pushed out of the scene and never picked
up. For a proof of convergence, see [23].
Strategy 3 is an interactive loop: (look, pick/push,
look, . . .). The control structure is shown in Fig. 9. The
central idea is to allow immediate interaction between the two
manipulation actions. Since pick is more effective than push,
priority is given to pick. Only if an object cannot be picked
up after several unsuccessful attempts is the next immediate
action to push that object, and to immediately return to pick
Data structures used in machine vision are often
classified as either ''iconic'' or ''symbolic''. An iconic
data structure is one whose principal organization is
that of a two-dimensional array. The elements of the
array may be bits, integers, real or floating-point
numbers, or more complicated structures. Each cell
of an iconic data structure is implicitly associated
with a location in two-dimensional space, by virtue
of its being indexed by two numbers. On the other
hand, a symbolic data structure, although it may
represent pictorial information, takes the form of a
scalar, list, graph, string, or table. Unlike an iconic
structure, spatial information, if it is to be included
in a symbolic data structure, must be explicitly repre-
sented, for example, by including coordinate pairs
in the structure. It is usual for iconic data structures
to be used for image data (including ''intrinsic
images'') and symbolic ones to be used for more
abstract information such as scene descriptions in
terms of regions and relationships, highly codified
shape descriptions, and semantic models or inter-
pretations.[')
Corresponding to the distinction between iconic
and symbolic data structures in machine vision, there
isa separation of ''retinotopic'' and ''nonretinotopic''
descriptions of neural areas in the neurophysiology
of the mammalian visual system. A retinotopic area
is one in which the neural activity is generally in
spatial correspondence (i.e. mapped in continuous
fashion) with the activity in the retina. A non-
retinotopic area is one in which the activity shows
no general spatial dependence upon the distribution
of activity in the retina.
Several factors motivate the study of special iconic/
symbolic architectures. Most importantly, parallel
image-processing systems such as the CLIP4 and
the MPP,) while very effective for point-neigh-
borhood image transformations, lose much of their
speed advantages when they must communicate with
single-processor hosts in computations of such non-
iconic transforms as chain encodings, polygonal rep-
resentations, Hough transforms, region-adjacency
graphs, syntactic descriptions of shape, or schema
instantiations.
A second factor is the awkwardness of computing
''hypothesis maps'' for topdown image analysis on
existing architectures; such operations seem to call
for special symbolic-to-iconic hardware that can per-
form certain kinds of ''plotting'' very rapidly.
A third motivating factor is that a study of the
architectural problems of iconic/symbolic trans-
formations may suggest new computational models
for related information-processing activity in natural
(human and animal) visual systems, and conse-
quently, to improve our understanding of natural
vision.
We note that, in general, multiprocessor systems
exist which can support both iconic and symbolic
processing, but these are expensive, and are non-
optimal for iconic-to-symbolic processing. Examples
of these include the following commercial and
research prototype systems: the BBN Butterfly,
Columbia University's Non-Von, the NYU Ultra-
computer, the CalTech Cosmic Cube, and others. A
survey of some of these parallel systems may be
found in reference (4).
In the following sections we present a discussion
of the relative strengths and weakneses of special
architectures that support rapid iconic-to-symbolic
and symbolic-to-iconic transformations.
One can classify the proposed architectures for
iconic/symbolic processing into two types: (1) hard-
wired transformation devices and (2) interfaces. The
hardwired devices provide specialized computing
capability for particular iconic-to-symbolic trans-
formations. The interfaces, on the other hand, make
it easy for iconic processors and symbolic processors
to communicate efficiently and leave the actual pro-
cessing to these more conventional components.
Four particular approaches are reviewed in this
article. Two of these are hardwired transformation
devices and two are interfaces. Each of the four
approaches is presented here in a somewhat abstract
form. That is, each method is described with a com-
putational model which is somewhat simpler than
what one might actually implement. Nonessential
features are omitted from the models to facilitate
comparison and analysis of the basic approaches.
Each of our four models is based upon a proposed
device or system that has appeared in the literature.
Our ''Image-Function Inverter'' is modeled after the
'ISMAP'' (Iconic/Symbolic MAPper) proposed by
Kent at the National Bureau of Standards and sub-
sequently developed by Aspex, Inc. of New York.')
The ''Chain-Run Encoder'' described here is based
upon a device designed by Pfeiffer at the University
of Washington.''' (Pfeiffer is now at New Mexico
State University.) The ''Bi-Modal Memory'' system
has been proposed by Tanimoto, and the ''Tile-
Based Interface'' is a model inspired by the
''CAAPP/SNAP'' (a joint effort between the Uni-
versity of Massachusetts-'-' and the University of
Southern California'? which was an important step
in the development of the DARPA Image Under-
standing Architecture). The first two of these pro-
posals fall into the hardwired-device category. The
other two are essentially interface schemes.
At the time of this writing, one of these devices
has been built and tested. The ISMAP is the least
complicated and is now available as an option for
the PIPE from Aspex, Incorporated of New York.
Two others have had chips designed and fabricated
(the BMM and the chain-run encoder), but the chips
have not yet been integrated into an overall system.
These four approaches give one perspective on the
range of problems and possible solutions that arise
in studying the issue of iconic/symbolic architecture.
Figure 1 is a chart which offers an analysis of the
appropriateness of each of these four architectures
to the iconic-to-symbolic computations listed. It is
assumed that each iconic/symbolic architecture
would be used in the context in which it is proposed.
lLater sections of this article describe some of the
algorithms involved in this analysis.
The transformations between iconic and symbolic
representations of pictorial information constitute
what may be called ''intermediate-level'' computer
vision. This general problem of intermediate-level
vision has been explored in a collection of papers,'')
with a working definition given in the first of them.'!l!
The three levels of processing for computer vision
are therefore the following:
Although this report is concerned primarily with
the intermediate level, the other levels must be men-
tioned at least to the extent to which they constrain
or help to solve the intermediate-level problems.
One can organize a treatment of intermediate-
level computer vision by focussing on several selected
problems. Let us consider six:
terms of its runs. Such an encoding may consist of
the starting coordinates for a contour, and a list of
runs, each described by a symbol (direction) and a
repetition count. Alternatively, each run may be
described with a pair of endpoints (since the run
represents a straight line segment). Several vari-
ations are possible, making use of absolute or relative
coordinates, explicit or implicit coordinates, runs in
order or out of order, etc. A chain-run encoding may
be a preliminary step towards obtaining the chain
code, or it may be regarded as an alternate form of
the chain code which, for some images, may be more
compact a representation than the chain code.
4, Hough transform. In order to detect the pres-
ence of lines or curves in an image, the Hough
transform or a generalized form of the Hough trans-
form may be used. For lines, the Hough transform
works by quantizing the space of possible values of
the parameters for the line equation. Each ''bin'' of
the quantized space is regarded as an accumulator
which accumulates evidence for the presence of a
line with a particular pair of parameter values. For
each pixel of the source image, a set of bins is
identified and a weight is added to each bin; the
weight is proportional to the extent to which the
source pixel seems to be part of a line (e.g. how
bright the pixel is).
5. Partial Hough transform. Rather than compute
the evidence for each of all the possible lines in the
image,it is often sufficient to determine the evidence
for the most prominent (one or a few) lines. When
this is sufficient, much of the computation can be
eliminated.
6. Region-adjacency graph. Some kinds of
machine vision require that a symbolic description
of a scene be computed before the objects are ident-
ified in the scene. One basis for a symbolic descrip-
tion is a graph structure called a ''region-adjacency
graph'', The problem of computing a region-
adjacency graph (RAG) is to take a segmentation
map (a two-dimensional array in which each element
contains the unique region-number for the region to
which it belongs; each region is a four-connected set
of pixels), and to produce a graph having a single
node for each region, and having an edge between
two nodes if and only if the corresponding regions
are adjacent (share a common boundary). A region-
adjacency graph is typically represented using adjac-
ency lists.
These six problems form a representative sample
of iconic-to-symbolic transformations. We use them
here to compare the four model architectures for
iconic/symbolic computing.
Other sets of intermediate-level operations have
been suggested as a basis for comparing architectures
such as the set consisting of area, perimeter, con-
nected components, convex hull, closest points, and
diameter.'? This set emphasizes problems of com-
putational geometry, which are, no doubt, of wide
interest. The set presented in this paper, however,
focusses on pixel-value/coordinate relation inver-
sion, chain encoding, line detection and sum-
marization of region adjacency. Such a set covers a
variety of image processing styles that are more
typical of picture-processing applications, and this
set is particularly useful in bringing out major dif-
ferences in the architectures under discussion.
Several strategies can be used to come up with
machines that can efficiently handle iconic-to-sym-
bolic transformations. One approach is to begin with
an iconic image processor and then augment it gradu-
ally, adding capability for more and more symbolic
or abstract computations.This approach has received
the attention of theoretical computer scientists and
has supported some interesting mathematical
results,)
Another approach is to attempt to tightly integrate
radically different kinds of processors, each of which
is optimized for a particular style of processing. This
approach can produce powerful systems that are
practical and easier to program than totally new
architectures. However, they may be harder to char-
acterize theoretically in an elegant way. The com-
bination of a pipelined image processor with the IFI
is a good example of such an architecture. The BMM
together with an iconic subsystem and a symbolic
processor network is another.
A third approach is to try to make general MIMD
systems cheaper and more parallel than they are
now. If the cost of the general-purpose systems could
be brought down far enough, and the parallelism
increased enough, they would be more effective for
iconic-to-symbolic transformations than they are cur-
rently.
Next is a presentation of several particular archi-
tectures, and descriptions of how they handle iconic-
to-symbolic data transformations.
A device that we shall call the IFI (Image-Function
Inverter) is modeled after hardware proposed by
Kent in conjunction with Aspex, Inc of New York
to lend extra capability to the PIPE (''Pipelined
Image Processing Engine'') system. That device is
known as the 'ISMAP'' (Iconic/Symbolic MAPper).
The ISMAP is consistent with the PIPE convention
that computations are organized according to the
video field rate (i.e. one field is processed in each
one-sixtieth of a second). Consequently, the ISMAP
(and our abstraction, the IFI) scans an image at the
frame rate, in its normal operating mode.
Since the PIPE is a system which transforms
images into images, it is an iconic processing system,
and it does well at low-level computer vision tasks
such as filtering an image. On the other hand, the
IFI is designed to compute more global features of
an image such as the counts one obtains in a histo-
gram of pixel values. The IFI is illustrated schem-
atically in Fig. 2.
The normal operation of the IFI consists of three
steps. In the first step, the source image is scanned,
and a histogram of pixel values is produced by the
IFI in a special memory buffer. The second step
produces a cumulative histogram from the normal
histogram. The cumulative histogram contains, in
each bin, the number of pixels of the source image
that have a value less than or equal to the bin index.
In the third step, the image is rescanned and for each
pixel value, the IFI makes a list of the coordinate
pairs at which that pixel value occurs in the image.
The cumulative histogram gives the starting address
for each such list.
The IFI is an ideal device for inverting the pixel-
value/coordinates relation. In a sense, it transforms
the image representation from a coordinate-oriented
one into a feature-oriented one. The principal motiv-
ation for this operation is to permit the host to search
only the list of potentially relevant coordinates when
seeking global geometric relations among selected
types of features. The IFI could be extended to
permit its mapping to be inverted', however, we
shall not consider these possibilities here.
In spite of its simplicity, the IFI appears to be a
rather useful device. Two novel algorithms which
use it are described later.
A device has been designed?' which would accel-
erate the conversion of binary images into chain
codes, in conjunction with a ''systolic'' cellular array
computer. Pfeiffer's device, consisting of a VLSI
chip that receives inputs from the systolic array,
treats one column of a binary image at a time. It
rapidly scans the column, identifying the pixels in
which the value 1 is found. If, in the sequence of
columns processed, the value in a row changes from
one column to the next, then the chip outputs an
indication that a horizontal segment either began or
ended in that row (a status bit keeps the state of each
run). It is intended that two to four of these chips
be used for one iconic processor: one processing
columns (as described), another processing rows,
and optically two more of them processing diagonal
lines. The outputs of the chips would be collected by
one or several Von-Neumann-style processors and
sorted into run-length-compressed chain encodings.
For purposes of comparison with other
approaches, we model this device so as to omit the
details of how columns are scanned. Our model,
referred to as the CRE (for Chain-Run Encoder)
simply takes N binary inputs in each of N steps and
outputs a list of events where each event is of the
form (i,e)where itells which of the N inputs changed
and e tells whether the change was from a 0 to a l
or from a 1 to a 0.
Let us consider how two or four CRE units would
be used with an iconic processor and a symbolic
processor to determine chain encodings of edges in
an image. In order that each of the four encoding
devices receive only ones for the pixels that form
parts of edge segments in the appropriate direction,
the iconic processor must produce four separate
binary images, one for each direction. Figure 3 shows
how two chain-run encoders could be integrated
with a cellular-array processor and a collection of
microcomputers. It is possible to use a single chain-
run encoder for runs in all four directions (sequen-
tially) by providing additional switches and inter-
connections to permit either row data or column data
to be fed into the device.
In order to provide a high-bandwidth interface
between a parallel image processor (such an MPP or
CLIP4-like system, or a pyramid machine'''') and a
collection of microprocessors, a ''bimodal'' memory
system has been proposed''! that would support
image-wide, parallel transfers with the parallel pro-
cessor, and ordinary byte-at-a-time transfers with
the microprocessors. By suitably partitioning the
memory, many microprocessors could access por-
tions of the memory simultaneously, and the parallel
processor could access a portion while most of the
microprocessors access portions. A clean implemen-
tation of the bimodal memory requires a memory
chip not commercially available at present. An MPP
or Pyramid, plus BMM, would provide a system
that is powerful, yet conceptually simpler than some
proposed extentions to pyramids and arrays.'' At
the same time, it is less expensive than purely MIMD
systems such as the Ultracomputer,!''V The BMM is
shown between an iconic cellular array and a col-
lection of microcomputers in Fig. 4.
What the BMM permits is a flexible scheme for
sharing work between a parallel image processor and
a collection of microprocessors. One can imagine
that the images are stored in shared memory that is
accessible to the parallel image processor at the same
rate that the image processor's ordinary memory is
accessible. Similarly, the shared memory is accessible
to a microprocessor as fast as, and as simply as if it
were ordinary memory on the bus.
In order for such a system to compute a histogram
CELLULAR pyramid is an exponentially tapering stack of
arrays of processors (''cells''). Communication between cells
on successive levels of the stack allows global analysis of data
input to the base of the stack in log(base sie) steps. Cellular
pyramids support fast parallel algorithms for multiresolution im-
age analysis. See the books edited by Rosenfeld [31], Cantoni and
Levialdi [8], and Uhr [37]; for solving computational geometry
problems [25], with the image input to level 0, the base of the
pyramid.
Usually the cells on each level are connected to form a square
lattice but triangular or hexagonal grids have also been used
[2], [6], [14]. A cell on level 1 + 1 (the parent) is connected
to a K x K neighborhood of cells on level l (its children).
Neighborhoods associated with adjacent parents overlap by K-2
cells along both directions, yielding a fourfold reduction in
number of processors (twofold along each side of the square),
however, twofold reductions can also be achieved using modified
architectures [11], [18].
We restrict ourselves here to pyramids defined on a square grid
with fourfold reduction between successive levels. If an image
is input to the base of the pyramid, we can generate reduced-
resolution versions of the image at higher levels. Usually the
value of the parent is a weighted average of the values of its
children and the same set of weights is employed at every level.
Burt [7] defined rules for which the set of weights converges to
sampled Gaussians with increasing standard deviations. Optimal
weights have also been proposed [24].
Let the base of the pyramid be of size N + 2'' 2'*. Then the
lth level has size 2''' y 2*', so that the total number of cells
is less than }N%, The height of the pyramid, ie., the number of
levels, is n = log N. Many image analysis tasks which require
O(N%) operations on a single processor can be accomplished in
O(log N) on a cellular pyramid.
When a pyramid is used to reduce the resolution of an image,
features of the input image become smaller and move closer
together as one proceeds from the bottom level of the pyramid
to its apex. Thus at the appropriate level, local operations are
sufficient to detect and analyze global features (see [31] for
numerous examples). Reduced resolution representations are also
useful in image compression applications (e.g., [1]).
The case of K = 2, ie., nonoverlapping 2 x 2 neighborhoods,
is related to the quadtree description of an image [34]. The
reduced resolution representations can be severely distorted when
the input is shifted [36]. This problem is known in the quadtree
literature as the shift-dependence of the description [20]. In the
worst case a one pixel shift of the input image can lead to a
significantly modified quadtree structure [35].
The dependence of the low resolution representations on
the position of the sampling grid and the input image is also
important in image pyramid applications. The shift-dependence
phenomenon is not restricted to the case of nonoverlapping
neighborhoods. Bister [5] shows many examples of such artifacts.
The rigidity of the pyramid structure may give rise to ar-
tifacts when pyramids are used for tasks such as analysis of
line-drawings [19], object-background discrimination [10], or
compact object extraction [13], [16]. To compensate for these
artifacts, in many of these algorithms the parent-child links (or
link weights) are iteratively changed after the initial resolution
reduction stage. Recently Baronti et al. [4] proposed a modifica-
tion of this concept by increasing the size of the neighborhoods
associated with parents once an initial segmentation of the image
is obtained.
Another approach to compensating for the artifacts of pyramid
structure is to adapt this structure to the content of the input
image. In custom-made pyramids [28] weights are defined based
on a local ''busyness'' measure during the construction of the
reduced resolution representations. Rom and Peleg [29] and
Chassery and Montanvert [9] employed the Voronoi tessellation
defined by a set of randomly chosen lattice points to build the
coarsest representation of the image, which was then adaptively
refined. Note that the method computes the representations top-
to-bottom.
In this paper we also use irregular tessellations to generate
an adaptive multiresolution representation of the input image.
In our approach, however, the hierarchy of representations is
built bottom-up and is adapted to the content of the input image;
thus most of the properties of ''classical'' image pyramids are
preserved. We employ a local stochastic process to build the
lower resolution representations.
In Section II we introduce the graph formulation of irregular
tessellations and the concept of a stochastic image pyramid.
In Sections III and IV we give two applications of stochastic
pyramids: connected component analysis of labeled images and
segmentation of gray-scale images. Further issues are discussed
in Section V.
In image pyramids based on regular sampling, <.g., at points
on a square grid, artifacts caused by the rigidity of the sampling
structure are always present. On the other hand, an image
pyramid defined by an irregular sampling hierarchy can be
molded to the structure of the input image. Note, however, that in
such a pyramid the metrical relations among cells are no longer
carried implicitly by the sampling structure. A cell at level ! + 1
cannot know a priori where its neighbors on level I + 1 or its
children on level l are located relative to the original sampling
grid. To describe the structure of such an image pyramid it is
more appropriate to use the formalism of graphs.
The cells on level l of the pyramid are taken as the vertices of
an undirected graph G[l]. The edges of the graph describe the ad-
jacency relations between cells at level l. Thus G[l] = (V[l], E[l])
where V[l] is the set of vertices and E[l] is the set of edges. The
graph G[0] defined by the 8-connected square sampling grid on
level 0 is shown in Fig. 1(a). An example of a graph G[1] that
might represent level 1 is shown in Fig. 1(c).
We construct the pyramid by a sampling or decimation process.
Each level is constructed from the level below it by selecting a
subset of the vertices. Thus a vertex on any level can be regarded
as a vertex of G[0], the sampling grid of the original image. In
addition, when we decimate level I to construct level I + 1. we
associate each nonsurviving vertex with one of the surviving
vertices. Thus each vertex on level I + 1 is associated with a
set of vertices on level l (itself and the nonsurviving vertices
associated with it). Each of these vertices is in turn associated
with a set of vertices on level I - 1, and so on; thus a vertex o
any level is associated with a set of vertices, called its ''region,
in the original image. These regions define a tessellation of th
image.
If the pyramid is to be build recursively bottom-up we mus
define a procedure for deriving G[l + 1] from G[l]. Since th
number of vertices in G[l + 1] must be less than in G[l] we ar
dealing with a graph contraction problem. We must design rule
for:
In order to have any vertex (ie., cell) in the hierarch
correspond to a connected region of the image, the cell c[l -
1] g V[! 4 1] must represent a connected subset of cel'
(ca[l],ci[l],- ,c,[]} c V[I]. We shall use the conventio
cs[l] sz c[l + 1], ie., the surviving vertex of the subset is fir-
on the list. In pyramid terminology, [ca[l], ci[l], -,c,[]} a
the children of c[l + 1]. Note that the location of the parent o
the sampling grid of the original image always coincides wii
the location of one of its children.
In pyramid construction based on Voronoi tessellations [9
[29] the parents are initially chosen by a random process. TH
edges are given by the Delaunay diagram of the tessellation ar
the children are grid sites inside the tiles associated with tt
parents, The process can then be repeated for individual til
(by randomly choosing grid sites inside each tile) to obtain
finer description. Note that such a pyramid is built top-down ar
the definitions of parents and parent-children links are based c
nonlocal processes.
When we use graph contraction to construct a pyramid, tw
constraints must be satisfied if we want to employ only parall
local processes:
where c4, d are survivors on level I. Constraint (1) assures th
any nonsurvivor on cell at level I has at least one survivor in i
neighborhood and thus can be allocated to a parent by a loc.
decision. In the example shown in Fig. 1(b) this constraint
satisfied. Constraint (2) assures that two adjacent cells on lev.
I cannot both survive and thus the number of vertices mu
decrease rapidly from level to level. In Fig. 1(b) this constrai:
is not satisfied since the survivors da, es and co, 9o are adjacen
The construction of G[ + 1] can also be regarded as findir
a maximal collection of vertices of G[l] no two of which a:
adjacent. This is the maximal independent set problem for grapl
(eg., [21]); we will return to it in Section V.
A possible alternative method of constructing G[l + 1] fro
G[l] is to partition G[l] into connected subgraphs and the
select one cell in each subgraph as a survivor. However,
we do so, the first constraint no longer assures locality of th
processing. In Fig. 1(b) cell b, has survivor cg adjacent to i
but must be allocated to survivor b two sites away. Choosin
the survivor independently for each region may also violate th
second constraint since two adjacent regions can both have the
survivors at the border [Fig. 1(b)]. Thus the set of children shou:
be defined in G[l] only after the vertices of G[l + 1] tthe
parents) have been chosen.
The last step in constructing G[ + 1] is to define the edges
E[1 + 1]. Let the connected subsets ca[l],ci[l],- ,c,[l]} C
V[l] and [d.[4],di[l],-- ,d,[]} c V[l] be he children of wo
different parents. Our condition for an edge between vertices
c[{ + 1] s cs[] and d[l + 1] sE d[] in G[l + 1] is
In other words, two vertices are joined by an edge in G[I + 1]
if there exists a path between them in G[l] of length at most
three edges. (Note that by (2), the path cannot be of length 1.)
G[l + 1] is now completely defined. Fig. 1(c) shows, the graph
corresponding to the partition in Fig. 1(b).
The irregular sampling hierarchy is thus built recursively from
G[0] (the original sampling grid). The apex of the hierarchy G[m]
has only one vertex. Constraint (2) assures that the apex is always
reached.
In the next section we describe a probabilistic parallel algo-
rithm that constructs graph contractions satisfying (1) and (2).
The algorithm is analyzed in more detail in [22], [23].
We have seen that the derivation of G[l + 1] from G[l] must
start by defining the vertices of the new graph. Since V[l + 1] C
V[l] we are dealing with a decimation process, ie., only a subset
of the vertices V[I] are retained. We want the decimation to be
performed in parallel on G[l].
We will define a decimation process that is dependent on the
image data. We assume that every cell c; (a vertex of G[I]) carries
a value g, characterizing its region of the image-for example,
the average gray level of the region. Without loss of generality
we can assume that g; is a scalar value; the treatment of feature
vectors is identical. From now on the explicit indication of the
level l will be dropped to simplify the notation.
Let cell cs on level have r neighbors on level l, ie., let its
degree as a vertex of G[l] be r. (Note that for the moment cg is not
necessarily a survivor and the set of its neighbors has no relation
with the connected subset allocated to a parent.) We examine
every neighbor c;, i = 1,--,r of cs and decide whether or not
it belongs to the same ''class'' as c. This decision can depend in
any desired way on the values g,,i = 0,---,r. We associate a
binary number A;, i = 0,--,r with each neighbor, where A, == 1
if c; belongs to the same class as cs, and A, = 0 otherwise; note
that A4 = 1,
The decimation algorithm employs three variables for every
cell: two binary state variables p and , and a random variable
uniformly distributed between [0, l] with outcomes x. The sur-
vivors are chosen by an iterative local process. Let k =< 0, 1,---
be the iteration index. Initially all p,(0) = 0. A cell survives if at
the end of the algorithm its pg(k) state variable has the value 1.
Every'iteration has two steps. First ;4(k) is updated based on
the states p;(k - 1) of neighboring cells in the same class:
In other words, g4(k) becomes 1 if and only if there is no survivor
among the cells belonging to the same class in the neighborhood
of cg, Note that the neighborhood includes the cell itself. The
initial conditions always yield ;4(1) = 1. Then pg(k) is computed
on the updated values of ;(k):
To become a survivor the outcome of the random variable
x drawn by the cell must be the local maximum among the
outcomes drawn by the neighbors in the same class. Note
that only those neighbors are taken into account which do not
already have a survivor adjacent to them (q;(k) = 1). This
condition extends the region of influence of a cell beyond its
immediate neighborhood and yields faster convergence of the
algorithm. The local maximum property assures that (2) is always
satisfied. The state of a survivor is not reversible. Once a cell
is labeled pa(k - 1) = 1, at subsequent iterations the product
g4(k)g(k) (5) is always 0 by the definition of ;4(k) (4). Thus
in (5) the second condition, preserving the current state is used.
It can be shown [22], [23] that after a finite number of iterations
(at most five, in the experiments reported there) the algorithm
reaches a final global configuration in which the survivors satisfy
(1) as well.
The algorithm is entirely local, every cell computing its states
based only on the states of its immediate neighbors. Except at the
highest levels of the hierarchy, where due to the small number
of vertices artifacts may occur, the decimation ratio between two
consecutive levels exceeds four. This lower bound results from
the fact that two adjacent cells never survive. On the random
graph structure of higher pyramid levels the average degree of
a vertex is around 6 [23]. To satisfy the nonadjacency condition
(2) the number of vertices must be reduced by about the same
order relative to the previous level.
By employing the algorithm an irregular sampling hierarchy
can be built in parallel in log(class size) steps. (The distinction
between class size and image size becomes clear in the next
section.) The stochastic decimation is performed independently
within classes. In the next two sections we describe two appli-
cations of the process, one to connected component analysis of
labeled images, the other to segmentation of gray level images.
In a labeled image the pixels are classified into a small
number of classes distinguished by different labels. A connected
component is a maximal set of connected pixels sharing the same
label. For simplicity we will restrict ourselves to the case where
there are only two labels, i.e., to the case of a binary image,
but images with multiple labels can be handled in essentially the
same fashion.
Sequential algorithms for analyzing the connected components
in a binary image usually employ a row-by-row scan [30]. An
alternative approach makes use of the quadtree representation of
the image [34]. In this section we apply the techniques described
in Section II to obtain in log(class size) steps a description of the
connected components in a binary image. The description takes
the form of a graph whose vertices represent the components
and whose edges represent the adjacency relations among the
components.
The fact that the pixels are labeled makes classification of the
neighbors of a cell immediate. Let the label of cell c; be gi-
In the binary case the label can have only two values. Thus in
the neighborhood of cell c, we have for i = 0,---,r the class
membership variables
Note that (6) is symmetrical; cs gets the same value of A in the
neighborhood of c; as c; gets in cs's neighborhood. Since the
definition of A, is symmetrical it can be regarded as the weight
of the edge (ca, c;). The case A = 0 is equivalent to removing
the edge from E[l]. Let E'[l] be the set of edges having M = 1,
and let G'[l] = (V[l], E'[1}). The connected components in the
labeled image are represented by connected components in the
graph G'[l], for all 1 2 0.
The subgraphs of G' are processed independently, each sub-
graph being recursively contracted into one vertex, the root of the
connected component. The contraction process is based on the
technique described in the previous section: first the survivor
vertices are designated and then the nonsurvivor vertices are
locally allocated to survivors. If a nonsurvivor has more than
one survivor neighbor it chooses the one carrying the largest
outcome of the random variable x from the last iteration of
the decimation process. Because the neighbors are neighbors
in G', the survivors can only have children belonging to their
own class. Thus from each connected component of the input
image a pyramidal hierarchy of irregular tessellations is built in
O(log(component size)) steps.
The different hierarchies may have different heights, but in
log[max(component size)] steps the entire image is reduced to
roots, This situation is detected at the level m when E'[m]
becomes empty. Evidently component size can differ from im-
age 5ize. For example, a connected linear pattern passing through
every second row of the image has length N(N + 1)/2 pixels.
Since the hierarchy is built over the pattern the number of levels
depends on its intrinsic diameter.
At each level, the graph G[l] includes edges between cells
that arise from different labels; it preserves the spatial relations
among the connected components. At the root level, G[m] is
the adjacency graph of the original labeled image; it has one
vertex for each connected component and its edges represent the
adjacencies between these components.
Fig. 2.(a) shows an example of a graph G[l] superposed
on the binary image from which it was derived. The induced
graph G'[l] is shown in Fig. 2(b). Note that in G'[l] each
connected component corresponds to a connected subgraph. The
cells surviving level l and the allocation of the nonsurvivors are
shown in Fig. 2(c). The graph G[l + 1] of the next level is shown
in Fig. 2(d) and the corresponding graph G'[! + 1] in Fig. 2(e)
Level 1 + 2 is the root level and its graph G[m] is shown in
Fig. 2(f). It correctly represents the adjacency relations among
the three connected components of the image: the background
and the two blobs.
In Fig. 3 a checkerboard image and the adjacency graph of it
root level are shown, The checkerboard is a ''worst-case'' image
the two connected components (both defined by the relation o
eight-connectedness) being distributed across the entire input
Cibulskis and Dyer [10] employed a regular pyramid structur
to segment this image. In their results the ''white'' componen
was allocated to one root at the apex, but the representation o
the black squares had to be spread over several levels. The siz.
of the image is 64 x 64 and the two roots were obtained a
the eight level of the hierarchy. Recall that the height of th
hierarchy depends on component size. Since random processe:
are involved in the construction of the irregular tessellations th
location of the roots depends on the outcomes of local processes
Nevertheless, the same root level adjacency graph is alway
obtained at the top of the hierarchy.
The famous connectedness puzzle of Minsky and Pa
pert [27, Fig. 5.1] can be solved by our technique ii
O(log(component size)) steps. The pattern in Fig. 4(a) contain
three black and two white bands, while in the pattern in Fig. 4(b
the two white bands are connected, leaving only two black bands
The adjacency graphs obtained at the root level clearly show th
different topologies.
The irregular tessellations that arise in the hierarchies define
by the connected components do not convey meaningful rep
resentations at intermediate levels. Let us define the receptiv
field of a cell on level I as the set of all the pixels at level (
(input) associated with it. This field is always a connected se
and the image is the disjoint union of the fields. Also, each field
is a subset of a connected component of the image. In Fig. 5 the
receptive fields of four levels of a hierarchy derived from a simple
image are shown. The different fields are randomly colored to
emphasize their shapes. At intermediate levels the shapes of the
fields are arbitrary, since they depend on the outcomes of random
variables. At the root level each field is a complete connected
component.
Our multiresolution representation consists of several inde-
pendent hierarchies, each built independently over a connected
component. The shape, size, position, and orientation of the
connected component have no influence on the final result.
The individual hierarchies can be used for the fast recovery of
geometrical properties such as area or perimeter [32]. It should
be mentioned that Miller and Stout [26] also proposed a data
structure in which a separate ''essential'' regular pyramid is built
over every object.
All the discussion in this section was restricted to binary
images. If more than two labels are used the discussion is
essentially identical. In particular, our method can be used to
label the connected components of constant gray level in an
arbitrary digital image. In the next section we study the less well-
defined problem of segmenting a gray level image into ''natural''
regions.
In gray level images the difference between the values of
two adjacent pixels is bounded below only by the size of the
quantization step. In our technique, to build the hierarchies
the pixels in a neighborhood must be assigned to classes. The
class membership induces the graph on which the stochastic
decimation takes place. For labeled images the classes correspond
to the labels and the hierarchy always converges to the same final
representation: the adjacency graph of the connected components
defined by the labels. For gray level images it is no longer
obvious how to define the classes (unless our goal is to segment
the image into connected components of constant gray level). In
the first part of this section we discuss this problem.
The simplest approach is to define class membership by
thresholding the gray level differences between the center cell
c and its neighbors c;, i = 1,-,r. The class membership
variables A, are thus defined by
As in the labeled case, (7) based on an absolute threshold T is
symmetrical. This symmetry, however, can create artifacts when
we attempt to segment gray level images, as we show in the next
example.
Fig. 6 shows an object having four gray levels on a white
background. The graph G[l,] of an intermediate level is shown
superposed on the image in Fig. 6(a). Let the differences between
the gray levels be less than T and let the two lighter gray
levels be within T of the background. The resulting graph G'[l,]
is shown in Fig. 6(b). Note the edges connecting regions that
have different colors. The stochastic decimation algorithm selects
survivor cells and the nonsurvivors are allocated to their most
similar surviving neighbors. The survivors (parents) compute new
gray level values based on their children. After a few more levels
of the hierarchy we might arrive at the graph G'[l,], > i,
shown in Fig. 6(c). The difference between the gray levels of
the two cells located in colored regions now exceeds T and in
G'[lg] these regions are no longer connected. If a different set
of outcomes of the random variables had been employed in the
stochastic decimation process, a different set of surviving vertices
might be obtained, and the new parents might have different gray
level values, yielding a new graph at level l4 [Fig. 6(d)]. We
conclude that using a symmetric class membership criterion for
gray level image segmentation strongly influences the structure of
the hierarchy and therefore the final representation of the image.
Our next example, a ramp image, shows the severity of
the resulting artifacts. In Fig. 7 (top-left) the image of a ramp
going from level 0 (black) to level 255 (white) is shown. The
difference between adjacent rows of pixels is either four or
five gray levels, depending on the quantization error. The pixel
values are the same along each row. The receptive fields of the
root level obtained for T = 33 are shown in Fig. 7 (top-right).
The color of a region is the gray level value computed by its
and
The ability of microorganisms to regulate their cytoplasmic composition
is, of course, a metabolic necessity, and techniques designed to impair
such regulation therefore provide means for inhibiting microbial growth.
One technique, of particular importance in the context of food preserva-
tion, makes use of weak lipophilic acids, in low concentrations, to induce
cell acidification and disrupt osmotic balance. To explain and predict such
effects necessitates the development of general mathematical models for
the underlying ion transport mechanisms involved, and it is this problem
that is addressed here. Considerable work has been done on the transport
of small ions across cell membranes, [l]-[5], and it is well known that such
transport can be effected in a variety of ways: from simple diffusion
processes to energy activated membrane ''pumps'' [5], [7], [8]. By refer-
ence to the Escherichia coli bacterium a relatively simple model for the
central processes of hydrogen (H '') and potassium (K'') ion transport
across bacterial cell membranes is developed. This is analyzed and its
ability to produce kinetic behavior comparable to that observed is experi-
mentally demonstrated. The analysis provides some simple relations be-
tween many of the parameters appearing in the model and goes some way
to determining appropriate values for these. Further experimental and
theoretical work is, however, required to fit these parameters more pre-
cisely.
E. coli is known to transport a variety of ions and metabolites, the
situation being represented as in Figure 1, (see [4]). The existence of an
H* /K* Counter-Transport mechanism ([5], [9], [10]) means that the
transports of these two ions are interrelated and cannot therefore be
modeled independently. However, provided only HL (the external H't
concentration) and K,. (the external K'' concentration) are varied then
consideration may be limited to the transport of these two ions
alone-Symport and the transport of other ions such as Na' and Catt
need not be considered ([1], [4]). Further, the transport of H' and K' in
these situations is dominated by Primary Active Transport and Counter-
Transport, [2], [8], [l1], [12], and hence diffusive transport can be ne-
glected, [1], [2], [13], [14]. Also insignificant are changes in the internal H
and K' concentrations due to metabolic reactions and cyctoplasmic
buffering, [l]. [4], [6]. With these simplifications Figure 1 reduces to
Figure 2. Denoting the efflux of H via mechanism (1) by (0,, the influx
of K ' via mechanism (2) by (?g, and the influx of H' via mechanism (3)
by (04, the model for H ', K transport becomes simply
where H, and K, are the internal H'' and K' concentrations and V the
cell volume.
The fluxes (?;, (!4, (4 will respond to gradients set up across the
membrane and also to deviations of H, and K, from some preferred
internal values, H * and K *, [4], [5]. Thus we assume
Further, linearizing (7) about the node (1, 1) the eigensolutions for the
resulting [acobian become
and this suggests the constraints
The last of these indicates that n, 1 is appropriate.
Finally we should note that the coefficients ä;, äg, and 9 occurring in
the original model have units of mol s'' and therefore represent charac-
teristic flux rates. Since we expect these to be small (in comparison with
the optimal number of ions, VH* and VK, inside a cell) this leads to
o;. 8.Y.Yg 1.
various parameters go some way towards fixing our model but still leave
considerable scope. It is somewhat unfortunate that the type of detailed
experimental results needed to fit the parameters more precisely do not
appear to be readily available. It would be important to know, for example,
the equilibrium dependence of K, on both H,L and K, and the nature of
H, kinetics for low H' (H : K''= 10 mol 1').
4. CONCLUSION
On the basis of various ion transport mechanisms operating in E. coli
cells a simple yet plausible model for the kinetics of H' and K'' transport
in these bacteria has been developed. By performing some elementary
analysis on the resulting coupled, nonlinear ordinary differential equations
we have shown, by reference to experimental information, how the model
is capable of generating qualitatively sensible results. Further analysis and
development is, to some extent, restricted by lack of appropriate data.
Knowledge of the incident solar irradiation is of great importance when
designing any solar-energy system at a site. The best irradiation information
is that obtained by long-term measurements of solar irradiation on a
horizontal surface at the location in question. However, for places, especially
for developing countries, where such measurements are not available due to
a lack of the necessary equipment, solar irradiation can be estimated by the
use of theoretical models and empirical correlations. Several different
approaches have been used to estimate the global solar irradiation. The first
attempt at such an estimation was due to Angstrom,'- This correlation was
then modified by Prescott' and used by Page:' it has been the most
convenient and widely used. However, Black et al.,' Rietveld,' and Smith'
have also used the sunshine duration to evaluate the solar irradiation. On the
other hand. Glover and McC ulloch included the latitude in addition to the
sunshine duration in evaluating the global irradiation, while Swartman and
Ogunlade' used the relative humidity in addition to the sunshine duration.
Liu and Jordan,'% K rieth.'' and Whillier'* used the declination angle and
latitude in their correlation. Mateer'? and Bennett'* combined the sunshine
duration, the declination angle and latitude to develop their empirical
models. Reddy'' and Sayigh''' proposed the use of sunshine duration,
temperature and relative humidity in such estimations, In another formula,
Reddy' 'suggested the use of the number of rainy days, sunshine hours and a
factor that depends mainly upon the geographical location of the place
under consideration in estimating the global (total) solar irradiation.
Gopinathan'? related the monthly mean daily total solar irradiation on a
horizontal surface to the relative humidity, maximum temperature, latitude
and altitude, while on the other hand, Sabbagh et al.,''' related the daily solar
irradiation to these parameters. Hoyt' developed a complicated theoretical
model based on climatological data for total precipitate water, turbidity and
Bahrain. Thirdly, to investigate the accuracy of Page's,* Rietveld's,' Glover
and McCulloch's,' and Alnaser's' models, In undertaking this, the global
irradiation estimated by using Page's model has been calculated in four
different ways depending upon the method for estimating the regression
coefficients. The calculated data of the solar global irradiation obtained by
these methods were compared with the actual (measured) data of the solar
global irradiation for Bahrain. However, the accuracy of the outcome data
of H were tested by calculating the mean bias error, the root mean square
error, and the mean percentage error.'! Finally, a new method for evaluating
the direct solar irradiation (H,), which is one of the two components of the
global (total) irradiation, has been introduced. The other component is
called the diffuse solar irradiation (H,,). One common approach for
calculating the direct irradiation (H,) is by calculating the diffuse solar
irradiation (H,), firstly using an empirical equation*M- and then
subtracting it from the total solar (global) irradiation. The difference
represents the value of the direct solar irradiation.
The data of the monthly mean of the daily sunshine duration (S), the
maximum possible sunshine duration (S;), the monthly mean of daily
maximum temperature T (in 'C), and the monthly mean of daily relative
humidity percentage (R) were obtained from the records of the Bahrain
International Airport Station, averaged for a period of 42 years. The actual
(measured) monthly mean of daily global solar irradiation on a horizontal
surface (H) was obtained from the University of Bahrain using an Eppely
pyranometer, averaged for the period 1982 to 1989.
tb) Rietveld'' proposed the following equations:
(c) Gopinathan's'' equations are as follows:
where hh is the altitude of the location in km: I for Bahrain is taken as
2m.
(d) Zabara's' equations:
The subscripts 1, 2, 3, 4 stand for the four methods used for
calculating the regression coefficients a and h.
In this paper, three methods are used to estimate the declination angle (0).
They are named: method 1, method 2, and method 3 and are as follows:
Method 1
This method has been used by many authors.28.29.33. The appropriate
expression is as follows:
where N is the day number starting from 1 January (February is taken to
contain 28 days).
M/ethod 2
This method was proposed by Spencer.?%
where
Method 3
In this method, 8, is obtained from the Astronomical Almanac, which is
published annually by the Nautical Almanac Office or Her Majesty's
Nautical Almanac Office.
while Alnaser's equation, which is
is the best equation when a and h are not involved in the calculation
of H.
(4) Correlations of the form:
and
are recommended for the calculation of the monthly mean of the daily direct
solar irradiation on a horizontal surface in Bahrain.
The inverse of Laplace transform is a topic of fundamental importance in many areas of theoretical
and applied mathematics [1-8]. Pade approximants have been used for this aim by many authors
in recent years [2-7]. The main problem in this field is the investigation of inverse in the region
0 4C t eC oo, Asymptotic methods permit us to investigate inverse for t -- 0 and t -- oo
[T,8]. Methods of so-called 'asymptotically equivalent functions'' were used in [T]. The main
idea of these methods lies in construction of function, in which asymptotic behavior in the limit
case t -- 0 and t -- oo is the same as the inverse behavior. But the asymptotic equivalent
function construction is the open problem. We propose to use two-point Pade approximant for
this purpose. The notion of two-point Pade-approximant is to be defined [5]
Let
The two-point Pade-approximant is represented by the function
in which m-+1 coefficients of expansion in the Taylor series, when e -- 0 and m coefficients of
expansion in the Loran series, when e -- oo coincide with the corresponding coefficients of the
series (1) and (2). Let us consider three examples.
Let the Laplace transform be
where Hg, Y - Bessel's functions. The exact inverse is
The inverse asymptotics look like
One can obtain two-point Pade approximant
The numerical results are plotted in Figure l (curve 1) and coincide satisfactorily with the
exact solutions (curve 2). Rational function (3) may be considered as ''asymptotically equivalent
function'' in the sense of [7].
Now we consider function
where H(t) - Beaviside's function. The Laplace inverse of this function is
where Ky - McDonald's function. Inverse asymptotics and two-point Pade approximants may be
written in the form
The numerical results are plotted in the Figure 2, where curves l,2 plot the formulas (4),(5)
correspondingly.
There are power series asymptotics expansions for t -- 0 and t -- oo in previous examples.
Now we investigate the case, when inverse f(t) contains exponential functions.
Let us
Then
Power expansion (T) was used for t -- 0, and exponential function was taken into account for
t -- oo, Comparison of exact ((5), curve 1) and approximate ((9), curve 2) solutions is shown in
Figure 3.
The accuracy of the proposed solution may be increased if we remove essential transform
singularities by well-known methods [1,8,9].
Comparison to other approximate methods (see [1-9]) may be used for the error analysis.
The methods and strategy described earlier have been applied to other integral transforms
(Fourier, Bessel, Mellin and so on).
Many interactive software systems of today are very powerful. Unfortu-
nately, these systems are too complex and have too many commands and options
for the users to master even a portion of them. Usually, users of these systems
learn a subset of capabilities that suffices to get the job done and be satisfied
with what they know. In this fast-moving society, users are unwilling to spend
extra time to experiment with the capabilities of the software system. Moreover,
many even do not want to ''mess around'' with what is already working, be-
cause they cannot afford to lose what they already have.
Another obstacle is the current method of documenting software. Usually,
on-line help or printed documents of the product contain a lot of detailed infor-
mation that covers a lot of ground. A user sitting in front of the terminal needing
help has to walk through either the manual or the on-line information in order to
get the appropriate answer that he or she wants. It takes a lot of time to read
through these documents to find out the necessary information. Most of the
time, a user needs just a line or a short paragraph explaining the current situa-
interactive systems can be attributed to this inability to effectively communicate
with the users (Matthew and Biswas, 1985a; Nickerson, 1981; Senay and Sta-
bler, 1985).
One basic requirements to effective dialogue seems to be mutual understand-
ing, on the part of both the speaker and the listener, of each other's individual
way of thinking in relation to the topic under discussion, and the nature of
communication. Achieving this understanding may be difficult at times, but it
may be crucial in situations where, for instance, the speaker wants to teach the
listener about some concepts. In this instance, the teacher may have to frame the
dialogue in terms of what he or she thinks the listener will understand. As the
listener's level of understanding increases, the speaker's dialogue has to adapt to
meet this change.
We do not claim that interactive systems that do not include some user model
cannot have an adequate man-machine interface, because many interactive sys-
tems work quite well without an explicit representation of the user. However, for
an interactive system to really understand the needs and goals of the human
operator, it must be able to build and maintain an explicit model of that user.
Within this context, the term model refers to a representation of the user in terms
of the user's observed and inferred abilities, beliefs (perhaps restricted to beliefs
about the domain), goals and, perhaps in the log term, attitudes and emotions. In
other words, the user model is a representation of the user that the software
system perceives and reconstructs in its own terms, and then stores in the sys-
tem. This model is then used in conjunction with the context of the interaction to
guide the nature of the communication between the user and the software sys-
tem.
Designing an adequate man-machine interface, in instances such as the ex-
ample above and many other situations, is difficult, because users are a hetero-
geneous group with a wide variation in levels of expertise and varying degrees
of frequency of computer use. Moreover, these characteristics are not static but
change dynamically as users' expertise increases, task demands change, and
skills are forgotten (Cuff, 1980). They also depend on which tasks and which
part of the computer system are the current focus of activity. It is apparent,
therefore, that users' demands on a software system change correspondingly.
However, the benefits of enabling a computer to build and utilize models of
individual users will be shown in an increase in the quality of the man-machine
interface generally and thereby a more usable end product.
In other words, current interactive software systems possess the following
deficiencies (Matthew and Biswas, 1985a):
A syntax-directed programming environment has been a research field for a
period of years. There are software systems like the Programmer's Appentice
(PA) (Waters, 1982) and the Cornell Program Synthesizer (Teitelbaum and Reps,
1981) which are some of the most well-known software systems. Most of these
editors offer little or no advising facilities on the use of the editor itself. Some-
times the lack of these facilities eventually hinder the users in their actual tasks-
programming. Intelligent advising systems and a syntax-directed programming
environment are two different research approaches that try to offer the user a
more satisfying experience, but they do not support each other (Teitelbaum and
Reps, 1981; Fikes, 1986; Matthews and Biswas, 1985b). Current research is
trying to merge these two approaches together and, on top of them, incorporate
the user model to provide a preferable environment to the users.
In this paper, we survey the research that uses the concept of user modeling
techniques to solve the above paradigm. After presenting the research work in
building intelligent systems, we use our system as an example to demonstrate
how this kind of system can be beneficial to the user. By user modeling, we
mean that the system is trying to create an image of the user that the system
interacts with. This image or model is the representation of the user as the
system knows and is created during and through the interaction between the
system and the user. Different aspects of the user, such as skills, technical
knowledge, or perhaps emotional facts, are acquired into the system. The sys-
tem then uses this model to interact with the user. A practical analogy is the
human-human interaction. The different ways that we treat our friends depend
on how well we know them and, equally important, what we do not know about
them. The way we interact with a new friend is very different from the way we
interact with an old friend. All these different behaviors are the result of how
much we know about that person we interact with, and what kind of image we
project in our mind about the person. A user modeling system in a way has the
same idea.
The proposed interactive software system in this paper, called the Intelligent
Syntax-Directed Editor (ISE), has two major components: the intelligent assis-
tant part and the syntax-directed programming part. The intelligent assistant
monitors user interactions with the editor, spots errors and inefficiencies, and
then issues appropriate advice to the user. The system also tracks the characteris-
tics of the interactions between the user and the software system. This informa-
tion is then used to determine when and how to respond to the user with regard
to errors, inefficiencies, and advice. The syntax-directed assistant monitors the
syntax of the programming language typed in by the user, and guides the user in
developing a syntax-error-free program. The editing commands are also syntax
directed. A list of options that are available is always shown at the bottom of the
user's screen, When an option is selected by the user, a new set of suboptions, if
there is any, under the option that the user just selected will be replaced as the
bottom line of the user's screen. Therefore, users always have a view of their
options at the moment. The software system is able to assist the user in several
different types of programming languages specified by the user either by asking
the user what language will he or she be using or by distinguishing the type of
language used in the file name that the user has specified.
This paper is organized in the following order. The next section presents a
general idea of user modeling used in interactive software systems and surveys
interactive software systems that use user modeling techniques. The following
section presents the design philosophy of the proposed ISE. The functions, fea-
tures, and the knowledge representation scheme of the ISE is then discussed. We
then describe the data representation used in the ISE and then discuss software
development using the ISE. Finally, we show our conclusions and some future
research directions.
An excellent broad definition of a user model, which encompasses a wide
variety of modeling schemes, is given in Zissos and Witten (1985):
[A user model is] any information which a program has which is specific to the
particular student being taught. The infornation itself could range from a sim-
ple count of how many incorrect answers have been given to some complicated
data structure which purports to represent a relevantt part of the student s
knowledge of the subject.
To provide a more analytical insight into the nature of user models, we will
first examine a taxonomy of user models and then describe some different tech-
niques for inferring users' knowledge of individual concepts from their observed
behaviors. During this course of discussion, several user-modeling techniques
will be briefly reviewed.
The term user model can be used to describe a wide range of knowledge
about people. Rich identifies three major dimensions for user models:
The IE consists of four submodules: the User Input Director (UID), the
User Proficiency Acquiring Manager (UPAM), the Session Information Man-
ager (SIM), and the Editor Commands Supervisor (ECS). The Editor System
Knowledge Base (ESKB) provides the rules and the facts for inference and the
two databases are used to store user information. UID can be considered as the
receiving end of the Intelligent Editor from the user, and the ECS, as the
output end. The functions of the above submodules are described below.
The top level of the UID (Fig. 2) is divided into two modules: the Textl
Command Recognizer (TCR), and Command Redirection Unit (CRU). The
major function of the TCR is to guide the user to issue syntactically error-free
commands. This module contains the Command Type Recognizer (CTR)
pressed, a menu as shown in Fig. 13 is displayed. After selecting the lf-then-else
statement, the menu will disappear and Statement-List will be replaced by an if
statement template. A similar example using C is shown in Fig. 14.
ISE provides a system that assists users in the software development pro-
cess. It offers the user with assistance on using the development system itself
and with assistance on programming language syntax. ISE provides help on
using the system through the help system which gives the user help information
that is essential and through active advising which gives user suggestions on
inefficient commands. The goal of the ISE is to save the user's time on software
development, to reduce the user's frustration on using the system, and to guide
the user in learning to master the system while he or she is using the system
More work needs to be done on the knowledge representation scheme of the
STA module. Having different knowledge representation schemes housed in the
PLSKB for different languages is preferred, because the same knowledge repre-
sentation scheme may not be compatible for different types of programming
languages. Not all languages are structural oriented languages. Dealing with the
languages like LISP or SNOBOL may need a different approach than dealing
with C or Ada type languages.
For further research, there are a few areas we are considering developing.
Currently, the ISE is designed to give active advice. However, one thing we
had not considered at the beginning of the design was the ability of the system
to learn from the user. There are always ways to do something fast, some
shortcuts. The system should be given the capability to extract these efficient
command sequences from the user's input and upgrade its own knowledge
base.
In this paper we consider the Euclidean bottleneck matching
problem which is defined as follows: We are given a set of points V and our job
is to match each point with some other point such that the longest distance
between the matched points, measured by the Euclidean distance between two
points, resulting from this matching, is minimized. The Euclidean bottleneck
matching problem arises in approximate weighted matching which is used in
efficient implementation of Christofides' approximation algorithm for the Eu-
clidean traveling salesperson problem [2], [3]. Consider Figure 1.1. The matching
in Figure 1.1 is an optimal of bottleneck matching. The bottleneck matching
problem is usually defined on graphs. Lawler [5] gave an O(m'n) algorithm to
solve the bottleneck matching problem defined on bipartite graphs where m and
n are the numbers of vertices of the two vertex sets of the bipartite graph
respectively. Burkard and Derigs [1] gave a FORTRAN program which solves
the bottleneck matching problem on general graphs. They did not analyze the
complexity of their algorithm. Gabow and Tarjan [3] described an algorithm
which solves the bottleneck matching problem in general graphs in O((n log n)''%n)
time, where n is the number of vertices and m is the number of edges. To solve
the Euclidean bottleneck matching problem, a straightforward method is to
compute the distances among all points, construct a distance graph, and then
apply Gabow and Tarjan's algorithm to this graph. The number of edges in the
graph will be @(pn'). Therefore, the time-complexity of this straightforward algo-
rithm will be O(a? log'? np
In this paper we present an algorithm to solve the Euclidean bottleneck
matching problem. Our approach heavily utilizes the geometric properties of this
problem. The basic idea of our algorithm to solve the Euclidean bottleneck
matching problem can be easily explained by considering Figure 1.2. In Figure
1.2 there is a solution to the Euclidean bottleneck matching problem. Besides, in
We give detailed proofs later. No matter which condition is satisfied, there is a
corresponding transformation mechanism to transform the given optimal solution
into another optimal solution as shown in Figures 2.1-2.4. Repeating this transfor-
mation process, we finally obtain an optimal solution where no lune contains more
than 16 points. The detailed transformation procedure is described as follows:
Define the distance sequence of an optimal solution as its matched distances
sorted in nonincreasing order. Let M, and M, be any two optimal solutions. 4). .
4.:---.4,L: 4nd d4,4. :.----. a.Ls.; are distance sequences of M, and M,
respectively. Define the lexicographic order of M, and M ; as follows: M, is greater
than M 4, denoted by M, > M4, if and only if there exists a number t where
1 s tsUn/2] such that 44.: 44;, for 1 s i= t and d4., > 4s.4
LEMMA 2.4. Procedure T will terminate in a finite number of steps.
PgooF. Since the maximum distance of matched pairs deleted is greater than the
maximum distance of matched pairs added, it is easy to see that if M , appears in
Step 3 for transformation before M,, then M, > M,. Now if Procedure T will not
terminate in a finite number of steps, then there exists an optimal solution M,
which will appear in Step 3 for transformation more than twice. This is because
the number of optimal solutions is finite. Thus M, > M, which is a contradiction
to the fact that lexicographic order is irreflexive. Therefore, Procedure T will
terminate in a finite number of steps. 3
Having proved the above lemmas, we have the following theorem:
THEOREM 2.1. For the Euclidean bottleneck matching problem, there exists an
optimal solution where no lune of matched pairs contains more than 16 points.
3. Using a k-Relative Neighborhood Graph To Solve the Euclidean Bottleneck
Matching Problem. In this section we define the kRNG first and then propose
an algorithm for solving the Euclidean bottleneck matching problem based on
this graph. Let V be a set of points in the plane. Toussaint [6] defined the relative
neighborhood graph of V (denoted by RNG(V) or simply RNG when V is
understood) to be the undirected graph with vertices V such that, for each pair
p, ge V, (p, g) is an edge of RNG(V) if and only if no point of V is contained in
the lune of p and ;. Figure 3.1 shows a set of points V in the plane and RNG(V).
and
From Lemma 4.1, it is easy to see that
Therefore,
Thus, E1 184n. D
The following theorem shows that we can find kRNG from kGNG.
THEOREM 4.2. The set of edges of kRNG is a subset of the set of edges of kGNG.
PoOF. We prove this theorem by showing that every edge of RNG
is an edge of kGNG too. Suppose that (p, 4)is an edge of YRNG then ]LUN,,,(V)!
k. Assume that qe R,(p) without loss of generality, then we have N3(p) 9
LUN,,(V). Therefore, N3(p)! k and ; is a k-geographic neighbor of p. In other
words, (p. 4) is an edge of kGNG and the proof is concluded. 3
COROLLARY 4.1. The number of edges of kRNG is less than O(kn).
We propose the following algorithm to find kGNG and kRNG:
10 Let i be the number of region of point p where point ; is located.
11 if d,, kr(p) then
12 kG,p)== kGX,p)- !41
13 if d, krp) then
14 kGIp)== kGIp) !4}
15 if jkGIp)l 2 k then
16 krp)== max[4,,l4e kGlp)}
17 kGX,(p)== g'lg'e kGlp) and 4,, = krp)}
18 kGIp)o kGp) - kGX,(p)
19 for each ie Regions do
20 for each ;e kGI(p) u kGXp) do
21 E5= E5- .4l
22 ;Find edges of YRNG by checking edges of kGNG.
23 E5 == 0 ;E5 is the set of edges of VRNG.
24 for each (p, 4)e E7; do
25 count= 0
26 for each v e V - {p, 4} do
27 if d,, 4,,4n4 4,, = 4,,hen
28 count -= count + 1
29 i count k then E2 = E .u [N. 4l]
For each region i of each point p, Algorithm G keeps two lists of points kGX,(p).
kGI(p) and one distance kr,(p). Initially, kGX,(p) and kGIp) are empty and
kr(p) = o (lines 5-8). kGX;(p) and kGI,(p) keep potential k-geographic neighbors
of p in the ith regions. Any two points of kGX,(p) have the same distance away
from p. kr(p) is the distance between any point of kGX,(p) and p. The distance
between any point of kGI,(p) and p is less than krp). !kGI(p)l is kept to be less
than k. Let ; be a point currently examined in the for loop of line 9 and ; is
contained in the ith region of point p. If d, kr(p), then q is added into list
kGX,(p) (lines l1 and 12). If d,,< kr(p), then ; is added to list kGI(p) (lines 13
and 14)L Whenever a point is added into list kGI,(p), we check whether kGI,(p)! = k.
If ]kGIp)| = k, then lines 16-18 will form new kGI(p) and kGX,(p). Let 4i.
9.---. .-; be the sequence of points examined by the for loop of line 9 and let
q; be the point currently examined. It is easy to see that if kGI(p) and kGX,(p)
are k-geographic neighbors of p in the ith region based on the set of points
l4, 4:.---< 4;-)}, then kGX,(p) and kGI,p) are still k-geographic neighbors of p
in the ith region based on the set of points [44. 43.-- -. 4;} after , is examined. By
induction, it is clear that Algorithm G constructs kGNG correctly. Lines 24-29
check every edge of kGNG to see if it is an edge of kRNG. If the lune of an edge
of kGNG contains less than k points of V, then it is an edge of kRNG. From
Theorem 4.2, we know that the set of edges of kRNG can be found in this way.
Checking if a point ; is a k-geographic neighbor of point p is done in lines
10-18 and they are executed n(n - 1) times because they are contained in the for
loop of line 9 and line 9 is contained in the for loop of line 4 again. Lines 10-18
take O(k) time. Theorem 4.1 ensures that line 21 is executed O(kn) times. Thus
kGNG can be constructed in O(knf) time. Lines 24-29 check every edge of kGNG
Since 4,, * 4,, - 4.-
From the above formula, it is easy to see that d,, s, $r.
Define Z(p, r) to be a set of pomnts which satisfies the following constraints:
(ir For all ze zZZip, r). 4,, 2r.
(ii) For any two points z; and ze Z(p, r). 4,,., 22 r.
(iii) For any two points z, and z4e Z(p, r). 4,., 22 ,., - r and d,,., 2 ,., - r.
Let Z be a set of points, pg Z, define 6(Z, p) to be the angle of .z4Pz4 where
z; and ze Z and points of Z - {z4.4} are contained in .3;p4 (including its
boundary).
LEMMA A.1. Z s Zp, ri, / @Z, p) cos''4), hen (! s 2.
POOF. Let v be a point of Z farthest from p. Then sector area vpw can be divided
into two regions: one is region vww'v' and the other is SW'pv' where v' is the
intersection point of pv and C,. Properties (ii) and (iii) of Z(p, r) imply no point
of region vww'v' -- {w} is in Z. If dg,, s 3r/2, then, from (A.1) and (A.2), we know
x 2 cos'''(6)and 4,, s r. Thus, property (i) of Z(p, r)implies that no point Sw'pv'
is in zZZ if 4,, s 3r/2. Suppose !Z > 2, If 4,.. s 3r/2, then 2Z, p) 2 x 2 cos'')
This contradicts the fact that dZ, p) cos''(4). Therefore, 4,,, 2 3r/2. Since
6(Z, p) cos''4), Z - |v} must be contained in Swpv' because x 2 ''(4). The
distance between p and every point of Z - {v must be less than or equal to 3r/2
because 4,, s 3r/2. Therefore, %Z -- {v}, p) 2 cos''4). This contradicts the fact
that 0Z, p) cos'4). Thus Z! s 2. C
Bentonite is a clayey material of very fine particle size, composed mainly
of smectite and formed by the natural processes of weathering, hydrothermal
alteration, sedimentation, etc. Members of the montmorillonite-beidellite se-
ries commonly occur in bentonites (Grim and Guven, 1978).
The smectites show a range of properties which are highly desirable in phar-
macy and pharmaceutical technology (Galän et al., 1985). Smectites are used
for making tablets, as disintegrant agents (Wai et al., 1966) and in the for-
mulation of stable suspensions (Wai and Banker, 1966), because thixotropic
gels are produced even at concentrations as low as 5% (British Pharmaceuti-
cal Codex, 1973). Other uses are as stabilizing agents, because the active con-
stituents remain in the interlayer (Browne et al., 1980) e.g, the stability of
some antibiotics is prolonged in bentonite suspensions (Cuciureau et al.,
1972). Bentonite also forms cationic compounds whose active constituents
allow for delayed and sustained dosage release (McGinity et al., 1977); they
are also used as binder and filler, as face make-up in cosmetics, etc. They are
smooth and white and chemically inert and stable under a wide range of tem-
peratures (U.S. Dispensatory, 1973).
One aim of this study is to describe and evaluate two Spanish bentonites
for possible pharmaceutical uses. Another aim is the application of the usual
techniques for the description and characterization of bentonites (CEC, spe-
cific surface, XRD, etc. ), to improve and to complete the requirements in the
principal Pharmacopoeias (British Pharmacopoeia, 1988; U.S. Pharmaco-
poeia, 1990).
Bentonite 1 comes from sedimentary deposits and has been provided by
Tolsa S.A. (Madrid). It is a fine smooth green powder. Bentonite 2 comes
from hydrothermal and meteoric alteration deposits in Rodalquilar (Al-
meria) (Caballero et al., 1985) and it is a raw sample consisting of brown
fragments of different shapes and sizes.
Prior to the analytical study the samples were screened (less than 2 mm)
to obtain a fine sample.
The following analytical techniques were applied to the fine sample:
The b parameter was estimated from spacings corresponding to the (060 )
reflections; quartz (Fischer ) was used as an internal standard. This made it
possible to distinguish between dioctahedral and trioctahedral smectites
(Brindley, 1980).
Oriented aggregates of homoionic magnesium clays (reflections 001) were
prepared by sedimentation and drying on a glass slide. To identify and quan-
tify the composition of oriented specimens solvation treatments with ethyl-
ene glycol and dimethylsulphoxide were used as well as thermal treatment
(Gonzälez and Sänchez Camazano, 1968; Brown and Brindley, 1980).
In all cases, the semiquantitative X-ray mineral analysis was carried out by
the use of intensity factors (Klug and Alexander, 1976; Schultz, 1964). The
the patient's skin (Harry, 1973). The conductivity in sample 2 is higher than
1, in accordance with the content of the solutes. In both cases, the cation con-
tent sequence is: Na' > Mg't > Ca *S K'. pH values are alkaline, slightly
higher in sample 1. Owing to the surface ionization of the colloids (Bohn et
al., 1979) and the contact potential (Jackson, 1970), these pH values are not
equivalent to those measured in aqueous suspensions (Table 4).
The results of the British Pharmacopoeia ( 1988) and U.S. Pharmacopoeia
(1990) trials are shown in Table 4. The samples meet all requirements about
identification, microbial limits and arsenic and lead contents. Sample l has a
d(060) of 1.527 A larger than the required limit of 1.504 A (U.S. Pharma-
copoeia, 1990) but within the usual range for smectite 1.49-1.54 A (Brown
and Brindley, 1980, p. 323). The pH must be alkaline, between 9. 5 and 10.5
(U.S. Pharmacopoeia, 1990); this is the case with bentonite 1, but not with
bentonite 2. This could be explained in terms of its genesis (acidic hydrother-
mal alteration, Reyes, 1977) and/or by impurity minerals. The viscosity of
the suspensions, increases as the pH becomes more alkaline (Nogueira and
Correia, 1967); however a very alkaline pH may damage the patient's skin
(e.g, as face make-up) or cause incompatibilities with the active constituents.
For this reason, pH buffering may be needed at slightly lower levels without
causing significant flocculation (Martin, 1971 ). Loss on drying and coarse
particles exceed limits in both samples. With respect to the gel formation and
swelling power, values are satisfactory for sample 1, but not for sample 2,
because of its larger particle size (Table 1 ) and lower purity (Table 2).
Bentonite 1 did well in nearly all trials. It is a sample with high clay content,
more than 90% smectite (saponite) and cation exchange capacity and spe-
cific surface values near the usual limits for this kind of samples; it is there-
fore potentially suitable. Although it exhibits an unsuitable content of coarse
particles, organic matter and loss on drying, these properties can easily be
modified by appropriate treatments.
Bentonite 2 fails some trials. It is a raw sample, that requires additional
treatment and purification. However, it is a great potential, because, from a
mineralogical point of view, the finest particles reach a high level of purity in
smectite (84%) and its chemical composition indicates that it is a montmo-
rillonite, more suitable for pharmaceutical use than other minerals of the same
group (e.g, saponite) having a greater cation exchange capacity and adsorp-
tion capacity.
Some of the tests required for bentonites by the main Pharmacopoeias es-
timate certain physicochemical properties such as gel formation and swelling
power. More specific and quantitative descriptions, such as cation exchange
capacity and specific surface have been carried out in this study.
The use of chemical analysis for the calculation of structural formulae is
recommended in order to determine which is the main mineral, given that
not all types of smectite have comparable properties for pharmaceutical use.
Finally, the saturation extract is important in the formulation of gels be-
cause it points to the composition of the interstitial solution of suspensions.
DEFINrTION. A de Bruiin sequence of degree n (or span n) is a periodic
binary sequence of periodicity 2'' in which each of the 2' possible
subsequences of length n occurs exactly once (in each periodicity).
Well-known facts (see [1]):
In each periodicity of length 2'', there are:
THEoREM 3. Of he 2'%''periods'' of a de Bruiin sequence of periodic-
ity 2'', n 4, all but n - 1 of the ''periods'' have length s n - 2 and are
counted in Theorem 2.
Proof. By Theorem 2, the total number of periods of length p, 2 s p
E n - 2, is
Define:
Then:
Thus, to maximize m4, we make one of the l,'s as large as possible, at
the expense of the others; while to minimize m,, we make all the l, as
nearly equal as possible, to keep the largest l, as small as possible. DT
The success or failure of nuclear energy has recently
been shown to be highly dependent on the state of
public acceptance (PA), with public opinions and
responses being significantly affected by the quantity
and frequency of information reported by the news-
media. For example, at the time of the 1979 Three
Mile Island (TMI) accident, a distinct time correlation
existed between the amount of newsmedia infor-
mation released and the anti-nuclear fraction in public
opinion (Mazur, 1984). In addition, following the
announcement of the 1986 Chernobyl accident the
anti-nuclear fraction suddenly increased, as did the
amount of media information in every country--there
being a time correlation between the two
(Hohenemser and Renn, 1988; Margerison, 1988).
Previous studies have determined that distinct
relationships exist between the number of nuclear
accidents, the associated information which is re-
leased by the newsmedia and the public's reaction.
These are as follows:
Since these investigations are only subjectively
quantitative, a qualitative relation which models the
amount of newsmedia-supplied nuclear energy infor-
mation and its effect on nuclear energy PA in Japan
is required to accurately estimate future public re-
actions. This prompted the present paper, which
describes a novel quantitative, semi-empirical relation
which is applicable to Japan, thereby enabling these
two variables to be modeled.
Figures l(a) and (b), respectively, show the time
variations in the amount of pro- and anti-nuclear
energy information reported by one of Japan's nation-
wide newspapers, The Asahi, The time distributions
of the amount of nuclear information released by
other nationwide newspapers in Japan are quite simi-
lar. Intervals of 3 months are used for time, with the
amount of information representing a value pro-
portional to the newspaper's total article area summed
over 3 months. The pro-nuclear articles correspond
the amounts of pro- and anti-nuclear information at
time t,,, with a,, b,, G. 4,, ;, and r4, being constants.
The summations are from t,, = 1954 to t, = t. Equa-
tion (1) assumes pro(anti)-nuclear public opinions
were initiated and developed in conjunction with the
available supply of pro(anti)-nuclear information,
although being diminished by subsequently released
anti(pro)-nuclear information. Public opinion at a
given time is also assumed to be dependent only on
newsmedia information, with the influence from other
means (e.g. local communities) being regarded as neg-
ligible compared with the newsmedia.
Use of the non-linear least squares method enables
the constants a,, b,, c;, 4,, t;, and r4, to be determined
by fitting equation (1) to actual values. When the
fractions F,(t) are represented as a percentage, values
applicable to Japan are a, = 0.0017, b,, = --0.0003,
c, = 0.016. d, = 0.003. i, = 0.19. s, = 8.1,
a, = -0.012, b, = 0.013, c, = 0.0, d, = 0.003,
ri, = 0.20 and r4, = 8.0, where a,,, through d,,, rep-
resent the fraction (%4) of the overall amount of infor-
mation released in 3 months and r is in years. Figure
2 shows the results calculated by substituting these
values into equation (1). The contribution from the
second term in equation (1) shows longer timescales.
and therefore has a much smoother behavior with
respect to time than the first term. To measure the
reproducibility of equation (1), the deviation o is
defined as follows:
where N is the actual number of public opinion polls
carried out to date. The value of o is, respectively, 2.1
and 3.6%4 for the pro- and anti-nuclear fractions. Even
though public opinion polls are not entirely accurate,
and rough assumptions were incorporated, the cal-
culations are nevertheless believed to correlate well
with the actual values. As a result, equation (1) is
considered to be a promising function to correlate
media information releases with public response.
According to several public opinion polls also taken
by the Office of the Cabinet Secretariat, the number
of people who obtain new information from televised
media is approximately equal to or greater than those
who get it from newspapers, with only a small portion
of the public preferring to receive news from other
sources. Thus, the effect of television on the formation
of public opinion cannot be neglected. However, since
the time distribution of televised information was
found to be very similar to that of newspapers
(Ohnishi, 1992), the functional form of equation (1)
is expected to hold, having time periods r;, and rg;
which are expected to be close to 0.2 and 8 years,
respectively. In these time periods, the effects of
nuclear accident related newsmedia information on
public opinion decrease exponentially.
If an estimation of the amount of nuclear infor-
mation released up to any desired time can be made,
the public attitude towards nuclear power at that time
can be predicted using equation (1), and hence the
state of PA can also be predicted to some extent by
the change in public attitude. The prescribed function
is also believed to be applicable to other PA situations,
such as environmental disruption and industrial
pollution.
Public opinion in Japan from both a pro- and anti-
nuclear power stance was found to react to the news-
media in two time periods, i.e. 0.2 and 8 years. Public
opinion corresponding to 0.2 years showed a sharp
response to the supply of nuclear information (Fig.
2), whereas, in contrast, on the 8-year timescale the
newsmedia supply affects opinion for a long time after
the cessation of the information supply. The follow-
ing two reasons are considered to explain this
phenomenon :
The actual public response is probably a combination
of these two explanations, thus resulting in responses
falling into the above-described short and long time
periods.
Suppose we are given the nonsingular linear system
with A e ((''' and b e C''. Suppose also that A is in a p p block partitioned form, where the
diagonal blocks A,; are square and nonsingular of order n;, j = 11)p, 2.f.,tnn, n, Let
D = diag A;;. 85- --. 8;,,) and let
lLet A5 in (1.4) be the associated MAOR matrü with R and fl, det(f1) = 0, being defined in
(1.6). If A and u satisfy the relationship
then the following statements are true:
An equivalence of the two-cyclic MAOR and a certain two-step method will allow one to
study the convergence properties of either method via the other. Despite the fact that in the
cases of real and pure imaginary spectra o(T) we are concerned with in the sequel the study is
made by using the MAOR (the EMSOR, to be precise) method, for more general spectra it
would be more convenient if we used the two-step method. For such an equivalence to be
established a matrix analogue of the eigenvalue relationship (1.9) will provide us with the key
point needed (see, e.g., [2-4,8]). For this the following theorem is stated and proved:
Theorem 3.1. Under the assumptions of Theorem 2. 1, with p = 2, and the additional assumption
that RT}T' = sI, with s being a scalar, the following matrü identities hold
It can be found that
In order to determine the extreme values for A and B in (4.6) and (4.7) we have to determine
the sign of the expressions in (4.9). For these signs we must take into account not only the
intervals for a and a' defined in (4.6) and (4.7) but also the relative positions of a with respect
to 2, that of a' with respect to 2 and a/a - 1) and therefore the relative position of a with
respect to 1 and that of ar/ar -- 1) with respect to 2, Considering in the a, w' )plane all
possible subdomains defined in this way one can very easily study the behavior of A(pf) and
B(Lf) and therefore define the corresponding ranges for r. After a long and tedious elementary
analysis one ends up with Table 1, where the results obtained are given in the most compact
form. Consequently, the union of all the domains, defined by all the subcases of Table 1, which
Proof. The MAOR method converges if and only if the two roots of (4.2), with b, c being
defined in (4.3a)' for all gf e M, are less than one in modulus. This is equivalent to
Assuming that gu > 0 and working in a way similar to that for the real case we end up with
Table 3, where again the results obtained are given in the most compact form. The expressions
for A and B are the ones in (4.8). For g = 0, we can find the following domains of convergence
in the (o, ar', r)space.
For L = 0 we then have 0 a 2, 0 a' 2 and r is arbitrary. DT
The precise domains of convergence of the two-cyclic consistently ordered MAOR method
with o(T) real or pure imaginary, determined and presented in the previous section, are new.
We remark, however, that in very special cases, where the three parameters involved in the
MAOR method are somehow reduced to two, the corresponding domains can be directly
obtained from the ones in Section 4 and most of them have been known. Thus:
An interesting open problem, which is under investigation, is the one concerned with the
determination of the optimum parameters in the various cases of Section 4. Again it is noted
that the optimum parameters of the special cases, which we referred to in the end of the
previous paragraph, can be found in the literature and some of them in the references given
previously.
ustralia has a long and proud record in the
development of the GPS satellite navigation
system. The first recorded involvements took
place from the Orroral Geodetic Observatory
in 1974 when pseudo-range tracking of the US
Navy's TIMATION series of satellites was
undertaken, (Easton et al, 1974 and 1975;
Buisson et al 1976). These experiments
between the Division of National Mapping
(Natmap), the US Navy Research Laboratory
and Royal Greenwich Observatory were
instrumental in validating and defining preci-
sion cesium and rubidium, clock performance
on satellites and in using the concept of
pseudo-range as an observable for determin-
ing position and real time navigation products.
Australian geodesists and surveyors were
quick to make extensive use of the first opera-
tional satellites with Natmap and The School
of Surseying at the University of New South
Wales playing hosts to numerous overseas
experts. The visit of Bob King from the
Massachusetts Institute of Technology was the
catalyst for the first monograph on GPS sur-
veying (King et al 1985). The South
Australian Department of Lands used GPS to
densify control in the south east ot their state
(Morgan et al, 1986), while Natmap built the
OPOS reduction system URizos et al 1987.
Govind et al, 1987). The running was then
taken up by the the Public Works Department
of NSW Macleod et al 1988). while the Roval
Australian Survey Corps acquired a batch of
T14100 receivers, and Antarctic Division
introduced the technique to Antarctica (Rizos
et al ]990). Since these heady davs of firsts,
GGPS suryeving has become so routine that
there is hardly a survey group or team that
hasn't used or doesn't have access to GPS.
As with previous programs, Australia's
unique position in the southern hemisphere
was attractive to international organisations
desiring a global coverage. Permanent track-
ducted to test and evaluate the concept, that
all Member Countries participate to the best
of their ability, and that this activity be co-
ordinated as closely as possible with com-
parable global deployments by other
member associations, as well as those bv
other organisations, and requests that all
existing global geodetic systems such as
Very Long Baseline Interferometry LBI)
and Satellite Laser Ranging SLR) be used
to carry out intensive observing campaigns
in conjunction with the proposed IGS work.
Recognising that global geophysical and
geodetic studies are urgently needed to bet-
ter understand many aspects of global
change, Noting that with recent technologi-
cal advances, the collection and analysis ot
digital geodetic and geophysical data is now
possible on a global scale, and considering
the economies of scale and scientific bene-
tits which can be derived from optimally
located ground and seabed measurements,
including their integration with space-based
observations, urges that relevant organisa-
tions, agencies and Member Countries
should review the geographical distribution
of those geophysical stations under their
control which make continuous earth and
space observations, and should jointly
locate as many of these stations as practical,
so that data gathered on a global basis by
ground- and space-based measurements can
be optimised.
downward propagation of the IGS from its
global objectives to more local objectives.
The most important benefits that Australia
can reap from the IGS campaign are:
The concept of a very high quality fiducial
network as the basis for the next Australian
reference system was first aired at the AUS-
LIG/UNSW Geodesy Forum last year,
(Morgan, 1991). It is further developed in this
issue by Manning and Harvey (1992).
This Fiducial network, computed in and
linked to the global system, will act as the pri-
mary network to which existing state and
lower order networks can be attached with
contidence and traceability. This will in turn
mean that a consistent set of geographical or
map grid co-ordinates can be generated which
will not suffer from edge and other boundary
related discontinuities. Since the precision
requirements of these networks is lower than
that needed to support global change studies it
will be possible to adopt a set of parameters for
the network and to hold these values fixed
over a time period of the order of a decade,
before significant motion will degrade the net-
work sufficiently to necessitate an adjustment.
The adoption of a geocentric system will
mean that the next generation of civilian refer-
ence systems in Australia will be compatible
with the current and future World Geodetic
Systems used by the Australian Defence
Forces. There are significant cost and opera-
tional benefits in aligning these systems, at the
m1ap co-ordinate level, and in reducing the cor-
rections that ordinary applications currently
need to apply to GPS observations. This is
especially so with the escalating use of GPS as
the primary co-ordinate generating tool for
LIS/GIS studies.
The need for the highest precision global
network, and access to this network so that
studies of such diverse topics as global
change, rise in sea level and tectonic motion
can be undertaken with assurance, were suc-
cinctly argued by Gordon Homes, (Homes,
1992) and Tony Sprent, (Sprent. 1992), in the
last issue, No. 37, of the Australian Surveyor.
This need is very real and important in the
analysis of data that has both temporal and
spatial implications and which needs or may
need to be integrated with similar data sets.
We have not attempted to make estimates,
or even guestimates of the savings that will
accrue from participation in IGS. However we
believe that geodesy, surveying and our
knowledge of the Australasian region demand
that we seize this opportunity to participate in
the IGS campaign at the highest level and to
propagate IGS technology, data and results to
lower levels as fast as we are able.
A widely adopted procedure when analysing
direction measurements is to reduce FL (face
or circle left, measured clockwise) and FR
(face right, measured anticlockwise) obser-
vations to a mean and then find the grand
mean from all arcs (sets) at that site. This
method has been used for many years. One
early reference is Jordan (1893). This 'station
adjustment' is a least squares adjustment of
the individual arcs that solves for directions
and orientations. An example of these calcu-
lations is shown in Table 1. The example is
given to show the mechanics of the calcu-
lations and for comparison with other
methods. As usual, the zeros of circle and
micrometer are changed between arcs.
The standard deviation of a grand mean
direction is then calculated from:
where a is the number of arcs and t is the
number of targets in an arc. The residuals v
are calculated in two steps. First, v' is the
difference of a reduced mean (of FL and FR)
observation from the grand mean direction.
Then v is calculated from v = v'-Dv'/t,
where Dv' is the sum of the v' for the arc.
ECv'/t is the orientation unknown of an indi-
vidual arc.
The standard deviation of a single mean
direction (mean of FL and FR) is calculated
from:
The grand mean directions can then be
entered into a least squares adjustment of a
network. This has been done since C.F,
Gauss used the procedure with equally
weighted directions in the Triangulation of
Hanover 1820-1830 (Jordan, 1888). In the
least squares adjustment of the survey net
one orientation parameter is determined for
this set of grand mean directions. The a
priori standard deviation assigned to the
grand mean directions is either derived from
s; above or an estimate of the population
standard deviation of the observations is
used. (Different populations apply for differ-
ent instruments, observers and observing
conditions). In the least squares process the
standard deviation of a direction is usually
increased to include effects of centring
errors,
Another method has been described by
Richardus (1984). In this case FL and FR
observations are not combined. Each face is
treated separately. Richardus advises to shift
the theodolite circle between faces and be-
tween arcs. The residuals are calculated by
taking the difference between a single face
observation (reduced to the RO) and the
grand mean. An example of these calcu-
lations is shown in Table 2. Note that Table 2
uses the same observations as Table l.
justments. In doing so, separate orien-
tation parameters should be included for
each arc and s,S used as a priori standard
deviations.
The use of means of single arcs (rather
than grand means) greatly increases the de-
gree of freedom of the adjustment without
compromising the stochastic model seriously
through the introduction of systematic er-
rors, Data snooping is greatly facilitated, as
single arc directions (rather than grand mean
directions) can be eliminated if required.
If grand means are used then surveyors
should continue to check the consistency of
single arcs when computing the station ad-
justment and the associated precisions.
URRENT radiofrequency (RF) radiation safety
standards (e.g., ANSI C95.1-1982) are based, to a
significant extent, on presumed rates of human whole-body
RF absorption. To date, whole-body absorption rates in
actual human subjects have only been measured by our
group. The experiments were performed using a TEM cell
as the exposure system [1]. Initially, the effect of frequenc
and grounding on the E-polarization absorption rates wa-
studied [2]. In that study, only the ideal free-space and
grounded conditions were simulated. In the present work,
the effect of different spacings from the ground plane on
the E-polarization absorption rates is reported. The other
two possible body orientations with respect to the wave, K
and H, will be ignored since their absorption rates are
much smaller than for the E orientation [1].
All measurements were performed using the modified
version of the TEM cell [3] in which all the TE resonances
are suppressed. Tests showed that the modified cell could
only be used reliably at frequencies below 25 MHz or from
40 to 42 MHz. Within the latter range, the ISM frequency
of 40.68 MHz was selected as the measurement frequency.
Absorbed-power measurements were performed with the
RF system previously described and evaluated [l].
All volunteers were adult males in good health. Ex-
posures were limited to one hour per day at a power
density not exceeding 13 yW -cm'' and no subject ever
absorbed more than one W,
M All experiments were performed with the body in an E
orientation (electric field parallel to the body length) and
the subject's feet closest to, or touching, the ground plane.
Both of the two possible orientations with respect to the
nagnetic field were employed: the EKH orientation, in
which the magnetic field was perpendicular to the chest;
and the EHK orientation, in which the magnetic field was
in the shoulder-to-shoulder direction. In general, the results
differed little between the two possible E orientations.
8. Most of the RF absorption rates are displayed in a
(6omparative way, i.e., as a ratio to a reference absorption
rate for the same subject. The reference value is either the
5free-space or grounded rate. Table I gives the reference
5figures for each combination of subject and frequency that
was used. Only three subjects were employed in the study
because other volunteers were not available for reasonable
periods of time and the results for the three subjects were
always found to be quite similar.
The five frequencies used for the experiments are also
given in Table I. Frequencies of 13.56 and 40.68 MHz were
employed because they are both ISM frequencies, at which
human occupational exposures often occur. The highest
usable frequency below all the interacting-resonance fre-
quencies of the cell is 23.25 MHz. The use of 10 MHz
; made a direct comparison possible between our measure-
gpments and the two relevant published theories; the cylinder
theory of Iskander et al. [5] and the block model calcula-
tion of Hagmann and Gandhi [6], [7]. Finally,7 MHz is the
lowest frequency for which reasonably accurate results are
possible.
g Since it was impractical to achieve a uniform-thickness
air gap between the subject's feet and the ground plane,
that ideal condition was simulated by using spacers of
, low-dielectric-constant materials. Two materials were used
(both of dielectric constant 1.03): expanded polystyrene,
and a hydrocarbon resin foam (ECCOFOAM PP-2 from
Emerson and Cuming, Canton, MA).
In the first RF absorption study, 40.68 MHz was found
to be at or near the grounded resonance. All frequencies
below 25 MHz were clearly in a different, below-resonance,
region. This distinction is also supported by the present
results.
Before the feet were separated from the ground plane,
the effect of separating the two feet on the ground plane
was tested. The results, in Table II, are clearly independent
of frequency and of which E orientation (EKH or EHK )
is employed. A small gap between the feet has only a slight
effect. A larger separation significantly reduces the absorp-
tion rate compared to the rate with the feet together. This
effect may be partly due to the reduction in effective height
of the subject as the feet are spread far apart. In the case of
subject L, used for Table II, his effective height is reduced
by 10 cm from its normal value of 173 cm when his feet are
90 cm apart.
The effect of an air gap on the normalized specific
absorption rate (NSAR) is shown in Fig. 1 for three
subjects exposed at the same frequency. Fig. 2 shows the
same plot for one subject in both E orientations and at
three different frequencies. It can be seen that the air-gap
effect depends only slightly on the subject and choice of E
orientation. Results for the two below-resonance frequen-
cies are very similar to each other, but strikingly different
from the results for the near-resonance frequency, 40.68
MHz. It can be seen in the two figures that, for below-reso-
nance frequencies, the absorption rate is reduced to half
the grounded value by an air gap of only three to six mm.
At the near-resonance frequency, on the other hand, an air
gap of 50 to 80 mm (based on Fig. 2 and other data) is
necessary to produce the same effect.
The two relevant existing theories are the approximate
cylinder calculation of Iskander et al. [5] and the block
model calculation of Hagmann and Gandhi [6], [7]. Since
the latter theory was only calculated for the EHK orienta-
tion at 10 MHz, the comparison between theory and
experiment was done for those conditions. The results are
presented in Fig. 3. The theories are normalized to NSAR(0)
for each theory, and not to the measured NSAR(0). This is
important to note because the two measured NSAR values
are both approximately four times the two calculated val-
ues (see Table I). This discrepancy will be addressed in a
later paper on improved models. Fig. 3 can be used,
however, to compare the relative decrease in absorption as
a function of separation distance from the ground plane.
For separations up to three cm, the measurements agr :e
with the block model and disagree with the cylinder mod.
At a separation of five or six cm, the two theories bc.hh
agree reasonably well with the measurements.
At higher frequencies, quantitative predictions were only
published for the cylinder theory. Since that model obvi-
ously does not distinguish between the EKH and EHK
orientations, the comparison was made to the average of
the measurements for the two E orientations. The results
are presented in Fig. 4. The measured curve for 23.25 MHz
is well below the cylinder theory, just as was found for the
other below-resonance frequency (10 MHz). The measure-
ments at 40.68 MHz are compared with the cylinder theory
for the same frequency, and also for 47 MHz because the
latter frequency is calculated to be at the grounded pec'.
for the cylinder. The measurements are seen to be close(
the 47-MHz curve for all separations measured. This agrec-
ment supports our contention that 40.68 MHz is actually
very near the peak for grounded human subjects.
The block model only made a semiquantitative predic-
tion for the resonant frequency [6, p. 25], one that is
obviously wrong: ''Several calculations made for the
grounded resonant frequency of 47 MHz show a fall-off in
magnitude of grounding effects with increasing distance
from the ground plane, which is similar to the results at 10
MHz.''
As far as the air-gap effect on the whole-body absorp-
tion rate is concerned, we conclude that the cylinder theory
is more accurate for the near-resonant frequency, and the
block model calculation is more accurate for the below-res
onance frequencies.
Finally, the block model calculation made one more
prediction that could be tested. That theory predicts [6,
fig. 5] that the SAR in the heel is much larger than in the
ball of the foot. This implies that most of the body's RF
current is carried to ground through the heel and that
contact with the ground plane is much more important for
the heel than for the ball of the foot. The data of Table III
prove that the opposite is the case. Most of the RF current
through the foot goes through the toes and ball of the foot.
This may be due to the larger contact area of that part of
the foot, or due to the complex bone structure of the ankle
and foot.
For small separations from the ground plane, the soles
of the subject's feet may be considered to form a parallel-
plate capacitor with the ground plane, The equivalent
circuit representing the exposure situation then consists of
a resistive human body separated from its image in the
ground plane by a capacitive impedance. Iskander et al. [5]
used this approach in calculating the RF absorption rate of
a cylinder. To test the capacitance idea, the functional
dependence of the absorption rate on capacitance is not
needed. It is only necessary to utilize the fact that the
capacitance of a parallel-plate capacitor is proportional to
the ratio of the material dielectric constant to the separa-
tion between the plates. Thus, the capacitance idea is
confirmed if the absorption rate depends only on the ratio
of the two quantities.
2 This idea was tested using different numbers of plastic
sheets between the subject's feet and the ground plane. In
(each test, the plastic sheets extended beyond the edge of
the feet by a distance at least as large as the total thickness
;, of the sheets. The two plastics that were used, and their
idielectric constants (measured in our lab), were: methyl
methacrylate (or lucite, K = 2.6) and cellulose acetate
(K = 40).
The results of these experiments are shown in Fig. 5. It
can be seen that most of the data points, after scaling for
the effect of the dielectric constant, lie on or close to the
curve for K = 1,0. The last point for cellulose acetate, at a
reduced separation of 1.6 cm, is likely off the curve because
the actual separation of.6.A cm is comparable to the width
of the feet; this violates the assumptions underlying the
parallel-plate capacitor model. Overall, the data of Fig. 5
adequately confirm the capacitance concept. That idea will
now be applied, in a qualitative way, to the practical case
of the effect of footwear.
The effect of different footwear on the nearly grounded
absorption rates was first tested at 23.25 MHz. The data
are presented in Table IV.
The results for socks alone are interesting for two rea-
sons. First, even thin nylon socks, only 0.65 mm thick,
reduce the absorption rate by a measurable amount, to 87
percent of the grounded rate. Secondly, the results for both
the nylon socks and the wool socks (1.7 mm thick) are the
same as the results for an air gap of exactly the same
thickness. This is a rather indirect way of proving the
well-known fact that the bulk of the volume of a sock
consists of air pockets, not material.
The use of shoes as well as socks further reduces the
absorption rate (Table IV). It is not possible to compare
the data with shoes and socks to the capacitor model
because: the heel is usually further from the ground plane
than the ball of the foot; the socks and shoes form a
dual-layer capacitor; and the dielectric constant of the
leather soles (which could not be measured easily) depends
strongly on its moisture content. For the experiments using
both shoes and socks;;the absorption rate compared to no
footwear was found tö'vary from 548 1 percent (for nylon
socks and dress shoes with leather soles) to 3281 percent
(for wool socks and rubber-soled athletic shoes).
The same two combinations of shoes and socks, which
represent the maximum rapge of the results for 23.25 MHz,
were used to study the effect of footwear as a function of
frequency. Those measufements are presented in Fig. 6.
Results are very similar at all the below-resonance frequen-
cies, At 40.68 MHz, however, footwear produces a much
smaller reduction in absorption; this is consistent with the
data of Fig. 2 for the effect of an air gap.
In occupational exposure situations, where the ground-
ing effect may occur, it is recommended that footwear
always be worn. This will provide some radiation protec-
tion at all frequencies: a rediuction in the RF absorption
rate of 15 to 35 percent neat resonance; and of 45 to 75
percent at below-resonance frequencies. For the commonly
used ISM frequency of 27.12 MHz, the reduction is esti-
mated from Fig. 6 to be between 40 and 60 percent. A
second radiation protection alternative is the employment
of a thick rug, rubber mat, plastic sheet, or any other
insulating material over the ground plane.
The rate of change of the absorption rate with separation
from the ground plane is seen in Fig. 1 to diminish
considerably for separations greater than about 2 cm. It is
obvious that the graphical analysis of Fig. 1 is poorly
suited to extending the measurements to the free space
situation, which is simulated in our TEM cell by a separa-
tion of 90 cm from the ground plane. A much more
suitable plotting scheme, illustrated in Fig. 7, was found.
Absorption rates are plotted as a function of the inverse
separation distance d''. Two advantages of this method
are that the extrapolation curves are linear (at below
resonance frequencies) and that the absorption rate for the
ideal free-space condition (d = oo) is simply the intercep'
of the regression line with the ordinate scale.
The data of Fig. 7, for subject L, overlap the data of
Fig. 1 for the same subject. The three points on the
right-hand side of Fig. 7(a) are the same as the three points
on the right-hand side of Fig. 1. It is seen in Fig. 7 that the
linear extrapolation to free space is valid for d''' g 0.5
cm'' or d a 2 cm. Graphs similar to Fig. 7 were also
plotted for the other two subjects at the same frequency.
All the lines fit reasonably well (R > 0.9 for all six lines).
The regression-line slopes and intercepts for the three
subjects in both E orientations are compared in Table V.
Slopes range from 2.80.5 to 7.430.3 cm. Additionally,
the slope is not consistently larger for either of the two E
orientations. The reason for these variations is not known.
The farthest a subject can be from the ground plane in
the TEM cell is 90 cm, when the subject is located halfway
between septum and wall. The subject's length of about
180 cm half fills the distance from septum to wall, The
question of whether or not these spacings adequately
simulate the ideal free-space situation can be answered by
comparing the intercepts of the regression lines (d = oo)
with the last data points (d = 90 cm). In every case, the
3difference was less than 10 percent and not statistically
%significant. The average difference between the two values
was 43 2 percent. This result proves that, for below-reso-
%nance frequencics, a septum-to-wall separation of twice a
7body length provides an exposure situation which very
closely simulates the ideal free-space situation.
The linear extrapolation process which was found to
work so well for below-resonance frequencies did not work
g for 40.68 MHz. The measurements for subject L, shown in
Fig. 8, are clearly not on a straight line. Neither are similar
plots for subject I. This difference in behavior at near-reso-
nance frequencies is not surprising since it is also observed
for small separations from the ground plane.
For the near-resonance frequency, a separation of 90 cm
$from the ground plane may not be considered equivalent to
the ideal free-space condition. Based on the four available
curves, it is estimated that NSAR([90 cm]'') is 10335
percent greater than the extrapolated intercept NSAR(0),
, and that this difference is real. Thus, all our simulated
free-space absorption measurements at 40.68 MHz should
be reduced by 10 5 percent. This has the effect of re-
ducing slightly the frequency esponent n (NSAR o: f') for
g. the free-space absorption curves from 18 to 41 MHz. The
corrected mean exponent is 2.7 0.2, in comparison to the
Ivalue of 2.90.2 originally reported by us [2, table 3].
t A complete set of absorption measurements for one
subject in the EKH orientation is shown in Fig. 9. The data
for the grounded condition and the smallest separation (0.6
cm) are fit by a single (weighted) regression line on the
log-log scale. The data for the free-space condition are fit
with two regression lines, as was previously found neces-
sary [2]. Two lines were also found to give a better fit to the
data for a separation of 5 cm.
. In terms of both the positions of the curves and the
number of required regression lines (one or two), it can be
; seen that the data for a 0.6-cm separation are similar to the
grounded results, while the measurements for a separation
g of 5 cm are more like the free-space results. This supports
our previous observation that a separation of about two cm
is the dividing line between the nearly grounded and nearly
free-space behaviors.
The results of this study fall conveniently into four
distinct categories, depending on whether the frequency is
near the grounded resonance ( f 4 40 MHz) or below it
(f s 25 MHz) and whether or not the subject's feet are
within two cm of the ground plane.
Near the ground plane, the decrease in NSAR with
increasing separation is very rapid. The absorption is re-
duced to half the grounded value by a separation of only 3
to 6 mm. The results agree very well with the predictions of
the block model for all separations out to 6 cm, while they
only agree with the cylinder model at a separation of 5 or 6
cm.
The idea that the soles of the feet and the ground plane
effectively form a parallel-plate capacitor was proved by
measuring RF absorption rates with different thickness of
three different materials between the two surfaces. Natu-
rally, the capacitor model only works for separations from
ground which are less than the width of the foot.
Ordinary footwear provides practical radiation protec-
tion by reducing the RF absorption rates, compared to
grounded, by 45 to 75 percent, depending on the choice of
footwear.
Finally, the absorption rates were found, when plotted
against inverse separation distance, to extrapolate in a
linear manner to the ideal free-space limit. The linear
relationship permitted the inference that a separation of 90
cm, the maximum possible in our TEM cell, is a very good
approximation to the free-space condition.
For near-resonance frequencies, the decrease in NSAR
with separation from the ground plane is an order of
magnitude slower than for the below-resonance frequen-
cies, The curve agrees fairly well with the predictions of the
cylinder model, but disagrees with the block model. Simi-
larly, footwear provide much less RF radiation protection;
the RF absorption rates compared to grounded are re-
duced by only 15 to 35 percent, depending on the choice of
footwear.
Finally, the absorption rates could not be extrapolated
to free space in a linear maner, and it appears that a
separation of more than 90 cm is needed to properly
simulate free space for frequencies near the grounded
resonance.
The author would like to thank J. A. Walsh for perform-
ing many of the measurements and S. J. Allen for review-
ing the manuscript. He is also grateful to the Division of
Biological Sciences, National Research Council, Ottawa.
for providing the facilities where the work was done.
Noise source identification and ranking are important
elements of any engineering noise reduction program. Any
effort to reduce machine noise can only be successful if the
most significant contributors to overall machine noise are
treated. The most useful way of ranking noise sources is in
terms of sound power, This is fortunate, because it has
become possible recently to measure the sound power of a
source in its acoustic near-field. Another advantage of
sound power is that it can be used to calculate sound
pressure levels as various distances from a machine in
several different kinds of acoustical environments.
The traditional method of noise source identification has
been the selective-wrapping or lead-wrapping technique.
The entire machine is wrapped in an acoustical absorbing
material with an outer layer of lead sheet. Various com-
ponents of the machine are then selectively exposed and the
contribution of each to the total noise is measured, There
are several problems with the lead-wrapping technique: the
method is slow, difficult, and time consuming, expensive
acoustic facilities are required; and, accuracy is a problem
at low frequencies and with less intense sources. For these
reasons, many researchers have been looking for alter-
natives to the lead-wrapping approach. An ideal noise
source identification method would be quick, accurate, ap-
plicable under a wide range of acoustical conditions, and
1nexpensive.
Recently developed methods of measuring acoustic in-
tensity in the near-field of a source come very close to
fulfilling the requirements of an ideal noise source iden-
tification method, Although fairly sophisticated FFT com-
puters and software are required, the measurements can be
made in almost any acoustic environment. Over the range
of frequencies normally of interest in noise control pro-
blems, the results are reported by many authors to be ac-
curate and repeatable'.1.'', In addition, acoustic intensi-
ty measurements can be made much more quickly and easi-
ly than measurements with the traditional lead-wrapping
technique.
For this investigation of source identification using the
acoustic intensity technique, preliminary studies were con-
ducted using a loudspeaer installed first in an anechoic
room and then in a reverberation room, After the comple-
tion of the preliminary work, a six-cylinder, in-line, tur-
bocharged, 350 horsepower diesel engine was used to com-
pare three noise source identification techniques: lead-
wrapping, surface intensity and acoustic intensity. The
results of the lead-wrapping and surface intensity source
identification experiments have been published previously,
so these results will be discussed in the current article only
for comparison purposes,''
in the frequency domain and substituted into Eq. (2), the
final result is:
where f,,; is the Fourier transform of acoustic pressure
signal 1, F%; is the conjugate of the Fourier transform of
acoustic pressure signal 2, and Im ( ) denotes the im-
aginary part of the argument. In terms of the cross-
spectrum oi acoustic pressure signals 2 and l, G3;, ihe in-
tensity is:
This formulation of acoustic intensity was first published
by Fahy in 1977 . Several derivations have since been
published which arrive at the same result.
The overail sound power of a source can be calculated
by integrating the component of the acoustic intensity l,,
normal to any arbitrary control surface area enclosing the
no1se source:
where A is the control surface area enclosing the source.
This may be achieved by always aligning the line joining the
two microphones normal to the arbitrary control surface.
A complete derivation of the intensity equation with
intermediate steps is available in Ref. 3.
Intensity measurements are subject to several kinds of er-
ror. One is a systematic bias caused by instrumentation
phase mismatch, while another is random sampling error.
Both kinds of error can become large enough to cause
serious measurement problems, particularly at low fre-
quencies. A third type of error is caused by the finite dif-
ference approximation used in calculating the pressure gra-
dient and the average pressure between the two
microphones. This error will be considered in a separate
section. Additional errors can occur when the intensity is
measured very close to complex sources,' '+, These errors,
and errors due to the operation of the FFT system, will not
be considered here.
As a sound wave passes over the microphone pair, they
each sense the same wave but with two separate phase
angles. Thus the measurement of intensity depends directly
on the accuracy of this phase angle measurement. Any
phase mismatch in the measurement system will lead direct-
ly to errors in the calculated intensity. Because of the large
wavelengths of low frequency sound pressure waves, a
closely spaced microphone pair will sense only a very small
difference in phase angle. Thus, at low frequencies, a small
instrumentation phase mismatch can lead to huge errors in
the calculated acoustic intensitv.
Sevbert has shown that random error is a function of the
frequency, coherence between microphone signals,
microphone spacing, and the number of averages used in
the measurement. '' Definine the normalized standard er-
rOT as'
Seybert showed the normalized standard error to be:
where ;; is the number of ensemble averages used in the
measurement, 5 2 is the coherence between the two
microphone signals, and oop; represents the measured phase
angle of the cross-spectrum. o ; is a function of the
microphone spacing.r, the frequency f = u/2, and the
direction of the sound waves as they strike the
microphones. For example, in the far-field of a simple
source the phase angle increases linearly with increasing fre-
quency:
where 6 is the angle between the line joining the two
microphones and a line in the direction of sound propaga-
tion. Thus, if the sound waves are propagating in the direc-
tion of the line joining the microphone pair, o ;; will have
its maximum value. When sound propagates perpendicular
to the line joining the two microphones, o ;; (as well as the
actual intensity) is zero. For real sources measured in the
near-field, the measured value of o 1; will always be less
than the maximum value of 360' (L .r/M).
Equation (9) shows that increasing coherence and in-
creasing the number of averages both reduce random error.
An increase in o ;; will also reduce random error. This can
be accomplished by increasing the microphone spacingtr,
but larger values of ..r tend to reduce the coherence, so a
balance must be found, o;; becomes small at low frequen-
cies because M becomes large. This means that large random
errors can be expected at low frequencies. Since both Y- and
op; can be calculated from the same data used to determine
the acoustic intensity, it is possible to check the random er-
ror of measurements in everyday practice, This can be very
useful in locating the source of measurement problems. A
similar alternative, approximation of random error, has
been published by Jenkins and Watts,''
Two basic methods of eliminating instrumentation phase
mismatch error have been developed, Chung described a
technique which requires microphone switching,' The in-
tensity measurements must be made twice, but the phase
mismatch error is, in principle, eliminated. Several other
researchers use a transfer function measurement
technique.'1.%, In this case the phase mismatch is
measured once and then used to correct the intensity data,
The drawback to this system is that it is quite difficult to
provide both microphones with identical signals for the
calculated from the assumption that a particular sound
source is completely removed. The results are shown in
Table 3.
The results of Table 3 clearly lead to the conclusion that
research needs to begin at the spindle; however, additional
noise reduction should be obtained at the gearbox, fan and
possibly at the ring-traveller system.
The rather low contribution of the ring-traveller system
in Table 3 deserves some comment. The values were found
under specific conditions-yarn count rather fine, rings
and travellers in good condition, and European standard
rings, Stewart and Bailey found a possible reduction of ring
noise of up to 6 dB for somewhat different conditions.'
This statement is not supported by our measurements. The
reasons for the difference might well be as described.
Figure 6 shows a cross-section through the Rieter spindle
type M. Of main interest, for our problem, is the roller
bearing at the top of the bolster, This bearing has to take
the load of the belt and the instability of the cops. The in-
evitable inaccuracy of the metallic parts leads to impact
generated shocks when the rollers are circling on the cylin-
drical races. These shocks produce vibrations of the shaft
and bolster. Vibrations will be significantly higher at the
resonance frequencies of the systems, as Stewart and Bailey
pointed out.' The vibration caused by the roller bearing is
rather high because the clearance of the shaft must be large.
This is necessary because the lower end of the shaft may
move in a radial direction in order to compensate for bob-
bin unbalance. Therefore, the fast running roller bearing
with rather high clearance, together with the lightweight
spindle structure, leads to high vibration of the whole spin-
devote very little discussion to the
electronic systems used to achieve the
results obtained.* Perhaps this is
because of the explosive advance of
modern solid -tate and computer
technoloeies. These technoloeies seem
crucial to the practical realization of
active noise control systems, and they
require a level of expertise that may
not be available to manv acousticians,
On he oher hand, perhaps the
authors simply wish to be secretive
about their achievements, In anv case,
two general types of electronic con-
trollers mav be identified:
ln the first type of controller, most
often found in the literature, sets of
active filters may be preconfigured to
fit a given application from acoustical
measurements ot the problem. Alter-
nately, the filters may be designed to
maximize some given acoustical
parameters which are believed to be
general in nature, In the adaptive con-
troller, the filters themselves are con-
figured within the electronic package
based on data sampled from the
acoustic environment. Such adaptive
systems may be designed to operate on
either by trial-and-error (see Ref, 70),
or may make use of deterministic
algorithms (sse Ref. 69). Development
of electronic controllers with reliabili-
ty and ruggedness suitable for field
use is an important practical goal,*'.*
Similarly, the literature really tells
very little about physical, geometrical
considerations. Details of best ar-
rangements are not given. This is
curious, as certain authors have at-
tempted mathematical treatments of
the problem; however, these
analytical approaches have always
been extremely general because of the
highly complicated nature of the pro-
blem. Thus, details suitable for design
purposes are not yet available.
Nevertheless, the physical geometry
has been shown to be extremely im-
portant in achieving high attenuation,
as illustrated in Fig. 11,t This is not
surprising since only geometries which
favor proper mixing of the cancelling
course smaller transformers do not make as much low fre-
quency noise as larger transformers. Therefore, the prog-
nosis will also be dependent on the size of the transformer.
Measuring the noise produced by transformers may lead
to the problem of interference ofpure tones, see Fig .3. Even
one transformer can lead to interference caused by image
sources situated behind walls or the ground. Close attention
should be given to the measurements and they should be
taken in different locations in the environment of the
measuring position. Standing wave patterns for 100 (120),
200 (240), 300 (360) Hz, etc., are mainly found around
transformer stations.
It is rather difficult to measure transformer noise at a cer-
tain distance, especially if there is more than one
transformer. In relation to the above mentioned in-
terference problem, there is considerable doubt as to the
reduction of sound level as a function of distance, Figure 4
shows a rather conservative curve (based, however, on
many measurements) for larger transformers (>40 MVA).
The measurements approximate a Gaussian distribu-
tion-for distances less than 30 m there is a standard devia-
tion o = 2 dB(A); between distances of 30 and 100 m, o in-
creases from 2 to 4 dB(A); and, for distances greater than
100 m, a = 4 dB(A).
Therefore, in order to make a prognosis one starts with
equations such as (1), (2) or (3), and with the aid of Fig. 4,
one can establish the sound levels around a transformer or a
transformer station. If the levels are to be decreased by bar-
rier screening effects, one has to take into account the dif-
ferences in radiation of both the cover and the walls of the
tank (for example, see Fig. 5).
processes using a combination of hydrocyclones, drum
filters, washings thickeners, and countercurrent decanta-
tion thickeners, the solid/liquid separation process being
facilitated by the addition of flocculating agent and glue.
The solids are separated and discharged as tailings and
the pregnant liquor clarified and filtered to prepare for
ion exchange or solvent extraction.
Generalized flowsheets for acid and alkaline leaching
are shown in Figures l and 2respectively.
Both ion exchange and solvent extraction are used in
conjunction with acid leaching in processing Canadian
uranium ores. lon exchange can be performed batchwise
or continuously, using strong-base anion exchange resins,
which can be placed in fixed or moving beds. After
passing through the resin bed, the emerging solution,
which has been stripped of uranium, can be directed to a
process in which yttrium is recovered. used as a washing
liquid in the previous processes, or discharged. Elution
of the uranium adsorbed on a strong base anion
exchange resin is effected by an acidic solution of nitrates
or chlorides, which displaces the uranium sulphate
complexes LUO;(SO,$', UO4(SO,') from the resin.
Iron, which is weakly adsorbed by the resin, will be
displaced and will appear in the first fractions, which
can be recycled back to the leaching stage or precipitated
by addition of ammonia, magnesia or lime, The subsequent
fractions, after a thickening and filtration step, are ready
for precipitation with ammonia or magnesia to give the
end-product of the process, yellowcake.
Solvent extraction works on the same principle as ion
exchange; in this case, instead of a solid resin, the
extracting agent is contained in an organic solvent. In
the Amex procesg88-4', the extracting agents are alkyl
amines. ln this process, the sulphuric acid leach liquor
containing about l,2 g of U,Og per litre is passed
continuously through 3 to 5 stages of mixer-settlers in
the opposite direction to a stream of alkyl amines in
kerosene. The aqueous phase, containing less than 0,005 g
of U4O4 per litre, is drained off as the raffinate, while
the extract in the organic phase is passed on to a
stripping process in which the stripping agent is acidic
chloride or nitrate solution. The resulting extract contains
about 40 g of U4O4 per litre, from which uranium can be
recovered by precipitation with ammonia, calcium oxide,
or magnesia. The amine can be regenerated in a separate
cycle and used again in the extraction step. Solvent
extraction is at present used at Agnew Lake and
Espanola, Ontario, and the Gulf Minerals mine at
Rabbit Lake, Saskatchewan.
Other forms of the solvent extraction process include
the Dapex'' process which uses a solution of D-2-EHPA
(di-2-ethylhexylphosphoric acid) and TBP (tributylphos-
phate) in kerosene. The function of TBP is to act
synergistically with D-2-EHPA, as well as to facilitate
separation of phases. In the Eluex' process, a com-
bination of ion exchange and solvent extraction is used.
Sulphuric acid, which is the eluant in the ion exchange
of 6, and finally thorium with 5 per cent rare earths at a
pH value of 10.
Alternatively, thorium and rare earths can be recovered
from solutions or further purified by solvent extraction
or ion exchange.
Nitric and hydrochloric acid leaches dissolve radium
from the ore almost quantitatively. The efficiency of
Both solvent extraction and ion exchange depend on
the ability of one of the two phases in tontact in the
system to concentrate one or more of the substances
present. In solvent extraction of uranium, the extracting
phase is usually a solution of an aliphatic amine or an
alkyl phosphoric acid dissolved in an inert organic solvent
such as kerosene. In ion exchange, this phase consists
of beads of resin that incorporate in their polymeric
structure functional groups such as the quaternary
ammonium group. The fundamentals and applications
of solvent extraction have been extensively reviewed in the
literature'9e-1 14
The kinetics of uranium extraction by di(2-ethylhexyl)
phosphoric acid are reviewed by Coleman and Roddy''
In the various mechanisms for solvent extraction of
metal compounds''-1 the mode of action depends
on inter-ionic processes such as ion association and
chelating, which render a complex uranium ion less polar
in the aqueous phase and therefore preferentially solvated
by the organic phase of lower dielectric constant. In an
aqueous solution, uranium(VI) oxide is in equilibrium
with the uranyl ion, UOj', which, in turn, sets up
equilibria with the anions present (sulphate ions, for
example), forming first the neutral uranyl sulphate.
UO,(SO,, then a series of complex uranyl sulphates,
precipitation as a removal mechanism is also almost
quantitative. Whether these characteristics can be in-
corporated into a process - so avoiding the discharge of
large masses of solid tailings containing -Ra - depends
on economics, on second order technical considerations
with respect to the leaching process, and on the implica-
tions of the subsequent separation process for the uranium
(ion exchange or solvent extraction).
UO;(SO,);P, n = 2, 3,... It is known that certain
amines, especially those containing Cg-C4istraight chain
alkyl substituents, readily form ionic association com-
pounds with uranyl sulphate anions in acidic solutions.
In practice, these ionic association compounds are ex-
tracted into an organic phase (kerosene), which also
serves as a diluent for the amine. This anionic extraction
of uranium is represented by the following equation
(using UO;(SO,1$' as an example):
Cationic species, such as the uranyl ion UOj'', can be
extracted with a number of alkyl phosphoric acids,
among which are D-2-EHPA. dii2-ethyl-hexyl) phos-
phoric acid, or TBP, tributyl phosphate. Thus D-2-EHPA,
expressed as (RO),POOH, reacts with UO{' in the
following way:
2RO),POOH + UOj* = [uRO),POO],UO4 + 2H'
giving [(RO),POO],UO4, which enters the organic
phase. Neutral compounds, such as UO,SO,''* and
UOj(NO4)4.6H,O'' can also be extracted into an
organic solvent,S,for example, as the molecular compound
UO;(NO4)4,mH4O.nS.
The chelating actions of many reagents have been
utilized as a means of extracting uranium in well-
developed analytical procedures of separation and quanti-
fication'-' The formation of uncharged chelates is
especially suitable for the extraction process. Palei''' has
compiled an extensive list of instability constants for
various uranyl complexes. A partial list is shown in
Table 4.
Of some interest are developments in the use of EDTA
as a masking reagent in the extraction of uranium from
many coexisting metal cations. This procedure takes
advantage of the fact that EDTA forms stronger com-
plexes with most interfering ions than with uranium, from
which the former can be removed selectively under
carefully controlled conditions. The synergistic action of
reagents acting in concert with extractants for the
extraction of uranium is reviewed by Healy' and D'
An example of this synergism is the 10'-fold enhancement
in the rate of extraction of uranium by TBP upon the
addition of the chelating agent HTTa19 From an
extensive observation of synergism in such systems,
Irving and Edgington'' postulated the necessary con-
ditions under which action can occur. The antagonistic
effect exhibited by certain combinations of reagents also
could be conceivably utilized in maintaining some selecti-
vity in separations by extraction.
Experimental studies of the extraction process in
terms of ion association or complex formation is difficult,
mainly for the following reasons:
Yagodin and Tarasoy'' have reviewed experimental
techniques applicable to kinetic studies. De, Khopkar, and
Chalmers'''%; amongst others, have extensively studied
the physical chemistry of the extraction process in general,
as well as specific extractions involving chelating agents
or solvating solvents. The extraction process may be
summarized as the formation of a complex H,,M,,,lL;
(H4O).S, between metal M, ligand L, and solvating
solvent S, and proton H, the coefficient of extraction, E
defned asi[M]..,%[M].,, beingespressed as
and
A plot of log E versus log [H''] produces a straight
line whose slope is equal to ma, 47, from which
the nature of the extracted species can be deduced
(equation [ 1]). For example. a slope of + 1 indicates that
the extracted species contains one proton per molecule
of extracted complex.
For the extraction of chelates, De, Khopkar, and
Chalmers''' derive an expression to account for the rapid
increase of extraction coefficient within a narrow pH
range, assuming the existence of a neutral, unsolvated
species MLL,, and using Ringbom's z-coefficients method
to calculate the effect of side-reactions on equilibria.
From this expression, it is possible to calculate the
pH value at which 50 per cent of the extraction will take
place.
For the formation of an extractable solvated complex
M(NOss), (HsO).S, by solvents such as TBP (represented
by S), through the equilibrium:
where (w - sx)HyO represents the water transferred
from the aqueous phase into the organic phase, and
sS(HO),, the hydrated solvent, an application''* of the
general treatment of Irwin, Rosottii and Williams produces
the expression:
where s measures the average solvating number in the
organic or the aqueous phase. Since 8., 0, the slope
of a plot of log E versus log [S] is equal to -aa,,the number
of S in the extracted solvated complex.
The theory'? and thermodynamic analyses'A.4' jn
connexion with the UO,'/HNO,/TBP system have
been well developed and reviewed.
The mechanism of extraction of uranyl salts was
studied by the shifts of infra-red absorption of the P=<O
vibration band (40 to 60 cm'') owing to its bonding to
the acid in the organophosphorus/uranyl/acid system'
The relation between molecular structure and extract-
ability was studied. Similarly, visible and ir spectro-
photometry'' was used to determine the structure of
the uranyl complexes of tetradecylammonium sulphate
in alkaline solutions.
Mechanisms have been proposed for the synergistic
as well as antagonistic effects observed in systems such
as the H,BuPO,/Bu,PO, and the H,BuPO,/tri-n-octyl-
phosphine oxide systems''; the tri-n-octylamine/2,6,8-
trimethylnon-4-yl phosphoric acid system''', the diben-
zoylmethane/neutral donor solvent system''%; and other
organophosphoric acid/amine systems'*
Closely related to the extraction process is the stripping
process, which works on the same principle. In this case,
the uranium extract is treated with aqueous solutions of
a stripping agent that forms still stronger ion association
compounds or coordinate complexes with the extractants,
thus removing the uranium ions back to the aqueous
layer.
When applied to the practical problem of processing
of the candidate alloys is shown in fig. 3. The existing
rules of water reactor materials are not relevant for the
high temperature alloys to be used in HTR applications
due to the different material characteristics and operat-
ing conditions of water cooled reactors.
At temperatures where no strain hardening is ob-
served, the 3S,,,-criterion used in the ASME code for
limiting cyclic plastic deformation is also not relevant.
At 800 C and above, it is therefore suggested that
components should be assured against a flow stress
limit. The permissible stress limit can then be de-
termined using an appropriate formula incorporating
elastic and creep behaviour. Below 800 C, plastic defor-
mation and strain hardening are observed in the tensile
test and the methods of ASME code, case N47, may be
permissible.
From the metallurgical point of view, high creep
strength and resistance to loss of room temperature
ductility are to a certain extent contradictory. The
standards for room temperature ductility which are
applied to water reactor components cannot therefore
be applied to creep resistant high temperature alloys.
For HTR components the expected loadings at room
temperature up to the service temperature do not re-
quire a high level of deformability, provided that start-
up and shutdown operations can be carried out without
inducing short term impact loading.
INCONEL 617 is the principle candidate alloy for
the IHX tubes and the hot gas header due to its high
creep strength [5]. Typical creep curves at 950 C are
shown in fig. 4. A well-defined secondary creep stage,
which is obtained for austenitic materials in the temper-
ature range 500-600 C, is not obtained, and the onset
of tertiary creep cannot be reliably determined. For the
evaluation of such curves, consideration of the following
is proposed:
The onset of tertiary creep should not be introduced. In
fig. 5 the 1% strain limit and rupture strength at 950 C
are plotted. The S, values is calculated by taking the
lower of the following stresses:
These stresses are derived from creep tests, carried out
in the temperature range 800- 1000 C for times in ex-
cess of 20000 h (fig. 6), the data being eNtrapolated to
required service times by a modified Larson-Miller
parametric method [12]. Comparison of the available
data determined in air and in simulated HTR helium
show no significant influence of test environment on the
creep rupture strength parameters.
at 580%C. If the stabilization ratio is adequate (> 8.7)
decarburizing effects resulting from carbon pick-up by
the austenitic steel are insignificant. Investigations in
component test circuits confirmed this positive result
For the relations described below, the quantity of
carbonitrides and the intermetallic compounds are of
particular importance [8]. The degree of stabilization
Nb/(C + N) gives the weight ratio of Nb to carbon and
nitrogen. With stabilization ratios of Nb/C = 10 experi-
ence shows exclusively Nb-carbonitrides in the micro-
structures.
The free niobium content (3 Nb) gives the amount of
Nb which remains after the precipitation of all niobium
carbonitrides. This free Nb content required for oversta-
bilization is estimated according to the equation:
The aim of the overstabilization is the improvement of
thermal stability of the microstructure of the alloy by a
second precipitate, the so-called ''Laves-phase''
(NbFe,), a hexagonal intermetallic compound.
The solidification of 2.25 Cr-1 Mo-Nb steel can be
understood after studying the binary and ternary phase
diagrams iron, carbon and niobium (figs. 5, 6 and 7)
according to [9,10,11]. According to these diagrams
solidification starts with the growth of -iron (solid
solution) crystals from the liquid. The liquid phase
enriches in carbon and Nb according to the ternary
system (fig. 5) [9]. When the composition reaches the
Fe-NbC eutectic temperature solidification of primary
Nb-carbides arises in the divorced eutectic or eutectic
microstructure. If there is a high amount of niobium the
ternary Fe-NbC-NbFe,-eutectic microstructure may be
expected (fig. 5). After cooling to approximately 1210 C
and more than 1.2% Nb in the steel there occurs,
according to the Fe-Nb diagram (figs. 6 and 7), a
eutectoid transformation of the 8-iron to y-iron and the
Laves-phase (NbFe ). Up to now chromium rich phases
(carbides, intermetallic compounds) have not been
found.
Better known than the microstructural reactions
above Ac4 are those at lower temperatures [12-15].
According to the continuous cooling diagram (fig. 8) the
microstructures obtained under various cooling condi-
tions are ferrite and bainite; whereas martensite was not
demonstrated after an austenitzation at 1020 C [13].
The niobium and carbon content in solid solution
may increase during austenitization (see fig. 10).
According to [17] this affects the microstructure. After
austenitization the y -+ transformation will be delayed
due to an increasing amount of free Nb, The formation
of bainite occurs even at slower cooling rates, see fig. ll
I1S1
Due to the very low amount of carbon in solid
solution after austenitization the formation is reported
of a very low-carbon bainite without the presence of the
characteristic carbides in the bainites [13]. Under the
different quantities to be measured to enable a separa-
tion. During the measurement of magnetic quantities
stresses of second order are captured which may have a
high importance concerning the primary damage. The
separation of macro- and microstresses is relative easy
in looking at the spatial distribution. For more detailed
information see refs. [5-13].
The questions concerning flaw detection are answered
generally. The equipments and systems fulfill the re-
quirements. In addition an improved quality of the
material, specially the higher ductility and the limitation
of the stress level or the avoidance of stress concentra-
tion allow to tolerate more extended flaws. The in-
terpretation of the test results has improved too. There-
fore one can minimize amplitude compensation factors
which take into account coupling variation or scanning
path compensation to get a closer relationship between
amplitudes measured and actual flaw.
Despite a couple of problems remain to be solved
like detection sensitivity in testing austenitic weldings or
castings and in the quantitative testing of surfaces or
surface near zones. In daily nondestructive testing of
surface near zones and in the German codes one as-
sumes that
Therefore in testing thick walled components the ultra-
sonic technique and hereof the tandem technique are
required, With increasing resolution both for amplitudes
and TOF one can observe at globular flaws very small
satellite echoes. These are caused from sound paths
either around the circumference of a globular flaw or
from sound travelling back and forth at the surface of a
cracklike defect. These echoes contain significant infor-
mation upon the shape of the flaw. A survey is given in
fig. 41.
eddy current fields by using an AC-magnetizing yoke
where a low frequency eddy current has been impressed
upon-fig, 4.9. The magnetic fields belonging to the
eddy currents are sampled with a magnetic probe or
stored with the aid of a two-dimensional magnetic tape.
Fig. 4.9 shows a pore in an austenitic material and
magnetic image of very difficult detectable stress corro-
sion cracking. During the development of the magnetic
crack inspection of non-ferromagnetic material we have
thought and will do it in the future too, to replace the
electrical potential probes used in the electrical poten-
tial method. The high sensitivity specially in the case of
stress corrosion cracking was a very surprising result.
Despite this under the name of ''current pertubation''
the ''current inspection with contactless working mag-
netic probes'' is known for many years.
The eddy current method used for the inspection of
weldings in the surface zone has been studied by the
commission V of the International Institute of Welding
for many years, The result was a couple of recom-
mendations with the final content: ''Summarizing, there
Is a basis for quaNtitative nondestructive testing (of
austenitic and ferritic steels and nonferrous metals) using
pick-up coils and eddy cturrents''
More and more the multi-frequency eddy current
methods are used in the field to inspect the steam
generator tubes. It is now realized that the multi-
frequency eddy current method is able to solve the
problems of today. The most important region is the
transition of the pipe into the tube plate. Here the
detection of flaws is very difficult because of the de-
posit, We have to take into account three different kind
of flaws: thinning of the wall, pittings and cracks, All of
these flaws occur more often at the outer side of the
pipes. It is very important to detect these flaws in the
transition range of the tube plate, to classify and to
determine the flaw depth. The separation according to
the multi-frequency method with linear relationship or
with polygon-like approximation of monolinear parame-
ters is realized for years. The result of the method is a
fault removed scalar quantity which has no information
upon the kind of the flaw. Becker and Betzold intro-
duced the so-called multi-frequency method as a classi-
fication method. They combined two frequencies with
different capability to suppress unwanted signals, but
with different sensitivity for the different kind of flaws.
The two outputs-Mix l and Mix 2-allow to separate
more sensitive the kind of the flaws- as shown in fig.
4,10-and allow the distinction between surface- and
interior flaws. Today in the critical part of the tube
plate we have to detect:
A still more improved detectability is possible by
using a nonlinear signal pattern. Fig. 4.11 shows the
geometry together with three different positions of the
coil, and the distribution of the electrical field intensity
near position 2. These curves have been calculated using
the finite element model. At the bottom one can see the
dynamic curves of the pipe without any flaw, and
together with sediments in the gap of magnetite and of
copper powder. The positions of the coil are indicated
in the dynamic curves. One can see immediately the
strong curvature through the border of the plate and the
sediments as well as the linear effect of the transition
zone into the rolled zone. The inspection of the transi-
tion zone using rotating ultrasonic probes is less dis-
turbed compared to the eddy current testing, but this
method is very slowly. The adaption of the suppression
method to the nonlinear curves, the use of mathematical
models to analyze the testing procedure and the optimi-
zation of the methods and the coils should further
improve the detectability. It is possible too to replace
the cylindrical coils by an array of small coils, which
will be switched electronically. This would allow not to
slow down the inspection velocity. The '' nonlinear sup-
pression of unwanted signals'' is in a developping state.
Therefore we expect in the near future the solution of
this very complexed and difficult problem.
Reconstruction is the procedure of extracting out of
the flaw indications, the size of the flaws, a procedure
which is mentioned in the KTA-codes but because of
the lack of substance not discussed in detail. We have to
differentiate between two classes of methods:
We like to concentrate in the following upon the meth-
ods with synthesized aperture because these are more
important. Aperture methods collect data along one line
or inside a two-dimensional aperture. Then a software
program is applied which allows to get a very precise
image of the flaws. The display can show different
cross-sections or some perspective views. The most im-
portant synthetic aperture methods are listed in fig.
4.12. They are:
ALOK-Phased Array-Holography--SAFT.
There exist more algorithms which can be applied to the
data, but they are not yet ready to be used to solve
actual problems.
The one which is very close to the inservice inspect-
ion performed today with pulse echo and in tandem
technique is the ALOK-method. ALOK uses the same
manipulator and probes, but instead of evaluating the
amplitude most of all it uses the time-of-flight (TOF)
have been implemented in this way have also been adap-
tive Ferguson meta-assemblers.
Meta-assemblers are a special case of meta-compilers.
Theoretically, a meta-compiler can be used as a meta-
assembler. However, it seems that there have been some
problems when existing meta-compilers have been used
this way. Besides these problems, meta-compilers are not
widely available. Nonetheless, attempts in this direction
have been reported,.1 We should note that meta-
assemblers can be used to complement compilers and
meta-compilers. In this case, the high-level programming
language is compiled into assembly language,'' which is
then translated into machine code. The second step can
be performed by a meta-assembler.
For various reasons, the translation of assembly pro-
grams into machine code is done in three separate phases.
In the first phase (assembly phase), an ''assembler''
transforms the assembly program into a form of code
called ''assembled code.'' In the second phase (linking
phase), a ''linker'' transforms the assembled code into
another form of code called ''linked code.'' In the third
phase (relocating phase), a ''relocating loader'' trans-
forms the linked code into still another form of code
called ''loadable code.'' The loadable code contains the
machine code in absolute form; it can be loaded into the
memory of the target computer with the help of another
tool called the ''absolute loader,'' Note that the linker
and relocating loader can be combined into one tool, the
relocating linking loader.
Splitting the translation process into the subprocesses
described above is desirable because it allows program
modules from different language processors (e.g., as-
semblers, Fortran compilers) to be combined, and it
greatly facilitates the development of modular programs.
Another translation scheme has the assembler directly
produce loadable code, in which case no linkers and
relocating loaders are required. With this scheme,
however, we lose the advantages cited above.
If the assembler-linker-loader scheme is adopted, then
a meta-assembler should be accompanied by a ''meta-
linker,'' a ''relocating meta-loader,'' and an ''absolute
meta-loader'' (or by a ''combined meta-linker relocating
meta-loader'' and an ''absolute meta-loader''). These
tools facilitate the development of linkers, relocating
loaders, and absolute loaders in the same way meta-
assemblers facilitate the development of assemblers.
Special meta-languages are required for this purpose,
namely a ''meta-linking language'' for the meta-linker
and a ''meta-loading language'' for the relocating meta-
loader. The first is used for describing linkers to the
meta-linker and the second for describing loaders to the
relocating meta-loader, in the same way that a meta-
assembly language is used for describing assemblers to
the meta-assembler.
It has been demonstrated that the linking process is
target-computer-independent,'. 12-14 Hence, meta-
linkers can be constructed as simple linkers and no meta-
linking language is required.
A list of existing meta-assemblers is presented in Table
1. Although I do not claim that the list is complete, 1
believe that it is a good approximation to a complete list.
The table presents general characteristics of meta-
assemblers, such as category, intended usage, and port-
ability. Also given are details about the construction of
the meta-assemblers, such as who constructed them, on
which host computer they were developed, and in which
language they are coded. More important, the table is set
up to provide answers to questions about the assemblers
that can be implemented by each meta-assembler: What
sort of assembly languages do they assemble? Can they
be readily used or does the user have to develop com-
plementary tools such as linkers and loaders?
The table is divided into five sections headed general,
construction, assembly langtlage, meta-assembly lan-
4ge, and complementary tools.
The general section is divided into five columns. The
first gives the name of the meta-assembler. The second
gives the class of the meta-assembler according to the
classification scheme shown in Figure 4. The third col-
umn indicates the meta-assembler's usage as intended by
its constructor-that is, the kind of assemblers that it can
implement: software assemblers, firmware assemblers,
or both. The fourth column gives a ''yes/no'' answer to
the question of the portability of the meta-assembler.
mation about the assembly languages that can be assem-
bled by the assemblers that can be implemented by each
meta-assembler. This section is divided into seven sub-
sections. The first subsection gives a ''yes/no'' answer to
the question of whether action verb notation (column
one) or list notation (column two) can be used. The sec-
ond subsection concerns entitv notation and indicates
''yes,''''no,'' or ''not applicable'' for the various entity
haps most important, the court said that the market
must be analvzed in terms of the OEM customer's initial
choice and not in terms of the choices open to him after
his commitment (and lock-in) to RDOS. It is as if the
court thought the OEM sinful or stupid to write applica-
tions programs to run under RDOS and then to be un-
willing to rewrite them to run under another operat-
ing system, In such circumstances, the court seemed to
say, the OEM customer deserves whatever happens to
him, and so too do the plaintiffs for wanting to sell him
a Nova emulator. The court appeared to be suggesting
that the OEM should never have got locked into RDOS
in the first place and that he ought to immediately ex-
tricate himself from the lock-in by rewriting all his ap-
plication programs. What the court seemed to be saying
was that we should not look at the market as it now is,
but at what it should have become or what it was several
years ago. That is unsound.
Surely, the tie-in arrangement stands on no better
legal footing than does an exclusive dealing contract or
requirements contract, and probably it deserves even
less favorable legal treatment.' In effect, Data General
has got a number of OEM customers to agree to buy
their CPUs from Data General, and by doing so has
foreclosed the Nova-emulator manufacturers from sell-
ing to those OEMs. It is immaterial in such a case
whether the OEMs were unwise to sign up exclusively
with one seller or whether at the time they signed up they
had many other alternatives. Indeed, the coercive effect
on the OEMs is probably not as important to the general
competitive process as the foreclosure effect, if any, on
suppliers prevented from selling to those OEMs.l In
cases involving exclusive dealing contracts, and by
similar reasoning in this case too, it is clear that the
market should be viewed at the time of the the suit or the
alleged wrongful foreclosure of which the plaintiff com-
plains, rather than at the time the defendant first signed
up the OEMs.14 The Data General court is on unsound
ground when it insists that the only time to view the
market is the latter time.
The treatment of timing and the concept of ''the''
relevant market badly flaw the analysis in the Data
General opinion. Unfortunately, the opinion does not
provide the data that would permit one to put a revised
relevant market analvsis forward. We know from Data
General's brief and arguments that it had a market share
of 80 to 90 percent of the alleged Nova/Nova-emulator
market, a share of about 17 percent of the total mini-
computer market, and obviously a smaller share of any
iarger market. We can infer that the 80- to 90-percent
figure should be downgraded somewhat, but we do not
know to what extent. For example, a weighted-average
market share, derived by assigning lesser and lesser
weights to the sales volumes of other computers as they
fall competitively more and more distant from Novas
and Nova emulators, could in principle be calculated.l3
That figure might be 40, 20, 10, or 5 percent, depending
on the weighting factors, on other assumptions,!% and of
course on the unavailable sales data.
Going through that exercise, even if we had the data
and techniques, would be of questionable value.'? An
alternative approach is probably more useful. At least
one proper test articulated by the Supreme Court has
been ''whether the seller has the power to raise prices . ..
with respect to any appreciable number of buyers within
the market,''16 The Data General court assumed, for the
sake of argument, that ''barriers prevent competitors
from developing compatible software (RDOS emula-
tors) and that the costs of converting to . . . noncom-
patible software are substantial,''1' [f this assumption is
puter functions as a task processor-it executes user-
and operating-system tasks. Fifty-six kilobytes of ran-
dom access memory are supplied to each LSI-11 as its
private program and data memory, Attached to the task
processor via a direct memory access channel is a front-
end communications processor. This front end, built
around a Zilog Z80 microcomputer, provides an inter-
face between the task processor and each of the two
buses that connect this node to other identical nodes.
The details of managing both the DMA channel and the
link-level bus protocol are assigned to a Signetics 8X300
microcontroller. Each node's hardware consists of a
power supply and a single backplane that accepts all of
the microcomputer circuit boards, including any inter-
face cards for terminals and storage devices.
The Micronet is intended to be a vehicle for ex-
perimentation with interconnection structures and
distributed control. Its architecture is fixed only at the
node level. The nodes can be interconnected in countless
ways by manually changing their bus connections. How-
ever, because each bus is electrically limited to support-
ing about seventeen nodes, large networks will tend to
be built up out of the same repeated structure; i,e,, they
wili consist of many small groups of nodes commun-
icating over a local bus, with the other bus port of some
of the nodes being used to link up ever larger clusters of
groups. Several such cluster topologies, including the
hypercube and X-Tree,* have been generalized by Wu
and Liu,' Figure 2 shows one possible configuration of
16 nodes in the Micronet, with each node sharing one
horizontal and one vertical bus.
Network computers should be able to support fault-
tolerant parallel computation and provide machine ex-
tensibility. However, achieving these goals depends not
so much on the construction of the network computer
hardware as on the availability of adequate operating
systems.% But difficult questions about such operating
systems remain unanswered, They concern decomposi-
tion, distribution, and synchronization of tasks; parallel
applications languages; internodal communications
facilities; task scheduling by global versus local pro-
cedures; static and dynamic deadlocks; and software
recovery from hardware failures. Techniques for dealing
with these questions should be general enough to work
collected from Altadena is ultimately can-
celled and sorted.
A map of the area with a scale of one
inch to six hundred feet was procured.
Small markers were used to designate the
main post office and other points to be an-
alyzed. A rolling mechanical map-meas-
urement device costing under $12.00 was
used to measure distances for inclusion in
the data matrix. Local managers created
the matrix shown in Table 1.
Two trips were conducted. Trip 1 cov-
ered Points 1 through 9, returning to
Point 1 (Figure 1). A second trip begin-
ning at Point 1 (the main office) collected
mail at Points 10 through 13, deposited
mail from both trips at Point 14, and re-
turned to Point 1. Trip ! returned to Point
1 to deposit funds from a contract station
located at Point 5. In the original tour,
these funds would be picked up from
Point 5 just prior to returning to Point 1,
for security reasons. The original path of
travel for both trips, with point-to-point
distances given in feet, is indicated in
Table 2. It should be noted that Alta-
dena's management had restructured the
evening collection less than one year be-
fore this analysis. The superintendent who
gubcables around a central core made up of two strips
yf Nitronic 40. The use of two strips makes the core
more flexible than a single thickness strip would be,
facilitates winding, and reduces eddy-current losses
when an insulating material sheet is sandwiched
between the two strips. Each subcable consists of six
insulated superconducting strands around a stainless
steel strand. The stainless steel strand increases the
tensile strength of the subcable. The turn-to-turn
insulation consists of G-10 spacers co-wound with the
conductor.
The layout of a superconducting ring coil is
shown in Fig. 11, Key design features of these coils
are: a 50-kA cable conductor; pancake winding; a
stainless steel, ring-stiffened case to provide structural
support and to contain helium; sliding pedestal
supports to transmit dead weight, seismic loads, and
the resultant out-of-plane magnetic load to intercoil
support structure; a coil case designed to equilibrate
magnetic bursting load and magnetic bending loads;
dielectric breaks in the case to reduce induced
poloidal currents; and joints sealed by welded omega
seals or double O-rings. The conductor being used for
the superconducting ring coils is similar to that
chosen for the solenoid, i,e., a 50-kA bath-cooled (by
liquid helium at atmospheric pressure), cabled NbTi
conductor.
The key features of the normal (internal) ring
coils are: water-cooled copper conductor, two bolted
jOints per turn, and structural support from the torus
permanent spool. The conductor cross section is
depicted in Fig. 12. The cross section has been chosen
to be as large as can be practically drawn with
extruded holes so as to minimize the number of
joints, The conductor is spirally wound into two
pancakes separated by a pancake-to-pancake insulat-
ing sheet. The conductor is wrapped with turn-to-turn
insulation prior to winding, or an insulating strip can
be co-wound with the conductor. Two joints per
turn are planned so that the coils can be initially
assembled in the bore of the TF coils. Should replace-
ment be necessary, the space constraints in the bore
by the nearby grounded surfaces. The lower energy
ions are deflected before exiting the gas cell.
A magnetic shield composed of 15 cm of ingot
iron surrounds the gas cell changer (-2 X 6 X 8 cm).
This shielding isolates the ion source, gas cell, and
direct recovery equipment from the stray magnetic
fields. The magnetic field is reduced from -1 kG to
sC8 G in the vicinity of the gas cell.
A nuclear shield composed of stainless steel, lead,
boron carbide, and water surrounds the drift duct and
transition sections. The NBI box is housed inside an
lgloo of similar composition or borated concrete. The
shield thickness is 60 to 100 cm and weighs -350
tons, Access panels allow removal of the replaceable
units of the ion source when the beamline is in place.
The FED electrical systems consist of power
handling and conversion; energy storage; and plasma
diagnostics, instrumentation, data acquisition, and
control. Each of the electrical systems is discussed
below.
Table XIV shows some of the photon and particle
fluxes, synchrotron radiation, and tritium levels
expected with nominal operation of the FED. It
shows that even the D-D plasma represents a severe
radiation environment.
Certain of the tokamak sectors are dominated by
maior subsystems, as shown in Fig. 32. Of the ten
sectors, four have ICRH launchers, one has a fueler,
and two have been designated as test sectors. Three
sectors will be dominated by plasma diagnostics. It is
expected, however, that there will be some diagnostic
instrumentation in all sectors.
Vertical access to the plasma from the top
requires penetration of the shield, the spool structure,
the segment vacuum boundary, and the armor tile. If
the diagnostic requires a path through the plasma and
through the bottom of the torus as well, then the
penetration passes through the limiter blade, the
vacuum duct, more shielding, more structure, and
finally the floor support structure. If a long colli-
mator is necessary or if the diagnostic detector must
be in a low magnetic field, then either the upper or
lower vacuum chamber related to the cryogenic
system would have to be penetrated as well as the
bridge-like structure that holds the TF and PF coil
system in place.
There are basically three kinds of penetrations
required for FED plasma diagnostic instruments.
These are shown in Fig. 33. The first hole penetrates
the shield but may be sealed at either or both ends.
This hole can be used for neutron measurements,
Detectors in this location have to be designed to
operate in a high magnetic field (3 to 5 T) and high
temperature (up to 300'C).
The second hole is one that provides direct access
to the plasma for a radiation signal or particle flux
and/or provides direct access for a detector. Instru-
ments requiring such access are neutral particle
analyzers, x-ray detectors, bolometers, and vacuum
ultraviolet (VUV) spectrometers. These instruments
will be exposed to high-energy neutrons, gamma rays,
and tritium, as well as to high levels of plasma radia-
tion such as synchrotron and bremsstrahlung.
The third type of hole involves the use of win-
dows and mirrors. Neutron and gamma-ray radiation
are important considerations. Tritium will be con-
tained since a vacuum boundary can be established
at the window. Instruments utilizing such holes are
interferometers, long-wave spectrometers, and syn-
chrotron detectors.
Since each type of hole will expose the diagnostic
windows, mirrors, seals, and detectors to high levels
of neutron and gamma-ray bombardment (and in
some cases tritium), the identification of acceptable
materials, components, and designs will require addi-
tional effort.
Table XV provides a listing of the presently
planned control diagnostics for FED and indicates the
information needed, the sensor, and the action and
shows the minimum set of instruments required to
ments in both heads and velocities. As Douglas, Ewing, and Wheeler [5]
demonstrate, however, the inclusion relationship between the divergence of the
velocity trial space and the head trial space is an essential fact in deriving these
error estimates.
The error estimates have implications for problems involving nonhomoge-
neous media. In standard formulations with spatially heterogeneous trans-
missivities the calculation u = -TVk calls for the multiplication of a function,
T, that may be rapidly varying for physical reasons, with another, h, that may
vary rapidly simply by virtue of its being the gradient of a spatially varying ap-
proximation. Such a product of rapidly varying functions may be quite poorly
behaved in numerical models. The mixed method avoids the numerical noise
associated with differentiation of heads and therefore does not compound physi-
cal fluctuations with artificial ones.
Douglas, Ewing, and Wheeler [5] also give theoretical justification to the
subtraction of singularities. In this case both the first- and second-order
schemes give global error estimates of the form
where, again, M, and M, are constants for a given boundary-value problem.
These estimates ensure that the velocities predicted by the mixed method will
converge to the exact velocities near pumped wells when the trial function ü
explicitly incorporates simple poles at the wells.
To illustrate the effectiveness of the mixed method we shall examine a simple
numerical example. Consider the equation
on f1 = (0, 1) x (0, 1) with (2 = 8(x - (1,1)) and u: v = 0 on af1. We
shall examine various pressure and velocity solutions for this boundary-
value problem.
Before discussing the numerical results, however, it is worth reviewing our
choice of bases for the trial spaces V, and W,. For convenience let us tempo-
rarily use the variable z to stand for either x or y, let the partition in the z-
direction be ,1 z4 < z, and call 3z, z, ,-M = 1,. ., N.
Define the functions [v,};2, 1 as follows. If y is even, v, is the standard
piecewise linear chapeau function having v,(z4) * 8,g. If y is odd, say y =
2K - 1, then v, is the piecewise quadratic given by
Finally, the mixed method gives good numerical results even in problems with
rather severe heterogeneities in medium properties.
This research is supported in part by Contract No. DAAG29-84-K-002 from the Army
Research Office, by Grant No. CEE-8404266 from the National Science Foundation,
and by a grant from the Wyoming Water Research Center.
In a recent paper Phillips and Rose [1] described a compact system of fi-
nite difference equations to treat the equilibrium of elastic bodies. For any brick
cell of volume 0(h') within the body these equations express, to second order
accuracy, the relationship between the traction forces and the displacements on
the faces of the cell which results when the cell is in isolated equilibrium.
Global equilibrium then occurs when the net traction force across a face
common to any two cells vanishes; this condition serves to eliminate the trac-
tion forces as variables and results in algebraic conditions for equilibrium
which are expressed in terms of the displacement values on the sides of neigh-
boring cells. As shown in [1], these algebraic equations seem particularly
suited to solution by iterative techniques which can exploit the parallelism of
computer architectures.
In this method, since each cell is in isolated equilibrium, the potential energy
in the cell is equal to half the work done by the traction forces and displace-
ments on the surface of the cell. The total potential energy within the body
can then be calculated, and the principle of minimum potential energy simply
results in the condition that the net traction forces vanish across common
cell faces.
This article examines this idea when general volume cells are employed. An
approximate equilibrium condition for each cell is obtained by means of the
construction of a transmission matrix which, when applied to displacement
values on the faces of the cell, yields approximate traction forces on cell faces
thus producing equilibrium in the cell. We call these equations compact finite
element equations. Equilibrium throughout the body is then obtained by deter-
Figure 6 illustrates a square cell on whose sides the indices (i t 1,j t 1)
indicate points corresponding to the centerpoints of sides of length 2h.
The traction values at the centerpoints of the sides, labeled counterclock-
wSe, are
The transmission matrix [T7] provides the relationship between [ p7! and Iu,,].
Introduce the standard finite difference notations
with corresponding definitions for ,s, ;. R-.,,. In [5] the following compact
scheme was described:
The fact that the approximation (73) leads to (76) may be verified by noting
that each of the functions [1,, y,x' -- yf] in the approximation basis satisfies
these equations. We may write (76) as
The definition (75) for [p2] then allows (76) to be expressed in the form
I71 = IT7Ht,1
The algebraic equations which result from the balance of traction force con-
ditions between cell sides have been studied in [5]. Standard iterative methods
appear to work quite successfully.
(ii) A condensation method: The representation (60) for the transmission ma-
trix for a triangular cell relates the traction values [ p.] on the sides of a triangu-
cooling of such a liquid one will expect a separation into
two liquids at the moment the temperature reaches the
limit of the miscibility gap. Ihe precipitation of a
second liquid will mostly take part in the same way as a
normal precipitation, i.e. by nucleation and growth.
Assuming that a nucleus has been formed in the bulk
of the liquid, the nucleus probably will grow in a
spherical shape and the growth will be diffusion con-
trolled. With small supersaturations the growth rate
can, according to Zener,' be described by the following
equation:
where R = radius of the Bi-rich droplet, V the
molar volume of the Bi-rich liquid, V@ = the molar
volume of the ZZn-rich liquid, A4; = the concentration
of Bi in the original liquid, X j' = the concentration of
Bi in the Zn-rich liquid in equilibrium with Bi-rich
liquid, X ;' = the concentration of Bi in the Bi-rich
liquid in equilibrium with Zn-rich liquid.
In our case there are so many particles that there is an
impingement between the particles. The solute in the
liquid x4; will thus decrease during the precipitation
and the growth rate will decrease. This means that the
supersaturation decreases with time, we will in the
future denote Xg;, with Ag!. Ng! can be derived from the
following material balance
where N = the number of Bi-rich droplets per unit volume.
Combining Eqs. [l] and [2] one gets:
This equation can be compared with Eq. [l]. One can
see that if we ignore the second term in the brackets
(this can be done if we have very few and/or very small
particles) the equation is equal to Eq. [1]. If there are
many large particles the growth rate for each particle
will be small and the left hand side of the equation can
be ignored. This means that the particle size will be
determined by the number of particles or by the level
rule which is equal to Eq. [2] if Ag! is substituted with
4'
In order to obtain information about the particles size
as a function of time and cooling rate, Eq. [3] was
numerically solved.
In the calculations Xj;' and A;{' have been related
to the temperature by the limit of the miscibility gap.
Fig. 1. The temperature as a function of time has been
taken from the experimental data, Figs. 2 and 3. During
the calculations the temperature has been divided in
discrete intervals during which the time-temperature
curve as well as the limit lines of the miscibility gap
have been considered to be linear.
F igures 20 and 31 show the caiculated particle size as
a function of the number of particles per cm. Figure 20
shows the results of the experiments with xg, 0.02 and
the two cooling rates,
The diagram shows that the particles have grown to a
maximum size of 85 gm at the normal cooling rate. This
is somewhat lareer than the mean size observed in this
sample, Fig. 5, but twice as small as the maximum
cooling rate in the sample. For the same alloy with the
lower cooling rate the difference between the calculated
and the observed maximum particle size was even
larger, four times in fact.
Figure 21 shows the calculated results for the alloy
with 24 pct Bi, A comparison between the calculated
(110 ym) and the maximum particle size measured (500
um) shows a difference of roughly 5 times. In this case it
is also of interest to compare the calculated maximum
particle size (110 um) with the maximum particle size
from the casting experiments (1000 um). The cooling
rate was roughly 40 times larger in the casted sample
than in the case considered during the calculations but
in spite of that the particle size was 9 times larger.
The calculated particle size are thus much smaller
than the observed particle size, This must be due to a
coarsening mechanism during the cooling sequence.
Such a coarsening mechanism can either be due to a
reduction of the surface energy (i.e. Ostwald ripening),
or due to a freezing of particles at a collision. To get an
idea about the most dominating mechanism in our case
the following calculations were made.
We will first consider the effect of Ostwald ripening.
According to Lifshitz' and Wagner' the predicted time
dependence of the mean radius is found to be
where R, is the original mean particle size at onset of
coarsening. In our test of the importance of the ripening
we will assume that the alloy is kept isothermally at a
temperature just above the monotectic temperature.
Taking a value of 10*J/mfor the surface tension and
putting in the data for alloy one into Eq. [4i and
rearrangmg one gets
R/R,, is plotted in Fig. 22 as function of R,, for three
different holding times (1, 10 and 100 s). The figure
shows that there are no or very little coarsening if the
particle size are larger than l um even at the longest
holding time.
The large differences between the observed and
calculated particle size must thus be due to a collision
between particles during the growth process. This was
often observed in the casted samples, see for instance
Figs, 7 and 10. However, a collision coalescence has
probably also occurred in the samples cooled under
microgravity. Some indications in that direction were
given by Figs. 19 and 20. In space, there is some gravity
left. In our case the gravity was roughly 10 m/s% In
evolve in the direction of maximum surface energy
decrease.
The subspace of vectors perpendicular to N has
codimension one, and must contain i (since i is tangent
to the constant volume surface). Of all these possible
values for i, there is one that tends to bring r closest to
the origin (see Figs. 1 and 2). Let this new value of r be
y, Then, since the perpendicular distance is the shortest,
the vector v must be perpendicular to i and parallel to
N, Thus v = 5N for some 5 0. We can write
where y is positive and may possibly vary with time, or
The value of f is obtained by taking the dot product
of the above equation with respect to N and recognizing
thati . N = 0. Thus.
If we now define a critical radiusr as
Eq.[5] becomes
or, rewriting for the i' particle.
The constant o is analogous to the constant K in the
GLSW equation with the proviso that it may vary with
time, Equation [9] is the geometric equation for coarsen-
ng.
The expression for dS ,/dt can now be derived from
Eqs. [3]and [4].
Let 9 be the angle between the vectors r and N. Then,
and the above equation becomes
which is negative because the surface area is decreasing.
The GLSW equation for diffusion controlled
coarsening 1s
From the condition that the volume fraction is
constant, it can be shown that r' = r, the average
particle size.
Application of Eq. [3] to the GLSW equation yields
or
The surface area is decreasing because 1/r s 1/r.
fracture surface was composed of grain boundary facets
covered with many small dimples and was similar to
creep-ruptured surface with cavity-type cracks,'
The influence of grain size on the fatigue lives at
700 C for eight kinds of austenitic stainless steels is
shown in Fig. 10. For comparison of the fatigue lives
assigns the boundary atoms the same charge q as the
bulk atoms, instead of 0.75g as we have assumed for the
calculation of E}},, However, it can be shown that the
true value of E', must be very close to the average of
the value we have calculated and the value calculated in
the same manner but with all charges set at 0.75q.
Therefore, the error of assuming all charges to be ; is
approximately one-fourth of E7A,, This does not sub-
stantially affect the results. We have nevertheless used
the average in Table II, thereby decreasing the error
further.
In the case of CuZn, our value of 40 ergs/cmis
substantiallv smaller than Potter's value' of 60
ergscm'; which was based on models of pair-bonding
energies. As with the order-disorder theory, our values
have the advantage of not depending on experimentally
derived data, other than crystal structure. This low
value of b ,-y 1S consistent with the experimental
superpartial dislocation separation in 5-brass. Accord-
ing to the usual theory, the mutual repulsion of two
superpartials is opposed by the surface tension of the
strip of A PB created between them: a basic calculation
results in the following expression for the equilibrium
separation S of screw superpartials-
Using values b = 2.56A'and C,, = 12.91, C, = 8.24,
C ,; = 10.97 (10'} dyn/cmA).'%we get S = 120A. This is
large compared to other estimated values; for instance.
Potter's' value is 10 to 40, Marcinkeowski's is 5634
and Umakoshi et al have estimated 2334, However,
Broom and Humble who were the first to view ß-brass
superpartial separation successfully, have suggested a
separation distance of 120&,4 which agrees with our
result.
We have demonstrated, by calculations of order-
disorder energies, critical temperatures, antiphase do-
main boundary energies, and superpartial separations,
the simplicity and accuracy of a new semiempirical
theory for the energy of formation of alloys. This theory
is superior to conventional Bragg-Williams, or inter-
action energy, approaches because it is relatively free of
experimentally derived parameters. It is expected that
this theory will be useful in calculating physical prop-
erties of a large class of alloys.
Following Hodges and Stott,* let's consider the
ground state of the alloy, with electron density p,(r) and
also a state with electron density p,(r) close to p,(r), the
two states corresponding to the same external (nuclear)
potential and same total electronic charge. The alloy is
assumed to have N atoms. U sing Density Functionai
Theory,' the difference in energy between the two states
can be obtained by carrying out a Taylor expansion of
E [p] about p = r,
where o(r) p,(r)- p,(r). The term in 3p(r) does not
appear in Eq. [A1] because [aE[p]!o()l,, s independent
of r, and then 2p(r) integrates to zero, We will oniy
consider the first term in Eq. [A1]. Using the notation
let's separate the double integral into a sum of integrals
over atomic cells (considering the partitioning of the
alloy volume defined by the prepared alloy)
where il. is the volume of the i-cell. Both sums in
Eq.[A3] run over all the cells, Let's separate Eq.[A3] as
To evaluate I(r,r'), let's write the total energy of the
alloy in cell form
where E, is an effective atomic energy of the i-cell and
b,, is the electrostatic Madelung energy associated with
the net charges in the cells. Williams et al'? have
recently demonstrated that such a decomposition is
possible, provided the electron density and the one
electron potential are taken to be spherically symmetric
in each atomic sphere. Note that E,, in Eq. [A5] is zero
ards, fish. snakes. seals, birds, elK, giraffes. dolphins. horses.
and turtles in the wild and dogs, cats, tats, tabbits, monkeys, and
baboons in the taboratory. Moreover, virtually every physiologic
parameter has been monitored with biomedical telemetry. Elec-
trocardiogram ECG), electro encephalogram (EEG), electromyo-
gram (EMG). hydrogen ion concentration (pH), temperature. pres-
sure, muscle contractile forces. gait, and blood flow are the
physiologic data that are most frequently transmitted with the
benefits of biomedical telemetry.
Miss Downs. my ninth grade geographyihistory teacher, once
said, ''Opportunity knocks once. The question is: Wlill you be tea-
dy?'' When asked to be the guest editor of this EMB Magazine is-
sue on Biomedical Telemetry, I recognized the opportunity to as-
semble the history, current techniques, and applications of
biotelemetry from the giants of the field in an effort to at least
partially explain to the uninitiated some of the art and mystery of
this special instrumentation technique. The contributors have
truly ''done it all.
Stuart Mackay was in biotelemetry from the very beginning
and gives us a glimpse of the early developments and evolution
of the field. Dr. Mackay's message is replete with examples and
applications to an impressively wide variety of animal species.
Many biotelemetry devices are implanted to obtain the most
natural physiologic data possible without physical restraints. Im-
planting biotelemeters places them in a very hostile environ-
ment, and they must be sealed against that warm, moist environ-
rment. Wen Ko and Thomas Spear are at the forefront of
packaging for implantable devices. They describe many ways to
encapsulate sensitive micropower implantable electronic de-
vices and include evaluations of their effectiveness.
For Francis Long and Richard Weeks. the world is their lab.
Their activities in wildlife biotelemetry have included studying
fish, elk, deer, birds, and badgers in naturai habitat to better un-
derstand migration, feeding, and reproduction patterns, as well
as the effects of stress (often induced by man).
Miniature and micropower are two cornerstones of modern
biotelemetry design and construction, Improvements in these
areas have closely paralleled the evoiution of semiconductor anc
microcircuit technologies. Reliable. stable integrated sensors
and biotelemeters on microcircuit ''chips'' are a reality today.
Jim Knutti describes some of the microcircuit designs and im-
plementations he has been involved with in recent years. The
work is truly state-of-the-art.
E]i Fromm has provided an example of a ''poor man's'' hybrid
biotelemeter to illustrate that- some rather sophisticated circuit
Operations can be done on a low budget and without extensive
microcircuit capabilities. His comments focus on a design for a
two channel, FM.FM formatted implanted biotelemeter for mul-
tiple channel monitoring using resistance type transducers.
The thrus1 of this issue is to introduce those who might be un-
familiar with biotelemetry to what the field is all about and to per-
rit those with some involvement to review what is currently hap-
pening. Biomedical telemetry like many other things began as a
''aboratory curiosity'' but has evolved to a useful, reliable tool
for data gathering. It has become an important, often complex.
part of physiologic monitoring, but it also can be exciting and a
lot of fun.
he purpose of biomedicai telemetry is
to monitor or study animals and hu-
mans with minimal disturbance to
their normal activity and to explore other-
wise inaccessible parts of the body. It cov-
ers a variety of situations.
Animal subjects range in size from bees
to whales; useful transmission distances
vary from a centimeter to a few thousand
kilometers and ttansmission times from a
few minutes to a few years to the life of the
Subject; frequencies range from 40 kH2 to a
few hundred rmegahertz; subjects range
from trees to humans and include animals
flying, burrowing in the ground or under
Snow, and swimming in fresh or salt water,
transmitters can be implanted surgically,
swallowed, inserted through other normal
body openings, or carried externally; power
can be induced inward for tissue stimula-
tion to energize transmitters and to pro-
duce mechanical motions; transmitters
monitor safety of workers in hazardous sit-
uations, carry signals from sterile regions,
mark animals with darts, and enhance or
reduce reproduction data; they have been
used during a variety of situations includ-
ing sleeping, loving, working, eating, lec-
turing, and diving. All this can be done with
biomedical telemetry without subject dis-
turbance.
Many experiments can be included un-
der the literal meaning of the term teleme-
try. One of the first authors to work in the
spirit of biotelemetry today was the
Frenchman Marey, who wrote a treatise on
terrestrial and aerial locomotion - ''Ani-
mal Mechanism'' the third edition was
published in 1883. This book describes
things such as a jockey carrying a spinning
kymograph while seated on a horse whose
gait is studied through four rubber tubes
running to bulbs attached to each hoof.
Figure 1 is taken from this treatise.'
In 1903 Einthoven transmitted electrocar-
diogram voltages over Leiden Telephone
System wires about a mile to a string galva-
nometer. In 1921 Winters transmitted
heart sounds over a marine radio link as a
demonstration for ships without a physi-
cian.% External transmitters of various sig-
nais evolved as electronic methods evolved
to produce smaller transmitters. Later, Sev-
eral groups inserted small coils and elec-
trodes into the skulls of animals so alter-
nating currents could be induced for a
primitive form of telestimulation.
The idea of transmitting signals from
within the body came to me in 1946 when
uncertainty about the pressure in the hu-
man bladder during voiding led to the sug-
gestion of placing a radio transmitter there.
This was done later when transistors were
invented.
The transmission of signals from within
a subject was a technique that evolved
slowly. On July 2, 1952, William Shockley
and Bell Labs sent me four experimental
point-contact transistors, which were diffi-
cult to power in a srmall package. (Junction
transistors were only available for military
use,) Thus, another approach was devel-
oped to provide for the totally passive
transmission of information.
A tuned circuit consisting of a coil con-
nected in paraliel with a capacitor has a
characteristic resonant frequency that can
be determined by bringing it close to a
grid-dip meter. This meter indicates more
power being abstracted when it is tuned to
the same frequency as the nearby circuit.
The resonant combination could be mod-
ulated to sense pressure information by
placing the coil near a diaphragm upon
which a ferrite or other magnetic core was
placed; motion caused by pressure
changes would alter the inductance and
the frequency. Bob Markevitch (who is cur-
rently Manager of the Systems and Analy-
sis Section of Ampex Corp.) took up this
project as an undergraduate student at the
University of California - Berkeley. Marke-
vitch's work was integrated with my pro-
gram on highly nonlinear circuits, and, as a
resuit, cyclic scanning of a grid-dip meter
was produced with barium titanate capaci-
tors in which capacitance and frequency
could be changed by applying a variable
voltage. The top part of Figure 2 is taken
from Markevitch's 1954 undergraduate re-
search report. The tuned circuit could be
placed in the mouth and its frequency mon-
itored from outside the face by the grid-dip
meter. Thus the circuits tested ty Marke-
vitch showed that signals could be trans-
mitted through the tissues of the body
from quite small coils placed within the
body.
My interest in glaucoma and continuous
monitoring of pressure within the eye led
to a masters thesis by Arthur Chen in 1963
relating to another passive telemetry sys-
tem. Carter Collins produced what is still
probably the world's smallest telemetry
transmitter. In connection with his doctor-
ate at Berkeley, Collins also used a scan-
ning grid-dip meter but scanned it in fre-
quency with the help of a mechanically
vibrating war-surplus variable capacitor.
For a transmitter, he used a piece of ordi-
nnary insulated wire wound into a spiral to
form a pair of pancake coils connected
only at their outer ends. This is shown at
the bottom of Figure 2. These coils could
be mounted in a small plastic bubble (2 mm
in diameter and 1 mm thick). When the
pressure was increased, the two coils
moved toward each other, thus increasing
both mutual inductance and distributed ca-
pacity, which lowers the electrically reso-
nant frequency for external detection.
Transmission distance need not be great to
carry a useful signal from within the eye.
One of Collins' recordings shows the
pressure changes within the eye of a rabbit
in response to a sudden sound (see Figure
3); other sensory modalities, including un-
expected sights, produced similar tran-
sients. Although Collins thesis in 1966 did
not stress techniques of constructing radio
transmitters, his study of pressure
changes within the eye could not have
been completed without the development
and use of these techniques.A.5 The trans-
mitter, which was surgically implanted in
the eye, could be checked at any time for
proper functioning by measuring its incre-
mental sensitivity. This was done by rais-
dog, the harness, implant or other device
must allow for drastic size changes.'
if a tesearcher just needs to collect data
rather than monitor instantaneous informa-
tion (such as for a diver or astronaut), te-
lemetry can be replaced by recording. Re-
searchers can record signals by variable
electroplating, by radioactive light variably
darkening film, by magnetic tape, and,
most recently, by semiconductor mem-
ories.- Recorders as well as telemetry
transmitters can be self-detaching for later
recovery,1.22 A recorder generally gives
less frequency response than a transmitter
for a given total time in a given package
size. Thus, for example, for a small animal,
a researcher might prefer to record heart
rate or temperature and transmit the com-
plete electrocardiogram.
Very stable transmitters can be made by
using quartz crystals to control frequency.
Ouartz crystals are the norm with tracking
transmitters where determining the posi-
tion of an animal by triangulation or other
process is accomplished. With artificial
earth satellites, the position of wide-rang-
ing animals in hostile environments can be
monitored to an accuracy of about a mile
by recording the Doppler shift in the re-
ceived frequency as the satellite pasSes
over the surface of the earth,43 Figure 8
shows a stable oscillator that was devel-
oped for this purpose. The entire tempera-
ture-compensated oscillator weighs 4.3
grams and has a maximum change of fre-
quency of 8 cycles per second at a frequen-
cy of 108 million cycles per second over
the substantial range of temperatures
shown, if lithium batteries are used, the en-
tire package weighs about O.2 kilograms,
which allows it to be carried on the back of
an albatross in flight.
Telemetry can De applied in other diverse
areas. Figure 9 shows a hypodermic dart
fitted with a radio transmitter that can be
shot from a rifle. Darts are a standard meth-
od of immobilizing animals during conser-
vation work. HOwever, the researcher may
loSe the animal if it falls unconscious in
underbrush, or the animal may drown if it
falls unconscious in water. The transmitter
in Figure 9 allows researchers to find the
animal after it has fatlen,4 COmpanies
building such transmitters have found that
a quartz crystal transmitter can be used
without the crystal shattering on impact if
the crystal is supported with four rather
than three wires within its case.
In other field studies, when collection of
physiologic data is required, the signal
from a small internal transmitter can be re-
ceived by a small external retransmitter
that picks up the weak signal and, after
shifting it in frequency, transmits it with
greater power to a remote point of recep-
ton,
A set of tongs, with a small transmitter
and sound sensor clamped to the handle,
can be used to remove a hard object rang-
ing from a kidney stone to a broken needle
because the collision of the tongs with the
rigid object generates a sound wave that
travels through the handle and is sensed
bby the transmitter, This is merely one ex-
ample of the use of telemetry to carry sig-
nals from sterile regions. A radio transmit-
ter can carry signals from the rotor of an
ultracentrifuge, too.4%
Transmitters in human or animal sub-
jects can be turned on and off in three
techniques acquired from research on implantable instruments
at the Electronics Design Center at Case Western Reserve Uni-
versity.
The research at CWRU involving biomaterial applications can
be divided into three categories: 1) hard-shell encapsulation
packages, 2) implantable transducers, and 3) stimulation elec-
trodes. A certain material is chosen to meet individual specifica-
tions for each type of device, but it must be integrated with other
materials to offer the best package available.'
Hard-shell encapsulation packages. This article will deal exclu-
sively with materials in this category - namely, the packaging of
passive materials. The major function of these materials is to
provide an impervious protective housing for the vulnerable elec-
tronic components contained within its rigid shell. Excluding en-
vironmental stresses, these pacKages are designed to bear little
or no ioad. In addition to hermeticity and biocompatibility, impor-
tant structural considerations for the package include mechnical
strength and stability, low weight, and reduced size and shape.
lmplantable transducers. Wlith this type of package, the bioma-
terial must meet two basic requirements. First, it must protect
the device from the influx of body fluids; second, it should pro-
vide minimal interference with the transduction of the desired
signal. In packaging most biomedical transducers, an insulating
conformal layer is deposited onto the device - in particular,
over electrically conductive and potentially corrosive areas. The
material (usually an adhesive rubber or resin) provides a thin, but
tough, film capable of guarding against environmental effects.
Unless a double package is constructed to provide further pro-
tection, these devices are only suitable presently for short-term
in vio experirmentation.
Stimulation electrodes. The noble metals (e.g., goId, platinum)
rank superior in this area. For design reasons andlor cost justifi-
cation, other materials remain popular: passivated metals (e.g.
stainless steels), carbon. and conductive polymers. These
materials must be able to effectively pass current to provide a
maximum signal response without showing adverse signs of fa-
tigue, tissue damage, or instability.
Figure 1 illustrates some microelectronic devices designed
and packaged at the CWRU Electronics Design Center. In each of
the three categories described above, similar materials are com-
monly used in the development of these devices. However, in
technical areas of methodology and application, only the pack-
aging materials used in the first category will be discussed in
depth in this article.
The second section of this article will be an overview of the ba-
sic requirements of packaging materials. In the third section,
packaging rmaterials commonly used today will be evaluated, and
the final section will focus on the applications of these rmaterials
to a prototype telemetry system developed at the Electronics De-
sign Center. This device has been used to successfully monitor
intracranial Dressure (ICP) in both animal and human subjects
and to transmit these signals to a computer via radio frequency.
All packaging materials should fulfill two major functions: 1)
Protection of electronic and mechanical components of the in-
strumentation systems from hazardous body environments.
The tank coil L serves as the radiating an-
tenna, When a modulating signal is applied
to the oscillator, the capacitance of the re-
verse biased collector base junction is
modulated. Since this capacitance is es-
sentially in parallel with Cg, the carrier can
be frequency modulated by the input mod-
ulating signal. Carrier deviations of 3 50
KHz are typical (for modulation index 5 5
and modulation frequency of 10 KH2). The
modulating signal can be a FM composite
audio, or PAM, PDM, PCM, etc. Modulation
is not perfectly linear and a varactor diode
can be incorporated to improve linearity. A
lack of linearity distorts the transmitted
signal, but this may not be a problem for
many applications when compromised for
circuit simplicity.
Receivers. There is a variety of good
quality FM receivers available commercial-
ly for the 88 to 108 MH2 FM radio band.
Probably for this reason, these frequencies
are popular for biotelemetry, and relatively
few receivers have been designed specif-
ically for biotelemetry on other frequen-
cies. SOme minimum requirements for a re-
ceiver for biotelemetry service are at least
2.5 V monaural FM sensitivity, at least a
108 KHz modulation signal bandwidth, a
signal-to-noise ratio of better than 30 dB, a
defeatable AFC (automatic frequency con-
trol), squelch, a signal tumimg meter, and a
signal strength meter. The AFC allows the
receiver to automaticaly follow small
amounts of telemeter carrier frequency
drift without constant manual retuning.
The signal bandwidth requirement is espe-
cially important when using pulse type en-
coding since too little bandwidth can dis-
tort the received pulses and cause
inaccuracies.
Biomedical telemetry has been around
for about 30 years and has become a useful
tool for obtaining restraint-free physiolog-
ical data from a broad spectrum of animal
Species and of monitoring settings. The de-
scribed techniques for biotelemeter circuit
design and construction are now well in
place, and it seems likely that future devel-
opment will be in the further miniaturiza-
tion and integration of biotelemeters and
transducers, improved power sources, and
improved packaglng..
ith advances in microelectronics and integrated circuits
designed to perform complex functions, the primary con-
trolling factor in determining the lifetime of any implanta-
ble electronic instrument today is the reliability of its package.
The most sophisticated biotelemetric device will perform only as
well as the materials used to protect its circuitry against an ag-
gressive environment such as that found in the bodies of both
humans and animals alike. It is difficult to categorize the best
packaging materials currently available. Simply stated, the opti-
mal biomaterial for all occasions has not yet been developed.
Many packaging materials may serve to fulfill special require-
ments, but no one material is superior for all applications. One
class of materials may outrank another in terms of performance
based upon a specific quality or a particular function. It remains
essential, however, that the packaging engineer not onty base
the selection on the inherent properties of the material, but also
carefully consider all possible consequences when the material
is interfaced with another material exhibiting dissimilar charac-
teristics. This article reviews the iiterature with emphasis on
years using these technologies. Figure 5 shows a system prior to
implantation, consisting of the main telemetry module, a battery
packlpower switch, and transducers. The remaining wires are RF
antennas for signal transmission.
Battery power sources are particularly suited for use in free-
rOaming animals. To extend battery lifetime to several months,
RF actuated power switches have been developed,. 10 The im-
plant is activated by a single RF burst, which latches the system
on for a preset period of time (Figure 6). After this period, the
Switch automatically turns off to prevent accidental battery
drainage. This approach extends battery life to more than several
months in many applications, and additional implant lifetime is
achieved by replacing the batterylswitch assembly through mi-
nor surgery.
Chronic implantation requires transducers that are long-term
stable, are not damaged by body fluids, and do not cause physi-
ological damage. Figure 7 shows an example of an implantable
ultrasonic transducer that has met these requirements for peri-
Ods greater than two years. It consists of a series of loosely fit-
ting perivascular cuffs, chosen to match vessel size, and ultra
sonic transducer inserts, containing a gold metalized ceramic
element with acoustic impedance matching and backing layers
for power efficiency.
Other sensors include pressure, temperature, biopotential, and
strain gauges. Significant limitations of present sensors include
power consumption, drift due to cable leakage, and size. En-
hanced integrated circuit sensors are under development to alle-
viate these constraints, '1.12
Totally implantable telemetry systems provide a unique capa-
bility to obtain repeated chronic measurements of physiological
parameters in undisturbed, unanesthetized research animals.
Small package size permits implantation of multiple transmitters
so that several variables can be monitored simultaneously. The
advantage of a long-term implant becomes apparent in studies
with slowly changing variables that are subject to conscious
control.
A prime example is in hepatic hemodynamics,'A when liver dis-
ease is induced during a period of months. In this case, the ani-
mal is disease weakened and would be extremely vulnerable to
infection from percutaneous leads, and hepatic blood flows are
influenced heavily by external stimuli such as noise or handling.
The rapid pace of innovation in VLLSI has provided an
implementation medium for highly parallel computer architectures.
To fully realite the tremendous.computing power of VLS]
technology requires that its characteristics, often subtle and
complex, be understood. First, the design complexity introduced by
the increasing chip sise and density calls for architectures composed
of repetitive, modular structures. Second, interconnections between
devices consume more power and require more area than the devices
(transistors) themselres. Third, the computing environment offered
by VLSI is I/O-bound, not compute-bound, due to the limited
number of I/O pins available in comparison to logic gstes. These
considerations have led to the development of highly parallel bit
level processor arrays, which are distinguished by their
communication strategy - digital signals are transmitted bit
sequentially on single wires as oppoeed to simultaneous transmission
on parallel busses - and their large number of simple bit serial
processing elemenu [PEa)L This leads to efcient commanication
both internally and between chips and provides a high degree of
parallelism. The regalar, repeatable structures inherent in bit level
arrays have the following adantages over other types of V,S]
gates and memory involed in a computation, and bit-level
pipelining result in very large throughputs [3].
The advantages from an algorithmic standpoint, resulting from the
Besibility at the bit level, include:
BBit level processor arrays are characterised by a ast of four
features:
Within the context of these features, ive classes of bit kvel arrays
and architectures representative of each class are discussd. These
five classes, shown in Tsble 1 with representatie systema, are:
Except for the class of image processing arrays, the arrays are
grouped according to a dominant architeetural characteristic. The
image processing class contains several arrays that have special
hardware features to speed up image operations, so this elass to
some estent represents special architecteral characteristies, Eaek
class is now described. The representative features are extraeted,
and trends and patterns surmised. Tsbles 2 and 3 summarise
information about each processor array discessed in the text.
iL level systolie arrays have been developed to perform
several digital signal processing tasks, including convolstion and
correlation, rank-order iltering, and the Discrete Fourier Traaslorm
[DFT) 2], 4]. Several of these chips are now sold as commercial
products. These arrays are specislised to perform one particular
algorithm, and each processing ekment is optimised for the
particular algorithm being impkemented. This fsct, alng with the
systolic concepts of extensive pipelining and local communicatiom
applied down to the bit level yield extremely fast clock speeds, as
the system clock cycke time is reduced to the cycle time (or a small
multiple of the cycle time) of the bit level processing elment.
Bit level systolic arrays show the potential of the bit level
processor array approach when a particular algorithm has the
proper characteristics. The regularity and local dats
communication evident in many signal processing aiggorithms, when
realited right down to the bit level, allows the integration of a very
large number of very simple PEs on a 4ingle chip. A bit-slice
correlator chip [2], for example, implemented in a 3 um CMOS/SOS,
process contained 840 PEa [cells). Special purpose arrays can be
constructed using the features of bit level processor arrays as
guiding principles. The resulting designs, while specialited for a
single algorithm, represent nearlr optimal use of the VLSI resource.
Image processing arrays include the NCR CPP
(Geometric Arithmetic Psrallel Processor) chip [5], Goodyear
Aeromsce MPP [Massively Parsllel Processor) [8], GEC GRD (GEC
Rectangular lmage and Data computer) chip [], and the University
Colege of London CLP (Cellular iLoggic Image Processor) [8]. These
bit level arrays are oriented towards inage processing as their
primary application, and have special image manipulation features
in hardware such as data reformatting buffers, bit plane 1/O, and
processing elements optitmmited for certain image transformations.
PAPD (Pyrsmi Architecture for Parsllel Image Analysis) [9] is a
processor array using a pyramid architecture to realize
multiresolution pipelined image processing; each PE in PAPL4
operates at the bit level, and has connections to 4 PEs [sons) on a
lower plane, 4 neighbor PEs at the same level, and a singe PE
(fsther) in a higher plane.
A common feature of these arrays is the lack of a large on-
chip locai memory for the PEs. The RAM sies vary from 32 bits for
the CLP array, to 255 bits for the PAPL array. Several reasons
for this pattern are evident. RAM chips are typically fabricated
using special, high volume, manufacturing processes that are fine
tuned to maximise RAM density and speed. These processes are
incompatible with the more general digital structures used in PR
design. For this reason, RAM densities for standard RAM chips are
much higher than RAM densities on chips including other logic. By
proridisg RAM of-chip, larger memories per PE are possible; no
RAM addressing pins on the PE chip are necessary; and higher
density RAMs may be used as they become available in the future.
The slower off-chip memory access time means that it is important
to do as many operations using the local PE dmemory as is possible.
This sitation is analogous to the problems associated with cache
memory in single CPU machines.
The characteristics of image processing algorithms for large
processor arrays are associated with this trend toward small local
memories. Subimages are loaded into the array, one pixel per PE,
and processed in sequence. These pixels typically require a small
number of bits. The availability of larger of-chip PE memories
allows an alternate approack, where eaeh PEE memory contains a
portio of the whole image. These larger memories also make it
poesibke to use Boating-point representations and arithmetic, a
necesssity in many applications. These larger PE memories may aso
redwee the I/O bandwidth required for certain algorithms, as it is
unnecessary to move partial resulte in and ont of demory as
compwtation proceeds il1]. The 1/O bottkneck in an n xnn processor
array, caused by the O[wf) growth in computation bandwidth versus
O[n) growth in I/O bandwidth, will become increasingly important
as increases with V,SI densities.
Several architectural features have been added to some of
these imagge processing arrays to speed up particular image
processing procedures. The GRD computer inclades a register and
special conters to perform a histogram operation; the CLP srray
PE hardware directly implements a global propagation function; the
GAPP snd AP srray have specisl I/O registers for transferring
bit planes and bufering input and output from the array; PAPA'a
pyramid network is specialised for multiresolution pipelined image
processing. Although each of these extra hardware features can be
justified in the sense of speeding up a particular operation, the costs
in terms of extra, more compiex hardware must be determined. For
a ixed amount of hardtare, a more complex PE reduces the
parallelism in an array in the sense that fewer tmay be constructed
compared to a simpler PE design. These costs must be weighed
against the increased speed for particular operations. This is similar
to the RISC critique of singh processor architectures: infrequently
used instructions should not be implemented in hardware as they
impact negatively on the performance of the frequently executed
instructions.
Vwriable parallellsm arrays have the ability to adapt to
the diferent degrees of concurrency available for dierent
algorithms and within the same algorithm. The PEs are grouped
into multibit processing unita of varying sie: a larger word widtk
per PE results in less parallelism for a gien array. This approack
holds promise for the efficient use of highly parallel processor
arrays. Algorithms which do not contain large amonts of
parallelism may use a reconfigured array with less parallelism, but
more powerful PEg. These arrays typically hae some form of kecal
control besides a masking bit to con figure the processor array. Bit
level processor arrays representative of this class inchude the fC,
DAP (Distributed Arrsy Processor) [12!, M'7'. AAP (Adsptire
Array Processor) chip [13], and the University of Sowthamstoa RP4
(Reconfigurable Frocessor Array) [14], In the latter two arrays,
part of the microcode word is held locally within each PE, allowing
some degree of independence in both computation amd in
communication with other PEs,
The RPA has several unique and interesting characteristics.
The PEs use a stack architecture to reduce the number of control
lines which must be routed to each PE, and to simplify the
controller by removing the need for address generatioa. Three
stacks are included in each PE: the Bitstack is one bit wide and 8
deep and provides general storage; the Activity stack, abso one bit
wide and 8 deep, contains the mask bit and provides a comvenient
means of implementing a context switch; the Wordstaek h used for
serial operations on word operands, and is conigsrabke so that it
may implement a variable word sise and varisble depth staek. The
P contains a mesh interconnection, with a twosbit bss betsees
neighboring PEs to increase inter-PE baadwidth; a tso-bit bus ie
also used internally for both operands and resulte. O$-chig RAM
storage is assumed. The variable parallelism natare of the RP4 he
supported by the Reconfiguration register. It provides the kcal
control that indicates if s PE e a MSB, LSB, or an intermediate bit
in a multibit processing unit, and the direction of data movement
during kft and right shifts. Additional local control takes the form
of a circuit to perform the mantissa aligmest amd esponeat
correction necessary in oating-point operatioms. Ths is a dicsl4
operation in most processor arrays using SIMD eoatrol 4ue to the
data-dependent nature of foating-point arithmetie. Th fPA ehip
contains 16 PEs and is implemented ia lam CMO.
The AAP, built by the NTT Corp. i 1988, k aa bd groeessoe
array implemented using a 2um CMOS process. A mesh'
interconnection network is implemented, akaag wi4h a roe ad'
column bus structure with bypasses which allows msitipka
transmissions on a single bus structare. As with te 8, kscal
control registers in each PE are used to cosßgare mellibit
procesing units within the array. RAM storage for each PE
amounts to 98 bits; an I/O register piane is also used to overlap
computation with I/O.
A VLSI version of the DAP has been ased by the AMT Corp.
to realise a DAP array as a backend processor to a desktop
workstation. Eack DAP chip contsins 54 PEs, and 14 DAP ehips
are used to realise a 3232 array. DAP memory is of-chip asd ca
be aried from 32 Kbits to 1 Mbit per PE. The DAP array uses a
mesh interconnection network along with row and colm bmses. It
can be configured in two modes: vector mode and matrit mode. In
matri mode, the full parallelism of the array is employed, each PE
performing a separate computation; in vector mode, a row or
column of PEs is considered to be a single multibit processing unit.
Vector mode is aided by a carry ripple connection between adjacent
PEEs in rows and columns.
Assoeistive processor arrays inciude the
Brunel Uniwerity SCAPE [Single Chip Array Processing Element)
15],the ASPRO (Airborne Associative Processor) [8], 1 VLSI version
of the STARAN associstive processor [18], and he UCSB VLSI
associative processor chip [17], The salient feature of these
processors is a content addressable memory used to perform bit
level operations in parallei. These arrays are particulariy adept at
fast and efficient searching. The classic associative processor
architecture included mask and compare registers which were
connected across each word in memory; a portion of each memory
word, specified in the mask register, indicates which bits are being
compared to bits currentlr in the compare register. The results of
the compare operation are placed in a search results register, which
has one bit for each word in memory. This basic architecture can be
enhanced by several features, including logic for each memory word
which can implement logical and arithmetic operations, The
associatire processor arrays are similar to other bit level processor
arrays in that a small amount of processing logic is associated with
a portion of memory. They difer in that the memory is accessed by
content, rather than by address, a feature which requires significant
extra hardware for each memory cell. Also, the PEs are typically
connected to their upper and lower neighbors, forming a linear
array.
Each of the three associative processor arrays contain features
which difer from the classical model. These features either increase
the performance of the array for certain operations, or help to
overcome limitations within the VLSI environment. Th+ UCSB
associatire processor chip is a 32w135 array realised in a 3um
NMOS process. It employs an on-chip decoder and segmented
compare and mask registers to reduce the number of I/O pins
required. The search results register may be shifted up or down one
word to realise bit serial communication. Signals are providded to
extend the irst responder signal and the search results register
between chips, zllowing multiple chips to be chained together.
SPO has a relatirely large processing element memory, 4
Kbits per PE, located o-chip. Eaeh ASPRO chip contsins 32
processing elements and a 32 bit Flip network. This network allows
standard RAM to be treated as multidimensional access memory, in
which data can be accessed in either the word direction or the bit-
slice direction, resulting in a very Bessible memory addressing
scheme. Hyhrid technology is employed to reduce memory psckage
sise, and eight hybrid RAM devices are required per module. There
are four PE/Flip network chips per modwk, and 16 total moduks
for the processor array plus one spare module. This results in an
array with 2048 PEa and 8 Mbits of memory. The SCAPE srray
chip contains 258 PEs, each with 37 bits of memory. The chip
empkoys a 2um CMOS process, requires 145,%00 transistors, and is
75mm* in area. The SCAPE has the most sophistieated PE of the
three associative processor arrays. It includes several registers to aid
in a sequence of bit operations on words in bmemory and to process
two-dimensional data. Additional PEE hardware includes registers to
save results of previous compare operations, a bit-serial adder and
carry latch, and an interconnection network between PEa, The
SCAPE chip also contains controller and clock generation kgic to
implement instructions broadcast from an external 3CAPE
controller.
Routing network arrays have the same basic architecture
as other bit level processor arrays, but in addition to a nearest
neighbor network these arrays will have, for example, a hypercube
or multistage cube interconnection network. These routing networks
provide high bandwidth communication paths between non-neighbor
processing elements, a requirement for many algorithms. Examples
of this clsss include the Conseetion Msehie [18] [hypereube) and
the DEC Massively Parallel Arehitectwre [19] (multistage cube).
These two architectures difer in that the Connectiow Mehine uses
a packet-switched, single-stage network with the routing hardware
implemented directly on the PE chip, while the Msaively Parullel
Arehiteetwre employs a cireuit-ewitched, multistage . network
implemented with separate routing chipe.
The 84K P Conneetioa Aekine employ a hypercube
network, with each node in the hy9ereube consisting of 16 PEa,
These 18 PEs are contained within a custom chip, which also
includes interfaces to the router, I/O, and mesh network, and
proportionate pieces of the mesh and hypercube network controllers.
O5-chip memory iss provided, with 84 K.bits of bit-addressable
memory per PE, The router hardware supports message pasiag
between PEs, so that one PE may send its data to the local memory
of another PE, or s PE may request data from another PE be
stored in its memory. Messages may traverse the netsork from
source to destination in one machie cycle, in which case the
network appears to be circuit-switched. If s message is blocked at a
particular node due to congestion, it will be stored at that node
until a path is open. Some dynamic load balancing is applied and
the router hardware may determine an alternate path and empkay
it in forwarding a message.
Th+ DEC Mssivelly Parallel Arckitectwre woul4 be the largest
of the processor arrays mentioned when fully implemented, with
282,144 PEs in a full system. n addition, each PE e 4 bits wide, so
in some sense the array is exteaded from the bit level processoe
array definition. Two chips are use to implement this architectsre:
a 2um CMOS, 242,D00 transistor, processing ekment chip, with 42
PEs in a 4x8 mesh configuration; and a 2gm CMOS router chip that
implements a circuit-switched mnltistage interconnection network.
Through the router network, any PEE may communicate directly
with any other PE s the overall machine. Eaek PEE eontains a
four-bit AlLU, 1Kh of ststic memory, and two programmable sise
shift registers. SIMD control is employed, snd a 4K memory mode ie
allowed in which a PEE may access the memory of ita neighbors to
allow for probkems reqsiring large memories per PE.
Fetre architectares could combine the features of varishle
parallelism arrays with routing network arrays to provide dynamic
architectures with varying degrees of parallelism ad Besihke
communication. This approach holds the promise of eciently
mapping a broader class of algorithms onto these arrays.
Despite the advantages of bit level processor arrays in terms
of both VLSI implementation and algorithm executios, they can be
diieult to program (in the case of an existing general perpose
architecture) or design (in the case of a specisl purpose
architecture). This problem is accentuated by the need to
impkment high level computations [e-44 matri computatioss,
convoltion) using bitwise operations. For the architectsres
described previouslr, several approaches to this probkem have been
used. These include:
These approaches lack portability among diferet smaehines ad
sometimes ignore optimisations possible at the bit kvelL It is ofte
dieult to prove the optimality of a gie mapplag ssisg these
methods. In order to sole the problems issoeiated with eurret
ap9roaches, it is desirable to devekp methodologies ad tooh which
enable the systematic mapping of algorithms onto proesssoe arrays.
In the past, several research eforts have been psrseed i this
direction and a good survey can be found ln 21]. Maay of these
methodologies, which were intended for word level processor arrays,
are applicable to bit level arrays. However, besides some of the
limitations that still characterise those methodokogies, systematic
bit level designs present additional problems. RAB [Reconfiguration
Algorithm for Bit level code)[10], an automated design tool which
maps a class of algorithms programmed in 'C' into bit level a :rays,
represents an attempt to understand and solve the open questions
and problems involved in the systematic design of bit level processor
arrays,
In practice, potentiai users of processor arrays are given an
algorithm and must devise a means for its execution using one of
the following options: (1) use an existing processor array, [2) design
a special purpose processor array, or [3) design an array that uses a
number of existing small processor array modules as the basic
componenta. O ption (1) requires mapping the algorithm into an
existing array taking into consideration site limitations, fixed
interconnection schemes, and predesigned processing elements. In
this option, which is referred to as full mapping, the programming
decisions are subordinated to the characteristics of the array.
Option (2) allows the user to design the hardware taking into
consideration only the characteristics of the algorithm and perhaps
some rather general VLSI design constraints (i.e., planarity, limited
pinout, etc). This option is referred to as full design. It corresponds
to the front end of a silicon compiler, and could provide the
structural and behavioral description of an array to the placement
and layout tools of the silicon compiler, O ption [3) is a compromise
between full mapping and full design, where the designer can decide
the overall organitation (i,e., shape, site, interfaces) of the array,
but uses given basic blocks which are themselves fully defined
'small' processor arrays, This option is referred to as partial
rmapping/design.
The input to RAB consists of C programs which describe word
level algorithms. These algorithms correspond to nested for loops
with static behavior, RAB 6rst expands the computations in the
input program into bit level operations as shown in Figure 1. This
expansion phase replaces word level computations with a bit levei
implementation of the arithmetic operations using a library of
macro expansions. This phase is followed by data dependence and
broadcast analysis using the Dependence Arc Set Analysis technique
[22]. The result of this analysi is a formal deseription of the
internal structure of the bit level algorithm. This structural
information is used to generate an algorithm transformation which
yields a restructured algorithm suitable for mapping onto a bit level
proceseor array. The mapping may be a full design of an
algorithmically defined array or full (partial) mapping for a fixed
(vsrisble) sise array corresponding to the fourth level of modules in
Figure 1. The transformed algorithm structure is then converted
into an intermediate representation which can be used to generate
code for several different architectures. The last two modules in
Figure 1, code generation and code optimisation, comprise the phase
in which code is generated from the intermediate representation for
a particular target architecture. This code is then optimised using a
standard compaction technique.
Esperience and insights provided by the development and
usge of RAB showed some of limitations of the approaches used for
the purposes described above, However, it aho revealed
opportunities for new improved techniques that can be applied to
the problems that RAB attempts to solve. This prompted the
authors and their co-workers to start the development of a new
software package which improves on RAB in the following regards:
8t-euel ezanswons: Currently, the optimality and generality
of the mappings provided by RAB is grestly afeeted by how bit-
level expsnsions are derived from word-level operations. This is due
to the very large number of expansions that are possible for any
single word-level operation. Because RAB needs to have a copy of
each possible expansion explicitly stored (i.e., the expansions are
stored and not generated when needed), it is infeasible to consider
every possible expansion. In addition, assuming that all poesible
expansions were available, RAB has no means of knowing or
estimating which expansion yields in the best design or mapping. As
a eonsequence, RAB needs to to be Tully executed for every single
poesible expansion in order to guarantee optimality. This is also
impossible in practice.
To deal with the problems mentioned above, autometK
programming and program transformation techniques are beiAI
considered which allow for the on-line generation of expansions from
a reduced number of 'basic'' stored expansions, The 'derived'
expansions are the result of applying transformations to the bssic
expansions by exploiting properties such as commutatiity,
distributivity and associativity of the bit-level operations. ln order
to estimate or identify the best expansion, some dependence analysiA
is performed and attached to each expansion. By analysing relevant
characteristics of data dependences, it is possible to arrive at
relative performance of diferent expansions and, furthermore, (uide
the generation of additional expansions with improved
characteristics. The knowledge of the dependence structure of tke
expansion used is also benefcial in the analysis phase, as discussed
Dependence Analysis: Currently, RAB analyses the full
expanded program by using the DASA technique which yields
structural and dependence information about the program ss a
convex set, The inequalities that define this convexz set are then
used to derive a matriss where each column is a dependence veetor.
This process has two dissdvantages. First, the analysis is
unnecessarily complex in the sense that no attempt is made i5 RAB
to exploit the fact that the 'complex' expanded program is the
result of s 'simple' known expansion applied to a 'simple'' program.
Secondly, information about the structure of the program is lost in
the process of generating the matrix of dependencies. ln addition, u
order to be able to use s tnatri representatioa for depeadeneies, ik
is often necessary to 'fuse'' loops or consider only thoee expansios
where loops are 'fused''. The problems mentioned above are
avoided in the new approach in the following ways, The dependeaee
analysis of the expanded program is obtained by combining the
results of the dependence analysis of the original word-kvel
program and the dependence analysis of the expasio. The
computational cost of this approach is smaller than that of
analysing the full program. With regard to the geaeration of tke
dependence matri, this featare will remain in the new approack
only for the purpose of interaction with the usar. However, for
purposes of optimisation in the synthesis phase, convez set
information is used instead. ln addition, loop fusion is not needed
explicitly, and, instead, constraints in the form of inequalities are
sufficient to convey how loops are arranged.
Masping and design plase: Currently, RAB h fslrly inBesibke
in the type of constrainte that it can take into account for mappiA$
and design purposes. The same can be said abont the optimisatios
criteris. This is due to the fact that dieret constrainte are
represented diferently and, to a certain extet, built into the code.
The same ean again be said of the objactire functions to be
optimised. The nee approsch provides a unige form foe the
representation of any type of constraint (im essessce, a ast ef
inequalities) and optimisatio procedures that are indepesdet of
what the constraints and objective fenctioas are (withis certai
limits which have to do with the convexity of tke objectire functios
and the space of feasibk soltions) Therefore, constraint4 asd
optimisation criteria can be easily added, deleted or chaged. By
having the expansion, anassis and synthesis phases of RA8 use tke
same convex set representation, a more modelar, efieiest amd
coherent mapping and design procedure results.
Meroeode generation ame optimiastiom: RAH does aot geaerate
microcode without user assistance. In the aew approack, a
intermediste form is generated from the outpet of the mapp$
module which is then translated to a microprogram speciie to the
target array.
This paper has described it level processor arrays and the
common characteristics that make them ideal for VLS! snd higk
speed computations. A taxonomy for current bit level arrays has
been developed that provides a perspectire on this class of
architectures, The design and programming problem as addressed,
and RAB, an automated design tool developed at Purdue, was
briefly discussed as a solution to some of these problems. RAB hss
he twin advantages of porta bility among different architectures
and systematic optimitation down to the bit level,
The IC design was specified from low level geo-
uetric masks to high level a lgorithm. Then, design
& updatiag can be performed more correctly, 4uickly
and inexpen sively via automatic syn thesization of
digital hardware with the bigh level bebavioral de-
scriptions. Ualike the computer-aided layout tool,
the designers can describe circuit behavior without
the knowledges of actual circuit structure. It can
be completely built with cosputer systess from the
construction of circuit architectures up to genera-
tion of layouss (1]-[5].
A cicuit modeled like a microprocesaor, can be
partitioned into two parts [3], vis., the data path
consisting of storage elements, operation units and
cosmmunication lines for storiag data and performing
operation s, and the controller for controlling data
transfer in the data path. Caly a few systems have
ever tried to optimize the data path structure at a
higher level [6]-[9]. Ia this study, a DPS has been
proposed for extractiag and embodyiag the data path
from an assemblylike program at ET level. Out put
of DPS a ppears as a data-path structure description
involviag the iaformation of pr ocessors utilization
and the coummunication n etwork structure. Also the
methods for implementations of processor allocator
and bus a llocator have been thoroughly in vestigsted
and discussed in section II. aad section III., re-
spectively. More details concerniag the DPS can be
found ia [10].
The purpose for implementation of the processor
allocator is to n omiaate a proper processor to each
statement. The processors can be iteratively allo-
cated stage by stage. Aad in each stage, there are
oae or more steps, in which all the statements caa
be assigned and performed by the processors. Then,
some possible allocation s of a ll statements to some
processors can be achieved. n put for DPS includes
a ssembly--like program as well as several processors
wthere the assembly-like program will be given by a
sample program wich can be proceeded by four basic
steps as follows:
While at least two ALUs are required to perfarm the
'4' or '4' operation. Since some stepe will be al-
located to all the stages sequentially. Therefare,
the allocating pr ocessors associated with thiss sar-
ple can be realized by four stages at moet. Aad the
possihle allocations of the statesents to the proc-
essors are listed and given ia Table I.
'o' us tne syao= ao or '+' 4 P%,.
stage, 'op2' is the syabolic name of the othes
ia 1-th stage, and same as for other ' opi' .
Before allocating, the following terms have to
be defined as:
Assume at the (i-1)-th stage, m circuits of data
paths with miniamum cost, namely: the NDP( 1-1,l1)8,
NDP(i-1,12), -+ . and NDP(i-1,lm)*, have been con-
structed. Then, m n ew circuits of data paths, viz.,
he NDP(i,).%), NDP(i,),%), -- , NDP(1,),s), shoul4
be con structed a ssociated with C(i,)). In order to
limit the n umber of data paths, decisioa should be
made in accordance with miaiamum cost, such that the
set of OP(i,c) can be generated as follows:
Then, the DPS selects data paths with least n uber
of cannection lines wbich will simplify the process
of bus allocation. Complete process of allocating
processors associated with the sample program will
be showa in Fig. l. and illustrated as follows:
Now the algoritha for allocatiag the processors
will be introduced as follows:
There are no recipes for designing efficient
algorithms. This is somewhat unfortunate
Irom the point of view of applications: Any
time we have to design an algorithm, we
may have to start (almost) from scratch.
However, it is fortunate from the point of
view of researchers: It is unlikely that we
are going to run out of problems or chal-
lenges.
Given a problem, we want to find an
algorithm that solves it efficiently. There
are three stages in designing such algo-
rithms:
(a) Shmathematics. Initially, we use some
8mple mathematical arguments to charac-
terize the solution. This leads to a simple
4lgorithm that is usually not very efficient.
h) Algorithmic Tools. Next, we try to
iply a number of algorithmic tools to
8peed up the algorithm. Examples of such
tools are 'divide and conquer'' and dynamic
programming [Aho et al. 1974]. Alterna-
tively, we may try to find a way to reduce
the number of steps in the original algo-
rithm by finding a better way to organize
the information.
(c) Data Structures. Sometimes we can
speed up an algorithm by using an efficient
data structure that supports the primitive
operations used by the algorithm. We may
even resort to the introduction of monsters:
very complicated data structures that bring
about some asymptotic speedup that is
usually meaningful only for very large prob-
lem size. (For a real-life monster see Galil
[1980].)
In these three stages we sometimes use a
known technique: a certain result in math-
ematics, say, or a known algorithmic tool
or data structure. In the more interesting
problems we need to invent new techniques
or refine existing ones for our purposes. We
may need to prove new shmathematics, find
an appropriate way or invent new ones, or
use algorithmic tools in a new way.
A word of caution about shmathematics.
In many cases it is not deep; however, that
does not mean that its results are trivial.
What counts in our case is not how deep or
elegant a theorem is, but whether it is use-
ful for improving our algorithm.
Usually, we use all three stages of the
design process in the order shown above,
but this is not always the case. We do not
always use all three. Once we have a quite
efficient algorithm, we may reuse any of
the three and not necessarily in this order.
In particular, we may use shmathematics
again and again: first to characterize the
solution, and then to analyze the running
time by justifying an algorithmic tool or by
proving the properties of certain data struc-
tures.
In this paper we exemplify the design of
efficient algorithms by surveying algo-
rithms for the four closely related problems
of finding a maximum cardinality or
weighted matching in general or bipartite
graphs. Mathematically, these are all spe-
cial cases of the problem of weighted
matching in general graphs. However, we
consider them in increasing order of di
culty because the easier the problem, ti
faster or simpler its solution.
The input consists of an undirected grapl
G = (V, E) with ] V[ = n and E] = i
The vertices represent persons, and eadj
edge represents the possibility that its end
points marry. A matching M is a subset 4
the edges such that no two edges in M sha1
a vertex; that is, we do not allow polygams
Probem 1: Max Cardinality Matching i
Bipartite Graphs. The vertices are parti
tioned into boys and girls, and an edge csi
only ioin a boy and a girl. We look for i
matching with the maximum cardinalit{
We can make Problem 1 harder in twi
different ways, resulting in Problems
and 3.
Problem 2: Max Cardinality Matching 4
General Graphs. This is the asexual casi
where an edge joins two persons.
Problem 3: Max Weiehted Matchine 4
Bipartite Graphs. Here we still have verj
tices representing boys and girls, but eacl
edge (i, j) has a weight uw,, associated witl
it. Our goal is to find a matching with th
maximum total weight. This is the welll
known assignment problem of assignin
people to jobs (disallowing moonlighting
and maximizing the profit.
Problem 4: Max Weghted Matching i
General Granhs. This broblem is obtaine4
from Problem 1 bv making it harder in botl
ways.
The four combinatorial broblems are im
portant and interesting in themselves?
Moreover, many combinatorial problem4
can be reduced to one of the above-de&
scribed four or can be solved by using, i
turn, the solutions to them as subroutines.
It was not clear initially how to solve
Problems 2-4 in polynomial time. The firef
polynomial-time algorithm for Problem 4
is due to Kuhn [1955]. The first polynoi
mial-time algorithms for Problems 2 and 4
are due to Edmonds. The later improved
algorithms are based on Edmond's monuy
mental work (Edmonds 1965a, 1965b).
An important notion for all four problems
is that of an augmenting path. We solve
each one of them in stages, and in each
stage we have a matching M. Initially, M is
empty. A vertex i is matched if there is an
edge (i, j) in M and single otherwise. An
edge is matched if it is in M and unmatched
otherwise. An alternating path (with re-
s9ect to M) is a simple path, such that
every other edge on it is matched. An aug-
menting path (with respect to M) is an
alternating path between two single ver-
tices, It must be of odd length, and in the
bipartite case its two endpoints must be of
different sex.
Consider Figure 1. The wiggly edges are
the matched edges. The path s, a, r, b, c, d,
-J,e, h is an augmenting path. Any contig-
uous part of this path (e.g., , c, d, i) is an
alternating path.
lhe ioliowing theorem is due to Berge
[1957! and Norman and Rabin [1959]
The motehing M has maximum cardinality
I and oniy if there is no augmenting path
t'ith respect to M.
One part of the theorem is trivial. If there
is an augmenting path, then by changing
the status of the edges on the path
(matched edges become unmatched, and
vice versa) we increase the size of M by 1.
We call this operation augmenting the
matching M. The other part of the theorem
is not trivial but is quite easy: We assume
that M is not a maximum matching and
show the existence of an augmenting path.
Let M' be a matching of cardinality larger
than M. Consider M @8 M ', the set of edges
in M or M ' but not in both. Hence M 8
M ' consists of alternating paths and cycles
(with respect to both M and M '). At least
one of them must be an augmenting path
with respect to M. (In all other types of
alternating paths or cycles the number of
edges from M is at least as large as the
number of edges from M '.)
Theorem 1 gives an immediate algorithm.
It consists of O(n) (at most n/2) stages. In
each stage a search for an augmenting path
is conducted. If any augmenting paths ex-
ist, the search finds one and the matching
is augmented. Since the search takes O(m)
time, the algorithm runs in O(mn) time.
The search is conducted as follows. Ver-
tices are labeled successively, boys with an
S label and girls with a T label. A labeled
boy (girl) is referred to as an S-boy (a
T-girl). After all labels from previous stages
are cleaned, all single boys are labeled with
S, We then apply two labeling rules itera-
tively:
The label also contains the vertex from
which the label has arrived. (In the case of
an S label this information is redundant.)
The search continues until the search
either succeeds or fails. The search suc-
ceeds if a single girl is labeled by T, The
search fails if we cannot continue anymore.
The following lemma can be proved by
induction.
By Lemma 1, if the search fails, there is
no augmenting path and the algorithm (not
only the stage) terminates. If a single girlj
is labeled by T, we have actually found an
augmenting path to j. The path can be
reconstructed by using the labels. The
search can be easily implemented in time
O(m) using any traversal of the graph that
starts with the single boys. The labels S
and T are also called even and odd or outer
and inner in the literature.
The best algorithm for Problem 1 is by
Hopcroft and Karp [1973]. They discovered
a way to find many augmenting paths in
one traversal of the graph. Their algorithm
is divided into phases. In each phase, a
maximal set of vertex disjoint augmenting
paths of shortest length is found and used
to augment the matching. So, a phase may
achieve the possible effect of many stages.
We now describe one phase. We use rules
R1 and R2 as before. Using breadth-first
search, starting from the single boys, we
identify the subgraph G of G consisting of
all the vertices and edges that are on some
shortest augmenting path. This subgraph
is ayered. In layer 2m (2m + 1) appear all
boys i (girls j) such that the shortest alter-
nating path from a single boy to i (j) is of
length2m (2m + 1). We finish the construc-
tion of G in one of two ways, Either a single
girl is reached and we complete the last
layer and delete nonsingle girls from it, or
we cannot continue. In the latter case the
algorithm (not only the phase) terminates,
which is justified by Lemma l.
In G we find a maximal set of disjoint
augmenting paths using depth-first search.
Every time we reach a single girk, we find
an augmenting path, erase its edges from
G, and start from another single boy. Every
time we backtrack alung an edge, we delete
it from G. It is quite easy to see that a phase
takes O(m) time. The importance of i
notion of a phase is explained by the foll
ing lemma [Hopcroft and Karp 1978].
The number of phases s at most O(Vn),
Consequently, Hopcroft gnd Karp's
gorithm runs in time O(m/n). :
It is interesting to note that the alj
rithm (not the time analysis, ie., n
Lemma 2) was actually known befoi
Problem 1 is a special case of the max fldj
problem for special networks. (Add a sour4
and a sink, connect the source to all ti
boys and the sink to all the girls, and taii
all capacities to be one.) Augmenting pati
correspond to the flow augmenting pati
in network flow, and the O(mn) algorithi
isjust the Ford and Fulkerson network floj
algorithm [Ford and Fulkerson 1956] f
these special networks. Similarly, Hopcrof
and Karp's algorithm is actually Dinic!
algorithm [Dinic 1970] applied to these spi
cial networks. This was first observed b
Even and Tarjan [1975] (ET).
As for Problem 1, Theorem 1 suggests
possible algorithm of O(n) stages. In esc
stage we look for an sugmenting path.'Thi;
search is complicated by the existence 4
odd-length cycles, which cannot exist ii
bipartite graphs. We start by labeling sl
single persons S and apply rules Rl and R
with the following two changes. First, w4
replace 'boys'' or 'girls'' by 'persons.' Seq3
ond, any time R1 is used andj is labeled b
T, R2 is immediately used to label thi
spouse of j with S. (Since j was not labeled7
before, it must be married and its spou
must be unlabeled.) We call this rule R12.
The search is conducted by scanning the
S-vertices in turn. Scanning a vertex mean
considering in turn all its edges except the
matched edge. (There will be at most one.
If we scan the S-vertex i and consider the
edge (i, j), there are two cases:
(C2 cannot occur in the bipartite case. The
case in which j is a T-vertex is discarded.
In case C1 we apply R12. In case C2 we
do the following: Backtrack from i and j,
using the labels, to the single persons s, and
s rom which i and j got their S labels. If s,
w s,, we find an augmenting path from s, to
s, and augment the matching. The trouble
begins (or life starts to be interesting) when
s, S;.
We next describe Edmonds' remarkable
work, in which the concept of blossoms is
introduced. Blossoms play a crucial role in
all algorithms for the nonbipartite case
croblems 2 and 4).
If s, s; s, let r be the first common
vertex on the paths from i and j to s, It is
easy to see that r is an S-vertex, that the
part of the two paths from i and j to r are
disjoint, and'that the parts from r to s are
identical. We have found an odd-length
alternating path from r to itself through
(i,j). We call this cycle B a blossom and r
its base (see Figure 1).
In this case the method of labeling intro-
duced so far might overlook an augmenting
path. For example, Figure 1 shows a possi-
ble labeling. The augmenting path s, a, r, b,
c, d, t. J. e, h, which 'goes around the
blossom B,'' will not be discovered.
Edmonds' idea (Edmonds 1965a) was
to shrink B: Replace it by a single super-
vertex B and replace the set A of edges
incident with vertices in B by the set A' =
l(B, j)lj e B, (i, j) e A]. Note that at
most one member of A' (incident with r) is
matched; none are matched if r = s, If C
is the graph obtained from G after such
shrinking, then the shrinking is justified by
the following theorem due to Edmonds
(which may be called the Blossoms Theo-
rem or the Main Theorem of Botany):
Jhere is an augmenting path in G if and
only if there i an augmenting path in C.
We do not know of any easy proof for
Theorem 2. Wedo nodiseuss de''aa6 Ir'
Part here (see Lawler [1976] or the second
b9ition o Tarjan (1983]). The tif part'' is
9bvious. Given an augmenting path in G, it
Unmediately yields an augmenting path in
G. If the path goes through B, then we do
the following: Replace the matched edge,
say (B, k), with (r, k); replace the un-
matched edge, say (B, j), with the edge
4, j), from which (B, j) originated (recall
the set A' above), followed by the even
alternating path in B from i to r. Such a
path always exists. If i was an S-vertexz
when B was formed, we use the labels and
backtrack from ito r. Otherwise, we use the
labels in reverse order around the blossom.
Storing B as a doubly linked list with a
marked base makes this very easy. Note
that, in Figure 1, as soon as the blossom B
is shrunk, the augmenting path can be
found. Note also that not every alternating
path from a vertex to itself is a blossom,
only those discovered by the algorithm (in
case C2).
The search for an augmenting path uses
a queue , where new S-vertices are in-
serted. During the search, vertices from
are scanned and new blossoms are occa-
sionally generated. A blossom is a recursive
structure because it can be nested. It is
convenient to refer to vertices that do not
belong to any blossom as (degenerate) blos-
soms of size 1. When a new blossom B is
generated from blossoms B;, . . ., Bi, we
call the latter the subblossoms of B. We do
not refer to them as blossoms anymore. As
a result, each vertex in the original graph G
always belongs to one blossom in the cur-
rent graph. The structure of each blossom
B is described by a tree, called the structure
tree of B. In this tree, the root is labeled
with B, and the sons of a node labeled C
are labeled with C4, . -., C, if at some time
the blossom C (which is now part of B) was
generated from subblossoms C4, . . ., C,.
Thus, the leaves of the structure tree of B
are the vertices that belong to B, The struc-
ture tree is represented by the collection of
the doubly linked lists of the various sub-
blossoms of B.
When a new blossom B is generated, it
is labeled by S and inserted into . Its
subblossoms that are still in ? are deleted
from . The search continues (in G).
If the search succeeds (in C2), we find
the augmenting path in the current graph.
Then we use the easy part of Theorem 2
and the structure trees to recursively un-
wind the augmenting path in the original
graph. We next augment the matching,
erase all labels and blossoms, and start a
new stage. Constructing the augmenting
path, augmenting the matching, and eras-
ing the labels take O(n) time. If the search
fails ( becomes empty), a repeated appli-
cation of Theorem 2 (each time a blossom
is shrunk) and an application of a modified
version of Lemma 1 (in which ''boys'' and
''girls'' are replaced by ''persons'') imply
thatthe current matching is maximum, and
we are done.
A naive implementation [Edmonds
1965a] takes O(n') (O(n') per stage). A
more careful implementation takes O(n')
[Gabow 1976a]: Since the blossoms are dis-
joint, the total size of all structure trees at
any moment is O(n). When we generate a
new blossom, we do not rename the edges;
edges retain their original names. In order
to find out quickly to which blossom a given
vertex belongs, we maintain a membership
array. When B becomes a blossom, we put
the T-vertices into the queue ; so we later
scan them instead of scanning the new
vertex B. The other vertices of B (the
S-vertices) have already been inserted into
, andwe do not delete them from Q. When
we consider an edge in C2, we ignore it if
both endpoints are in the same blossom. In
this implementation a stage takes O(n%)
time.
A slightly better bound can be obtained
as follows. If we find the base r of a new
blossom B more carefully, by backtracking
one vertex at a time, once from i and once
from j, marking vertices on the way, we
find the base and construct the blossom in
time O(h), where k is the number of sub-
blossoms of B. Hence, the total time per
stage devoted to finding bases and con-
structing blossoms is O(n). (Each node in
the structure tree is charged O(1).) Using
the 'set union'' algorithm to maintain the
sets of vertices in the blossoms for the
membership tests takes O(mo(m, n)) per
stage for a total of UU(mna(m, n)), where a
is the inverse of Ackermann's function
[Tarjan 1975].
The obvious question that comes to mind
is whether the ideas of the phases can be
realized in the nonbipartite case. Recall
that in one phase we discovered a maximal
set of vertex disjoint augmenting paths of
shortest length. This is important becsui
Lemma 2 holds for general (not necessari
bipartite)graphs. Executing a phase in gei
eral graphs is more complicated than in ti
bipartite case because of the exsistence.
blossoms.
Even and Kariv [1975] showed how
execute a phase in time min(n', m log g
This resulted in an O(min(n'', m4
log n)) algorithm. A more detailed versid
[Kariv 1976] is a strong contender for t8
ACM Longest Paper Award. (It will prol
ably lose only to Slisenko's real-time ps
indrome recognizer [Slisenko 1973].)
More recently, a simpler approach wi
found by Micali and Vazirani [1980].
phase i, i = 0, 1, .,, of the algorithm,
maximal set of (shortest) augmenting patHl
of length L = 2i + 1 is sought as follosi
An odd (even) level of a vertex u is define
as the length of the shortest odd- (even
length alternating path from a single verte
to v. Both types of level numbers are conmj
puted in linear time. Let the sum of a fri
(matched) edge be the sum of the eve
(odd) levels of its endpoints plus one.
bridge is an edge of sum L that belongs t
an augmenting path of length L. Ewej
gmenting path of length L is shown ti
contain a bridge. Bridges are identified ani
new search technioues applied to find dis
joint augmenting paths of length L. We dj
not give the details here.
The immediate implementation of Mical
and Vazirani's aleorithm takes O(mVno(ai
n)) time.The authors claimed that the psi
ticular case of the disjoint set union ussj
by their algorithm can be shown to requii
only linear time; as a result their algorithi
runs in time O(mmVVn). 9uite recently, Gel
bow and Tarjan [1983] found a linear-tim
algorithm for some special cases of the digg
joint set union. One of these special case9
is the one needed in Problem 2. (As a result
also the O(mno(m, n)) algorithm medß
tioned above can be implemented in time
O(nm).)
TThe most efficient algorithms for Problems
3 and 4 use some observations on dats
structures that we now review. A prioriff5
queue (p.4.) is an abstract data structure
consisting of a collection of elements, each
with an associated real-valued priority.
Three operations are possible on a p.q.:
An implementation of a p.q. is said to be
eificient if each operation takes O(log n)
time, where n is the number of elements in
the p.q. Many efficient implementations of
p.q.s are known, for example, 2-3 trees
[Aho et al. 1974; Knuth 1973].
In p.q.s elements have fixed priorities.
What happens if we allow the priority
of the elements to change? Obviously, an
additional operation that changes the
priority of one element can be easily imple-
mented in time O(log n). On the other hand,
it is not natural to allow arbitrary changes
in an arbitrary subset of the elements in
one operation simply because we have to
specify all these changes.
We consider two generalized types of
p.qs, which we denote by p.q. and p.4-4.
The first simply allows a uniform change
in the priorities of all the elements cur-
rently in it. The second allows a uniform
change in the priorities of an easily speci-
fied subset of the elements.
More precisely, p.4.4 enables the follow-
ing additional operation:
4. decrease the priorities of all the current ele-
ments by some real number ä.
A version of p.q.4 was used by Tarjan
(977L
To define p.4.4, we first need some as-
Sumptions. We assume that the elements
belong to a totally ordered set and that they
are partitioned into groups. Every group
can be either active or nonactive. An ele-
ment is active if its group is active. By
splitting a group according to an element e,
e mean creating two groups from all the
rlements in the group greater (not greater)
thn e Noe uiau ilike ue udai
9peration, we split a group according to an
8pent and no aceorminr to is pnerns.
The operations possible for p.q.s are
It may seem at first that one may need
up to n steps to update all the priorities
as a result of one change. However, it is
possible to implement p.4.; and p.q. effi-
ciently. In particular, the change of priori-
ties can be achieved implicitly in one step
(see Galil et al. [1986]):
p.4.4 and p.4.4 can be implemented in time
O(og n) per operation.
The implementation of the generalized
p.q.s uses regular p.q.s and a collection of
offsets to handle the change in priorities.
We also make use of Johnson's d-heap
[Johnson 1977], where d is the number of
sons of internal nodes. (The usual heap is
a 2-heap.)-We partition the primitive op-
erations into two types. Type 1 includes
inserting an element and decreasing the
priority of an element, and type 2 includes
deleting an element. Type 1 involves 'sift-
ing up'' the heap for a cost of O(loga),
whereas type 2 involves 'sifting down'' the
heap for a cost of O(d logAa). Consequently,
the following theorem holds.
Let d = Im/n + 1l, A d-heap supports m
operations of type 1 and n operations of type
2 in time O(m logaaa).
Introducing weights and maximizing the
weight make the problem much harder. One
can show that the following approach
solves Problems 3 and 4. Start with the
empty matching, and in each stage find an
augmenting path with the maximal in-
crease of the weight. One then can show
that after k stages, we have a matching of
maximum weight among matchings of size
k (eg., see Tarjan [1983]). In the case of
Problem 3, finding such an augmenting
path is relatively simple, because it can be
easily reduced to solving a single-source
shortest path problem for graphs with non-
negative edge lengths. But finding such an
augmenting path for general graphs is
much harder, so we choose a completely
different approach.
We use duality theory of linear progam-
ming (specifically the primal-dual method)
to derive the algorithm. We need linear
programming for motivation only. Once we
obtain the algorithm (for Problem 3 or 4),
we prove its correctness by a one-line proof.
Therefore, the description below is some-
what sketchy. (See Lawler [1976] or Papa-
dimitriou and Steiglitz [1982] for more
details.)
After defining the problem as a linear
program, we consider the dual problem and
then use complementary slackness to
transform our optimization problem into a
problem of solving a set of inequalities
(constraints). A pair of feasible solutions
for the primal and the dual problems are
both optimal if, for every positive variable
in the one, the corresponding inequality in
the other is satisfied as equality.
In the case of Problem 3, defining the
problem as a linear program is immediate.
We describe it as an integer program and
replace the integrality constraints x;; EE
[0, 1] by 0 ss x;;, Since the matriz of con-
straints is unimodular, we must have an
optimal integral solution.
We will have a primal solution, a match-
ing M, and a dual solution, an assignment
of dual variables u, u (corresponding to
boys i and girls j). For convenience we
define slacks ,; for every edge (i, j): ,;
ur t u; - u,;. The inequalities x,; 2 D are
the constraints of the dual problem.
(Whhenever we mention s;; below we always
assume that (i, j) 6 E.) By duality, M has
a maximal weight if 6.0-6.2 hold:
The sufficiency of 60-6.2 for optimality
can be proved directly as follows: Assume
that M, u., 4,. , satisfy 6.0-6.2 andd
N be any matching. Then ZuowN 4
2ZuuwN 4 + w - , s Ei wu + Eiw
6.0 and the fact that N is a matching),wl
2Zuws W, == Ei u. + E, w4 (hy 6.1
6.2 and the fact that M is a matchii
Consequently, M is a maximum wei
matching.
So, we only have to find a matchin{
and an assienment of the dual variai
that satisfy 6.0-6.2. We use the prin
dual method. This method starts witi
simple solution, which violates some of
constraints. The solution is then' modif
in a way that guarantees that the numi
of violations is reduced. In our case we sf
with M = C, u, maxsi uwfor bovs and
= 0 for girls. The initial solution satisd
60 and 6.1, and violates only 6.2, The
gorithm makes changes that preserve
and 6.2and reduce the number of violatiä
of 6.2.
The algorithm consists of O(n) stages.
each stage we look for an augmenting psi
as in the simple algorithm for Problemi
except that we use only edges with [
slack (j; = 0). If the search is successf
we augment the matching (ie., chai
the primal solution) and start a new stai
This is progress because one single boy gi
married.
If the search fails, we change the di
variables as follows. Let & = min(&4,
84 = IIDES-oy , &g n1TeS-bosJbwst
For an S-boy iwe set u. +- uu- 4, and fdj
T-girlj we set u, - u; + & It is easy to
that b > 0 and the change preserves 6.0
6.1, Also 4 = u, for sny single boy 4 (i
of all boys had the same initial value,
in each change of the dusl variables,
single boys had S label and their dual vi
ables were decreased by the same ß). If
&, then after the change 6.2 holds, and
are done. Otherwise, for each edge (i, il
an S-boy and j a free girl, with x,; =
(there exists at least one), ,; becomes zer3
and we can continue the search. Since
least one girl gets a T label as a resui
= 84 at most O(n) times per stage.
The naive implementation of the alg{
rithm above takes O(mn') time. The mos
costly part is maintaining &4. For eveg
free girl j, let , = minasw n; and
e; = (i, j) be an edge with i an S-vertex ai
., = ,, Then &4 min,nwea j, Note that
when we make a change of & in the dual
variables, x; is reduced by & and e; does not
ehange. Also, if & = &4 Y,, then the slack
of e, becomes 0 and it can be used for
continuing the search. By maintaining x;
and e, for all free girls j, an O(n') imple-
mentation of the algorithm follows.
In a different implementation, we main-
tuin the collection C = l(i, j)] , > 0, ian
N-boy,j a free girll as p.4-, since all these
v,,'s are reduced by & in a change in the
dual variables. Whenever we scan an
S-vertex i, we consider all edges (i, j), where
j is a free vertex. Those edges with x; > 0
are inserted into the p.q.4. Consequently,
this implementation takes O(mn log n)
time.
A small improvement is achieved if we
maintain , and e, (as above) for free girls j
in a ps.4. Then & is the minimum of this
p.q. One can see that the p.q. used here
satisfies the conditions of Theorem 4, and,
consequently, we get an O(mn log/s-1nn)
time bound, which dominates the two
bounds of O(n') and O(mn log n).
A closer look at a stage reveals that an
augmenting path is found using Dijkstra's
algorithm for all shortest paths from a sin-
ele source [Dijkstra 1959]. The source is a
new vertex, which is connected to all single
boys with new edges of length zero. The
lengths of other edges are the xj;'e at the
beginning of the stage. The reduction of a
Aage to a shortest-path problem is well
known [Gabow 1974]. In fact, each stage
discovers the augmenting path that causes
the largest increase in the weight of the
matching. The various implementations
of Dijkstra's algorithm are (1) the naive
mplementation: O(n'), (2) an imple-
ntation using p.s: O(m log n), and
[%A an implemeuion using TEeoeni 4e
'?m vz--wn), Hence, he eorresponding
time bounds for n stages follow immedi-
ately.
The main purpose of this section is to
=erve as a warm-up for the next section.
t we try to solve Problem 4 exactly as we
8oIved Problem 3, we immediately run into
problems. The linear program obtained by
dropping the integrality constraints from
the integer program for Problem 3 may
have no integer optimal solution. Edmonds
[1965b] found an ingenious way to remove
this difficulty, which led to a polynomial-
time algorithm for Problem 4. He added an
exrponential number of constraints of the
following form. For every odd subset of
vertices B,
These new constraints must be satisfied by
any matching, and, surprisingly, their ad-
dition guarantees an integer optimal solu-
tion, This fact follows from the correctness
of the algorithm, which can be proved di-
rectly.
We now proceed as before. We have a
primal solution, a matching M, and a dual
solution, an assignment of dual variables u;
for every vertex iand z,for every odd subset
of vertices B,. We now define slacks x;
slightly differently: xn, u, + u; - 4;
2üues,. (Again xi;2 0 are the constraints
of the dual problem.) By duality, M has
maximal weight if 7.0-7.3 hold:
In fact, as in Problem 3, we only need
duality theory for motivation. The suffi-
ciency of 7.0-7.3 for optimality can be
proved directly: Assume that M, u., , s
satisfy 7.0-7.3, and let N be any matching.
Then 2uaeN W, > 2ZueN (u: uu- ,
2Aiiuws.)= Z. u.+ Z,o0hy 70 and the
fact that N is a matching), while 2uowat Va
= S, u. Ei r. (hy 7.1-7.3 and the fact
that M is a matching).
We can use 7.0-7.3 to derive a polynomial
algorithm because we will have z, > 0 only
for blossoms or subblossoms, and their total
number at any moment is O(n). We main-
tain only z,'s that correspond to blossoms.
Since we consider only ,, for i, j not in the
same blossom, x,; u, t u; - u;, as in
Problem 3.
We again use the primal-dual method.
We start with M = , u, (maxw1 4n.0/2
and no z,s (no blossoms). The initial so-
lution violates only 7.2. The algorithm
makes changes that preserve 7.0, 7.1, 7.3
and reduce the number of violations of 7.2.
As in Problem 3, the algorithm consists
of O(n) stages. In each stage we look for an
augmenting path using the labeling R12
and the two cases C1, C2 as in the simple
algorithm for Problem 2, except that we
only use edges with x,; = 0. If the search is
successful, we augment the matching.
To preserve 7.3 we keep blossoms with
z4 7 0 shrunk at the end of the stage. As a
result, we have two new kinds of blossoms
in addition to the S-blossoms we had in
Problem 2. (Recall that a newly generated
blossom is labeled by S.) Since the labels
are erased at the end of a stage, we may
have free blossoms at the beginning of a
stage. During the search a free blossom can
become a T-blossom. (Recall that a blos-
som is just a vertex in the current graph.)
We call the vertices of an S-blossom
(a T-blossom or a free blossom) S-vertices
(T-vertices or free vertices). When, during
the search, a new S-blossom B, is formed,
the vertices in its T-blossoms (which now
become subblossoms) become S-vertices
and are inserted in the queue 0 (of
S-vertices). We also initialize a new z, to
If the search is not successful, we make
the following changes in the dual variables.
We choose & = min(b4, 4, 4,4,l, where
We then set
Such a choice of & preserves 7.0, 7.1, and
73
If& = &i, we expand all T-blossoms B
which the minimum was attained. (E:
corresponding z, becomes 0.) The epi
sion of blossom B is shown in Figure 2
stons beine a blossom and its subblossoi
become blossoms. All vertices of the ni
S-blossoms are inserted into .
If & = & (& = &4), we consider all edi
(i, j) with i an S-vertex and j a free verfj
(an S-vertex not in the same blossom)
which the minimum was attained. For es
such edge x;; becomes 0 and we can us8
for continuing the search. At the end
each stage we also expand all S-blossodd
B, with z, = 0.
Let us call each change in the dual vai
ables a substage. Each S-blossom cori
sponds to a unique node in one of tj
structure trees at the end of the stage. Eai
T-blossom corresponds to a unique node
one of the structure trees at the beginnin
of the stage. Consequently, for i = 2, 3,
& = &; at most O(n) times per stage: wh(
& = &, a blossom becomes a T-blossod
when 8 = , either the stage ends or a ng
S-blossom is generated, and when & = &,
T-blossom is expanded. Finally, & = '
most once. Consequently, there are O(
substages per stage.
The most costly part of a substage
computing &. The obvious way to compu4
it takes O(m) steps and yields an O(mn
algorithm. Edmonds' time bound wg
O(n').
The only parts that require more thsi
O(n') are maintaining &y and &. & is han
dled as in the O(n') algorithm for Proble4
3. To take care of &. we define for eved
pair of S-blossoms B,, Bi:
We record the edge e,a on which the ming,
mum is attained and maintain W, =
miniV?;, We do not maintain VV, ), but any
time we need it we compute it by using eu
Obviously &, = mine,. A change in th
dual variables and computing & costs O(n3
as for &;. We have to update ]V,] and el
any time an S-blossom B. is constructed-
from B., .-., B,. Recall that (r + 1)f4
of the subblossoms are S-blossoms and
(r - 1)f2 of them are T-blossoms. We
first ''make'' each T-blossom B, into an
S-blossom by considering all its edges and
computing for it lVWV,il and le,il. Then we
use the ,/s of B,,, . . ., B, to compute ,
and le,jl for the new blossom B, and to
update ]V,] for j w k. The total cost (per
stage) to make T-blossoms into S-blossoms
is G(m). We now compute the rest of the
cost T(N), where N is the number of S-
blossoms plus the number of non-S-vertices
in the graph. T(N) ss crN + T(N - r + 1)
because rN is a bound on the number of
,/s considered after making the T-blos-
soms into S-blossoms. T(N) = O(N) (by
induction on N), and the total cost of com-
puting & is O(n'). The discussion above
results in an O(n') algorithm [Gabow 1974;
lLawler 1976].
The most costly part of the algorithm is
the frequent updates of the dual variables,
which cause changes in l,]. Note that all
the elements that determine each &; are
4ecreased by & each change in the dual
*riables. We maintain 44,&, 4i by a p---
We use a p.qto maintain u, for T-verices,
and another p.q. for z,s of S-blossoms B,.
If we try to maintain ä4 by a p.q.4, we
tave nroblems. Consider Figure 3. Isitially
there may be a large free blossom B,. At
that time all edges in Figure 3 should be
99nsidered for finding the value of &,. Later
ti may become a T-blossom. Then these
%dges should not be considered for finding
he value of .. Later still, Bi may be ex-
Panded and one of its subblossoms, B,, may
eome tree. The ister may wubseuenuy
b+4 Tbosom.aGsä.kGiäiäiü
implementation requires the consideration
of each such edge a large number of times
(up to k in Figure 3).
To maintain &4,we have a p.q.4. For every
free blossom (T-blossom) B, we have an
active (a nonactive) group of all the edges
from S-vertices to vertices in B,. Note that
if (i, j) is in a nonactive group (i is an
S-vertex and j is a T-vertex), then xj; does
not change as a result of a change in the
dual variables. It is now easy to verify that
the eight operations of p.q.4 suffice for our
purposes.
Consider a group g, which corresponds to
a blossom B. The elements of the group are
the edges l(i, j)li an S-vertex,je B]. The
order on the elements is derived from the
order on the vertices of 8. The latter is
taken to be the left-to-right order of the
leaves of the structure tree. The order be-
tween two edges (i, j) and (f, j) is arbi-
trary, The order enables us to split the
group g into the groups corresponding to
B), ..., B, when we expand B to its sub-
blossoms.
To maintain the generalized priority
queues, we make a change in the scanning
of a new S-vertex i We also take into
account edges (i, j) with x,; > 0 and have
three more cases in addition to C1 and C2
for edges (i, j) with ,, = 0. Assume that j
is in a blossom B with x,, 7 0.
We insert (i, j) with priority x,; into the
active (nonactive) group corresponding
to B.
(C5) B is an S-blossom and i e B.
We insert (i, j) with priority x,/2 to the
T.. that computes &4.
Since &; = u, for any single vertex , we do
not need a generalized p.q. to compute b.
Nevertheless, we have a p.q.4 for the u,'s of
the S-vertices and also a p.q.4 for the u,'s
of the T-vertices for computing x,; when
the edge (i, j) is considered.
We have a p.q.; for the z,'s for S-blossoms,
because at the end of a stage they all be-
come free, and in the next stage they may
become T-blossoms.
The p.q.) for computing &4 contains also
edges (i, j) with iandj in the same blossom.
We do not have time to locate and delete
such edges each time a new blossom
constructed. Consequently, if & = & ai
& x;, we first check whether i and j 4
in the same blossom. If they are, we del4
the edge and possibly compute a ni
(larger) &.
All edges (i, j) in the generalized p.:.'s thi
compute &4 or &4 have x; 7 0, since
element is deleted as soon as its priorii
becomes 0. Similarly, all z,'s in the pAi
that computes &. are positive. Consi
quently, 5 > 0.
To derive an O(mn log n) time bound, W
need to implement two parts of an alg
rithm carefully:
1. We maintain the sets of vertices
each blossom (for finding the blossom thai
contains a given vertex) by concatenab
queues [Aho et al. 1974]. Note that the
number of finds, concatenates and splitsi*
O(m) per stage, and each takes O(log 4
time.
2. In C2 we use the careful backtracking
described for Problem 2.
The O(mn log n) time bound is easily
derived as follows, There are at most n
augmentations (stages). Between two aug-
mentations we consider each edge at most
twice and have O(m) operations on (gen-
eralized) p.g.s, (This includes 1 and 2
above.)
We have considered four versions of the
maximum matching problem and discussed
the development of the most efficient al-
gorithms for solving them. By 'most effi-
cient algorithms'' we mean those that have
the smallest asymptotic running times. We
now mention briefly a number of closely
related additional topics and give some ref-
erences. These are intended to serve as
examples and certainly do not form an ex-
haustive list.
(a) Applications of Matching. We do not
list here the many applications of solutions
to Problems 1-4. For some applications see
Lawler [1976].
(b) Generalization of Matching. Prob-
lems 1-4 can be generalized in a number of
ways, For example, Gabow [1983a] has re-
cently considered similar problems where
some kinds of polygamy are allowed. He
found efficient reductions to the corre-
sponding matching problem. Stockmeyer
and Vazirani [1982] showed that several
natural generalizations of matching are
NP-complete.
(c)Special Cases of Matching. Many ap-
plications solve one of the Problems 1-4,
but only for special graphs. For example,
Problem 1 is used to find routing in super-
concentrators [Gabber and Galil 1981]. The
Rraphs that arise in this application have
Vertices with bounded degree, and hence
the solution given here takes time O('P')
ferhaps this can be improved. For better
Igorithms for some special cases of Prob-
8m 1, see Cole and Hopcroft [1982] and
9abow [1976b].
(d) Randomizing Agorithms. Several
lgorithms that work very well for random
Wraphs or for most graphs have been devel-
0ped. They are usually faster and simpler
than the algorithms discussed here [An-
gluin and Valiant 1979; Karp 1980]. An
interesting problem is to find improved
randomizing algorithms that use random
choices (rather than random inputs).
(e) Approximation Agorithms. As for
all optimization problems, we may settle
for approximate solutions. For cardinality
matching, the solution that uses phases
yields a good approximation by executing
only a constant number of phases. For sim-
ple, fast, and good approximation algo-
rithms for special graphs see Iri et al.
[1981], Karp and Sipser [1981], and
Plaisted [1984].
(f) Improvements. We next discuss
possible improvements of the algorithms
considered in this paper. All the time
bounds discussed in this paper can be
shown to be tight. One can construct fam-
ilies of inputs for which the algorithms
require the number of steps that is specified
by the stated upper bounds. There are no
known lower bounds for any of the four
problems. Improving the O(m/n) bound
for cardinality matching must involve the
discovery of a new approach that does not
use phases. Similarly, except for a logarith-
mic factor, improving the bound for
weighted matching requires the use of an
approach that does not make d(n) augmen-
tations. Perhaps the introduction of phases
may lead to improved algorithms for Prob-
lems 3 and 4. Note that the solution to
Problem 3 is slightly better than the solu-
tion to Problem 4, due to the use of
d-heaps. It may still be possible to find a
similar improved solution for Problem 4.
There are several theoretical questions
concerning Problems 1-4. Their solution
may lead to simpler or faster algorithms:
Assume that we have solved an instance
of a weighted matching problem and then
make a small change such as adding or
deleting some edges or changing the weight
of a few edges. It is not clear how to make
use of the solution to the original problem.
there is often a distribution of activation energies. This reflects the
multiplicity of 'reactions' that can lead to the final degraded product.
Even the collision frequency term X in the solid state involves vibration
(and rotational) kinetic energies which are much more difficult to
describe than the kinetic energies of an ideal gas. The orientation factor
Z is also very difficult to describe and is very much involved with the
orientational entropy of the reaction (as itis also in the ideal gas case).
It is impossible (without suitable experimentation) to know what, if
any, mechanical, physical or chemical properties of a geosynthetic
productwill be governed by a single reaction rate theory overa reasonably
wide temperature range. It remains for careful experimentation to sort
out the behavior of the reaction rates and to assess its behavior and
appropriateness for this type of analysis. A variety of different experi-
mental possibilities will be discussed in later sections of the paper.
Using the Arrhenius modeling approach described in the previous
section, the long-term degradation of a polymer product can possibly be
evaluated. An example of an actual experimental data set is shown in
Fig. 4, which is taken from Martin & Gardner (1983). Here is shown the
inverse of the half-life of the tensile strength for a glass-reinforced PT
plastic versus inverse temperature. The half-life is the time it takes to
aging at temperatures that are significantly lower than those used when
testing at ambient pressure.
The procedure for prediction of degradation using OIT measurements
requires incubation of a set of polymer samples at different temperatures.
The immersion medium should be the site-specific liquid. These samples
are removed periodically and tested for their OIT values. These values
are compared to the original, and unaged, OIT values and plotted as a
function of time. The results might appear as shown in Fig. 14b). An
arbitrary decision is made as to the limiting values (e.g. at 50% reduction).
from which times t) .s.t,iare obtained. Procedures followed from this
point are as described previously.
It should be noted that the use of OIT for lifetime' prediction has been
attempted hut in a different manner than just described. Cadwallader &
Metzger (1987) did the 'incubation' directly in a DSC cell under
temperatures of 130 C, 1600'C and 180 C, in oxygen and at a very high
pressure of 5-5 MPa (800 Ib/in). What the 'real life' significance is of
these conditions, is quite unknown.
The measurement of the amount of oxygen absorbed by a polymer test
specimen as a function of time is the classical method of oxidation
studies of polyethylene, as well as other polymers (Hawkins et al., 1959).
Howard & Gilroy (1975) provide data and include Arrhenius graphs
further illustrating the use of the predictive concepts.
The experimental set-up generally used to measure the oxygen uptake
is as follows (Bandyopadhyay et al., 1985). The sample is heated in an
oxygen atmosphere in a sealed tube with desired temperature and
oxygen concentration. The moles of oxygen reacted can be directly
measured from the pressure loss within the tube. Results from this type of
testing appear as shown in Fig. 15(a). What is needed for Arrhenius
modeling is a series of oxygen uptake curves from samples at various
constant temperatures immersed in the site-specific liquid. The resulting
curves might appear as shown in Fig. 15(b). From here an arbitrary value
of oxygen uptake is assigned and corresponding times are obtained for
which the Arrhenius plot is generated. The procedure at this point has
been described previously.
The carbonvl index is a value which is obtained from a Fourier
transform infrared (FTIR) device. The concept of FTIR is based upon
Tokyo.The second author gratefully acknowledges the financial support
of the Japanese Society for the Promotion of Science (JSPS) for his
sabbatical leave at the University of Tokyo, during which this study was
conducted. Mr T. Sato, technical staff of the Geotechnical Engineering
Laboratory, offered his kind advice in the design of the test apparatus.
Appreciations are also extended to Mitsui Petrochemicals Industries
Ltd, Rhone-Poulenc Fibers, and Reemay Inc., for supplying their
geotextiles. The testing apparatus was manufactured at the machine
shop of the Institute of Industrial Science. University of Tokyo.
Coherent optical transmission systems are currently of great interest due to many recent
advances in semiconductor laser technology as well as novel methods for increasing
optical receiver sensitivity. The two main advantages of coherent detection over direct
detection are improved receiver sensitivity and an enhanced frequency selectivity. These
advantages offer the possibility of long, repeaterless data transmission at potentially high
bit rates. The increased sensitivity of practical coherent systems over direct detection
bility in tuning the directional coupler as will be discussed later. In practice, a scheme
for general SOP detection is not necessary, as most practical coherent detection
schemes try to maintain the local oscillator in either the TE or TM mode. This greatly
simplifies the proposed receiver by eliminating the Mach-Zehnder, making it much
more feasible and easier to fabricate. The simplified configuration is shown in Fig. 2b.
Monolithic integration of the necessary density is currently achievable in LiNbO,
making the receiver with SOP control feasible for fabrication on a single LiNbO,
substrate using known technologies. Further discussion of the difficulties in fabricat-
ing such a system is presented in the next section and the last section of this paper. The
proposed system would have the mechanical and thermal stability advantages inherent
in a monolithically integrated system as well as low loss and optical fiber compatibil-
ity, The system should also be compact and have low drive voltages. Due to the slowly
varying, random nature of the polarization fluctuations on the order of a few hundred
hertz [31], the active devices in the scheme may be driven with conventional lumped
electrodes instead of a more complex traveling wave electrode design. The slowly
varying nature of the fluctuations also allows for the averaging of the polarization over
a few bit cycles thereby lowering the sensitivity penalty due to the signal tapoff for the
SOP detection stage. Further discussion of some of these advantages is presented in
the last section.
The proposed SOP control system consists of phase shifters, TE - TM mode con-
verter and 2fß directional couplers. In order to monolithically integrate the complete
SOP control system onto a single LiNbO, substrate, an x-cut or z-cut crystal orienta-
tion is desirable. Both x-cut and z-cut configurations require the use of a periodic,
interdigitized electrode structure in order to achieve TE - TM mode conversion via
the r,; electro-optic coefficient (Fig. 3). Such a device can be analyzed by using the
standard coupled mode equations [32, 33].
ments for both good impedance matching and low drive voltage. The calculated values
of Z, are also shown in Table 1.
For comparison, though there are different fabrication process conditions for LiNbO,
and LiTaO, substrates, the same electrode pattern is used. A schematic diagram of the
phase modulator fabricated on Y-cut LiNbO, and Y-cut LiTaO, is shown in Fig. 2. For
must be (S/N), 2 5.2 or 7 dB [2]. Kourtis [4] calculated the narrowband and wideband
jamming rejection capacity (process gain) of direct sequence spread spectrum systems.
For narrowband noise the process gain was found to be proportional to the code
frequency-to-message frequency ratio (f,/,,), and for wideband noise, or interference
caused by other spread spectrum signals, the process gain was found to be proportional
to 02) 0A).
Under these conditions, including the spread spectrum's process gain for wideband
jamming rejection, [4]
The more complicated the inductive construction of the data type, the longer these
rites.
Of course, in many cases the proof of the equality of two functions can be given
purely equationally without appealing to either induction or these algebraic tools
- otherwise no proof would be possible at all, since the common proof obligation
has the shape of a set of functional equalities. Somewhat surprisingly, it turns out
that often such a proof can also be substantially shortened by appealing to the
Unique Extension Property.
Not all functions on the naturals are homomorphisms. Attempts to prove
a (valid) functional equality for a non-homomorphic function by appeal to the
Unique Extension Property are doomed to fail, and, in fact, even for homo-
morphisms success is not guaranteed. An example is the factorial function fac:
there exists no simple function s such that II(fac) holds. However, there are simple
functions @ and z such that IITI( fac) holds, where IIII is the pattern given by
(Here @0 is a binary function; between two functions returning natureals @
denotes the application of @0 to the results of these functions.) The instantiation
that gives the factorial function is that in which @ is taken to be the operation such
that m 0@ n = m x (succ -n), and z is 1.
Like II before, IIII has a unique solution for each choice for the unbound
functions, in this case @ and z. So the following is a valid statement:
This can be shown to follow from the Unique Extension Property. But the proof of
this is (even for a simple type like the naturals) non-obvious, lengthy, and in fact a
new ritual that can be avoided by a properly designed extension of the theory.
Category theory provides some concepts that have proven indispensable in the
formulation of generic theory, paramount among which is the notion of a functor.
We give a treatment here slightly geared towards our purposes. In particular, we
handle only the unary case, although the type constructors x and -+ introduced
below are also (binary) functors.
A functor is a pair of functions, one acting on types, and one on functions, with
some further properties as stated below.
assigns to each type A a type At, and to each function fe A -- B a function
fte At-- 8t, where the latter mapping preserves function composition and
identity; more precisely,
Equality (2) requires that f-g is well-typed; this is viewed as a wellformedness
condition that applies in general to all constituents of functional expressions, and
is from now on left implicit. In denoting an identity function, as in (3), its type is not
stated, but in any context id is assumed to have a specific type, and so (3) stands for
as many equalities as there are types.
An appeal to these equalities will be indicated in the justification of a proof step
by ''functor''.
An important type construct is x. It has a corresponding action on functions.
(In [Mee89] I used different notations for x on types and on functions, which was
a bad idea.) It is informally defined by
We have the usual '' projection functions'' from A x B to A and B, which are
denoted as
We also need the combinator that combines two functions fe A -- C and
ge B-- C into one function
(The usual category-theory notation is (. g).)
It can be characterised by
Its relationship with x is given by
which can in fact be taken as the definition of x on functions. From these equations
all calculational properties of C, 2, a and x are easily derived.
The relevant properties that we shall have occasion to use are
A fact that we shall also use is that any mapping a F, i.e., mapping a function
f with the same domain as F to the function fs F, is a bijection, so that
For discussing the application of the theory we need the dual type constructor
-+, which forms the ''disjoint'' or ''tagged'' union. Informally,
The Calculus of Constructions of Thierry Coquand [Coq85, Coq87, CoH86a,
CoH86b, CoH88] is a system of typed i-calculus in which the second order poly-
morphic typed i-calculus can be interpreted. Furthermore, using the formulas-as-
types notion [How80], it is possible to interpret constructive higher order logic in
it. It differs from the type theory of Martin-Lof [Mar75, Mar82, Mar84] in two
important ways: it is impredicative, and it has only a small number of primitive
axioms and rules. Coquand proved [Coq85] the normalisation theorem for the
calculus of constructions, and later [Coq87, CoH88] extended this to a proof of
the strong normalisation theorem. He also proved [Coq86] that any one of three
The first two results show that fault-tolerance is introduced and preserved
by F-refinement transformations. From the next two results, it can be seen that
refinement transformations can be applied to the original program and fault-
tolerance introduced after that, using F-refinement.
Corollary 6.1, Given a program P and a fault environment F,
As an example of the first result in Corollary 6.1, let S4 be the initial state of
program P and
then
Program P []Py- is a fault-tolerant program such that whenever a fault occurs, the
execution of P will recover by re-starting from its initial state [JMS87, HeH87].
The following theorem introduces a useful F-refinement rule.
Theorem 6.2. Given a program P = A) [] .. . ]A, and a fault environment F, let I
be a non-empty subset of [1,. . .,n} and
where for each i e [1,...,n),
then
The next theorem provides a rule to allow a recovery action to be freely introduced
at any point in the program.
Theorem 63. For the program 9;(P) = A,[].. ,]A, given in Theorem 6.2, if an
action A, of 9;(P) can be written as at .4o:, let
Then
It is noted that for any command c
And we assume here that the fault environment of skip is always empty:
Types were first introduced into logic by Bertrand Russell in the early 1900s, and
into a i-calculus-based system by Alonzo Church [Chu40]. In Church's system the
only types are atomic constants (to denote particular sets) and types of form
(o -- r), where o and r are any already-built types. Each i-term has a unique built-
The problem of determining the forces on and heat transfer of
blunt-nosed bodies in supersonic jets and nonuniform flow fields is
of current interest because of ongo ing research on subjects as di-
verse as the behavior of the planets of the solar system, super-
sonic aircraft, and industrial plasma flows. In contrast to the
well understood [1-5] case of uniform supersonic flow fields rela-
tively little is known about the flow patterns generated in non-
uniform flow about various bodies. The unresolved problems in-
clude rearrangement (or transition in) of the wake flow between a
pair of bodies, the effect of the sizes of the bodies and the
distance between them on the distribution of pressure and heat flux
over the rear (trailing) body and on the drag of that pair of
bodies, as well as the effect of the depth of a cavity in and the
porosity of the rear body on the heating and aerodynamics of that
body. While scattered information has been published on these
topics, no comprehensive review of the theoretical and experi-
mental research on the supersonic flow around bodies situated in
wake flows at various distances behind other bodies has been
undertaken.
The problem of wake flow behind an object traveling through
the atmosphere at a supersonic velocity involves many fundamental
problems of both traditional and cutting-edge aerodynamics, i.e..
turbulent gas flow, flow separation and attachment, and nonequili-
brium flow. The work of Abramovich [6], Ginevskiy [7], and Vulis
and Loytsyanskiy [4] has played an important role in the develop-
ment of the theory of flow in jets and wakes. Prediction of detached
turbulent flows and wakes has been discussed by Gogish and
Stepanov [8], and Shvets and Shvets [9] review the experimental
Khlebnikov [20, 21, 23] has reported experimental data on the
distributions of pressure and heat flux (i.e., heat transfer coef-
ficients) over a sphere-and-disk system in flow at Mach numbers
N * 3 and 5 when the distances between the bodies were 1.6 s 1/d (
s 8, and ratios of their midsection diameters were 1.6 s /d s 3.2,
lLet us discuss these distributions for specific cases.
Distributions of pressure and heat flux on spheres of various
seeEssEE.ia-lESESES2E.E2EEESEEEEEEEESS.E.SEEESESSSSE5s- Tme
experimental data can be represented in the form of universally
applicable functions, shown in Figs. 2.1 go 2.3. The heat flux
distribution 4 8,g,N 7)is independent of and ML (7ig. 2.2), and the
pressure distribution P / V. M) is independent of l, A, and
M (Fig. 2.1). Here q is the specific heat flux measured in any
given point on the sphere. ..,, is the maximum q in a given run,
F is the pressure on the sphere. gy = the maximum P in a givet
run, and 8 is the central angle measured from the axis of symmetry
of the rear body. Similar results were also obtained under the
same flow conditions in the case of a sphere (or truncated cone)-
disk pair (Fig. 2.4), where Y is the ratio of the distance from
the center of the disk r to the disk radius F. When the data are
plotted in different coordinates, the pressure and heat-transfer
distributions cease to be generally applicable (see Fig. 2.5, where
%4, the specific heat flux at the stagnation point of a disk in
an unperturbed flow).
Figure 2.1a shows the experimental values of Fl,,,gg 8 a
spiked sphere in a supersonic flowfield, where the initial detach-
ment of the boundary layer occurs at the spike (data of Kharitonov).
These data, for various spike lengths and Mach numbers N,. are
was calculated from the following equations:
where the parameters with subscript = refer to the unperturbed
flow and those with subscript e refer to the outer boundary of the
mixing zone. g 4n4 %,. 4re the stagnation pressures in the flow
and behind the shock front, respectively. The computed functions
}g%.) for Mach numbers N, * 3 and 5 are shown in Fig. 2.7.
An engineering method for calculating the pressure and heat
flux at the surface of the rear body at l .. r4s suggested ir
it with respect to the variable 4 we find that '' 0. The
missing boundary conditions for this equation are obtained from
the first equation in (10.7), written for the surface of the blunt
body n = 0 and the surface of the shock wave n = 1. Integrating
P''! O, we obtain the stream function:
In the blown-in layer,
In the shock layer,
Setting f = 0 in (10.10) and (10.11), we obtain the thickness
'', of the blown-in layer and the thickness 7+ 9f the shock layer.
respectively:
Using 'L, ' 7; * 1. we can then compute the distance from (10.2):
We then obtain the following formulas for the distance + , f
the contact surface and the shock-wave distance expressed in
physical variables:
All that remains is to determine the pressure gradient at the
contact surface and obtain E P4%'L,)- We use the equation p. *
= 2G(P'y7 and rewrite it (following [60]) in the form
where we have used the stream function f as the independent variable.
We can then obtain the following relationship between f' and f by
substituting the solution obtained in (10.11) (which is accurate
(Fig. 6) defines the operation of the tunnel at different flow ex-
pansion ratios in the diffuser, and lines and define the limits
for the total hydraulic energy and the minimum pressure, respec-
tively. Experiments with variation of the expansion ratio of the
discharge diffuser in the open-loop water tunnel as a function of
the flow velocity have shown that the optimum operation is attained
at %g 4 1.5. Further increase in 4 bings about only an insignifi-
cant increase in the useful area of the operating diagram with V ,-
It was thus demonstrated that discharge into an adjustable-
vacuum chamber and the use of an adjustable expansion ratio dis-
charge diffuser with a constant optimum cone angle in a large
gravity-driven open-loop water tunnel with a 0.7 m x 0.4 m test
section allows attaining the following flow parameters: , 40
m/ sec, Eu > 0.01 and Re = 10. In this case we achieve inlependent
pressure control at low Eu, which significantly broadened the
operating capabilities of the tunnel. In fact, they approach those
of one with a closed loop. Gravity-driven open-loop water tunnels
deserve serious consideration, as they do not require high-power
driving pumps, they are relatively cheap to build, and operate,
produce low values of Eu at high Re and Fr, and operate with natural
water with natural cavitation nuclei. The rather broad range of
operating modes allow performing high quality experiments at re-
duced side effects.
considered in thermal convection problems, only in that the initial flow is already turbulent.
Hence the instability now should be determined by the turbulent Rayleigh number Ra, where
the eddy viscosity v+ is substituted instead of v. Accordingly the corresponding Re - 1. What
seems clear, however, is that the ensuing instability of turbulent flow is likely to destroy the
phase coherence necessary for normal spectral transfer of helicity and energy. In other words,
helicity pumped by means of the mechanism considered above would accumulate anomalously
at certain intermediate scales (of order of the scale of instability), and concurrently the energy
transfer to small scales should be reduced. Thus in accordance with section 4 we may expect
the large scale instability to occur.
Mathematically the problems is posed as follows. Coupled with the equation of motion
(5.1) in the Boussinesq approximation
is the heat transport equation. The temperature field is then excluded from (6.1) by means of
the heat transport equation. In the resulting equation, quite complicated to be sure, one
separates the small scale turbulent component from the weak large scale perturbation field by
the substitution vv = vv' + (v).
Since we consider the linear stage of instability, the turbulent component v' can be seen as
given. Moreover, it can be chosen as a simple Gaussian field. The latter would not affect the
whole analysis for the large scale component v), as long as the terms of the type a Ho are
generated. The impact of vastly simplifying statistics with the eddy viscosity v4 is of secondary
importance as we suggested previously. In the above approximations the Reynolds stress in
the equation for (v) can be calculated by closure, yielding (Tur et al., 1984, 1991; Moiseev et
al., 1988)
where e is the unit vector (0, 0, 1), Pr is the turbulent Prandtl number r,/. A4 is the
turbulent heat conduction coefficient, Ra = gAh''/v+YY+ is the Rayleigh number, A = y --
7e4i,. The parameters T, and T, are very complicated and generally depend on the choice of
model for the v,. This dependence is, however, reasonably weak and can be neglected at first.
Also, P,,, ,., - VW,/P& is the proiector operator.
Most essential for our purposes is understanding the rhs of (6.1). The parameter S is as
follows
The expression in square brackets is the dimensionless ratio of the total accumulated helicity
and the measure of spectral transfer. If S = 0, (6.2) becomes the usual equation for convective
instability with v and 1 substituted by their renormalized turbulent value v+ and Y4. On the
other hand, if S is sufficiently large, the character of instability following from (3.1) is
radically different from the usual convective one with characteristic scale of instability tending
to infinity for S > N,j,, In other words, the presence of helicity modifies completely the nature
of convective instability. On the other hand, and this is very important, by itself helicity does
not cause instability. This is reflected in the fact that the rhs of (6.1) itself is proportional to
Ra. In this sense Ra plays the role of the a parameter from section 4. It is fundamental
convection for y A Y4;; L4 > 0), i.e, the case of unstable stratification, that drives instability
ostensibly by destroying the normal transfer of helicity and energy to the small dissipative
scales.
To see more clearly a relation between the large scale instability as described by eq. (6.1)
and the constraint on the energy cascade imposed by anomalously slow helicity dissipation we
The vorticity field given by (1) is not divergence-free, however, a divergence-free vorticity
field can be reconstructed as the curl of the velocity field (3) (Novikov, 1983). When the y
vectors are well aligned with the reconstructed vorticity field, a vorton system is a good,
discrete approximation of a divergence-free vortex structure, and subsequent evolution closely
agrees with a divergence-free vortex evolution. This fact has been stressed by Greengard and
Thomann (1988). Aksman and Novikov (1988), and Pedrizzetti (1991). A parallel study of
vortex-body interaction with classical cut-off method and vortex singularity method has
clearly revealed the different characteristics of the two models (Pedrizzetti, 1991a).
Discretization errors and numerical approximations can drive the y vectors away from a
divergence-free configuration. Actually, even after a short time calculation, they usually
depart from it slowly, because there is no effect helping them to readjust in a divergence-free
configuration. This is a serious obstacle to long computations, and to simulations of complex
interactions (Pedrizzetti and Becchi, 1990).
For this reason we introduce a filtering procedure which gives y vectors a tendency to
realign with f x u vector field. Let us define the aligned intensity
We introduce an equation for evolution of the vorton intensities,
to be added to eq. (4b). The parameter is the characteristic frequency of the linear filter
operation (5). The filter time scale, T, = 1/, must not be longer than the timescale of the
physical phenomena otherwise it has no effect; on the other hand, it must not be shorter to
avoid artificial forcing, because a discrete vortex representation can pass through a not
perfectly aligned configuration during the evolution. Nevertheless, the results are not very
sensitive to the value of , once the order of magnitude is right. Typically for a curved vortex
filament m - T/R, R being the radius of curvature.
The effect of this procedure on the evolution of vortex particle structures is very encourag-
ing, giving the system a natural self-consistency which permits the computation of complex
interactions and the study of long-time evolutions, as will be seen from the results of section
We want to point out that this operation requires no additional calculation in a numerical
program. At each time step the velocity partial derivatives, u,/ x, with i, j = 1, 2, 3, are
computed for the evaluation of Vu, at each vorton location, from these we can compute Y x u
and . Then we update the intensities by Y4, (1 -- a)y + af, with a = m 2t, and 2t the
computational time step.
This divergence filtering procedure has no clear physical meaning yet (a study is in progress
with an analogy with the self-consistent field method in a system with weak interaction;
Landau and Lifshitz, 1967; Novikov, 1991), but its significance, from an operative point of
view, is clear as a methodology for making the system (4a, b) consistent with its physical basis.
In computational vortex models, when core size is evolved, the energy is not an invariant of
motion e.g. Agishtein and Migdal, 1986; Pumir and Siggia, 1987). This is obvious because
swirl velocity and axial flow are not taken into account in energy computation, and during
This study was partly supported by the Grant-in-Aid for Scientific Studies from the
Ministry of Education, Science and Culture (Category B, No. 03452120). The authors express
their sincere thanks to Mrs. Y. Tsutsubuchi for her help in the preparation of the manuscript.
Double diffusive layers may be formed in a fluid stratified by a stable solute gradient when
it is heated uniformly from below. A first mixed layer bounded by a sharp diffusive interface is
initially developed from the heated bottom up to a certain height. Owing to upward molecular
diffusion of heat, a thermal boundary layer is developed ahead of the advancing interface into
the undisturbed ambient stable solute gradient. Thus within this boundary layer a double
diffusive system consisting of a destabilizing temperature distribution and a stabilizing solute
gradient is developed; the temperature and lengthscale of this boundary layer increase with
time, until it breaks down and a second mixed layer is formed on top of the first mixed layer.
By a similar mechanism, subsequent secondary layers may be formed successively above the
second one producing a double diffusive layered system.
Turner (1968) studied this phenomenon by applying a constant heat flux at the bottom of
the stratified fluid. He observed that the thickness of the first mixed layer increases as 1?&
where t is the time elapsed from the initiation of heating, until this layer stops growing due to
the formation of a second mixed layer above it.
Turner (1968) developed a criterion for the onset of the second layer based on a Rayleigh
number defined in terms of the maximum height of the bottom layer before a second layer
starts forming above it, the heat flux supplied to the bottom, and the undisturbed salinity
gradient. The critical Rayleigh number was determined by Turner from visual observations of
In the past there have been analytical studies devoted to
symmetrical fault condition (1,2,3}. But in practice the
probability of occurrence of an asymmetrical faults is much
greater. As far as the author is aware in the past only one
attempt was made to study the asymmetrical faults [4]. There-
fore it is worthwhile to pay further attention to the asym-
metrical faults, because, the large induction motors form a
significant part of the system load. Development in protec-
tion system and increased installation of the fast switchgear
mmake it essential that electrical engineers to be able to
predict the maximum contribution from large induction motors
to the fault current. Also predict the resulting maximum
torque for design purposes. In practice usually engineers are
not keen to involve in time consuming digital simulations.
Therefore it will be valuable if simple, yet accurate,
analytical equations can be provided. This is the main objec-
tive of this investigation.
where , is the voltage across the jth diode.
Fig. T, shows a simplified flow chart for the new technique presented in
this paper.
For the simulation results, Euler method of numerical integration was
used. The time increment for numerical integration Dt was chosen to be
0.01 p.u. for the computer program executions. The time increment was
fixed for the majority of steps, but it is varied at the switching events
to give more accurate computation. For comparison purpose, results
obtained from real system are compared with those obtained from digital
simulation. Three tests were performed. The parameters in per unit are
given in Appendix A.
Test 1 :
This includes switching power supply on with duty cycle equal to 0.b, and
a load torque applied to the rotor shaft T, 0.383&L P-v-
Test 2 :
This includes a sudden change in duty cycle from 0.0 to 1.0 after steady-
state has been reached (i.e. the chopper state was in the 0FF mode all
the time, then suddenly, the state of chopper is changed to the ON mode
all the time), the load torque applied to the rotor shaft T,0.290,. P-w-
Test 3 :
This shows the steady-state performance for the machine with duty cycle
of the chopper equat to 0.0, and a load torque applied to the rotor shaft
%; - 0.2203 p.w.
Fig. S shows the system performance from the real system for test 1,
while Fig. 9 shows the predicted performance for the same test. Figs. 10
and l1 are the corresponding real and predicted performance respectively
for test 2. Figs. 12 and 13 are for test 3. Fig. 14 shows the relations
between the speed of the motor and load torque for different duty cycles
at steady-state. Fig. 15 shows the relations between the speed of motor
and chopper duty eycle for constant load torque at steady-state.
the characteristic polynomial is
Then
but during sliding regimes it is:
where
By comparing the matrices, from:
and
we obtained:
We choose parameters k+3, t and c depending on the current at peak torque
and on the sliding domain. Increasing the parameter t or k+3 we can improve
dynamic behaviour obtaining faster transient responses, but we observed two
important constraints:
The choice of c: may be, for instance, o, st to reduce the overshoot.
We have another free parameter, the proportional term k-, which has
critical influence only if the set-point is quickly changing. We observed that
this value must be very small if critical conditions are to be avoided.
Ia the secgnW4 4PP94W%. s i cslculated by using the Lagrange multipliers A4; obtained from
the preceding OPF solution. For each time interval, s y is calculated as fhe average over
the available thermal generating units:
The final average value is calculated for each iteration as
This second approach appears to be more logical as it employs information available for
the OPF study alone.
A computer program was implemented on a DEC VAX-11/780 system. The algorithm is
based on the well-known Newton-Raphson method adopting a sparse matrix solution. ' A
special procedure has been developed to produce the initial guess values for the unknown
variables. For details consult [7]. 'The optimal solutions were obtained for 5-bus, 14bus
and 30-bus systems, for different o values (ranging from 0.D to 1.0 in increments of 0.2).
The 5-and 14 bus systems include two generator buses each, with bus 1 assigned a thermal
plant, and bus 2 assigned a hydro plant. The 30-bus system includes three generator buses,
with buses 1 and 13 assigned identical thermal plants, and bus 2 assigned a hydro plant.
The fuel cost and water discharge model coefficients for all systems are the same as in [7].
The transmission loss B-coefficients for the 5-Bus system are:
The transmission loss B-coefficients for the 14-Bus system are:
The transmission loss B-coefficients for the 30-Bus system are:
The load curve in the 5-bus system is divided into three discrete intervals with load of 70
% of peak for 8 hours, peak for 8 hours and 90 % of peak for 8 hours. Two discrete intervals
8e used in testing the 14 and 30 bus systems with peak load for four hours and half load
for 20 hours.
We analyze results to address two issues. The first involes a comparison of results using
t9 r+o'rorosed apnroaehes tor e+äiualaa dme eo ot iosses, anß he scond deals wk
the effecf of varying the relative weights assigned to each cost component
THe seat cann be adjusted up/ down d epend in g upon oper ator' s height and
for ease of operation.
It is possible for slight laterel adjustment of the seat by unscrew-
ing the kn ob heads, pr ovided at the bottom moet, below the seat. Thi s
adjustment is not normally called f or.
The main aseembly of the open-loop contr oller is a 'Voltage-level
Indicator'. This is housed in a weat her protected Contr ol Bax fitted
to the handle of th e ped alling eqwipment.
The volt age- level-indicator is designed for eaey indication of voltage
r ang es or voltage ban ds and regulate the ped allinn g accordingky. bince
ths gener atar is of persn ent Mgnet rotor construction, any c lobed
feed-hac k in the voltage circuit ie not possible.
H529EESAG- Th e voltage- level indic ator is design ed function ally to
4ndicate thr ee volt age levels or voltage ban ds throu gh c oloured uight
faittin g Diodes (L ED) lamps. They ar e as followws 4
In each of the above leve l indication s, there are again addition al
ub-leveis. The vo ltage levels can be adjusted by two potentiometers
iwne tor medium voltage and a oth er ror anove norwl voitage) broviaed
for this purpoe.
$tarting from the previous considerations, it is possible to observe the following:
Aith sinusoidal supply the no-load electrical power absorbed by the motor can be written
with square wave voltage supply in the same load condition the no-load electrical power
Wbsorbed by the motor is given by the following relationship:
The same analytical procedures were followed to generate the
rotor current waveform which was then analyzed for its harmonic
content both theoretically and experimentally for diffent firing
angles.
The waveform of the rotor current numerically generated for a
firing angle of 20% is plotted in Figure 9. The oscillogram of the
rotor current at the same firing angle is shown in figure 10. The
figures are basically the same.
Figure i1 displays the harmonic content in the rotor current in
percent of the fundamental harmonic for different firing angles
where
Now, for he surface magnet rotor, L. L, , and L,L, 0.25 o 0.35. For a well-designed
ALA rotor, we have L./-, 0.97 (L. ,). 3 to 4, Thus, L, of a PM machine is related to L. of
an ALA-rotor machine by:
verter, motor and drive efficiencies are 86.2%, 79.9% and 68.9%, respectively. The principal
reason behind the efficiency improvement afforded by freewheeling is the reduction in inverter
losses through the reduction in the number of chops. Reducing the number of chops allows
the optimiation algorithm to expand the conduction interval (%,;y - ,A,) while reducing the
rms motor current. This helps to improve motor efficiency as well. While this evidence is
important, it must be considered in the context of the intended duty of the drive.
This discussion on the importance of freewheeling indicates that not only is it very im-
portant that the trajectory through the flux linkage-current state space enclose the required
area to provide the needed time-average torque, but that how that area is swept out is very
important. Figure 13 shows the superposition of the flux linkage-current trajectories for the
monofilar and bifilar drives for point 1 of Table 2. The monofilar drive operates with lower
currents, and therefore does not push the motor as heavily into saturation. The idea of shap-
ing the energy conversion cycle through the use of both magnetic saturation and inverter
freewheeling needs additional study.
At the time of development, the bifilar drive was considered optimal because it minimized
drive cost. This still appears to be the case when considering only the cost of the drive.
A persuasive argument can be made, however, that the cost of energy must be included in
the equation. In electric vehicle propulsion the cost of energy is very high, implying that if
the monofilar drive is only 5% more efficient than the bifilar drive on average, then perhaps
doubling the cost of the inverter is warranted.
This paper has addressed issues related to the excitation of VEM drives. The excitation
strategy governs how the voltage (or current) is applied to the motor through the switching
inverter as commanded by the controller. The effects of a given excitation are generally
not unique in a VRM drive. That is, several sets of excitation parameters may produce
way technically, economically and, above all, socially. The dependence on agriculture of
the national labour forces of the 31 selected countries is again clear from Table 2.
Indirect energy in the forms of energy-intensive fertilisers and pesticides is also vital
for good crop yields. Indeed, the new high-yielding cereal varieties now on the market will
only be high-yielding given access to sufficient fertiliser treatment. Table 2 gives the
artificial fertiliser consumption figures for the 31 countries in 1980 on a per hectare of
arable land basis including temporarily fallow land but with double-cropped areas counted
only once. It should be noted that China, the nation with by far the highest value, supplies
only 15% of the amount of New Zealand, the world's largest fertiliser consumer per hect-
are; though the Chinese figure surpasses the weighted means for both the industrial market
(Western) and East European non-market economies. Nevertheless, China is a special case
here, as in many other respects, and it is more instructive to observe the Indian figures
ata quarter of the Western mean, while nations such as Uganda, Rwanda, Guinea, Chad,
the Central African Republic, Burundi, Niger, Sierra Leone, and Zaire (again all African
states) have virtually no access to commercial fertilisers at all.'
While it is true that grain production increased from 1958 to 1980 by 70%( to 775 Mt
in the developing countries due to a 20%( increase in harvested area and a 40%( increase
in yield giving almost 2t/ha, this progress was not evenly distributed. Latin America, the
Indian subcontinent and China did better than 70%( but Africa as a whole did rather
poorly.'? This bears out the findings of Table 1, yet Table 3 shows that it is the African
the past, the main concern of energy policy was to ensure that lack of energy did not
acome an obstacle to economic growth. Since the overall development strategy was to
Bild a strong industrial base, most efforts were directed toward providing a stable supply
if cheap energy to the critical sectors of the econommy. Thus, energy plaved a sunportive
iHe in a nation's industrialization. In this situation, energy policy was limited simply to
Bijusting supply to demand and did not require sophisticated analysis.
This type of energy policy was made possible by the continuous availability of relatively
xpensive imported oil supplies, so that energy constraints on economic growth were
jnimal until the increase of world oil prices in 1973. The lack of proner characterization
pergy as a factor in production is not surprising considering that, prior to the autumn
1973, the scarcity value of energy products received scant attention. Energy was a topic
pphysicists, not of planners and decision makers. However, the fivefold increase in the
l price of crude oil since 1973 has elevated energy expense to a significant portion of
oduction costs in many industries, and hence it has raised disturbing questions about
prospects of industrialization. The direct and indirect energy cost of producing many
3puts has recently risen to the point where this cost presumably will become a principal
erminant of the location of world productive capacity.
'Today, the future adequacy of oil for world production and trade is in doubt. There
s. been no major expansion of exploration activities for new energy sources nor major
Evestment in alternatives such as coal, synthetic fuels, or nuclear power. As a result, the
Railability of new energy sources in sufficient quantities to take the place of oil is not
pected to become a reality before the 21st century. There is a growing expectation,
herefore, that the world may be entering an energy trough, a period characterized by
Wncertain supplies of energy.
The price and availability of energy having become significant constraints on world
9roduction, consumption, and trade since 1973 necessitates a new approach to economic
evelopment, one that takes more explicit account of the role of energy. Because energy
%5ervades all aspects of economic activity and its demand is derived from the structure and
Srowth of the whole economy, the policy goal of optimal mix and conservation of energy
[94n be realized only if supported by adequate management of demand in the consuming
%Sectors (industry, transportation, and residential and commercial). Therefore, it is essential
that the energy implications of alternative development policies in all these sectors be
many potential processes for large-scale production of heavy water have been investigated.
Of these, very few have been pursued beyond the laboratory stage, and still fewer have
been used for production on the order of tonnes per year. As of 1975, cumulative
production of heavy water in WOCA was between 8000 and 9000 tonnes, with just over
90% having been made by hydrogen sulfide-water exchange (the GS process). The
remainder has been produced by hydrogen-water vapor exchange plus electrolysis (6',),
hydrogen distillation (2%(), ammonia-hydrogen exchange (1%(), and water distillation
(0.3%). The production since 1975 has been even more heavily weighted towards the GS
process. As these figures indicate, this process has a position in the field of heavy-water
production similar to that of the gaseous diffusion process for uranium enrichment. The
stuation can be expected to change somewhat with the operation in the next several years
of three plants in India and one in Argentina using various realizations of the
4mmonia-hydrogen exchange process (see Table 4). In particular, the water-fed hydrogen-
Ammonia process being implemented in Argentina captures the major advantage of the GS
Rocess, i.e., a large-scale production capability, albeit at a higher cost. Two other chemical
%change processes, hydrogen-water and hydrogen-amine, also show promise for com-
Aercial production, but a discussion here would carry us too far afield, and we refer the
Rterested reader to an authoritative review article by Rae.'' Laser methods have also been
RWestigated for heavy-water production,' However, there is much less leverage in applying
ese methods in deuterium separation as compared with uranium enrichment. This is
%wse present wparative costs ot si40ISWU ana s214/kg D,0 are equivalent wo about
Wem mole ir u a omnare4 wm abou si+i aoie at d. Hoev, wser
hniques may be economically competitive for stripping tritium from heavy water.' In
fiowing weuon, we bnnenny ascuss oe os ana nyaogen-ammonia eeange rrocess
The GS process. In chemical exchange processes for heavy-water production, deuterium
E+-+Gä44%4SG444ü.
8=Ne+ w+assS<.e+6mäas4mäi' c+4+++4
=== +++eie4ii.s+S++c+s4+W
ange tower (see Fig. 6a). The recovery of deuterium from the feed in this type of
wmmw rocsssaromaes I1 - ./), wnerea,,a,ae mesennarauon ness or
The purpose of INRET as stated in the agreement is: ''In order to promote an
internationally beneficial and mature future for peaceful nulcear energy development,
nuclear energy technology transfer between allied countries must be encouraged to the
extent of common interests, within the constraints of safety and international safeguards,
and that a system of collaboration between the UNM and the U'T on an academic basis
shall be established to promote international nuclear research, education and training
programs.. .''.
The basic framework of INRET is that of an academic consortium among university-
based organizations in the Pacific Basin countries. The INRET committee will for now
consist of members from UNM, UT, MIT, the East-West Center, and the universities in
the Asia-Pacific region. The main office of INRET is at present located at UT.
INRET activities will include: (1) examination of regional needs, roles, and impacts
of nuclear energy (elucidation of possible future developments with necessary
conditions, including nuclear nonproliferation as a social imperative and development
of preparedness for nuclear abnormal occurrences in the region, for example);
(2) basic research, development, and training, including use of a research reactor and
development of a small-powered reactor; and (3) basic technology developments in the
fields of recovery operations for accidental conditions such as TMI-2, safety of power
reactor systems, environmental radiation safety, safeguardability of power and research
facilities, efficient use of LWR fuels, extension measures of reactor life, development
of small reactor systems, low-level radiation effects, waste management and disposal, and
the like.
This proposal calls for long-term and broader global perspectives into the next century
in the Asia-Pacific region, complying with IAEA jurisdiction.'
International training program at research reactor facilities. The role of research
reactor facilities is significant in the education and training of nuclear power spec-
ialists, as well as of those in basic and applied research: even though a research reactor is
very restricted in its functions and different from a power reactor, so much is shared
in common that detailed analysis of research reactor operations can provide
good examples of safety-related research and meet the requirements of operational
personnel. International sharing of a research reactor facility through bilateral and
multilateral agreement is already being seen.t Another related function of a university
research reactor facility like the University of Tokyo Nuclear Engineering Research
Laboratory (UTNL) would be to introduce general scholars and researchers to the nuclear
fuel cycle activities of nuclear industries and power utilities. Some foreign researchers
have already stayed at UTNL as a forward base and visited nearby nuclear facilities
[JAERI, Power Reactor and Nuclear Fuel Development Corporation (PNC), and Japan
Atomic Power Company Ltd. (JAPC)].
Need to share research resources. Most basic nuclear research can be pursued satis-
factorily in the advanced countries within the framework of the existing national and
nternational nuclear laboratories and societies; however, countries that started nuclear
sience and engineering belatedly need first to send scientists and engineers to the advanced
countries such as the United States and Japan in order to catch up.
Nuclear engineering developments and safety research in a developing country will cost
much more than such a country can afford and inevitably will be pursued through bilateral
or multilateral agreements, or both. It is natural that the ideas of international cooperation
engineering, with classical engineering fields such as mechanical engineering and instru-
mentation following closely. Finally, Table 6 gives more detailed results for the disciplines
included within the engineering and technology group.
The study described herein is an approach towards a systematic structural analysis of
energy R and D activities and assessing likely impacts of disciplines of science and
technology. It also serves as a preliminary step for more comprehensive studies by
providing an example for the use of formal methods in formulating science and technology
policy.
Delineation of objectives, which are energy R and D activities in our study, is an
important phase of the general method. After identification of major R and D fields, a
comprehensive but compact breakdown of activities within each field, based on the nature
of the R and D activities, was proposed. In assigning weights to objectives, which is
different from the UNESCO approach, a priority structure was not utilized. This
procedure derives from the need for a diversified energy R and D policy, since it is apparent
that the transition in Turkey is towards the use of a varied energy supply mix instead of
a single plentiful energy source. The appropriateness of a strict priority structure is further
questionable since the relative importance of R and D activities within a field is liable to
change during the time span under consideration, even if the development goals concerning
this field of energy are likely to remain the same.
The weighting scheme is based on experts' ratings of the relative importance of various
fields and activities. Since the results are influenced by the actual weights utilized, the
sensitivity of the results to the weights must be investigated. This phase of the study is in
progress and further refinements on the methodology, such as incorporation of the time
element into the weighting scheme, are being considered.
Acknowledgements-The authors gratefully acknowledge the support provided by the NATO-TU-ENERGY
project and the assistance of N. Said Unlü in developing the needed computer programs.
The methods of caustics and photoelasticity have been
widely used in the last two decades to determine the
dynamic stress-intensity factor. The general procedure in
obtaining the dynamic stress-intensity factor is to assume
that the near-tip area in which the photoelastic fringes
are formed or the region from which the caustic is formed
is dominated by the asymptotic field and the stress-
intensity factor is then extracted as a fitting parameter.
The two methods are based on different physical phen-
omena, namely, wave interference in the case of photo-
elasticity and light refraction in the case of caustics, and
experimental conditions could be such that the results
obtained with one method would not match the results
obtained with the other. Just such a comparison of the
two methods was performed by Nigam and Shukla.'
Their main conclusions were that while both techniques
yielded comparable results for static loading conditions
they lead to dissimilar values of the instantaneous stress-
intensity factor when applied to running crack problems.
The reliable measurement of the dynamic stress-intensity
factor is extremely important in dynamic fracture charac-
terization since this factor is taken to be the driving force
in dynamic fracture mechanics. In this paper, we in-
vestigate this issue further and define the conditions under
which the methods of photoelasticity and caustics can be
used accurately. In the next section we briefly describe
the nature of the dynamic crack-tip stress field. Then, a
brief description of the experimental scheme is provided.
The following section deals with the determination of the
dynamic stress-intensity factors from photoelasticity and
caustics respectively. Finally a comparison of the results
of the two techniques is provided and the impact on the
dynamic stress-intensity-factor measurement is discussed.
The Cartesian components of the asymptotic stress
field near the crack tip under symmetric loading is given
where K,(t,v) is the dynamic stress-intensity factor and
Ja(8,v) is the known angular distribution of the stresses.
It is common in dynamic fracture literature to consider
the above stress field to be sufficient to characterize the
crack-tip state, particularly since the energy flux into the
crack-tip region (r - 0) can be characterized only in
terms of the dynamic stress-intensity factor: K,(t,v).
Thus, experimental and numerical simulations of dynamic
fracture have been directed at determining the instantaneous
dynamic stress-intensity factor and its relationship to the
instantaneous crack-tip speed. However, the question of a
unique description of dynamic crack growth in terms of
an instantaneous relationship between K,(t,v) and the
velocity has been hotly debated in recent years. The main
causes for nonuniqueness were attributed to difficulties in
experimental measurements of either K,(t,v) or v or both.
In particular, it was noted that the effect of higher order
terms and three-dimensional effects in the region of
experimental measurements made estimates of the dynamic
established before crack initiation, and that within the
front another fringe loop pattern is now reestablished, as
shown in Fig. 4. The region enclosed by this second shear
wavefront is the only area of the whole specimen where
information on the crack initiation, crack speed and
moving boundary conditions are felt. As a consequence,
using the least-square's regression analysis. The displace-
ment gradients at the point of interest are obtained as
follows; the partial derivative of the cubic equation is
obtained with respect to ?., Y, and Z,, and then, the
values of X,, Y, and Z, at the point of interest are sub-
stituted for ?,, Y, and Z, in the equation resulting from
partial derivatives. The displacement gradients estimated
in this way are substituted into the lLagrangian equation
expressed by the following equation.
Finally, we obtain the three-dimensional strains at the
grid point of interest by substituting the strain components
calculated through eq (1) for the corresponding terms in
the following equations,'- =
where we assumed that the volume was invariable and
that two of the shear strains, y,, 4nd y,,, were zero near
the crack tip on the specimen surface. In addition, we
also calculated the equivalent strain, e.,. from the
following equation.
We can obtain the three-dimensional strains at any
arbitrary grid point by repeating the same procedures for
the other 25 grid points surrounding the grid point of
Interest.
The displacements u and v were measured by removing
the replica films from the area around the creep-crack tip
and by photographing them with the appropriate
magnifications.
Figures S(a) and (b) show the measurements of the
localized deformation around the precrack in materials A
and B creep tested at T' = 650 C and a... = 180 MPa,
and at T 600 C and o.-. = 150 MPa, respectively. The
creep cracks started to grow at the precrack tips in
materials A and B at the stage presented in both figures.
The distributions of the equivalent strains corresponding
to the localized deformations shown in Figs. S(a) and (b)
are presented in Figs. 6(a) and (b), respectively. According
to these figures, the strain is concentrated around the pre-
crack and the creep-crack tips, and the equivalent strains
reach about 60 x 10:% at these areas. Moreover, the
relatively large strains, about e., = 50 x 10*%, occur in
the areas fairly distant from the crack tip. The size of
such a strain-concentrated domain is larger in the material
B than in the material A.
The creep crack originates after the great blunting
which occurred at the precrack tip. At that time, the
creep deformation, caused mainly by the grain boundary
sliding, also occurs in the area distant from the precrack
tip. The grain-size dependency of the above domain size
corresponds with that of the grain-boundary-sliding
distance. The area surrounded by the location along
which the sliding occurs is larger in the material B than in
the material A.
Figures 7(a), (b) and (c) show the distributions of the
strain e, in the direction of the load axis (Y axis) around
the precrack tip in the material A, where the three stages
in the creep-crack growth process are presented. These
figures show that the intense strain concentration occurs
in the area ahead of the precrack tip inclined at an angle
conservative. This topic will be further discussed in
future publications by the authors.
The panels were visually inspected during compressive
loading. Because the facings were not supported laterally
between two consecutive corrugations, localized buckling
of the facings is possible. Intercorrugation instability was
indeed observed on the concave side of the panel in the
highly stressed vertical edge regions at loads approaching
the panel collapse load. The localized buckles had a nearly
square shape with the half wavelength equal to the half
wavelength of the corrugations. The collapse initiated at a
vertical edge close to one of the loaded, horizontal edges.
By inspection, the collapse zone had the appearance of a
compressive failure of the facing. After initiation of the
collapse zone, it propagated towards the center of the
panel. This observation thus indicates that corrugated-
board plates fail as a result of compressive failure of the
most highly stressed facing, accompanied by compressive
failures of the core and the opposite facing. The site of
failure initiation varied from panel to panel which indicates
that the discrete edge supports shown in Fig. 3 did not
induce severe stress concentrations or friction.
In this study a test fixture for evaluation of the pre-
and postbuckling response of simply supported, nearly
flat rectangular panels subjected to edge compression has
been evaluated by several methods.
The shadow-moire method verified that the panels
possessed symmetric out-of-plane displacement and
buckled in the first buckling mode. Evaluation of the
compressive response of isotropic polycarbonate panels by
a nonlinear, multiparameter regression model enabled
experimental determination of the critical buckling load,
P,,, and a postbuckling parameter, A, both of which are
sensitive to the boundary conditions. The magnitudes of
Pg, and A were in close agreement with their theoretical
values for simply supported plates using the flat-load
introduction configuration, but not for the grooved one.
Consequently, these results suggest that the flat-load
introduction closely approximates simply supported
edge conditions.
Corrugated board panels were loaded in the cross-
direction (Fig. 1) until collapse occurred. Localized
buckling of the facing between the corrugations was
observed on the concave side of the panel at loads
approaching the collapse load. Collapse occurred at loads
larger than those required to initiate localized and elastic
overall buckling, respectively. The panels failed by
compressive failure of the board materials in the highly
stressed vertical edge regions.
The authors are grateful for the financial support
provided by SCA Research, Sundsvall and the many
stimulating discussions with the program monitor, Dr.
Alf de Ruvo. In addition, we would like to thank Mr.
Thomas Nordstrand for useful suggestions with respect
to the regression analysis and preparation of the manu-
script. Professor Ake Samuelsson's guidance throughout
this study is gratefully acknowledged. The suggestions
provided by the reviewers were extremely useful.
Baum et al,'' found empirically that the geometric
mean of the two in-plane Poisson's ratios of paper is
approximately constant.
Furthermore, they found that the in-plane shear modulus
can be approximated by
where E, and E, are the principal Young's moduli.
These relationships are used to estimate the bending-
stiffness elements D,, and D,, for corrugated board from
measured values of principal bending stiffnesses D,, and
D44, and orthotropic plate theory.'' The relations so
obtained are
solution gives lower values since it is valid only for an
infinite plate. The constraints of the model boundaries
cause the loads to be confined over a smaller area
producing higher stresses and hence higher fringe orders.
Previous researchers found difficulty in determining
fringe orders at high stress gradients. The maximum
fringe gradient found at 0.2 mm from the boundary of the
hole was estimated to be 1.9 fringe/mm. This is com-
parable to the fringe gradient measured by Allison'' of
2.5 fringe/mm.
Sanford'% found difficulty in measuring isochromatic
fringe orders of less than 0.3. Consequently additional
analysis was carried out on a 4-mm disk of CT200
(mixed as previously stated) which had been loaded in a
V-groove (Fig. 10). The fringes were obtained using a
circular polariscope set for a dark field. The dark point in
the lower half of the disk is an isotropic point. Figure l1
shows the results of measurements along the line AB
shown in Fig. 10 as measured using the spectral analyzer
and a circular polariscope with the Tardy-compensation
method. The results of the two methods agree closely at
both high fringe gradients (an estimated maximum of
1.4 fringes/mm) and at low birefringence. It is possible
to see the isotropic point at approximately 32 mm from
the top loading point. The low fringe orders from 40 mm
to 4 mm are caused by edge stresses that occurred in the
disk prior to analysis.
A method using a spectral-contents analyzer to carry
out photoelastic analysis on frozen stressed samples has
been described. An advantage of this method over other
systems is that an expert photoelastician is not required as
the fringe order is determined uniquely by the spectral
signature of the point analyzed. Each point on Figs. 9 and
11 obtained using the spectral analyzer was a mean of five
measurements. In the case of the semi-infinite plate (Fig.
9) the standard deviations of the means over 1.7 mm
from the hole were all less than s 0.005 fringe. At points
closer than 1.7 mm, the standard deviation attained a
maximum value of s0.009 fringe. The higher standard
deviations occurred at fringe gradients greater than
about 1 fringe/mm.
A heat transporter is a device which uses a mediating fluid to receive heat from a heat source at
one location, to transport the heat to a second location, and to deliver the heat to a heat sink at
the second location. The mediating fluid is contained in a service pipe, referring herein to one
having both heat exchanger and heat transport functions.
A liquid thermosiphon is a closed cycle heat transporter in which a mediating liquid is warmed
by receipt of heat from the heat source at a lower elevation, flows as a less dense liquid to a higher
elevation at which it is cooled by delivery of heat to the heat sink, and finally flows back to the
heat source as a more dense liquid to complete the cycle. The circulation of the mediating liquid
is driven by the differences in liquid densities and elevations.
Chen [l] has made a theoretical study of this well-known configuration in which warmer, less
dense mediating water flows upward in one pipe segment, and cooler, more dense water flows
downward in another, the segments being connected by a heat sink at the upper, cooled end, and
by a heat source at the lower, heated end. The temperatures shown are idealized but similar to those
obtained at optimum conditions on the novel device described herein. The LLT has been used
practically by Cronin [2] to preserve the frozen condition of the permafrost under a building in
Barrow, Alaska. The Balch tube (Fig. 1b) is also an LLT. However, it is arranged to have small
tubes within one large tube so that the Balch tube can be used as a structural compression member
as well as a means of stabilizing permafrost soils in the frozen condition [3, 4]. The soil is the heat
source and the highly subzero winter air temperature is the heat sink.
A quasi LLT is also used in hundreds of thousands of natural circulation solar water heaters
in Israel, Australia, and Japan. In these units heating occurs in the solar collector at the bottom
Advanced nuclear energy complexes have been proposed previously [9, 10] based on large
High-Temperature Gas-Cooled Reactor (HTGR) plant concepts. With focus now on smaller,
simpler, and safer plants the nuclear refinery concept would have as its genesis the steam cycle
MHR plant (Fig. 5) which is close to commercial realization [l 1]. The potential of MHR operation
at much higher levels of temperature (than the 700*C associated with the steam cycle variant) is
well known [12], and this is essentially viewed as a follow-on class of plants taking advantage of
a commercial data base from the aforementioned steam cycle plant. The reactor module in the
refinery complex, based on an annular reactor core embodying prismatic fuel elements, would be
very similar to the steam cycle plant, but engineered for temperatures perhaps as high as 1000C
(1832*F). As discussed below the nuclear heat source would retain the juxtapositioned steel vessel
configuration.
The focal point of the refinery is the nuclear reactor which has two basic functions to meet the
product(s) demand: (1) provide very high grade heat, and (2) facilitate electrical power generation.
The energy complex would likely have a multiplicity of reactor modules with some dedicated to
process heat and others for power generation; and perhaps some hybrid units [13] to give added
operational flexibility. Heat source configurations have been discussed for both coal gasification
[14] and power generation using a direct cycle gas turbine [15], and it is convenient to describe them
from the composite (hybrid) configuration shown in Fig. 6. Salient features of the nuclear refinery
concept are given in Table 1.
The reactor core is installed in the center vessel, and is coupled (via coaxial ducts) to the thermal
loop (on the left) and power generation loop (on the right). In the thermal loop the reactor thermal
Taking into account a system approach, the main calculated stages realize some mathematical
models. A system approach demands that one considers VCRS as elements of a system including
the environment in their interconnection and interaction.
The VCRS circuit heat calculation includes mathematical models showing the dependence of
some output thermodynamical parameters of VCRS elements on some input ones. It also includes
A area (m')
8 slope of the adsorption isobar (1/K)
c specific heat capacity (J kgK *')
C parameter in equations (7) and (1 1)
E heat capacity flow rate (W K:')
COP coefficient of performance
h enthalpy (J kgT')
L bed length (m)
p pressure (Pa)
9a isosteric heat of adsorption (J kg '')
g? heat amount (J)
s bed thickness (m)
t time
%as cycle time
temperature
w fluid velocity (m s')
w parameter in equation (12)
Greek letters
a dimensionless wavelength
f, 8* dimensionless temperature
y constant
T adsorbed amount (kg kg''')
6,, 8, temperature changes (K)
e parameter
( parameter
2 parameter
Subscripts
z zeolite bed
c, con condenser
ef effective
f fluid
in inlet
out outlet
cy cycle
eq equilibrium
e evaporator
sh superheat
One of the main problems in the well-known basic adsorption heat pump cycle is its very low
coefficient of performance (COP). Much attention has been paid to the investigation of the so called
regenerative process in which the internal heat recovery is utilized to increase the process efficiency.
The rapid pace of innovation in VLSI has provided an
implementation medium for highly parallel computer architectures,
To fully realite the tremendous computing power of V,SI
technology requires that its characteristics, often subtle and
complex, be understood. First, the design complexity introduced by
the increasing chip sie and density calls for architectures composed
of repetitive, modular structures, Second, interconnections between
devices consume more power and require more area than the devices
(transistors) themselves. Third, the computing environment offered
by VLSI is I/O-bound, not compute-bound, due to the limited
number of I/O pins available in comparison to logic gates. These
considerations have led to the development of highly parallel bit
level processor arrays, which are distinguished by their
communication strategy - digital signals are transmitted bit
sequentially on single wires as opposed to simultaneous transmission
on parallel busses - and their large number of simple bit serial
processing elements [PEg). This leads to efficient communication
both internally and between chips and provides a high degree of
parallelism. The regular, repeatable structures inherent in bit level
arrays have the following advantages over other types of V[,S]
structures:
gates and memory involved in a computation, and bit-level
pipelining result in very large throughputs [3].
The advantages from an algorithmic standpoint, resulting from the
flesibility at the bit level, include:
Bit level processor arrays are characterited by a set of four
features:
Within the context of these features, five classes of bit level arrays
and architectures representative of each class are discussed. These
five classes, shown in Table 1 with representative systems, are:
Except for the class of image processing arrays, the arrays are
grouped according to a dominant architectural characteristic. The
image processing class contains several arrays that have special
hardware features to speed up image operations, so this class to
some extent represents special architectural characteristics. Each
class is now described. The representative features are extracted,
and trends and patterns surmised. Tables 2 and 3 summarite
information about each processor array discussed in the text.
Bit level systolic arrays have been developed to perform
several digital signal processing tasks, including convolution and
correlation, rank-order filtering, and the Discrete Fourier Transform
(DFT) [2], [4]. Several of these chips are now sold as commercial
products. These arrays are specialited to perform one particular
algorithm, and each processing element is optimited for the
particular algorithm being implemented. This fact, along with the
systolic concepts of extensive pipelining and local communication
applied down to the bit level yield extremely fast clock speeds, as
the system clock cycle time is reduced to the cycle time (or a small
multiple of the cycle time) of the bit level processing element.
Bit level systolic arrays show the potential of the bit level
processor array approach when a particular algorithm has the
proper characteristics. The regularity and local data
communication evident in many signal processing algorithms, when
realied right down to the bit level, allows the integration of a very
large number of very simple PEs on a single chip. A bit-slice
correlator chip [2], for example, implemented in a 3 gm CMOS/SOS,
process contained 640 PEs (cells). Special purpose arrays can be
constructed using the features of bit level processor arrays as
guiding principles. The resulting designs, while specialized for a
single algorithm, represent nearly optimal use of the VLSI resource.
Image processing arrays include the NCR GAPP
(Geometric Arithmetie Parallel Processor) chip [5], Goodyear
Aerospace MPP (Massively Parallel Processor) [5], GEC GRD (GEC
Rectangular Image and Data computer) chip [7], and the University
College of London CLP [Cellular Logic Image Processor) [8]. These
bit level arrays are oriented towards image processing as their
primary application, and have special image manipulation features
in hardware such as data reformatting buffers, bit plane I/O, and
processing elements optimied for certain image transformations.
PAPL [Pyramid Architecture for Parallel Image Analysis) [9] is a
processor array using a pyramid architecture to realize
multiresolution pipelined image processing; each PE in PAPL
operates at the bit level, and has connections to 4 PEs (sons) on a
lower plane, 4 neighbor PEs at the same level, and a singe PE
(father) in a higher plane.
A common feature of these arrays is the lack of a large on-
chip local memory for the PEs. The RAM sites vary from 32 bits for
the CLIF array, to 256 bits for the PAPA array. Several reasons
for this pattern are evident. RAM chips are typically fabricated
using special, high volume, manufacturing processes that are fine
tuned to maximite RAM density and speed. These processes are
incompatible with the more general digital structures used in PEE
design. For this reason, RAM densities for standard RAM chips are
much higher than RAM densities on chips including other logic. By
providing RAM off-chip, larger memories per PE are possible; no
RAM addressing pins on the PE chip are necessary; and higher
density RAMs may be used as they become available in the future.
The slower off-chip memory access time means that it is important
to do as many operations using the local PE memory as is possible,
This situation is analogous to the problems associated with cache
memory in single CPU machines.
The characteristics of image processing algorithms for large
processor arrays are associated with this trend toward small local
memories. Subimages are loaded into the array, one pixel per PE,
and processed in sequence. These pixels typically require a small
number of bits, The availability of larger off-chip PE memories
allows an alternate approach, where each PEE memory contains a
portion of the whole image. These larger memories also make it
possible to use floating-point representations and arithmetic, a
necessity in many applications. These larger PE memories may also
reduce the I/O bandwidth required for certain algorithms, as it is
unnecessary to move partial results in and out of memory as
computation proceeds [l1]. The I/O bottleneck in an n xn processor
array, caused by the O(nf) growth in computation bandwidth versus
O(n) growth in I/O bandwidth, will become increasingly important
as n increases with VLSI densities.
Several architectural features have been added to some of
these image processing arrays to speed up particular image
processing procedures. The GRD computer includes a register and
special counters to perform a histogram operation; the CLIP array
PE hardware directly implements a global propagation function; the
GAPP and MP array have special I/O registers for transferring
bit planes and buffering input and output from the array; PAPA'a
pyramid network is specialised for multiresolution pipelined image
processing. Although each of these extra hardware features can be
justified in the sense of speeding up a particular operation, the costs
in terms of extra, more complex hardware must be determined. For
a fixed amount of hardware, a more complex PR reduces the
parallelism in an array in the sense that fewer may be constructed
compared to a simpler PE design. These costs must be weighed
against the increased speed for particular operations, This is similar
to the RISC critique of single processor architectures: infrequently
used instructions should not be implemented in hardware as they
impact negatively on the performance of the frequently executed
instructions.
Variable parallelism arrays have the ability to adapt to
the different degrees of concurrency available for diferent
algorithms and within the same algorithm. Th+ PEs are grouped
into multibit processing units of varying sie: a larger word width
per PE results in less parallelism for a given array, This approach
holds promise for the efficient use of highly parallel processor
arrays, Algorithms which do not contain large amounts of
parallelism may use a reconfigured array with less parallelism, but
more powerful PEg. These arrays typically have some form of local
control besides a masking bit to configure the processor array. Bit
level processor arrays representative of this class include the ICL
DAP (Distributed Array Processor) [12!, NTT'a AAP (Adaptive
Array Processor) chip [13], and the University o f Southampton RPA
(Reconfigurable P rocessor Array) [14]. In the latter two arrays,
part of the microcode word is held locally within each PE, allowing
some degree of independence in both computation and in
communication with otheF PEs.
The RPA has several unique and interesting characteristics.
Th+ PEs use a stack architecture to reduce the number of control
lines which must be routed to each PE, and to simplify the
controller by removing the need for address generation. Three
stacks are included in each PE: the Bitstack is one bit wide and 8
deep and provides general storage; the Activity stack, also one bit
wide and 8 deep, contains the mask bit and provides a convenient
means of implementing a context switch; the Wordstack is used for
serial operations on word operands, and is configurable so that it
may implement a variable word site and variable depth stack. The
RPA contains a mesh interconnection, with a two-bit bus between
neighboring PEs to increase inter-PE bandwidth; a two-bit bus is
also used internally for both operands and results. Of-chip RAM
storage is assumed. The variable parallelism nature of the RPA is
supported by the Reconfiguration register. It provides the local
control that indicates if s PE is a MSB, LSB, or an intermediate bit
in a multibit processing unit, and the direction of data moverment
during left and right shifts. Additional local control takes the form
of a circuit to perform the mantissa alignment and exponent
correction necessary in floating-point operations. This is a difficult
operation in most processor arrays using SMD control due to the
data-dependent nature of foating-point arithmetic. The RPA chip
contains 18 PEs and is implemented in 3um CMOS.
The AAP, built by the NTT Corp. in 1982, is an 8s8 processor
array implemented using a 2um CMOS process. A mesh
interconnection network is implemented, along with a row and
column bus structure with bypasses which allows multiple
transmissions on a single bus structure. As with the RPA, local
control registers in each PE are used to configure multibit
processing units within the array. RAM storage for each PE
amounts to 96 bits; an I/O register plane is also used to overlap
computation with I/O.
A VLSI version of the DAP has been used by the AMT Corp.
to realize a DAP array as a backend processor to a desktop
workstation. Each DAP chip contains 64 PEs, and 16 DAP chips
are used to realie a 32s32 array. DAP memory is off-chip and can
be varied from 32 Kbits to 1 Mbit per PE. The DAP array uses a
mesh interconnection network along with row and column buses. It
can be configured in two modes: vector mode and matrix mode. In
matrix mode, the full parallelism of the array is employed, each PE
performing a separate computation; in vector mode, a row or
column of PEs is considered to be a single multibit processing unit.
Vector mode is aided by a carry ripple connection between adjacent
PEs in rows and columns.
Associative processor arrays include the
Brunel University SCAPE [Single Chip Array Processing Element)
j15],the ASPRO [Airborne Associative Processor) [6], a VlLSI version
of the STARAN associative processor [16], and the UCSB VLS1
associative processor chip [17]. The salient feature of these
processors is a content addressable memory used to perform bit
level operations in parallel. These arrays are particularly adept at
fast and efficient searching. The classic associative processor
architecture included mask and compare registers which were
connected across each word in memory; a portion of each memory
word, specified in the mask register, indicates which bits are being
compared to bits currently in the compare register. The results of
the compare operation are placed in a search results register, which
has one bit for each word in memory. This basic architecture can be
enhanced by several features, including logic for each memory word
which can implement logical and arithmetic operations. The
associative processor arrays are similar to other bit level processor
arrays in that a small amount of processing logic is associated with
a portion of memory. They difer in that the memory is accessed by
content, rather than by address, a feature which requires significant
extra hardware for each memory cell, Also, the PEs are typically
connected to their upper and lower neighbors, forming a linear
array.
Each of the three associative processor arrays contain features
which difer from the classical model, These features either increase
the performance of the array for certain operations, or help to
overcome limitations within the VLSI environment. The UCSB
associative processor chip is a 324135 array realited in a 3gm
NMOS process. It employs an on-chip decoder and segmented
compare and mask registers to reduce the number of I/O pins
required. The search results register may be shifted up or down one
word to realie bit serial communication. Signals are provided to
extend the first responder signal and the search results register
between chips, allowing multiple chips to be chained together.
ASPRO has a relatively large processing element memory, 4
Kbits per PE, located off-chip. Each ASPRO chip contains 32
processing elements and a 32 bit Flip network. This network allows
standard RAM to be treated as multidimensional access memory, in
which data can be accessed in either the word direction or the bit-
slice direction, resulting in a very ffexible memory addressing
scheme. Hybrid technology is employed to reduce memory package
sie, and eight hybrid RAM devices are required per module. There
are four PE/Flip network chips per module, and 16 total modules
for the processor array plus one spare module. This results in an
array with 2048 PEs and 8 Mbits of memory. The SCAPE array
chip contains 256 PEs, each with 37 bits of memory. The chip
emplöys a 2um CMOS process, requires 145,000 transistors, and is
75mmf in area. The SCAPE has the most sophisticated PE of the
three associative processor arrays. lt includes several registers to aid
in a sequence of bit operations on words in memory and to process
two-dimensional data. Additional PE hardware includes registers to
save results of previous compare operations, a bit-serial adder and
carry latch, and an interconnection network between PEg, The
SCAPE chip also contains controller and clock generation logic to
implement instructions broadcast from an external SCAAPE
controller.
Routing network arrays have the same basic architecture
as other bit level processor arrays, but in addition to a nearest
neighbor network these arrays will have, for example, a hypercube
or multistage cube interconnection network. These routing networks
provide high bandwidth communication paths between non-neighbor
processing elements, a requirement for many algorithms. Examples
of this class include the Connection Machine [18] (hypercube) and
the DEC Massively Parallel Architecture [19] (multistage cube).
These two architectures difer in that the Connection Machine uses
a packet-switched, single-stage network with the routing hardware
implemented directly on the PE chip, while the Massively Parcllel
Architecture employs a circuit-switched, multistage network
implemented with separate routing chips.
Th+ 64K PE Connection Machine employs a hypercube
network, with each node in the hypercube consisting of 16 PEe.
TThese 18 PEs are contained within a custom chip, which also
includes interfaces to the router, I/0, and mesh network, and
proportionate pieces of the mesh and hypercube network controllers.
OB-chip memory is provided, with 64 Kbits of bit-addressable
memory per PE5, The router hardware supports message passing
between PEs, so that one PE may send its data to the local memory
of another PE, or a PE may request data from another PE be
stored in its memory. Messages may traverse the network from
source to destination in one machine cycle, in which case the
network appears to be circuit-switched. If a message is blocked at a
particular node due to congestion, it will be stored at that node
until a path is open. Some dynamic load balancing is applied and
the router hardware may determine an alternate path and employ
it in forwarding a message.
Th+ DEC Massively Parallel Arehitecture would be the largest
of the processor arrays mentioned when fully implemented, with
282,144 PEs in a full system. In addition, each PE is 4 bits wide, so
in some sense the array is extended from the bit level processor
array definition. Two chips are use to implement this architecture:
a 22um CMOS, 242,000 transistor, processing element chip, with 32
PEs in a 4x8 mesh configuration; and a 2um CMOS router chip that
implements a circuit-switched multistage interconnection network.
Through the router network, any PE pmay communicate directly
with any other PE in the overall machine. Each PE contains a
four-bit ALU, 1Kb of static memory, and two programmable size
shift registers. SIMD control is employed, and a 4K memory mode is
allowed in which a PE may access the memory of its neighbors to
allow for problems requiring large memories per PEE.
Future architectures could combine the features of variable
parallelism arrays with routing network arrays to provide dynamic
architectures with varying degrees of parallelism and fiexible
communication. This approach holds the promise of efficiently
mapping a broader class of algorithms onto these arrays.
Despite the advantages of bit level processor arrays in terms
of both VLSI implementation and algorithm execution, they can be
difficult to program (in the case of an existing general purpose
architecture) or design (in the case of a special purpose
architecture). This problem is accentuated by the need to
implement high level computations (e-gg-, matrix computations,
convolution] using bitwise operations. For the architectures
described previously, several approaches to this problem have been
used. These include:
in a general way, without using bit level optimisations.
These approaches lack portability among different machines and
sometimes ignore optimizations possible at the bit level, It is often
difficult to prove the optimality of a given mapping using these
methods. In order to solve the problems associated with current
approaches, it is desirable to develop methodologies and tools which
enable the systematic mapping of algorithms onto processor arrays.
In the past, several research efforts have been pursued in this
direction and a good survey can be found in 21]. Many of these
methodologies, which were intended for word level processor arrays,
are applicable to bit level arrays. However, besides some of the
limitations that still characterize those methodologies, systematic
bit level designs present additional problems, RAB [Reconfiguration
maps a class of algorithms programmed in 'C' into bit level arrays,
represents an attempt to understand and solve the open questions
and problems involved in the systematic design of bit level processor
arrays.
In practice, potential users of processor arrays are given an
algorithm and must devise a means for its execution using one of
the following options: [1) use an existing processor array, [2) design
a special purpose processor array, or [3) design an array that uses a
number of existing small processor array modules as the basic
components. Option (1) requires mapping the algorithm into an
existing array taking into consideration size limitations, fixed
interconnection schemes, and predesigned processing elements. In
this option, which is referred to as full mapping, the programming
decisions are subordinated to the characteristics of the array.
Option (2) allows the user to design the hardware taking into
consideration only the characteristics of the algorithm and perhaps
some rather general VLSI design constraints (i,e., planarity, limited
pinout, etc). This option is referred to as full design. It corresponds
to the front end of a silicon compiler, and could provide the
structural and behavioral description of an array to the placement
and layout tools of the silicon compiler. Option (3) is a compromise
between full mapping and full design, where the designer can decide
the overall organization (i,e., shape, sive, interfaces) of the array,
but uses given basic blocks which are themselves fully defined
'small' processor arrays, This option is referred to as partial
mapping/design.
The input to RAB consists of C programs which describe word
level algorithms, These algorithms correspond to nested for loops
with static behavior, RAB first expands the computations in the
input program into bit level operations as shown in Figure 1. This
expansion phase replaces word level computations with a bit level
implementation of the arithmetic operations using a library of
macro expansions. This phase is followed by data dependence and
broadcast analysis using the Dependence Arc Set Analysis technique
[22]. The result of this analysis is a formal description of the
internal structure of the bit level algorithm. This structural
information is used to generate an algorithm transformation which
yields a restructured algorithm suitable for mapping onto a bit level
processor array. The mapping may be a full design of an
algorithmically defined array or full (partial) mapping for a fixed
(variable) site array corresponding to the fourth level of modules in
Figure 1. The transformed algorithm structure is then converted
into an intermediate representation which can be used to generate
code for several different architectnres, The last two modules in
Figure 1, code generation and code optimization, comprise the phase
in which code is generated from the intermediate representation for
a particular target architecture, This code is then optimized using a
standard compaction technique.
Experience and insights provided by the development and
usage of RAB showed some of limitations of the approaches used for
the purposes described above. However, it also revealed
opportunities for new improved techniques that can be applied to
the problems that RAB attempts to solve. This prompted the
authors and their co-workers to start the development of a new
software package which improves on RAB in the following regards:
Bit-level ezpansions: Currently, the optimality and generality
of the mappings provided by RAB is greatly affected by how bit-
level expansions are derived from word-level operations, This is due
to the very large number of expansions that are possible for any
single word-level operation. Because RAB needs to have a copy of
each possible expansion explicitly stored (ie., the expansions are
stored and not generated when needed), it is infeasible to consider
every possible expansion. In addition, assuming that all possible
expansions were availa ble, RAB has no means of knowing or
estimating which expansion yields in the best design or mapping. As
a consequence, RAB needs to to be fully executed for every single
possible expansion in order to guarantee optimality. This is also
impossible in practice.
T'o deal with the problems mentioned above, automatic
programming and program transformation techniques are being
considered which allow for the on-line generation of expansions from
a reduced number of ''basic'' stored expansions, The ''derived''
expansions are the result of applying transformations to the basic
expansions by exploiting properties such as commutativity,
distributivity and associativity of the bit-level operations, ln order
to estima te or identify the best expansion, some dependence analysis
is performed and attached to each expansion. By analyting relevant
characteristics of data dependences, it is possible to arrive at
relative performance of diferent expansions and, furthermore, guide
the generation of additional expansions with improved
characteristics. The knowledge of the dependence structure of the
expansion used is also beneficial in the analysis phase, as discussed
Dependence Analysis: Currently, RAB analyzes the full
expanded program by using the DASA technique which yields
structural and dependence informa tion a bout the program as a
convex set, The inequalities that define this convex set are then
used to derive a matrix where each column is a dependence vector.
This process has two disadvantages. First, the analysis is
unnecessarily complex in the sense that no attempt is made in RAB
to exploit the fact that the ''complex'' expanded program is the
result of a ''simple'' known expansion applied to a ''simple'' program.
Secondly, information about the structure of the program is lost in
the process of generating the matrix of dependencies. In addition, in
order to be able to use a matrix representation for dependencies, it
is often necessary to ''fuse'' loops or consider only those expansions
where loops are ''fused'', The problems mentioned above are
avoided in the new approach in the following ways. The dependence
analysis of the expanded program is obtained by combining the
results of the dependence analysis of the original word-level
program and the dependence analysis of the expansion. The
computational cost of this approach is smaller than that of
analyzing the full program. With regard to the generation of the
dependence matrix, this feature will remain in the new approach
only for the purpose of interaction with the user. However, for
purposes of optimiation in the synthesis phase, convex set
information is used instead. In addition, loop fusion is not needed
explicitly, and, instead, constraints in the form of inequalities are
sufficient to convey how loops are arranged.
Mapping and design phase: Currently, RAB is fairly inflexible
in the type of constraints that it can take into account for mapping
and design purposes. The same can be said about the optimiation
criteria. This is due to the fact that different constraints are
represented differently and, to a certain extent, built into the code.
The same can again be said of the objective functions to be
optimied. The new approach provides a unique form for the
representation of any t3pe of constraint (in essence, a set of
inequalities) and optimization procedures that are independent of
what the constraints and objective functions are (within certain
limits which have to do with the convexity of the objective function
and the space of feasible solutions). Therefore, constraints and
optimitation criteria can be easily added, deleted or changed. By
having the expansion, analysis and synthesis phases of RAB use the
same convex set representation, a more modular, efficient and
coherent mapping and design procedure results,
Merocode generation and optimization: RAB does not generate
microcode without user assistance. In the new approach, an
intermediate form is generated from the output of the mapping
module which is then translated to a microprogram specific to the
target array.
This paper has described bit level processor arrays and the
common characteristics that make them ideal for VLSI and high
speed computations. A taxonomy for current bit level arrays has
been developed that provides a perspective on this class of
architectures, The design and programming problem was addressed,
and RAB, an automated design tool developed at Purdue, was
briefly discussed as a solution to some of these problems. RAB has
the twin advantages of portability among different architectures
and systematic optimization down to the bit level,
The IC design was specified from low level geo-
metric masks to high level a lgorithm. Then, desigo
& updating can be performed more correctly, 4uickly
and inexpen sively via automatic syn thesization of
digital hardware with the high level behavioral de-
scription s. Ua like the computer-aided layout tool,
the designers can describe circuit behavior without
the kn owledges of actual circuit structure. It can
be completely built with computer systems from the
construction of circuit architectures up to genera-
tion of layouts [1]-[5].
A circuit modeled like a microprocessor, can be
partitioned into two parts [3], vis., the data path
con sisting of storage elements, operation units and
communication lines for storing data and performing
operation s, and the coatroller for controlling data
tran sfer in the data path. Only a few systems have
ever tried to optimize the data path structure at a
higher level [6 ]-[9]. In this study, a DPS has been
proposed for extractiag and embodyiag the data path
from an assembly-like program at RT level. Out put
of DPS a ppears as a data-path structure description
involving the information of pr ocessors utilization
and the communicatioa n etwork structure. Also the
methods for implementation s of pr ocessor allocator
and bus allocator have been thor oughly in vestigated
and discussed in section II. and sectioa III., re-
spectively. More details concerning the DPS can be
found in [10].
The pur pose for implementation of the pr ocessor
allocator is to n otminate a pKroper processor to each
statement. The processors can be iteratively allo-
cated stage by stage. And in each stage, there are
one or more steps, in which all the statements can
be assigned and performed by the pr ocessors. Then,
soae possible allocation s of all statements to some
processors can be achieved. In put for DPS includes
a ssembly-like pr ogram a s well a s several pr ocessor s
where the assembly-like pKrogram will be given by a
sample program which caa be proceeded by four basic
steps as follows:
While at least two A lLUs are required to perform the
'4' or '4' operation. Since some steps will be a l-
located to all the stages sequentially. Therefore,
the allocatLing processor s a ssociated with this sam-
ple can be realized by four stages at most. And the
possible allocation s of the statemen ts to the pr oc-
essors are listed and given in Table I.
'opl' is the symbolic name of '+' in 1-th
stage, 'op2' is the symbolic name of the other '4'
in 1-th stage, and same as for other ' opi'.
Before allocating, the following terms have to
be defined as:
Assume at the (i-1)-th stage, m circuits of data
paths with minimum cost, namely: the NDP( i-1,l1)8,
NDP(1-1,12)8, +- . and NDP(i-1,lm)3, have been coa-
structed. Then, m new circuits of data paths, viz.,
the NDP(i,).1), NDP(i,),), -- , NDPi,),m), should
be con structed associated with C(i,j). n order to
limit the n umber of data paths, decision should be
made in accordance with min imum cost, such that the
set of OP(i,c) can be generated a s follows:
Then, the DPS selects data paths with least n umber
of connectioa lines which will simplify the pr ocess
of bus allocatioa. Complete process of allocatiog
processors associated with the sample pr ogram will
be shown in Fig. l. and illustrated as follows:
Now the a lgorithm for allocating the processor s
will be introduced as follows:
There are no recipes for designing efficient
algorithms. This is somewhat unfortunate
from the point of view of applications: Any
time we have to design an algorithm, we
may have to start (almost) from scratch.
However, it is fortunate from the point of
view of researchers: It is unlikely that we
are going to run out of problems or chal-
lenges.
Given a problem, we want to find an
algorithm that solves it efficiently. There
are three stages in designing such algo-
rithms:
(a) Shmathematics. Initially, we use some
simple mathematical arguments to charac-
terize the solution. This leads to a simple
algorithm that is usually not very efficient.
(b) Agorithmic Tools. Next, we try to
apply a number of algorithmic tools to
speed up the algorithm. Examples of such
tools are 'divide and conquer'' and dynamic
programming [Aho et al. 1974]. Alterna-
tively, we may try to find a way to reduce
the number of steps in the original algo-
rithm by finding a better way to organize
the information.
(c) Data Structures. Sometimes we can
speed up an algorithm by using an efficient
data structure that supports the primitive
operations used by the algorithm. We may
even resort to the introduction of monsters:
very complicated data structures that bring
about some asymptotic speedup that is
usually meaningful only for very large prob-
lem size. (For a real-life monster see Galil
[1980].)
In these three stages we sometimes use a
known technique: a certain result in math-
ematics, say, or a known algorithmic tool
or data structure. In the more interesting
problems we need to invent new techniques
An important notion for all four problems
is that of an augmenting path. We solve
each one of them in stages, and in each
stage we have a matching M. Initially, M is
empty. A vertex i is matched if there is an
edge (i, j) in M and single otherwise. An
edge is matched if it is in M and unmatched
otherwise. An alternating path (with re-
spect to M) is a simple path, such that
every other edge on it is matched. An aug-
menting path (with respect to M ) is an
alternating path between two single ver-
tices. It must be of odd length, and in the
bipartite case its two endpoints must be of
different sex.
Consider Figure 1. The wiggly edges are
the matched edges. The path s, a, r, b, c, d,
4J, e, h is an augmenting path. Any contig-
uous part of this path (e.g., b, c, d, i) is an
alternating path.
The following theorem is due to Berge
[1957] and Norman and Rabin [1959].
The matching M has maximum cardinality
f and only if there is no augmenting path
uith respect to M.
One part of the theorem is trivial. If there
is an augmenting path, then by changing
the status of the edges on the path
(matched edges become unmatched, and
vice versa) we increase the size of M by 1.
We call this operation augmenting the
matching M. The other part of the theorem
is not trivial but is quite easy: We assume
that M is not a maximum matching and
show the existence of an augmenting path.
Let M ' be a matching of cardinality larger
than M. Consider M 68 M ', the set of edges
in M or M ' but not in both. Hence M 68
M ' consists of alternating paths and cycles
(with respect to both M and M '). At least
one of them must be an augmenting path
with respect to M. (In all other types of
alternating paths or cycles the number of
edges from M is at least as large as the
number of edges from M '.)
Theorem 1 gives an immediate algorithm.
It consists of O(n) (at most n/2) stages. In
each stage a search for an augmenting path
is conducted. If any augmenting paths ex-
ist, the search finds one and the matching
is augmented. Since the search takes O(m)
time, the algorithm runs in O(mn) time.
The search is conducted as follows. Ver-
tices are labeled successively, boys with an
S label and girls with a T' label. A labeled
boy (girl) is referred to as an S-boy (a
T-girl). After all labels from previous stages
are cleaned, all single boys are labeled with
S, We then apply two labeling rules itera-
tively:
The label also contains the vertex from
which the label has arrived. (In the case of
an S label this information is redundant.)
The search continues until the search
either succeeds or fails. The search suc-
ceeds if a single girl is labeled by T. The
search fails if we cannot continue anymore.
The following lemma can be proved by
induction.
By Lemma 1, if the search fails, there is
no augmenting path and the algorithm (not
only the stage) terminates. If a single girl j
is labeled by T, we have actually found an
augmenting path to j. The path can be
reconstructed by using the labels. The
search can be easily implemented in time
O(m) using any traversal of the graph that
starts with the single boys. The labels S
and T are also called even and odd or outer
and inner in the literature.
The best algorithm for Problem 1 is by
Hopcroft and Karp [1973]. They discovered
a way to find many augmenting paths in
one traversal of the graph. Their algorithm
is divided into phases. In each phase, a
maximal set of vertex disjoint augmenting
paths of shortest length is found and used
to augment the matching. So, a phase may
achieve the possible effect of many stages.
We now describe one phase. We use rules
R1 and R2 as before. Using breadth-first
search, starting from the single boys, we
identify the subgraph G of G consisting of
all the vertices and edges that are on some
shortest augmenting path. This subgraph
is ayered. In layer 2m (2m + 1) appear all
boys i (girls j) such that the shortest alter-
nating path from a single boy to i (j) is of
length 2m (2m + 1). We finish the construc-
tion of G in one of two ways. Either a single
girl is reached and we complete the last
layer and delete nonsingle girls from it, or
we cannot continue. In the latter case the
algorithm (not only the phase) terminates,
which is justified by Lemma 1.
In G we find a maximal set of disjoint
augmenting paths using depth-first search.
Every time we reach a single girl, we find
an augmenting path, erase its edges from
G, and start from another single boy. Every
time we backtrack along an edge, we delete
it from G. It is quite easy to see that a phase
takes O(m) time. The importance of the
notion of a phase is explained by the follow-
ing lemma [Hopcroft and Karp 1973].
The number of phases is at most O(Vn ).
Consequently, Hopcroft and Karp's al-
gorithm runs in time O(mVn).
It is interesting to note that the algo-
rithm (not the time analysis, ie., not
Lemma 2) was actually known before.
Problem 1 is a special case of the max flow
problem for special networks. (Add a source
and a sink, connect the source to all the
boys and the sink to all the girls, and take
all capacities to be one.) Augmenting paths
correspond to the flow augmenting paths
in network flow, and the O(mn) algorithm
is just the Ford and Fulkerson network flow
algorithm [Ford and Fulkerson 1956] for
these special networks. Similarly, Hopcroft
and Karp's algorithm is actually Dinic's
algorithm [Dinic 1970] applied to these spe-
cial networks. This was first observed by
Even and Tarian [1975] (ET).
As for Problem 1, Theorem l suggests a
possible algorithm of O(n) stages. In each
stage we look for an augmenting path. This
search is complicated by the existence of
odd-length cycles, which cannot exist in
bipartite graphs. We start by labeling all
single persons S and apply rules Rl and R2
with the following two changes. First, we
replace 'boys'' or 'girls'' by 'persons.'' Sec-
ond, any time R1 is used and j is labeled by
T, R2 is immediately used to label the
spouse of j with S. (Since j was not labeled
before, it must be married and its spouse
must be unlabeled.) We call this rule R12.
The search is conducted by scanning the
S-vertices in turn. Scanning a vertex mesns
considering in turn all its edges except the
matched edge. (There will be at most one.)
If we scan the S-vertex i and consider the
edge (i, j), there are two cases:
C2 cannot occur in the bipartite case. The
case in which j is a T-vertex is discarded.
In case C1 we apply R12. In case C2 we
do the following: Backtrack from i and j,
using the labels, to the single persons s, and
s; from which i and j got their S labels. If s,
s s;, we find an augmenting path from s; to
s; and augment the matching. The trouble
begins (or life starts to be interesting) when
S; 8;.
We next describe Edmonds' remarkable
work, in which the concept of blossoms is
introduced. Blossoms play a crucial role in
all algorithms for the nonbipartite case
(Problems 2 and 4).
If s; = s; s, let r be the first common
vertex on the paths from i and j to s, It is
easy to see that r is an S-vertex, that the
part of the two paths from i and j to r are
disjoint, and that the parts from r to s are
identical. We have found an odd-length
alternating path from r to itself through
(i, j). We call this cycle B a blossom and r
its base (see Figure 1).
In this case the method of labeling intro-
duced so far might overlook an augmenting
path. For example, Figure 1 shows a possi-
ble labeling. The augmenting path s, a, r, b,
c, d, t. J, e, h, which 'goes around the
blossom B,'' will not be discovered.
Edmonds' idea (Edmonds 1965a) was
to shrink B: Replace it by a single super-
vertex B and replace the set A of edges
incident with vertices in B by the set A' =
lB, j)]) e B, 3 (i, j) e A]. Note that at
most one member of A' (incident with r) is
matched; none are matched if r = s. If
is the graph obtained from G after such
shrinking, then the shrinking is justified by
the following theorem due to Edmonds
(which may be called the Blossoms Theo-
rem or the Main Theorem of Botany):
There is an augmenting path in G if and
only if there is an augmenting path in G.
We do not know of any easy proof for
Theorem 2. We do not discuss the ''only if''
part here (see Lawler [1976] or the second
edition of Tarjan [1983]). The 'if part'' is
obvious. Given an augmenting path in G, it
immediately yields an augmenting path in
G. If the path goes through B, then we do
the following: Replace the matched edge,
say (B, k), with (r, k); replace the un-
matched edge, say (B, j),. with the edge
i, j), from which (B, j) originated (recall
the set A' above), followed by the even
alternating path in B from i to r. Such a
path always exists. If i was an S-vertex
when B was formed, we use the labels and
backtrack from ito r. Otherwise, we use the
labels in reverse order around the blossom.
Storing B as a doubly linked list with a
marked base makes this very easy. Note
that, in Figure 1, as soon as the blossom B
is shrunk, the augmenting path can be
found. Note also that not every alternating
path from a vertex to itself is a blossom,
only those discovered by the algorithm (in
case C2).
The search for an augmenting path uses
a queue !, where new S-vertices are in-
serted. During the search, vertices from ?
are scanned and new blossoms are occa-
sionally generated. A blossom is a recursive
structure because it can be nested. It is
convenient to refer to vertices that do not
belong to any blossom as (degenerate) blos-
soms of size 1. When a new blossom B is
generated from blossoms B4, . - ., Bu, we
call the latter the subblossoms of B. We do
not refer to them as blossodms anymore. As
a result, each vertex in the orginal graph G
always belongs to one blossom in the cur-
rent graph. The structure of each blossom
B is described by a tree, called the structure
tree of B. In this tree, the root is labeled
with B, and the sons of a node labeled C
are labeled with C4, . - ., C, if at some time
the blossom C (which is now part of B) was
generated from subblossoms C4, . - ., C,,
Thus, the leaves of the structure tree of B
are the vertices that belong to B, The struc-
ture tree is represented by the collection of
the doubly linked lists of the various sub-
blossoms of B.
When a new blossom B is generated, it
is labeled by S and inserted into . Its
subblossoms that are still in are deleted
from . The search continues (in G).
If the search succeeds (in C2), we find
the augmenting path in the current graph.
Then we use the easy part of Theorem 2
and the structure trees to recursively un-
wind the augmenting path in the original
graph. We next augment the matching,
erase all labels and blossoms, and start a
new stage. Constructing the augmenting
path, augmenting the matching, and eras-
ing the labels take O(n) time. If the search
fails ( becomes empty), a repeated appli-
cation of Theorem 2 (each time a blossom
is shrunk) and an application of a modified
version of Lemma 1 (in which ''boys'' and
''girls'' are replaced by ''persons'') imply
that the current matching is maximum, and
we are done.
A naive implementation [Edmonds
1965a] takes O(n') (O(n') per stage). A
more careful implementation takes O(rn)
[Gabow 1976a]: Since the blossoms are dis-
joint, the total size of all structure trees at
any moment is O(n). When we generate a
new blossom, we do not rename the edges;
edges retain their original names. In order
to find out quickly to which blossom a given
vertex belongs, we maintain a membership
array, When B becomes a blossom, we put
the T-vertices into the queue ; so we later
scan them instead of scanning the new
vertex B. The other vertices of B (the
S-vertices) have already been inserted into
, and we do not delete them from Q. When
we consider an edge in C2, we ignore it if
both endpoints are in the same blossom. In
this implementation a stage takes O(n-)
time.
A slightly better bound can be obtained
as follows. If we find the base r of a new
blossom B more carefully, by backtracking
one vertex at a time, once from i and once
from ), marking vertices on the way, we
find the base and construct the blossom in
time O(k), where k is the number of sub-
blossoms of B. Hence, the total time per
stage devoted to finding bases and con-
structing blossoms is O(n). (Each node in
the structure tree is charged O(1).) Using
the ''set union'' algorithm to maintain the
sets of vertices in the blossoms for the
membership tests takes O(ma(m, n)) per
stage for a total of O(mno(m, n)), where aa
is the inverse of Ackermann's function
[Tarian 1975].
The obvious question that comes to mind
is whether the ideas of the phases can be
realized in the nonbipartite case. Recall
that in one phase we discovered a maximal
set of vertex disjoint augmenting paths of
shortest length. This is important because
Lemma 2 holds for general (not necessarily
bipartite) graphs. Executing a phase in gen-
eral graphs is more complicated than in the
bipartite case because of the existence of
blossoms.
Even and Kariv [1975] showed how to
execute a phase in time min(n', m log n],
This resulted in an O(min(n'', m/n
log n)) algorithm. A more detailed version
[Kariv 1976] is a strong contender for the
ACM Longest Paper Award. (It will prob-
ably lose only to Slisenko's real-time pal-
indrome recognizer [Slisenko 1973].)
More recently, a simpler approach was
found by Micali and Vazirani [1980]. In
phase i, i = 0, 1, .., of the algorithm, a
maximal set of (shortest) augmenting paths
of length L = 2i + 1 is sought as follows:
An odd (even) level of a vertex u is defined
as the length of the shortest odd- (even-)
length alternating path from a single vertex
to u, Both types of level numbers are com-
puted in linear time. Let the sum of a free
(matched) edge be the sum of the even
(odd) levels of its endpoints plus one. A
bridge is an edge of sum L that belongs to
an augmenting path of length L. Every
augmenting path of length L is shown to
contain a bridge. Bridges are identified and
new search techniques applied to find dis-
joint augmenting paths of length L. We do
not give the details here.
The immediate implementation of Micali
and Vazirani's algorithm takes O(m Vn(m,
n)) time.The authors claimed that the par-
ticular case of the disjoint set union used
by their algorithm can be shown to require
only linear time; as g result their algorithm
runs in time O(mVn). Quite recently, Ga-
bow and Tarjan [1983] found a linear-time
algorithm for some special cases of the dis-
joint set union. One of these special cases
is the one needed in Problem 2. (As a result,
also the O(mno(m, n)) algorithm men-
tioned above can be implemented in time
O(nm).)
The most efficient algorithms for Problems
3 and 4 use some observations on data
structures that we now review. A priority
queue (p.q.) is an abstract data structure
consisting of a collection of elements, each
with an associated real-valued priority.
Three operations are possible on a p.q.'
An implementation of a p.q. is said to be
efficient if each operation takes O(log n)
time, where n is the number of elements in
the p.q. Many efficient implementations of
p.4.8 are known, for example, 2-3 trees
[Aho et al. 1974; Knuth 1973].
In p.q.s elements have fixed priorities.
What happens if we allow the priority
of the elements to change? Obviously, an
additional operation that changes the
priority of one element can be easily imple-
mented in time O(log n). On the other hand,
it is not natural to allow arbitrary changes
in an arbitrary subset of the elements in
one operation simply because we have to
specify all these changes.
We consider two generalized types of
p.qs, which we denote by p.4.4 and p.4-4.
The first simply allows a uniform change
in the priorities of all the elements cur-
rently in it, The second allows a uniform
change in the priorities of an easily speci-
fied subset of the elements.
More precisely, p.q. enables the follow-
ing additional operation:
4, decrease the priorities of all the current ele-
ments by some real number &.
A version of p.q.4 was used by Tarjan
9771.
To define p.4-4, we first need some as-
sumptions. We assume that the elements
belong to a totally ordered set and that they
are partitioned into groups. Every group
can be either active or nonactive. An ele-
ment is active if its group is active. By
splitting a group according to an element e,
we mean creating two groups from all the
elements in the group greater (not greater)
than e. Note that, unlike the usual split
operation, we split a group according to an
element and not according to its priority.
The operations possible for p.q.4 are
It may seem at first that one may need
up to n steps to update all the priorities
as a result of one change. However, it is
possible to implement p.q.1 and p.q. effi-
ciently. In particular, the change of priori-
ties can be achieved implicitly in one step
(see Galil et al. [1986]):
p..; and p.. can be implemented in time
O(log n) per operation.
The implementation of the generalized
p.qs uses regular p.q.s and a collection of
offsets to handle the change in priorities.
We also make use of Johnson's d-heap
[Johnson 1977], where d is the number of
sons of internal nodes. (The usual heap is
a 2-heap.) We partition the primitive op-
erations into two types. Type 1 includes
inserting an element and decreasing the
priority of an element, and type 2 includes
deleting an element. Type 1 involves ''sift-
ing up'' the heap for a cost of O(loga),
whereas type 2 involves 'sifting down'' the
heap for a cost of O(d logaA0. Consequently,
the following theorem holds.
Let d = Im/n + 1l. A d-heap supports m
operations of type l and n operations of type
2 in time O(m logAn).
Introducing weights and maximizing the
weight make the problem much harder. One
can show that the following approach
solves Problems 3 and 4. Start with the
empty matching, and in each stage find an
augmenting path with the maximal in-
crease of the weight. One then can show
that after k stages, we have a matching of
maximum weight among matchings of size
k e.g., see Tarjan [1983]). In the case of
Problem 3, finding such an augmenting
path is relatively simple, because it can be
easily reduced to solving a single-source
shortest path problem for graphs with non-
negative edge lengths. But finding such an
augmenting path for general graphs is
much harder, so we choose a completely
different approach.
We use duality theory of linear progam-
ming (specifically the primal-dual method)
to derive the algorithm. We need linear
programming for motivation only. Once we
obtain the algorithm (for Problem 3 or 4),
we prove its correctness by a one-line proof.
Therefore, the description below is some-
what sketchy. (See Lawler [1976] or Papa-
dimitriou and Steiglitz [1982] for more
details.)
After defining the problem as a linear
program, we consider the dual problem and
then use complementary slackness to
transform our optimization problem into a
problem of solving a set of inequalities
(constraints). A pair of feasible solutions
for the primal and the dual problems are
both optimal if, for every positive variable
in the one, the corresponding inequality in
the other is satisfied as equality.
In the case of Problem 3, defining the
problem as a linear program is immediate.
We describe it as an integer program and
replace the integrality constraints x,; E
[0, 1] by 0 ss x,;, Since the matrixs of con-
straints is unimodular, we must have an
optimal integral solution.
We will have a primal solution, a match-
ing M, and a dual solution, an assignment
of dual variables tu., ; (corresponding to
boys i and girls j). For convenience we
define slacks j; for every edge (i, j): j;
u f u; - u;;. The inequalities x.; 2 0 are
the constraints of the dual problem.
(Whenever we mention x;; below we always
assume that (i, j) 6E E.) By duality, M has
a maximal weight if 6.0-6.2 hold:
The sufficiency of 6.0-6.2 for optimality
can be proved directly as follows: Assume
that M, u.. 4, r, satisfy 6.0-6.2 and let
N be any matching. Then 2.i-N W4 >
2.owN . + u, - =w s E. u. E, w, hy
6.0 and the fact that N is a matching), while
2aow Wa = E u. + Z, w 0hy 6.1 and
6.2 and the fact that M is a matching).
Consequently, M is a maximum weight
matching.
So, we only have to find a matching M
and an assignment of the dual variables
that satisfy 6.0-6.2. We use the primal-
dual method. This method starts with a
simple solution, which violates some of the
constraints, The solution is then modified
in a way that guarantees that the number
of violations is reduced. In our case we start
with M = 3, u; maxs1 44i for boys and u,
=0 for girls, The initial solution satisfies
6.0 and 6.1, and violates only 6.2. The al-
gorithm makes changes that preserve 6.0
and 6.2 and reduce the number of violations
of 6.2.
The algorithm consists of O(n) stages. In
each stage we look for an augmenting path,
as in the simple algorithm for Problem 1,
except that we use only edges with zero
slack (,; = 0). If the search is successful,
we augment the matching (i.e., change
the primal solution) and start a new stage.
This is progress because one single boy gets
married.
If the search fails, we change the dual
variables as follows. Let & = min(84, &),
8 IiTiaS-soy , 84 TiaS-boy,):treewi 3-
For an S-boy iwe set u; e- u;- , and for a
T-girl j we set u; +- u; + 8. It is easy to see
that b > 0 and the change preserves 6.0 and
6.1, Also &4 = u for any single boy i, (u,'s
of all boys had the same initial value, and
in each change of the dual variables, all
single boys had S label and their dual vari-
ables were decreased by the same &). If & =
84, then after the change 6.2 holds, and we
are done. Otherwise, for each edge (i, j), i
an S-boy and j a free girl, with ,, = &
(there exists at least one), ,; becomes zero,
and we can continue the search. Since at
least one girl gets a T' label as a result,
& = 8 at most O(n) times per stage.
The naive implementation of the algo-
rithm above takes O(mnf) time. The most
costly part is maintaining &y. For every
free girl j, let m; min,sw+ 4 and let
e; (i, j) be an edge with i an S-vertex and
x x,, Then 84 = min,n++aa s,, Note that
when we make a change of & in the dual
variables, m, is reduced by ä and e, does not
change. Also, if 5 = &4 x,, then the slack
of e, becomes 0 and it can be used for
continuing the search. By maintaining x,
and e; for all free girls j, an O(n') imple-
mentation of the algorithm follows.
In a different implementation, we main-
tain the collection C = (i, j)| , > 0, i an
S-boy,j a free girll as p.4.1, since all these
xj;'s are reduced by & in a change in the
dual variables. Whenever we scan an
S-vertex i, we consider all edges (i, j), where
j is a free vertex. Those edges with x,, > 0
are inserted into the p.q.1. Consequently,
this implementation takes O(mn log n)
time.
A small improvement is achieved if we
maintain x, and e; (as above) for free girls j
in a p.4.4. Then &4 is the minimum of this
p.q. One can see that the p.q. used here
satisfies the conditions of 'Theorem 4, and,
consequently, we get an O(mn l0gr-/4+11nn)
time bound, which dominates the two
bounds of O(n') and O(mn log n).
A closer look at a stage reveals that an
augmenting path is found using Dijkstra's
algorithm for all shortest paths from a sin-
gle source [Dijkstra 1959]. The source is a
new vertex, which is connected to all single
boys with new edges of length zero. The
lengths of other edges are the m;;'s at the
beginning of the stage. The reduction of a
stage to a shortest-path problem is well
known [Gabow 1974]. In fact, each stage
discovers the augmenting path that causes
the largest increase in the weight of the
matching. The various implementations
of Dijkstra's algorithm are (1) the naive
implementation: O(n'), (2) an imple-
mentation using p.q.s: O(m log n), and
(3) an implementation using Theorem 4:
O(m logr-/snn). Hence, the corresponding
time bounds for n stages follow immedi-
ately,
The main purpose of this section is to
serve as a warm-up for the next section
If we try to solve Problem 4 exactly as we
solved Problem 3, we immediately run into
problems. The linear program obtained by
dropping the integrality constraints from
the integer program for Problem 3 may
have no integer optimal solution. Edmonds
[1965b] found an ingenious way to remove
this difficulty, which led to a polynomial-
time algorithm for Problem 4. He added an
exponential number of constraints of the
following form. For every odd subset of
vertices B,
These new constraints must be satisfied by
any matching, and, surprisingly, their ad-
dition guarantees an integer optimal solu-
tion, This fact follows from the correctness
of the algorithm, which can be proved di-
rectly.
We now proceed as before. We have a
primal solution, a matching M, and a dual
solution, an assignment of dual variables u,
for every vertex i and z,for every odd subset
of vertices B,. We now define slacks .;
slightly differently: ., = u; t u; - w, +
2.,pes,. (Again ,; 2 0 are the constraints
of the dual problem.) By duality, M has
maximal weight if 7.0-7.3 hold:
In fact, as in Problem 3, we only need
duality theory for motivation. The suffi-
ciency of 7.0-7.3 for optimality can be
proved directly: Assume that M, u., G, 4
satisfy 7.0-7.3, and let N be any matching.
Then ZiuoeN W, > CawN 4. u; - =, +
2.<oea,4}= E, u.+ E,ro.hy 70 and the
fact that N is a matching), while 2....aes a
= E, wi + Ee re. (hy 7.1-7.3 and the fact
that M is a matching).
We can use 7.0-7.3 to derive a polynomial
algorithm because we will have z, > 0 only
for blossoms or subblossoms, and their total
number at any moment is O(n), We main-
tain only z,'s that correspond to blossoms.
Since we consider only ,; for i, j not in the
same blossom, m,; u; h u; - J;;, as in
Problem 3.
We again use the primal-dual method.
We start with M = C, u, = (maxsr 4e0/2
and no z,'s (no blossoms). The initial so-
lution violates only 7.2. The algorithm
makes changes that preserve 7.0, 7.1, 7.3
and reduce the number of violations of 7.2.
As in Problem 3, the algorithm consists
of O(n) stages. In each stage we look for an
augmenting path using the labeling R12
and the two cases C1, C2 as in the simple
algorithm for Problem 2, except that we
only use edges with m,, = 0. If the search is
successful, we augment the matching.
To preserve 7.3 we keep blossoms with
e, > 0 shrunk at the end of the stage. As a
result, we have two new kinds of blossoms
in addition to the S-blossoms we had in
Problem 2. (Recall that a newly generated
blossom is labeled by S.) Since the labels
are erased at the end of a stage, we may
have free blossoms at the beginning of a
stage. During the search a free blossom can
become a T-blossom. (Recall that a blos-
som is just a vertex in the current graph.)
We call the vertices of an S-blossom
(a T-blossom or a free blossom) S-vertices
(T-vertices or free vertices). When, during
the search, a new S-blossom B, is formed,
the vertices in its T-blossoms (which now
become subblossoms) become S-vertices
and are inserted in the queue ? (of
S-vertices). We also initialize a new z, to
zero.
If the search is not successful, we make
the following changes in the dual variables.
We choose ä = min(84, &, 4, 4i), where
We then set
Such a choice of & preserves 7.0, 7.1, and
73
If& = 6., we expand all T-blossoms B, on
which the minimum was attained. (Each
corresponding z, becomes 0.) The expan-
sion of blossom B is shown in Figure 2. B
stops being a blossom and its subblossoms
become blossoms. All vertices of the new
S-blossoms are inserted into .
If 5 = 64 (& = 84), we consider all edges
(i, j) with i an S-vertex and j a free vertex
(an S-vertex not in the same blossom) on
which the minimum was attained. For each
such edge x,; becomes 0 and we can use it
for continuing the search. At the end of
each stage we also expand all S-blossoms
B, with z, = 0.
Let us call each change in the dual vari-
ables a substage. Each S-blossom corre-
sponds to a unique node in one of the
structure trees at the end of the stage. Each
T-blossom corresponds to a unique node in
one of the structure trees at the beginning
of the stage. Consequently, for i = 2, 3, 4,
& = 8; at most OO(n) times per stage: when
6 = by, a blossom becomes a T-blossom;
when 8 = 84, either the stage ends or a new
S-blossom is generated, and when & = &., a
T'-blossom is expanded. Finally, = 4 at
most once. Consequently, there are O(n)
substages per stage.
The most costly part of a substage is
computing &. The obvious way to compute
it takes O(m) steps and yields an O(mn)
algorithm. Edmonds' time bound was
O(n').
The only parts that require more than
O(n') are maintaining &4 and &4. & is han-
dled as in the O(n') algorithm for Problem
3. To take care of &4, we define for every
pair of S-blossoms B,, Bj:
We record the edge e,r on which the mini-
mum is attained and maintain W =
min;W,;. We do not maintain Y,j, but any
time we need it we compute it by using es. 1-
Obviously &4 = min,. A change in the
dual variables and computing &4 costs O(n')
as for &4. We have to update !V,] and ew 1
any time an S-blossom B, is constructed
from B.,, .-., B,,. Recall that (r + 1)/2
of the subblossoms are S-blossoms and
(r - 1)/2 of them are T-blossoms. We
first ''make'' each T-blossom B,, into an
S-blossom by considering all its edges and
computing for it V,,i! and le,,l. Then we
use the V,,,s of B, , . .., B,, to compute 4
and le,)] for the new blossom B, and to
update ]9,] for j = k. The total cost (per
stage) to make T-blossoms into S-blossoms
is O(m). We now compute the rest of the
cost TN), where N is the number of S-
blossoms plus the number of non-S-vertices
in the graph. T(N) s crN + TiN - r + 1)
because rN is a bound on the number of
V/s considered after making the T-blos-
soms into S-blossoms. T(N) = O(N%) (by
induction on N), and the total cost of com-
puting &4 is O(n'). The discussion above
results in an O(n') algorithm [Gabow 1974;
Lawler 1976].
The most costly part of the algorithm is
the frequent updates of the dual variables,
which cause changes in lm,;]. Note that all
the elements that determine each &; are
decreased by each change in the dual
variables. We maintain &4, &, &i by a p.4.1-
We use a p.q.; to maintain u, for T-vertices,
and another p.q.; for z,s of S-blossoms B,.
If we try to maintain &y by a p.4.1, we
have problems. Consider Figure 3. Initially
there may be a large free blossom B;. At
that time all edges in Figure 3 should be
considered for finding the value of &. Later
B; may become a T-blossom. Then these
edges should not be considered for finding
the value of &. Later still, B; may be ex-
panded and one of its subblossoms, Eg, may
become free. The latter may subsequently
become a T-blossom, and so on. A simple
implementation requires the consideration
of each such edge a large number of times
up to k in Figure 3).
To maintain &, we have a p.q.4. For every
free blossom (T'-blossom) B, we have an
active (a nonactive) group of all the edges
from S-vertices to vertices in B,. Note that
if (i, j) is in a nonactive group (i is an
S-vertex and j is a T-vertex), then ;; does
not change as a result of a change in the
dual variables. It is now easy to verify that
the eight operations of p.q.4 suffice for our
purposes.
Consider a group g, which corresponds to
a blossom B. The elements of the group are
the edges l(i, j)] i an S-vertex,j e B]. The
order on the elements is derived from the
order on the vertices of 8, The latter is
taken to be the left-to-right order of the
leaves of the structure tree. The order be-
tween two edges (i), j) and (i, j) is arbi-
trary. The order enables us to split the
group g into the groups corresponding to
B;, ..., B, when we expand B to its sub-
blossoms.
To maintain the generalized priority
queues, we make a change in the scanning
of a new S-vertex i, We also take into
account edges (i, j) with ,; > 0 and have
three more cases in addition to C1 and C2
for edges (i, j) with ,, = 0. Assume that j
is in a blossom B with ,; > 0.
We insert (i, j) with priority ,, into the
active (nonactive) group corresponding
to B.
(C5) B is an S-blossom and i g B.
We insert (i, j) with priority m,;/2 to the
p.q. that computes &4.
Since ä4 u, for any single vertex ii4, we do
not need a generalized p.q. to compute ä4.
Nevertheless, we have a p.q.; for the u,'s of
the S-vertices and also a p.q. for the u,'s
of the T-vertices for computing xi; when
the edge (i, j) is considered.
We have a p.q.; for the z,'s for S-blossoms,
because at the end of a stage they all be-
come free, and in the next stage they may
become T-blossoms.
The p.q. for computing &4 contains also
edges (i, j) with iand j in the same blossom.
We do not have time to locate and delete
such edges each time a new blossom is
constructed. Consequently, if & = 4 and
8g x;;, we first check whether i and j are
in the same blossom. If they are, we delete
the edge and possibly compute a new
(larger) &.
All edges (i, j) in the generalized p.q.'s that
compute 4 or 84 have m,; > 0, since an
element is deleted as soon as its priority
becomes 0. Similarly, all z,'s in the p.4-
that computes &i are positive. Conse-
quently, & > 0.
To derive an O(mn log n) time bound, we
need to implement two parts of an algo-
rithm carefully:
1. We maintain the sets of vertices in
each blossom (for finding the blossom that
contains a given vertex) by concatenable
queues [Aho et al. 1974]. Note that the
number of finds, concatenates and splits is
O(m) per stage, and each takes O(log n)
time.
2. In C2 we use the careful backtracking
described for Problem 2.
The O(mn log n) time bound is easily
derived as follows. There are at most n
augmentations (stages). Between two aug-
mentations we consider each edge at most
twice and have O(m) operations on (gen-
eralized) p.q.s, (This includes 1 and 2
above.)
We have considered four versions of the
maximum matching problem and discussed
the development of the most efficient al-
gorithms for solving them. By ''most effi-
cient algorithms'' we mean those that have
the smallest asymptotic running times. We
now mention briefly a number of closely
related additional topics and give some ref-
erences. These are intended to serve as
exzamples and certainly do not form an ex-
haustive list.
(a) Applications of Matching. We do not
list here the many applications of solutions
to Problems 1-4. For some applications see
Lawler [1976].
(b) Generalization of Matching. Prob-
lems 1-4 can be generalized in a number of
ways. For example, Gabow [1983a] has re-
cently considered similar problems where
some kinds of polygamy are allowed. He
found efficient reductions to the corre-
sponding matching problem. Stockmeyer
and Vazirani [1982] showed that several
natural generalizations of matching are
NP-complete.
(c)Special Cases of Matching. Many ap-
plications solve one of the Problems 1-4,
but only for special graphs. For example,
Problem 1 is used to find routing in super-
concentrators [Gabber and Galil 1981]. The
graphs that arise in this application have
vertices with bounded degree, and hence
the solution given here takes time O(H').
Perhaps this can be improved. For better
algorithms for some special cases of Prob-
lem 1, see Cole and Hopcroft [1982] and
Gabow [1976b].
(d) Randomizing Algorithms. Several
algorithms that work very well for random
graphs or for most graphs have been devel-
oped, They are usually faster and simpler
than the algorithms discussed here [An-
gluin and Valiant 1979; Karp 1980]. An
interesting problem is to find improved
randomizing algorithms that use random
choices (rather than random inputs).
(e) Approximation Algorithms. As for
all optimization problems, we may settle
for approximate solutions. For cardinality
matching, the solution that uses phases
yields a good approximation by executing
only a constant number of phases. For sim-
ple, fast, and good approximation algo-
rithms for special graphs see Iri et al.
[1981], Karp and Sipser [1981], and
Plaisted [1984].
(f) Improvements. We next discuss
possible improvements of the algorithms
considered in this paper. All the time
bounds discussed in this paper can be
shown to be tight. One can construct fam-
ilies of inputs for which the algorithms
require the number of steps that is specified
by the stated upper bounds. There are no
known lower bounds for any of the four
problems. Improving the O(m/n) bound
for cardinality matching must involve the
discovery of a new approach that does not
use phases. Similarly, except for a logarith-
mic factor, improving the bound for
weighted matching requires the use of an
approach that does not make @ß(n) augmen-
tations. Perhaps the introduction of phases
may lead to improved algorithms for Prob-
lems 3 and 4. Note that the solution to
Problem 3 is slightly better than the solu-
tion to Problem 4, due to the use of
d-heaps. It may still be possible to find a
similar improved solution for Problem 4.
There are several theoretical questions
concerning Problems 1-4. Their solution
may lead to simpler or faster algorithms:
Assume that we have solved an instance
of a weighted matching problem and then
make a small change such as adding or
deleting some edges or changing the weight
of a few edges. It is not clear how to make
use of the solution to the original problem.
The most important contributions to crack problems in plane elasticity were collected and summarized in
[1-5]. More recently, numerical results obtained in fracture analysis were edited and compiled in [6]. From
these publications we see that the investigators pay more attention to the problems with some complicated
geometry, loading and material conditions. For example, problem of a cracked half-plane with mixed
boundary value condition has been solved by a rational function of the mapping function [7].
The singular integral equations for problems of a single and multiple curved cracks have been
proposed in [l]. It is probable that the solution of these problems is a weak point in analysis, Also very
few numerical results in this field have been carried out. There are some inherent difficulties for
evaluating the Cauchy principal value integral. For example, the existing quadrature rules are mainly
limited to the case of the integrals along a straight line.
In this paper a more convenient approach for solving the singular integral equation is proposed,
ie, we let the displacement jump function along the crack take the form of a polynominal multiplied
by a weight function. Therefore, the involved singular integrals can be integrated in a closed form, and
the single-valuedness condition of displacements is satisfied automatically. After letting the integral
equations be satisfied at some discrete points on the two curved cracks, the undetermined coefficients
in the polynomial can be obtained immediately. Finally, the interaction effect between two curved
cracks isevaluated, and the K -factors at the crack tips are obtainable. To demonstrate the efficiency of
the proposed approach, several numerical examples are given. Obviously, the proposed approach can
be used for multiple curved cracks in an infinite plate without any difficulty.
In the following analysis the problem of two curved cracks in an infinite plate with zero remote
stresses is considered. It is assumed that the tractions along the two crack faces have the same
magnitude and opposite direction.
To formulate the singular integral equation for this problem Muskhelishvili's method for plane
elasticity is used [8]. According to this method the stresses o+s,0, 0++, the resultant force function,
X. Y, and the displacements u,c can be described by two complex potentials @z) = 4(2) and
Pi2) = w'2), see [8], as
where G is the shear modulus of elasticity, A = 3 -- 4v for the plane strain problem.
w = (3 -- y)/(1 -+ v) for the plane stress problem, and v is Poisson's ratio.
For the case of single curved crack, shown in Fig. Al of the Appendix, the dislocation distribution
is defined as
Clearly, the single-valuedness condition of displacements leads to
After some manipulation the singular integral equation for this case takes the form (see Appendix)
where p(t,,) and q(t;)represent the normal and tangential tractions applied along the crack faces, and
Obviously, the problem of two curved cracks shown in Fig. 1 can be considered as a superposition
of two single curved crack problems. Therefore, the resulting singular integral equations
Ifx = 0, the two curved cracks becomes two collinear cracks. The calculated results are compared
with those obtained in [6]. The comparison is listed in Table l. It proves the high accuracy that has
been obtained by using the suggested approach.
In the second example, one curved crack has a parabolic configuration and the other sine
configuration (Fig. 5). The two curves are expressed by
respectively. As before, under the action of o = p, the calculated stress intensity factors can be
expressed by
The calculated results of F;i,ts4 -., fss are plotted in Figs. 6-9, respectively.
If Y = 0, the two curve cracks becomes parallel cracks. The calculated results are compared with
those obtained in [6]. Also here the comparison proves that high accuracy has been obtained by using
the suggested approach. The comparison is listed in Table 2.
where
Clearly, from the single-valuedness condition of displacement, the following constraint condition
should be satisfied:
It is of interest to point out that the traction applied along the curve L; in Fig. A 1 can also be
evaluated by the same equation as (A 12), ie.
However, for t is moved along the curve L and t,is located on L;,and thus, the integrals in (A 18) are
regular.
mathematician, your association with Courant, the Robbins-Monro process, empirical
Bayes, compound decision theory, power one tests, adaptive sequential estimation, and
optimal stopping problems were covered in the paper by Lai and Siegmund (1986). So,
I feel I can leave those aspects in the background and concentrate on the human and phi-
losophical side, which I hope you will share with me. Would you care to comment on
this approach?
A: The trouble is that scientific personalities tend to be rather dull, with a few excep-
tions like Richard Feynman and James Watson.
Q: You were trained as a mathematician, and your first publications were in mathemat-
ics, How did you get interested in statistics? Was it Harold Hotelling's influence?
A: He called my attention to the fact that little as I knew about mathematics, most sta-
tisticians in this country at that time knew even less, so I might have some success in
statistics. I took him up on that suggestion and started as a statistician in 1946 in Chapel
Hii.
Q: What had been your area of specialization in mathematics? Was it analysis? Wih
whom did you work in mathematics? Was it Courant?
A: Originally I worked in topology. I never worked with Courant except in writing that
book. I never studied with him.
Q: Then who influenced your mathematical education?
A: There were a number of mathematicians who were very important to me in my early
years. Marston Morse made a big impression on me, although I never took a course with
him.
Q: Was he the famous topologist!
A: His field was the calculus of variations. He created the Morse theory, one of the
great achievements of American mathematics.
Q: Was he at Harvard?
recommend to his students to go into biostatistics because the control of statistical
methods at NIH is so much in the hands of doctors. The statisticians are regarded as
computer technicians, rather than being respected by the medical profession as fellow
scientists. Of course, the doctors are the authorities in matters of healh. They know
what's good for you. But we have to help them more, whether they like it or not.
I thank Brian Moses for transforming tape-recorded informal conversations into
the present manuscript.
In the mid 1940's, J.M. Sengupta and his colleagues at the
Indian Statistical Institute employed an empirical method for
estimating the area of fields under rice paddy in Bihar [Sengupta
(1953)]. Sengupta's procedure may be described as follows. The
measuring apperatus is a line segment or line grid of length L.
Let T be a sufficiently lerge erea (taken to be rectangular for
convenience) within which all the unknown areas D, (i=1. . .. ,k) are
assumed to lie. Select a point P at random in T. W1th P as its
left hand end point the grid L is placed parallel to a fixed
direction (taken here as the x-axis). If L 1s the length of the
intersection of L with D,, L EL, multiplied by a constant factor
is used as an unbiased estimate of the total area
Soon af ter Sengupta' s paper appeared I was asked to look into
this problem by Professor P.C. Mahalanobis who was also interested
in a formula for the variance of L, In view of the fairly
arbitrary shapes of the fields occurring in area sampling problems
of the kind mentioned above, the advan tages of the line grid
estimate over a planar or rectangular grid are obvious. A detailed
comparison be tween the two me thods with par ticular reference to
bias and cost has been made in Sengupta (1953).
M. Masuyama who derived an expression for EL using integral
geometric methods assumed the following condition of ''unbiasedness''
that holds between the large rectangle T, the grid L and the area D
[Masuyama (1953)!. If L has a nonempty intersection with D then L
is contained within T.
In this article I shall briefly indicate how the first two
moments of the line grid estimate of a single area can be obtained
from the formulas of Herber t Robbins on measures of random sets
[Robbins (1944) and Robbins (194)]. I have not tried to obtain
results for the most general possible areas D, but only for those
The problem, simplified for our purposes, is set up as
Follows4 Let R be a rectangle with sides 0 $ N, S A, (1=1,2), T 1s
a larger rectangle with sides -4, S 8, $ A,*, (1=1.2) (8,70), The
measuring device is a rectangular grid r with sides O $ N, ;-
The center of this grid is placed at a point in T chosen at random
with density . with the side [0,a,) parallel to the x,-axis, It
is fur ther assumed that
This condition corresponds to the unbiasedness condition in the
linear grid problem. Let X be the (two-dimensional) random set
T R. In Robbins (1945) Robbins has computed the first two
moments of X. Actually, he considers a more complicated problem in
which R is a parallelepiped in n space and the measuring appara tus
consists of N n-dimensional parallelepipeds r,. %, = r,fi R and X=&,
u..4 %
Consider a rectangular grid r of (horizontal) length L and width e.
We make the stronger assumption L ( min(8, .4,). Let be the
random intersection of r with R. Denote the length of the
horizontal side of X by L, Letting X' be the rectangle with sides
L and e we write
Mecke (1987) and the references given there). Matheron has
introduced a topology in 4 which makes t a Polish space.
Convergence of sequences of sets in Z to closed one dimensional
sots in 4 on hen be use4 to show that the indt= 'p,,P11
jointly measurable in , and a relative to the appropria te product
o-field.
General formules for Ei,(L) and E[u,(L)f) tor the esttatton
problem of D when D is the sum of several areas as in Section S are
given by the following
Here u P o g', the measure on the space of lines induced by the
measurable map L(u). In the problem considered in this article,
the choice of P (uniform distribution in the rectangle T) leads to
a ''kinematic'' measure v. However, as seen from (23) and (24) more
general measures on lines can also arise. An explicit evaluation
of the right hand sides of (23) and (24) for speciftc choices of P
and D; would give rise to suitable extensions of Crof ton-type
formulas.
Research suppor ted by Air Force OfFice of Scientific Research
Contract No. F49620 &5C O144.
THE METHOD of input-output linearization prov-
ides a natural framework for the design of
tracking controllers. This technique has in fact
been successfully implemented in several practi-
cal applications, such as flight control (Asseo,
1973; Meyer and Cicolani, 1975; Meyer and
Cicolani, 1980; Singh and Schy, 1980; Lane and
Stengel, 1988) and the control of rigid robots by
the so-called computed torque method (Freund,
1975). The theory is now well developed and
understood (Isidori, 1989).
One of the major obstacles to the direct
application of this theory is the fact that it relies
on a nonlinear version of pole-zero cancellation.
Of course, the nonlinear pole-zero cancellation
implicit in these techniques is only a problem
when the cancellation is one involving unstable
zero dynamics (introduced in Byrnes and Isidori
(1984) and made precise in Isidori and Moog
(1989) and Isidori (1987)). In this paper, we
focus on this problem with specific emphasis on
the aircraft control problem.
While several researchers have applied the
methods of nonlinear control to the aircraft
problem (see Lane and Stengel (1988) for a nice
summary), most have neglected the small
moment-to-force coupling without proper jus-
tification. This coupling provides dynamic effects
that cannot be assumed to be bounded! Due to
the fact that we are building a closed loop
feedback system, we must carefully analyze the
effects of this coupling to guarantee that small
changes in this parameter do not result in drastic
changes in behavior such as the bifurcation
behavior that can result from inertial coupling
(Hacker and Oprisiu, 1974) or high angles-of-
attack (Mehra et al., 1977). In this paper, we
provide rigorous justification for the common
practice of ignoring the moment-to-force cou-
pling in the design of the controller.
This paper is organized as follows: Section 2
discusses some modeling issues for aircraft
dynamics and presents a simplified planar VTOL
aircraft that will be used in the main discussion.
We then work through the details of both exact
and approximate input-output linearization for
the simplified aircraft and present illustrative
simulations showing the qualitative behavior of
this system. In Section 3, we develop the
rudiments of a theory for the approximate
input-output linearization for slightly non-
minimum phase systems.
The complete dynamics of an aircraft, taking
into account flexibility of the wings and fuselage,
aeroelastic effects, the (internal) dynamics of the
engine and control surface actuators, and the
multitude of changing variables, are quite
complex and somewhat unmanageable for the
purposes of control. A useful first approximation
is to consider the aircraft as a rigid body upon
which a set of forces and moments act.
Then, with r, R, and ar being the aircraft
position, orientation (rotation matrix), and
angular velocity, respectively, the equations of
motion can be written as
where f, and r, are the force and moment acting
on the aircraft expressed in the aircraft reference
frame. Here, the a subscript means that a
quantity is expressed with respect to the aircraft
reference frame.
Depending on the aircraft and its mode of
flight, the forces and moments can be generated
by aerodynamics (lift, drag, and roll-pitch-yaw
moments), by momentum exchange (gross thrust
vectoring and reaction controls to generate
moments), or a combination of the two. The
flight envelope of the aircraft is the set of flight
conditions for which the pilot and/or the control
system can effect the forces and moments
needed to remain in the envelope and achieve
the desired task.
While the function mapping the control inputs
to the forces and moments is a highly nonlinear
state-dependent function, it is useful to note that
this function can normally be decomposed as
where e $'' denotes the state and c e $''t
denotes the control input and 3;%''-- %';
: R''-- 9t*'', and u :H'' s R'''--''' are (con-
tinuous) functions. In particular, for each in
the function u(x, -): %'''- }''' is one-to-one and
hence (algebraically) invertible. The value of the
function u(:, ) can often be taken to be the
components of the force and moment that the
actuators were designed to produce.
As a example, consider the YAV-8B Harrier
produced by McDonnell Aircraft Company
(McDonnell Douglas Corporation, 1982;
McDonnell Aircraft Company, 1983) depicted in
Fig. 1 (aircraft frame--A, runway frame-R).
The Harrier is a single-seat transonic light attack
V/STOL (vertical/short takeoff and landing)
aircraft powered by a single turbo-fan engine.
Four exhaust nozzles on the turbo-fan engine
provide the gross thrust for the aircraft. These
nozzles (two on each side of the fuselage) can be
simultaneously rotated from the aft position
(used for conventional wing-borne flight) for-
ward approximately 100 degrees allowing jet-
borne flight and nozzle braking. The throttle and
nozzle controls thus provide two degrees of
freedom of thrust vectoring within the xr-z plane
of the aircraft. (If the line of action of the gross
thrust does not pass through the aircraft center
of mass, then this thrust will also produce a net
pitching moment.)
In addition to the conventional aerodynamic
control surfaces (aileron, stabilator (stabilizer-
elevator), and rudder for roll, pitch, and yaw
moments, respectively), the Harrier also has a
reaction control system (RCS) to provide
moment generation during jet-borne and transi-
tion flight. Reaction valves in the nose, tail, and
wingtips use bleed air from the high-pressure
compressor of the engine to produce thrust at
these points and therefore moments (and forces)
at the aircraft center of mass. The design of the
aerodynamic and reaction controls provides
2 x 2 rotation matrix depending on the roll angle
The study of this simple planar model provides
important insight that extends naturally to the
more complicated six degrees-of-freedom aircraft.
Consider the PVTOL aircraft system given by
(6). Since we are interested in controlling the
aircraft position, we choose x and y as the
outputs to be controlled. We seek a (possibly
dynamic) state feedback law of the form
such that, for some y = (yj, y)',
Here, v is our new input and z is used to denote
the entire state of the system (including
compensator states, if necessary).
Proceeding in the usual way, we differentiate
each output until at least one of the inputs
appear. This occurs after differentiating twice
and is given by (rewriting the first two equations
of (6))
Since the matrix operating on u (the so-called
decoupling matrix) is nonsingular (barely-its
determinant is -e!), we can linearize (and
decouple) the system by choosing the static state
feedback law
The resulting system is
This feedback law makes our input-output map
linear, but has the unfortunate side-effect of
making the dynamics of 6 unobservable. In
order to guarantee the internal stability of the
system, it is not sufficient to look at input-
output stability, we must also show that all
internal (unobservable) modes of the system are
stable as well.
The first step in analyzing the internal stability
of the system (11) is to look at the zero dynamics
(Byrnes and Isidori, 1984; Isidori and Moog,
1989; Isidori, 1987) of the system. The zero
dynamics of a nonlinear system are the internal
dynamics of the system subject to the constraint
that the outputs (and, therefore, all derivatives
of the outputs) are set to zero for all time.
Constraining the outputs and their derivatives
to zero by setting v; vg = 0 (and using
appropriate initial conditions), we find the zero
dynamics of (11) to be
Equation (12) is simply the equation of an
undamped pendulum. Figure 5 shows the phase
portrait (6 vs 6) of the pendulum (12) with
e =1. The phase portrait for e . 0 is simply a
horizontal m-translate of Fig. 5. Thus, for e > 0,
the equilibrium point (6, 6) = (0, 0) is unstable
and the equilibrium point (, 0) is stable but not
asymptotically stable and is surrounded by a
family of periodic orbits with periods ranging
from 2Ve to =. Outside of these periodic orbits
is a family of unbounded trajectories. Thus,
depending on the initial condition, the aircraft
will either rock from side to side forever or roll
continuously in one direction (except at the
isolated equilibria).
Nonlinear systems, such as (11), with zero
dynamics that are not asymptotically stable are
called non-minimum phase. Figure 6 shows the
response of the system (11) when (;, Us) is
chosen (by a stable feedback law) so that r will
track a smooth trajectory from = 0 to = 1
with y remaining at zero. The bottom section of
the figure shows snapshots of the PVTOL
aircraft's position and orientation at 0.2 sec
intervals. From the phase portrait of 6 (Fig. 6e),
we see that the zero dynamics certainly exhibit
pendulum-like behavior. Initially, the aircraft
rolls left (positive 6) to almost 2xY. Then, it rolls
right through four revolutions before settling
into a periodic motion about the -3T
equilibrium point. Since v; and vy are zero after
t = 5, the aircraft continues rocking approxim-
ately sx from the inverted position. Note that,
even if the resulting zero dynamics were
acceptable, the control law itself is unacceptable
value of 0.9 for e means that the aircraft will
experience almost lg (the acceleration of
gravity) in the wrong direction when a rolling
acceleration of 1 rad sec'' is applied. For the
range of e values that will normally be expected,
the performance penalty due to approximation is
small, almost imperceptible.
Note that, while the PVTOL aircraft system
(6) with the approximate control (16) is stable
for a large range of e, this control allows the
PVTOL aircraft to have a bounded but
unacceptable altitude (y) deviation. Since the
ground is hard and quite unforgiving and vertical
takeoff and landing aircraft are designed to be
maneuvered in close proximity to the ground, it
is extremely desirable to find a control law that
provides exact tracking of altitude if possible.
Now, e enters the system dynamics (5) in only
one (state-dependent) direction. We therefore
expect that one should be able to modify the
system (by manipulating the inputs) so that the
effects of the e-coupling between rolling
moments and aircraft lateral acceleration do not
appear in the y-output of the system.
Consider the decoupling matrix of the true
PVTOL system (6) given in (9) as
To make the y-output independent of e requires
that the last row of this decoupling matrix be
independent of e. The only legal way to do this
is by multiplication on the right (i,e, column
operations) by a nonsingular matrix V which
corresponds to multiplying the inputs by V '. In
this case, we see that
is the desired transformation. Defining new
inputs, ü, as
we see that (9) becomes
Following the previous analysis, we set e = 0 and
linearize the resulting approximate system using
the dynamic feedback law
Note that this control law will approximately
linearize the true system. The true system inputs
satisfy
for some k C o:.
Proof. Similar to that of Theorem 3.1. 3
As in the SISO case, the stronger conclusions
of Theorem 3.2 can be stated when the control
objective is stabilization and the approximate
system has no zero dynamics.
In this paper, we have described the
application of techniques of exact input-output
linearization of nonlinear control systems to the
flight control of vertical take-off and landing
aircraft. We saw that the application of the
theory to this example is not straightforward. In
particular, the direct application of the theory
yielded an undesirable controller. We remedied
the situation by neglecting the coupling between
the rolling moment input to the aircraft
dynamics and the dynamics along the y-axis.
The example of the vertical takeoff and
landing aircraft is an example of a system which
is slightly non-minimum phase. Thus, the exact
lienarization technique resulted in a system
which was internally unstable. We generalized
the lessons learned from this application to
define, informally, slightly non-minimum phase
systems and gave methods to linearize them
approximately.
To convert the units from gravity potential to linear ones, these potential values
(actually potential differences from the datum) are scaled by the mean value of
gravity along the vertical to the geoid/ellipsoid between the point P and the
datum surface to give the adDcdialLiGLDelUDl.
Gravity therefore plays a central role in the levelling operation:
Another distinctive feature of the levelling process is that, in a sense, it makes no
assumptions concerning the shape of the earth, or even the geoid. The field
operations, the data reductions and the final elevation value assigned to a point
are all free of the influence of the uz5ISD4lllüGl4.
The reference ellipsoid is a mathematical approximation of the real earth, and is
central to most other geodetic techniques. Although measurements by, for
example, theodolite or EDM, are made in the real world, the observations are
Their location, and the intersite distances, is illustrated in Figure 7. Pt. Lonsdale
in Victoria was one of the tide gauges used in the definition of the AHD
(mainland). The Burnie tide gauge was used in the definition of the AHD
(Tasmania). The AHD level adjustment is unlikely to have distorted the levelling
loops within the hundred or so kilometres of these two gauges. We can
therefore assume that the levelled (orthometric) heights of the Tide Gauge
Benchmarks in Victoria are strongly related to the MSL datum at Pt. Lonsdale
(one of the tide gauges used to fix the AHD to MSL), and the Tide Gauge
Benchmarks in northern Tasmania are related to the MSL datum at Burnie. In
other words, MSL at the three tide gauges in Victoria (defined mathematically by
zero orthometric height, and physically by excavating below the Tide Gauge
Benchmarks to a depth equal to the benchmarks' orthometric height) can be
assumed to relate to the same geopotential surface. The same assumption can
be made concerning the three Tasmanian MSL estimates. Therefore, if there is
any slope in the MSL across Bass Strait relative to the geoid (that is, a Sea
Surface Topography slope) then the comparison of the results of GPS/geoid
determination at the 6D5EDIDg of Victorian gauges with those obtained from the
D5SIDDIlg of Tasmanian gauges should provide an estimate of its magnitude.
(The three gauges at King lsland, Flinders lsland and Kingfish B have no direct
role to play in the AHD (mainland) --> AHD (Tasmania) connection.)
Three WM102 dual-frequency GPS receivers were used to track the GPS
constellation during a daily 6-8 hour observation window. To survey the 9
gauges, a 'leap frogging'' receiver deployment procedure was used, in which
one or two of the sites were observed on consecutive days, while the other
modelled by eauation (9) (KEARSLEY, 1988; MITCHELL, 1990). The 'inner''
zOne contribution therefore is entirely short wavelength geoid information,
represented mathematically by:
where.
The integration is carried out over a cap, centred at the computation point, with
radius 44y. Usually this is of the order of a degree (-110km) or so.
For the computation of the Bass Strait geoid, 35375 point gravity observations
were selected from the Australian Gravity Data Base, from within the area -
35.9 sos-43.2? and 139.4 SAs150.4'. The computation procedure and software
used was developed by A/Prof. A.H.W. Kearsley of the School of Surveying,
University of N.S.W., and is described in HOLLOWAY (1988) and KEARSLEY
(1988).
In Table 6 we summarise the results of the determination of Sea Surface
Topography ( g+) at the tide gauges on the Australian mainland and along the
north coast of Tasmania (derived from equation (6)) using the various ellipsoid
height determinations by GPS (Table 5), and the determination of geoid height
by the gravimetric technique ( N;gsi 1
give optimum single-session solutions using a variety of dual-frequency data
processing strategies that aim to resolve all (or at least some) of the cycle
ambiguities.
The tasks of orbit determination, improved tropospheric modelling and
overcoming the reliance on Portland's coordinate value, are best tackled
together using a suitable GPS reduction package. Presently undergoing testing
is a revised GPS reduction package, based on the earlier USMASS programs
(RI22OS & STOLZ, 1988), capable of the simultaneous processing of GPS data
from a global network of stations (in addition to the Bass Strait sites) to estimate
orbits and station coordinates. By ''floating' all the Bass Strait sites, and holding
the CIGNET stations ''fixed'', we can define a good datum for the Bass Strait
network, while at the same time ensuring the satellite orbits are determined
reliably. It is envisaged that this software system will incorporate the algorithms
presently employed by the P.C. software system described in RI22OS et al (1990).
Several graduate students are involved with these analyses as part of their
thesis projects. The results of the recomputational effort will be available in the
next 12 months.
The Australian Research Council supported this project. Many people have
generously provided assistance during the planning and execution of the GPS
survey. We would like to particularly acknowledge the support of several
postgraduate students from R.M.!. T. (N. Talbot, G. Gerdan, C. Hill) and U.N.S.W.
(W. Fu & P. Hung). Mark Groskops of Sydney University assisted with the
observations. Special thanks are given to Rod Eckels of Wild-Leitz Pty. Ltd.,
Sydney, for his advice and help with the operation of the WM102 receivers, as
well as with the PoPST and GEOLAB software. The Antarctic Division,
University of Melbourne, kindly made available their WM102 receivers for the
field campaign.
The range of computer applications in the law is
wide. It extends from general applications, of use to
lawyers, to applications designed specifically for the
law. This paper is concerned only with those systems
that make use of artificial intelligence (A1) techniques
to solve legal problems.
lLegal Al systems can usefully be divided into two cate-
gories: legal retrieval systems and legal analysis systems.
Legal retrievalsystems allow lawyers to search through
databases, containing details of statutes and decided
cases, for information. AI techniques may be employed to
simplify this task (egby searching for keywords which
have not been input by the user but are deducted to be
equivalent to, or sufficiently related to, the input key-
words).
Legal analysis systems take a set of facts and deter-
mine the ramifications of those facts in a given area
of law.
(McCarty (1980a) identifies a third category of
legal AI systems: integrated legal systems. He cites as
an example computerized title registration systems
which make decisions about people's rights and obli-
gations. It is hard to see why such a system could not
be usefully classified as a legal analysis system, albeit
with some of the features of a legal retrieval system.)
Mehl (1959) claims that there is no fundamental
difference between these two categories (legal re-
trieval systems and legal analysis systems)--that the
difference is one of degree only. However, Shannon
and Golshani (1988) point out that the difference
between systems based on a ''conceptual model of
legal analysis'' and text-retrieval systems is that the
latter do not ''understand'' any area of the law.
This paper will be concerned with legal analysis
systems.
Legal analysis systems can be divided into two cate-
gories:
The idea of a judgment machine was raised over
A number of projects have focussed on representing
the provisions of a statute as a set of rules. When
these rules are applied to the facts of a case (i,e. by
instantiating previously free variables) an inference
engine can produce an answer which represents the
effect of the statute on the given facts. For example,
the British Nationality Act has been encoded as
a PROLOG program (Sergot, Sadri, Kowalski,
Kriwaczek, Hammond and Cory, 1986).
Shannon and Golshani (1988) claim that extract-
ing the rules from a statute in an ad hoc fashion is
unsatisfactory because:
Although it is not possible to check such rules for
correctness, it must be remembered that lawyers are
similarly unable to check their own interpretation of
a statute for correctness. It is up to the knowledge
engineer to check that the rules are an accurate rep-
resentation of the statute. Shannon and Golshani
Suggest that it is unlikely that a mechanical method
can be developed for transforming statutory language
into formal rules, but methods have been developed
which (if followed rigorously) reduce the likelihood of
error. The use of such methods would also reduce the
scope for dissimilar rule formulations. The first prob-
lem (being unable to trace the evolution of the statute
to the rules) can be obviated by the sensible use of
comments.
No area of law is covered exclusively by statute. Even
a new statute, which has not specifically been the
subject of any case, is interpreted in the light of
previously-decided cases. Case law (or the common
law) is fundamental to the Australian legal system,
which relies heavily upon the doctrine of precedent.
This doctrine states that each decided case is not
merely an example that later judges may choose to
follow, or to ignore: that case, itself, becomes part of
the law. This means that any useful LES must take
account of the law embodied in previously-decided
cases.
The problem of representing case law is different
from, and more complex than, that of representing
statutory provisions. If a statutory provision is open-
textured, the courts give meaning to that provision.
When faced with this open-textured concept, the LES
builder has two options:
The first option is satisfactory only if the LES is being
used by a legal expert who is (presumably) in a pos-
ition to answer the question. This approach would
surely reduce the LES's usefulness. Shannon and
Golshani (1988) opt for a combination of both
approaches:
This model allows some room for reasoning with the
facts but relies on the user for input when no clear
inference is found. We add depth to our model by filling
in the basic rules and definitions of the statute with
additional factual examples from decided cases. (p. 311)
This simplistic solution to the problem of open tex-
ture has severe limitations, as discussed in $4.2.1.
Tyree, Greenleaf and Mowbray (1988), with their
FINDER system, take a completely different approach
to the problems posed by the common law, The area
of law which they chose to model is based entirely on
cases. They claim that the number of decided cases
in any given area of law is usually so small that
inductive tree generation algorithms cannot be used.
Further, they suggest that it is inappropriate to model
case law using a rule-based system:
It is not that it is theoretically impossible to write such
rules, but that it is not the natural way in which lawyers
reason with cases. (p.232)
In fact, it is not possible to formulate production
rules which will adequately represent case law,
because such a rule-based system would be of little
use to a lawyer. It is not just that rule-based reasoning
''is not the natural way in which lawyers reason with
cases'' Such a system may be capable of producing
an answer (possibly with an attached estimate of its
probability). But a lawyer is not interested in a de-
finitive answer-even if it is strongly suggested by a
long line of legal authority-because it doesn't assist
the formulation of their legal argument.
As discussed in $3.2, a lawyer reasons with cases by
arguing that there are no legally significant differ-
ences between the instant case and a previously-
decided case whose result is desirable, andlor that
there are legally significant differences between the
instant case and a previously-decided case whose
result is not desirable.
No amount of reason extraction from an inductive
rule-base will provide the informaton that a lawyer
needs to argue in this fashion. Hence, attempting to
reduce the results of previously-decided cases to rules
which can be simply added to a statutory rule base is
an inappropriate approach to the problem of open-
textured concepts. As Tyree et al. (1988) state, such
an approach does not reflect the way in which law-
yers reason about cases. But, more importantly, it
makes for an inadequate LES.
The FINDER system of Tyree et al. (1988) takes the
following approach to cases. Expert knowledge is
used to determine the most important cases in a
given (fairly small) area of law, and the attributes
which are of legal importance to the outcome of
those cases. These attributes are given weights--not
by a legal expert, but by examining the extent to
which each attribute differs across the cases. Using
these weighted attributes it is possible to measure
statistical nearness (similarity) between the cases.
When the facts of the instant case (l.e. those attri-
butes which are of legal importance) are entered, the
nearest previously-decided case (the nearest
nneighbour) is ascertained. If the attributes of the
nearest neighbour are the same as those of the instant
case then the advice is clear. When the attributes of
the cases differ, FINDER gives details of the nearest
neighbour, and lists the differences. The system also
finds the nearest case which reached the opposite
conclusion to that of the nearest neighbour (the near-
est other). That case, and the differences between it
and the instant case, are explained. To reduce the
chance of giving bad advice, several statistical tech-
niques are employed to ensure that the nearest case is
not greatly different from the instant case.
Two distinct forms of knowledge representation in
LESs have been identified. Rules are appropriate for
representing statutory law. They can also be used to
represent case law, but this approach is inadequate.
Alternatively, a set of attributes can be identified for
each of the relevant cases. By comparing these attri-
butes with those of the instant case, a statement can
be made about the common law as it relates to the
1nstant case.
None of the systems discussed in this paper (and,
to the author's knowledge, no previously developed
system) has incorporated both of these methods of
legal knowledge representation: a rule-based system
combined with a case-based system.
sHYSTER (Popple, 1990c) is a prototype of an LES
which combines a rule-based system with a case-
based system (similar to the case-based FIINDER system).
When the rule-based system encounters an open-textured
concept, the case-based system is employed to produce a
legal argument as to the meaning of that concept.
This eclectic approach has a number of benefits:
This paper has discussed previous developments in
LES design and has shown how a purely rule-based
approach is inappropriate if an LES is to be of use to
a lawyer A better approach (combining rule-based
methods with case-based methods) has been outlined
and it is suggested that LESs which incorporate this
approach will prove to be fruitful objects of research.
The author would like to thank Roger Clarke, Peter
Drahos, Bob Moles, Malcolm Newey, Robin Stanton
and Neil Tennant for their comments and suggestions.
This research is supported by an Australian
National University PhD Scholarship (funded by the
Centre for Information Science Research) and by a
Sigma Data Postgraduate Research Award.
James Popple is a PkD student at the Department of
Computer Science. Australian National University He
has degrees in law and comptuter science, and is inter-
ested in the areas where those two disciplines overlap:
computer applications in the law: and the legal impli-
cations of computer technologyy Mr Popples doctoral
thesis research focusses on the development of legal
expert systems.
Based on the decision for this design concept, ap-
propriate technical specifications, which describe
basic requirements for the design, can be written
out. At this stage it is still a rough ''conceptual de-
sign,'' and any specific system configuration (e.g..
process plan, material-handling equipment, and
control systems) has not been determined yet. Such
details will be determined in the next two steps.
Step 4: Select feasible design alternatives, In this
step, the designers specify more technical details of
the design and find feasible alternatives for their
design. At the same time they impose some con-
straints on the design stemming from the design ob-
jectives and/or the analysis tools. This stage is one
of the most important steps in a DA-based design
process, The designers need to think of the feasible
design alternatives in conjunction with the analysis
tools applicable to the design. Thus the designers
are required to possess sufficient knowledge of the
analysis tools as well as the key design variables in
the design, so that they are capable of analyzing the
design alternative by using the tools. The design
constraints arising from the design objectives and
strategies can eliminate alternatives which do not
achieve the design objectives; also, the design con-
straints stemming from the analysis tools can elimi-
nate alternatives which cannot be analyzed by the
tools. As a result, only the alternatives which sat-
isfy the design objectives and the analysis feasibility
will be selected for the futher detailed design activi-
ties. It is important to note, however, that there
might be some cases where screening alternatives
by analysis feasibility will not conform to the design
objectives. For example, if the limitation of the tool
diminishes the major design objectives, screening of
alternatives by the tool is not appropriate. The de-
signers need to think of other tools for analysis, or
even of the validity of the design project, due to the
inability for analysis.
Step 5: Select and specify design parameters,
After the proper design alternatives are chosen, the
parameters of the design must be selected and spec-
ified. Those parameters could be (again, in case of
manufacturing-systems design) parts produced,
process plan, types of machines, number of ma-
chines, routings, lot sizing, system control logic,
and so forth. Just like the design alternatives, de-
Sign parameters can also be the objects of the con-
straints. As exemplified by the fixed instruction
length in the RISC case, the design strategy can
constrain the design parameters, or (as in the geo-
metric features in the PADIL case) the design tool
can constrain the design parameters.
Step 6: Evaluate the design, When the designers
complete their initial design, they then evaluate the
design by using a simple tool. A quick evaluation by
the simple tool is the key in the DA-based design
process. The results of the evaluation will show the
designers directions for improving the design,
which might be a change in design parameters to
adjust (or hopefully optimize) the design to satisfy
the design requirements, or a major change in
choosing the design alternatives. The designers iter-
ate along this process until the design satisfies the
primary design needs.
Step 7: Implement the design. The completion of
the above process allows the designers to imple-
ment the design. The implementation could be man-
ufacturing of the designed product or installation
and operation of the designed manufacturing sys-
tem.
As described, the major distinction between the
traditional design process and the design process by
DA is the use of the design constraints and simple
tools, as the rounded blocks in Fig. 13 show. The
presence of these blocks in the design process
makes DA unique and yields the design with the
hypothesized benefits discussed in the earlier sec-
tion.
Our case studies, motivated by the idea of DA,
brought forward many interesting issues, of which
we discuss three main ones here: (1) use of con-
traints for design; (2) use of simple tools for analy-
sis; and (3) DA as a methodology to use both design
constraints and simple tools effectively. Figure 14
shows the relationship of these three issues. We
think each one of these issues is worthy of further
investigation. To conclude this paper, we refer to
these issues together with some future research di-
rections.
First, the case studies certainly pointed out the
benefits of using constraints for design or, in short,
''constrained design.'' Whether the design con-
A team of designers at a well-known computer man-
ufacturer was developing a new VLSI chip for a line
of workstations. In the past, this firm had routinely
used a particular CAD tool for the design of layouts
for printed wiring boards (PWB). Following the lay-
out design, other groups in the firm would design
the manufacturing process and test methods. There
existed computerized interfaces from this CAD tool
to the CAM tool used by these other groups, which
made it very easy for these other groups to analyze
the impact of different processes and testing ap-
proaches on the production of PWBs.
In this instance, however, the design team felt
that the existing CAD tool did not have sufficient
functionality, and they decided to use a new CAD
tool for their layout. As a result, they were able to
design more sophisticated layouts. On the other
hand, after the design was completed and the other
teams started to work on the process and testing,
the chip development ran into some major prob-
lems. Most of all, because of the lack of compatibil-
ity of the database of the new CAD tool with the
existing CAM tool, several additional man-months
of work were required to design the test proce-
dures. This delay caused more harm than merely
the cost of the extra time. The product was targeted
to reach the market in a very short time; thus a
schedule slip had the potential of enormous dam-
age. A test program was finalized within the overall
D&D time requirements, but because of the time
pressure on the test designers, its quality remained
to be seen. The potential cost associated with poor
test coverage and diagnosis could be far greater
than the benefit gained by using the new CAD tool.
Of course, there are lessons from this case study
for the fields of ''simultaneous engineering'' and
''design for manufacturability''; however, what do
we have to learn from this case study as far as DA is
concerned? First, our concepts would suggest that
the new CAD tool did not satisfy the requirements
of DA, especially if we define analysis in the wider
sense of ''analysis done at all stages of the D&D
cycle,'' With this view, the new CAD tool should
have been ruled out as offering only ''local'' im-
provements (i.e., to the designers), rather than
''global'' improvements (i.e., to the whole D&D
process). Second, this case also teaches us that for
effective application of DA, we must be careful to
apply the yardstick of DA to the whole D&D pro-
cess, and not just to a portion of it. Finally, we
should note the relation between this case study and
the PADIL case. In the PADIL case, one could con-
ceive of the original part designer discarding PADIL
1.0 in favor of a more ''sophisticated'' PADIL sys-
tem to have more functionality in the design tool.
That case study shows that by staying within the
limitations of PADL 1.0, the designer would actu-
ally have designed better parts. Similarly, in this
case, staying within the constraints of the existing
CAD layout tool would have led to a better design
process.
Car bodies are composed of subtly expressed aes-
thetic surfaces. Traditionally, the design of such a
body has depended on repeated trial and error, con-
suming many man-hours, trial clay models, and
stamping dies. This design process is quite long: it
takes about 2 years for an automobile manufacturer
to complete the styling design (Fitzgerald, 1987).
Shortening this design process is therefore crucial
to every automobile manufacturer. A CAD/CAM
system would be a powerful tool for this; however,
standard CAD/CAM systems do not have appropri-
ate modeling and analysis capabilities to cover the
complicated design process. Thus, Toyota devel-
oped its own CAD/CAM system for the car body
design process (Ohara, 1984). The Toyota CAD/
CAM system, consisting of four subsystems, is
comprehensive: it covers all processes for car body
engineering and replaces many previous time-con-
suming processes. Among the four subsystems, two
provide interesting case studies from the point
of DA.
3.4.1 Toyota-case l: car body styling design.
The entire car body design process starts with body
styling design. For such design, it has been common
to use spline curves to model free-form surfaces of a
car body. This method, based on the interpolation
of a point sequence, yields smoothly connected
curves; however, such curves do not necessarily
satisfy designers' aesthetic requirements for car
bodies. For these aesthetics, the curves must be
smooth in the curvature distribution and in the high-
light lines. [Highlight lines are the image reflected
from the car body surface when it is illuminated by
parallel fluorescent lamps placed above it. (see Fig.
9)]. Also, if a CAD system for such design is in-
tended to be used in an interactive mode, the sys-
tem should be easy to manipulate and quick to re-
spond to designers' inputs, To satisfy these
requirements, the CAD system development team
needed to have both clear technical requirements
ing evidence at that time, but still, based mainly on
our intuition and engineering knowledge, we pro-
ceeded to extend the original hypothesis about the
benefits of DA. Specifically, we hypothesized that
DA as an overall methodology could result in the
following, somewhat less obvious but no less impor-
tant, benefits.
Anyone familiar with current trends in design and
manufacturing will immediately recognize that
these benefits could give a corporation significant
competitive advantage. Thus in our thinking about
DA, we should view this concept not just as a de-
sign refinement approach, but rather as a (poten-
tially) strategic weapon that could be a significant
addition to the arsenal of a manufacturing firm
(Suri, 1988b).
The purpose of the above sections has been to
clarify our thinking on the basic concepts of DA.
We now proceed to the case studies on design pro-
cesses that will assist us in solidifying the ideas and
hypotheses of DA.
The five case studies presented below are drawn
from design of actual products or systems. Note
that they are not necessarily situations where DA
was precisely practiced. Rather, they allow us to
deduce how some of the principles of DA influenced
(or might have influenced) the design processes.
Thus our efforts in analyzing the case studies will be
to understand the influence of these principles.
Recently, the development of computer-based tools
for the design of mechanical parts has been quite
remarkable, especially in the area of solid modeling
(see, e.g., Voelcker, 1988). Constructive solid ge-
ometry (CSG), one of the more promising solid
modeling methods, uses Boolean combinations of
simple primitive solids to describe an object. In the
development of CSG systems, a natural question
that arose was, ''What type of geometric primitives
are necessary and how well do such primitives rep-
resent actual parts?'' In order to gain insight into
this question, in 1974-1975 the Univerity of Roch-
ester along with Xerox Corporation made a careful
survey of the geometrical characteristics of the
functional parts used in a tabletop copier (Samuel et
al., 1976; Voelcker, 1988). The purpose of the sur-
vey was to generate data on part geometry to guide
the design of PADL (Part and Assembly Descrip-
tion Language), a CSG-based modeling language.
The following versions of the PADIL language were
used for studying a total of 128 mechanical parts.
Each version of PADL includes a different set of
primitives, as follows (with version 1.0 being the
simplest):
Figure 5 (Voelcker, 1988) shows some results of the
survey. It plots the percentage of surveyed parts
that could be modeled as a function of the number
of primitives used. For example, PADIL 1.0 could
describe about 30% of the parts, with the largest
PADL 1.0 definition requiring about 45 primitive
instances; while PADL 2.8 could describe 99% of
the parts, with the largest definition requiring some
500 primitive instances.
An important finding here, related to D/A, is the
curve labeled ''Redesigned to PADL 1.0.'' Origi-
nally, PADIL 1.0 could represent just 30% of the
parts. However, when a designer was given the task
of trying to redesign the remaining parts just with
PADIL 1.0, he was able to redesign about another
30% of the parts even under the constraints im-
questions. The specialists are en-
vironmental experts in the aggre-
gate industry and at least one per-
son fields calls from 8 a.m. until 4
p.m. every weekday.
Minimally, the specialist asks a
routine set of format questions
and the plant in question receives
a follow-up ''green note.'' A green
note describes in brief what re-
portedly happened at the plant,
and includes a generic set of prob-
lem-solving guidelines. Also, the
plant manager meets with people
from the environmental affairs di-
vision of the company. Only in se-
vere situations are the employees
asked to meet with the environ-
mental affairs representatives.
So far, Kissley reported, em-
ployees have been using the hot-
line on a regular basis. The hot-
line program has been initiated
throughout the Michigan, Ohio,
and Indiana operations, and after
an evaluation period, it will be in-
stalled in all the 50 or so company
operations in the Midwest.
The 28th International Cement
Seminar will be held in San Anto-
nio, Texas, at the Marriot River
Center from Nov. 30 to Dec. 3,
1992.
Speakers interested in making
a presentation at the seminar are
requested to submit a summary or
outline of their presentation to
Richard S. Huhta, no later than
July 30, 1992, for review.
For further information on sub-
mitting papers, please contact:
The National Lime Association's
(NLA) annual convention, held
near Orlando, Fla., in April, was
marked by the organization's
92nd birthday
The four-day convention in-
cluded lectures on coal technology
and global warming, asphalt re-
search, the removal of lead in lime-
treated drinking water, and more.
This year's convention, which
nearly 145 NLA members attend-
ed, bore little resemblance to the
association's first assembly at
Cincinnati. Fewer than 15 people
attended that 1902 meeting,
which was organized by then Rock
Products Editor Edwin H. De-
febaugh. By 1903, the group had
grown to nearly 30 members, and
its first slate of permanent officers
was elected. Today, the organiza-
tion claims 34 member companies.
The Portland Cement Association
(PCA), in conjunction with Rock
Products magazine, has scheduled a
gregate operation's strongest and
loudest critics. ''Without this ap-
proach, you won't have the credi-
bility to make something like this
go,'' said Van Overbeek;
In response to President Bush's
call for regulatory review by gov-
ernmental agencies, the National
Stone Association (NSA) sent two
letters, one to the Department of
Labor and another with the Envi-
ronmental Protection Agency
(EPA), stating that regulations
placed burdens on aggregate com-
panies, and the American econo-
my in general, without concur-
rently benefitting either the
workers or the environment.
In the 12-page letter sent to the
Department of Labor and ad-
dressed to Roland H. Droitch,
Deputy Assistant Secretary for Pol-
icy the following points were made:
This paper attempts to estimate the rate of
return on R & D investment in the Japanese phar-
maceutical industry, The industry is an interest-
ing object for such a study because, first and most
obviously, it is the most R & D intensive of all
industries in virtually any country. In Japan in
1987. R & D expenditures as a ratio of sales were
7 percent in the pharmaceutical industry, far
higher than the manufacturing average of 3.1
percent and more than one percent-point higher
than the second most research-intensive industry,
communications and electronics equipments (5.8
percent). This tendency is common elsewhere: in
the US in 1985, the same ratio was 8.4 percent in
the pharmaceutical industry as compared to 4.2
percent for all industries. Whether such a high
research intensity is justified by the rate of return
is a question worth investigating.
Second, R & D in the pharmaceutical industry
can be assurmed to be purely product-oriented.
Generally speaking, the aim of industrial R & D is
to increase the demand by developing new prod-
ucts (or improving existing products) or to de-
crease costs by improving production processes.
In many industries it is extremely difficult to
estimate how much of the R & D expenditures are
aimed at product innovation and how much at
process innovation. In the pharmaceutical indus-
try, however, it is reasonable to assume that basi-
cally all the R & D efforts are directed at product
innovation, that is, to invent new drtugs, Our
estimation of the rate of return will be made
taking advantage of this fact, that is, by estimat-
ing the contribution of R & D expenditures to
new drug development.
Third, a common complaint among pharma-
ceutical companies is fast imitation by generic
producers which reduces the returns to original
inventors. Thus, the private returns are argued to
be srmaller than the social returns, and govern-
mental support is called for to make up for the
reduced private incentive to research. It is there-
fore important from the public policy viewpoint
to estimate the social rate as well as the private
The marginal contribution of R,, on N,'. to be
denoted by af, can be estimated by multiplying
the estimated a, coefficient with the average
value of N,*/R,;.
In the return function we assume that profits
in year t, P,, depend on the stream of new drug
inventions in the past, N, N, ,, N,.:- -.,. The
effect of N, ., may increase with j in the begin-
ning because doctors may be at first unaware of
the drug or uncertain of its effect. However, after
several years, the effect will start diminishing
because rival drugs will appear either by imitation
or by developing substitutes. Taking this fact into
account and assuming a Koyck-lag structure for
the diminishing part, we have
which can be transformed into
As explained earlier, the government sets the
prices with which the NHI reimburses doctors for
their use of drugs. These listed prices are ex-
pected to affect the profitability of pharmaceuti-
cal companies, even though the firms offer dis-
counts to doctors depending on market condi-
tions. The listed prices are revised every year,
mostly in a downward direction, reflecting cost
decreases, development of substitutable drugs,
and so forth. The revision is modest in most
years; however, a substantial cut of 17 percent on
average was made in 1984. To examine if this cut
has reduced the profitability of the companies
and the industry, we added a dummy variable D
(= 1 in 1984 and the following years) to eqn.
(3)
We estimate this equation assuming further
that the coefficients on the N variables of the
right-hand side follow a second-order Almon lag
structure, and putting a constraint that the near-
end coefficient must be zero, that is, the effect of
N,.; on P, must be zero, Again there is a trade-off
for the choice of j; a larger j is desirable in itself
but reduces the degrees of freedom in view of the
limited length of the available time-series data.
After some experiments we have settled on j = S.
Once eqns, (1) and (3) are estimated, the cal-
culation of ROR is straightforward. Suppose that
the firm increases its R & D expenditures by 1/af
in vear t - k. Then bv (1) the number of NCEs is
expected to increase by l in year . This increase,
in turn, is expected to increase future profits
through (2). Therefore, the internal rate of return
on R & D, i, can be calculated by solving the
following equation and using the estimates of af,
bb,.- . ,4 and 6: '
The data for all the variables, N, R, H and P,
can be obtained both at the company level (for
public companies) and at the industrial aggregate
level, We thus estimate eqns, (1) and (3) at both
of these two levels. Equation (1) estimated with
company data can be interpreted as showing the
private productivity of R & D activity while the
one estimated with industrial data can be inter-
preted as showing the social productivity. The
difference arises due to the spillover of knowl-
edge. That is, R& D in company A may benefit
company B through the spillover of general or
specific knowledge that A's research generates,
and this effect is included when industrial re-
search output is regressed on industrial research
input but excluded when only A's research output
is regressed on A's research input.
Similarly, the coefficients of eqn. (3) estimated
with company data are interpreted as showing the
private marginal returns on NCEs while those
it is not enough that the inflow affects the R & D
productivity or the profit contribution of new
drugs. It has to affect it differentlv between the
ten largest firms and smaller firms. There appear
not many reasons to expect this to be happening.
The drugs developed outside of Japan, classified
as licensed NCEs, have to be approved by the
Japanese government before going to the market.
Since our study only considers the contribution of
in-house NCEs on company or industry profits,
the imported or licensed drugs can affect the
difference between the company and industry
cquations only if licensed NCEs affect the profit
contribution of in-house NCEs through, say,
product substitution and, in addition, this effect
is different between the ten largest firms and the
smaller firms. We find little reason to expect such
differential effect and therefore assume that our
comparison of private and social ROR is inde-
pendent of the inflow of knowledge or new drugs
from abroad.
The data for N and H were obtained from the
Ministry of Health and Welfare. For the company
data, R& D expenditures and profits were ob-
tained from the companies' financial statements.
For the industrial data, R & D expenditures were
taken from the Report on the Surrey of Research
and Development and profits from the Census of
Manufactures. Interviews with companies suggest
that some of the expenditures made to test NCEs,
such as payments to those hospitals participating
in clinical tests, are often reported as a part of
general administrative and marketing expenses
rather than R & D expenditures. To the extent
that the R & D expenditures are thus underval-
ued, the coefficient of R,, in the R & D function
may be overestimated.
Profits are defined as net of capital costs where
capital costs were calculated, for company data,
by multiplying the equity capital by the ROR on
the Public Utility Bonds (in addition to interests
paid on debts) and, for industrial data, by multi-
plying the fixed assets by the same ROR. Because
only the cost of capital on fixed assets could be
subtracted in the industrial data, there is a possi-
bility that industrial profits are overestimated.
These profits are gross of R & D expenditures,
and both profits and R & D expenditures are de-
flated by appropriate price indexes to the 1980
price (in billion yen). The data for NCEs were
available for the period 1967-86 and that for
R & D expenditures, 1966-86. '' We estimate the
two cquations with these time-series data but
because of the limited data period and because of
the lag structure imposed on either equation, the
degrees of freedom are small.
The estimation results for the R & D function
in table 2 indicate that, both for the private and
the social (except the case of nine-year lag), the
fit improves and the coefficient of R increases as
a longer lag is taken. The difference across alter-
native lags may be due to the different sample
periods (shown in the far right of the table) which
were the longest available for respective estima-
tions. To eliminate this effect we re-estimated
eqn. i) using the same period as in (vi). The
result is eqn. (vii). In the private equation this
result is better than (i) with a positive coefficient
of R, but the fit is poorer than (vi). Hence, the
R & D-NCE lag appears in fact large in the pri-
vate equation, In the social equation too, the
coefficient of R is significant in (vi) but not in
vii), indicating that a longer lag with a fixed
sample period (for NCE) improves the fit.
Another finding in table 2 is that the coeffi-
cient of R is always larger in the social equation.
It is therefore suggested that the spillover effect
is present. The coefficient of H, the proportion
of in-house development, is always positive in the
social equation as predicted but not in the private
cquation.
Since the estimation was made with a log-lin-
ear specification, the coefficient a, gives the elas-
ticity. The table shows that, with the lag of eight
to nine years, this elasticity is in the range of 0.4
to 0.5, suggesting diminishing returns. The result
differs from Jensen's [5] finding with a panel data
of 28 firms and 11 years in the US that the
elasticity is not significantly different from unity
except for very small firms. As discussed in foot-
note 5 above, the use of time-series data in the
than eight years. Also, the best cstimation for the
social ROR was obtained with a lag of eight
years. Hence, the lag between major R & D ex-
penditures and the approval of NCEs appears to
be eight years or more. This inference was sup-
ported by the interviews the authors had with thc
Japanese pharmaceutical company executives,
This paper estimated both private and social
rates of return on R & D in the Japanese pharma-
ceutical industry, by estimating the R & D func-
tion, that is, the contribution of R & D expendi-
tures to new drug development, and the return
function, that is, the contribution of new drugs to
profits. The main finding was the excess of the
social ROR over the private ROR. We thus in-
ferred that both spillover and generic effects are
present, where spillover effects refer to the
spillover of one firm's scientific and technological
knowledge to other firms, and generic effects
refer to the contribution of a new drug to indus-
trial profits through generic production. Because
the estimated social ROR is the quasi-social ROR
that does not take consumers' surplus into ac-
count, the true social ROR must have been even
higher. Therefore, one may be tempted to argue
that measures should be taken to protect the
inventor's interest further and limit the generic
producer's entry so as to increase the incentive
for innovation, or to promote government sup-
port in the form of research subsidies or govern-
ment research institutions, such as the US Na-
tional Institute of Health.
However, one should be aware that the excess
of estimated social ROR over the private ROR
may have been caused by the definitional differ-
ence between industry and company data, and
the heterogeneity between large and small firms.
In the estimation, sample size was limited and the
statistical fit was not always satisfactory.
Furthermore, other external effects of new
drug development have to be examined, for in-
stance, the effects to the markets of substitutes
and complements, the effects on unemployment
and income distribution, and the effects on R & D
in technologically related fields: see Mansfield et
al. [6]. It is very difficult to assess these effects
because the invention of a new drug can influ-
ence other markets in many different ways. For
instance, if an anti-cancer drug is invented, it may
increase the demand to hospital beds by delaying
death or decrease it by curing patients at an early
stage. Although it is beyond the purpose of this
study to inquire into these effects in detail, such
inquiry is imperative before any policy proposal is
to be made.
The private rate of return was estimated to be
19 percent with the lag of nine years. This esti-
mate seems reasonable in two regards, First, many
estimates of ROR by means of regressing TFP
growth on R & D intensity (see the references in
section 3) have been about the same magnitude
or, if anything, slightly higher. Second, if the risk
premium is subtracted to take account of the
inherently risky nature of pharmaceutical R & D,
the estimated ROR appears comparable to the
ROR on physical or financial assets. Hence, the
cqualization of risk-free ROR over different types
of assets seems roughly realized, and the high
R & D intensity of the industry seems to be the
result of rational corporate behaviour.
lof repair of the cables after their failures, time to first,
second, . . .,sixth failures were considered to be
ASSociated with binary covariates ;, - . . , respec-
itively. For a particular time to failure only one of
fthese covariates will have a value of one to indicate
3whether that time to failure is first, second,.. . , or
sixth, failure. In most of the cases two types of repair
were done on the cable, either a new joint was
provided or some repair work at the place of the old
joint was performed. To estimate the effect of these
two types of repair one more covariate, 4, Was
considered with the value one if a new joint was
hazard' were plotted against time for both types of
cables. There was no reasonable deviation from
parallelism of the two curves, one for the first failure
'snd the other for the remaining failures, as shown in
Fig. 3. It is clear from Fig. 3 that the hazard rate for
the time to the first failure is less than compared with
the time to the next failures for the cables as the curve
representing the hazard rate for the time to the first
failure always lies below the curve representing the
times to the second and higher failures.
Again, taking the cable types (covariate z;) as
strata, the estimated log-cumulative hazard rates were
plotted against time to failures to verify the
proportionality assumption of the hazard rate for
cable type A and cable type B. These curves are
approximately parallel as shown in Fig. 4. The figure
suggests that the hazard rate for the cable type B is
less than the cable type A as the curve representing
the estimate for the log-cumulative hazard rate for the
cable type B always lies below that for the cable type
Hence, on the basis of the above results it can be
recommended that cable type B be used so that
unplanned interruption of production can be reduced.
The PHM technique can be used as an explanatory
tool to seek explanations for undiscovered facts and
on that basis decisions for selecting the suitable
material or design for an item or a component can be
taken.
The authors want to thank Mr. Mats Larsson, Mr.
Owe Söderstedt and Mr. Lars Sterner, LKAB Kiruna
Mine, Kiruna, for providing data and other help
during this study. The authors also thankfully
acknowledge financial support from the research
organization 'Gruvteknik 2000'.
Since the overall optimization process requires a
simultaneous satisfaction of the objective function and
the constraints, a decision or selecton of a set of
design variables is made by assuming that the
constraints are non-interactive and the logical and
operator corresponds to intersection. Because of
symmetry of this procedure with respect to goals and
constraints, there is no longer any distinction between
the objectives and the constraints of a decision
process.
This paper presents a method for the solution of the
fuzzy multiobjective problem based on: (i) finding the
solution of the individual single objective optimization
problems; (ii) determining the best and worst
solutions for each of the objective functions; (iii)
using these solutions as the boundaries of the fuzzy
ranges in the corresponding fuzzy optimization
problem; and (iv) solving the resulting fuzzy
optimization problem. The computational details are
presented through the following steps:
Both the crisp and fuzzy optimization methods
outlined earlier find the solution of continuous
variable problems. However, the reliability and
redundancy apportionment problems involve both
continuous and integer design variables. Due to the
lack of suitable techniques to solve mixed integer
nonlinear programming problems in an efficient
manner, a heuristic scheme, termed Approach 1, is
used in conjunction with the techniques outlined in
the previous sections. A descriptive flow diagram of
the computational procedure in Approach 1 is given in
Fig. 1 in the context of maximization of reliability.
This diagram can be suitably modified for handling
multiobjective and fuzzy optimization problems.
Two of the most significant forms of air pollution
in the home are environmental radon and environmental
tobacco smoke (ETS), also termed passive smoking. The
National Academy of Sciences'' estimates that the life-
time probability of fatal lung cancer following exposure
to 1 working level month (WLM)% is approximately 350
x 10-%, With a mean concentration of 1 pCilL of radon
in the United States,' the lifetime probability of fatal
cancer would then be 4.6 x 10- This estimate assumes
continuous exposure to age 70, a latency period of 20
years, and an equilibrium fraction (ratio of density of
alpha emissions from radon progeny to the density which
would be produced under conditions of complete radio-
active equilibrium) of 0.5.' Although risk estimates for
ETS are less well established, a recent review of such
estimates suggests that the lifetime probability of fatal
cancer is increased by 30% for never-smoking individ-
uals living with a smoker.' If it is assumed that radon
exposures are distributed evenly across populations ex-
posed and unexposed to ETS, this value of 30% may be
attributed primarily to ETS exposure alone rather than
to differential exposure to radon in the ETS-exposed
groups. The National Research Council report'? esti-
cigarette smoke,''*' radionuclides,'% and mixtures of
the two,' The model utilizes the morphology of Yeh
and Schum'' and provides for incorporation of en-
hanced deposition at bifurcations in the passageways.'%
It also includes explicit modeling of the flow of mucus
in the passageways, an important consideration in relat-
ing the site of progeny decay to the site of deposition.
The details of the dosimetric model have been given
elsewhere- and are not repeated here.
It is important to bear in mind that risk estimates for
exposure to radon progeny depend upon an assumption of
the site of cancers in the lung. A common practice in the
field of radiation protection has been to calculate doses
from radon progeny in the fourth generation of the trach-
eobronchial region (see, .g., Ref. 1). This choice has been
based on the finding of the predominance of cancers within
this generation among the mining populations.
Two problems arise in this regard. The first is the
high prevalance of cigarette smoking within this group,
Suggesting that (at the very least)the fourth generation may
be the target site only among smokers. Epidemiological
evidence indicates that the site of lung cancer among non-
smokers is more distally located.-' In the present discus-
sion, generations 4 and 16 will be considered in estimating
the effect of ETS on risk (providing a form of sensitivity
analysis for conclusions generated here).
The second problem concerns differences in particle
size and breathing characteristics between the mine and
home environments. The miners generally were exposed
to larger particles which tend to deposit preferentially in
the upper passageways, while particle sizes in the home
are smaller. As shown in a recent paper,'' dose rates
from radon progeny as calculated through use of the
deposition model used here are almost uniform through-
out the generations of the tracheobronchial region for
ambient aerosols. In addition, recent studies have sug-
gested that the unattached progeny, ypically assumed to
deposit preferentially in the upper passageways, may be
of little significance in home exposures due to the pre-
dominance of nasal breathing with its attendant filtration
of particles, atoms or molecules with large diffusion
coefficients. For the reasons given above, both the 4*
UTONOMOUS mobile robots are one of the important
areas of application of computer vision. The advantages
of having a vehicle that can navigate without human inter-
vention are many and varied, ranging from vehicles for use
in hazardous industrial environments, battlefield surveillance
vehicles, and planetary rovers.
The problems associated with navigating mobile robots in an
indoor structured environment are reasonably well studied, and
a number of different approaches have been suggested [1]-[6].
Outdoor navigation of a mobile robot in an unstructured envi-
ronment is a more complex problem, and many issues remain
open and unsolved. The DARPA ALV project [7], the CMU
NAVLAB [8], and the University of Massachusetts mobile
robot project [9] are among some of the significant research
attempts in the area of outdoor navigation for autonomous
vehicles.
Position estimation, or spatial localization as it is alternately
called, is one of the primary requirements of any autonomous
navigational task. The environment in which the robot is to
navigate and the information available about the environment
affect the problem significantly. The techniques for an indoor
office type of environment are quite different from those called
for in an outdoor scenario. Identifying landmarks, measuring
range and/or attitude to these landmarks from the robot, and
using this information to do positional estimation is one
feasible way of spatial localization [10}-[13]. However, if the
robot does not have the ability to detect any landmarks in the
environment, these approaches do not work. The robot may
be aided in its navigational task by a preloaded map of the
environment. The problem to be solved in such a case is to
establish a correspondence between the map and the images
taken by the robot.
This paper considers the problem of an autonomous mobile
robot navigating in an outdoor mountainous environment,
equipped with a visual camera that can be panned and tilted. A
digital elevation map (DEM) of the areas in which the robot is
to navigate is provided to the robot. The robot is also assumed
to be equipped with a compass and an altimeter to measure
the altitude of the robot. In this paper, we present a method to
estimate the position of the robot in such a scenario. Typical
applications could be that of an autonomous land vehicle, such
as a planetary rover. The approach presented formulates the
position estimation problem as a constrained search problem.
The horizon line contour (HLC) is extracted from the images
and used to search the DEM for the possible locations of
the robot. Geometric constraints derived from the shape and
position of the HLC and the known camera geometry are used
to prune the search space significantly. The search strategy is
a two-stage process. Stage l is a coarse search that prunes the
search space using geometric constraints; stage 2 refines the
position using a curve matching strategy. The search algorithm
presented is made robust to the errors in the various search
parameters. Examples of the position estimation strategy using
real terrain data are presented. Preliminary results of this work
were presented in [14] and [15].
Section II briefly describes the position estimation problem
of a mobile robot and summarizes the various approaches
studied so far. Section III discusses the general problem of
image/map correspondence and previous work in this area.
Section IV describes the constrained search used in this paper
and all the details of the search algorithm. Section V discusses
the compuational complexity issues of the search algorithm
and presents the average case, worst case, and best case order
of the complexities of searching the DEM. Section VI studies
the effects of the errors in the various search parameters on
the position estimation and details methods to make the search
algorithm robust by accounting for the errors. Section VII
presents results using real terrain data. Finally, Section VIII
summarizes the work presented in this paper.
Determining the position of the robot in its environment is
one of the basic requirements for autonomous navigation. The
problem of self-location has received considerable attention,
and many techniques have been proposed for solving it.
These techniques vary significantly depending on the kind
of environment in which the robot is to navigate, the known
conditions of the environment, and the type of sensors with
which the robot is equipped.
In the following discussion, we consider a Cartesian coor-
dinate system in which the position refers to the location of
the robot (1,y) on the ground plane and the pose refers to the
orientation of the robot, i.e., the rotation about the z axis.
Most mobile robots are equipped with wheel encoders that
can be used to get an estimate of the robot's position at
every instant. However, due to wheel slippage and quantization
effects, these estimates of the robot's position contain errors.
These errors build up and can go grow without bounds as
the robot moves, and the position estimate becomes more and
more uncertain. So, most mobile robots use some other form
of sensing, like vision or range, to sense the environment and
to aid the position estimation process.
Broadly, we can classify the position (position and pose)
estimation techniques into the following types: 1) landmark-
based methods; 2) methods using trajectory integration and
dead reckoning; 3) methods using a standard reference pattern;
and 4) methods using the a priori knowledge of a world model
and then matching the sensor data with the world model for
a position estimation.
Using landmarks for position estimation is a popular ap-
proach [10][13], [16[19]. The robot uses the knowledge
of its approximate location to locate the landmarks in the
environment. The landmarks can be naturally occurring in
the environment, like the tops of buildings, road edges, hill
tops, etc. in an outdoor scenario [17], [18] or can be iden-
tifiable beacons placed at known positions to structure the
environment [12]. Once these landmarks are identified and
their rangelattitude relative to the robot is measured, the
robot's position and pose generally can be triangulated from
these measurements with a reduced uncertainty. This technique
suffers from the disadvantages of requiring the availability of
the landmarks in the environment and the reliance on robot's
ability to detect them.
In the second type of technique, the position and pose of
a mobile robot are estimated by integrating over its trajectory
,and dead reckoning, ie., the robot maintains an estimate of its
current location and pose at all times and, as it moves along,
updates the estimate by dead reckoning [1]-[5]. However, this
technique necessitates the robot's being able to establish a
correspondence between the features detected by the sensors
at the current location and those at the previous location (to
compute the trajectory of the robot). This is, in general, not
a easy problem.
The third method of accurately estimating the position and
pose of the mobile robot is to place standard patterns in
known locations in the environment. Once the robot images
and detects these patterns, the robot's position can be estimated
from the known location of the pattern and its geometry. The
pattern itself is designed to yield a wealth of geometric in-
formation when transformed under the perspective projection.
Different researchers [20}-[23] have used different kinds of
patterns or marks, and the geometry of the method and the
associated techniques for position estimation vary accordingly.
These methods are only useful for indoor robots in structured
environments.
In the fourth approach, the robot is aided in its navigational
tasks by providing a priori information about the environment
in the form of a preloaded world model. The basic idea is
to sense the environment using on-board sensors on the robot
and then to try to match these sensory observations to the
preloaded world model. This process yields an estimate of the
robot's position and pose with reduced uncertainty and allows
the robot to perform other navigational tasks. The problem in
such an approach is that the sensor readings and the world
model could be in different forms. For instance, given a CAD
model of a building and a visual camera, the problem is to
match the 3-D descriptions in the CAD model to the 2-D visual
images. This is the problem Kak et al. [24] address in their
work on the PSEIKI system. Tsubouchi and Yuta [6] discuss
the position estimation techniques used in their indoor robot,
YAMABICO. The robot is equipped with a color camera and
a map of the building where it is to navigate. Both are indoor
robots for navigating in a building.
In this paper, we consider the position estimation problem
of an autonomous land vehicle navigating in an outdoor
mountainous environment. Out approach falls into the fourth
type of technique outlined above. A DEM of the area in which
the robot is to navigate is assumed to be given. The robot is
assumed to be equipped with a camera that can be panned
and tilted, an altimeter to measure the robot's altitude, and
a compass. No recognizable landmarks are assumed to be
present in the environment where the robot is to navigate.
The DEM is a 3-D data base. It records the terrain elevations
for ground positions at regularly spaced intervals. The images
recorded by the camera are 2-D intensity images. The problem
is to find common features to match the 2-D images to the 3-
D DEM, thereby estimating the position of the robot. Since
we assume the robot has a compass in this work, we only
estimate the position and not the pose.
As pointed out earlier, one of the key issues involved in
determining the position of a mobile robot given a world
model is to establish a correspondence between the world
model (map) and the sensor data (image). Once such a
correspondence is established, the position of the robot in
the environment can be determined easily as a coordinate
transformation. In order to solve this problem, we need to
extract a set of features from the sensor data and identify the
corresponding features in the world model. The problem is
further complicated by the fact that the image and the map are
usually in different formats.
Indeed, this problem of imagelmap correspondence is of
fundamental importance not only to the mobile robot position
estimation problem but also to many other computer vision
problems in general, such as object recognition, pose esti-
mation, airborne surveillance and reconnaissance, etc. Other
work addressing this image/map correspondence problem is
described in [17], [24}29]. The work by [27}-[29] is elated
more closely to our work in this paper.
Freeman and Morse [28] consider the problem of searching
a contour map for a given terrain elevation profile. Such a
problem is encountered, for example, when locating the ground
track of an aircraft (the projection of the flight path on the
ground) given the elevation of the terrain below the aircraft
during the flight. The authors describe a solution that takes
ladvantage of the topological properties of the contour map. A
graph of the map topology is used to identify all the possible
contour lines that would have been intersected by the ground
track, So, the topological constraints of the terrain elevation
profile and the geometric constraints of the flight path are
used in estimating the location of the elevation profile in the
given map. Ernst and Flinchbach [27] consider the problem
of determining the correspondence between maps and the
terrain images in low-altitude airborne scenarios. They assume
that an initial estimate of the three-dimensional position is
available. Their approach consists of partially matching the
detected and expected curves in the image plane. Expected
curves are generated from a map, using the estimate of the
sensor position, and they match the simulated curves with the
curves in the image plane. In contrast, our method does not
assume the availability of an initial estimate of the sensor
position. Instead we derive a set of possible positions by using
a constrained search paradigm to prune the search space of
possible locations. Rodriguez and Aggarwal [29] also consider
the problem of matching aerial image to DEM's. They use
a sequence of aerial images to perform stereo analysis on
successive images and recover an elevation map. Then they
present a method to match the recovered elevation map to
the given DEM and thereby estimate the position and pose of
the airborne sensor. All the above work, however, is concerned
mainly with an airborne sensor. Our current research considers
the problem of the sensor on an autonomous land vehicle. We
use a constrained search strategy to isolate the robot's position.
One approach to this problem is to extract features from the
images and then search the map for the occurrence of corre-
sponding features. Once this correspondence is established, the
position can then be computed. However, an exhaustive search
of the entire map is usually prohibitively expensive if the map
is very large, as in our case. As an alternative, we formulate
this correspondence problem as a constrained search problem.
We extract features from the images taken by the robot, and
instead of searching for the corresponding features in the map,
we search the space of possible robot positions for locations
from where such features could be imaged. Once we isolate
these sets of possible locations, then the expected features from
these positions are generated; using a matching technique,
the exact robot location is isolated from among these sets
of possible locations. We use the horizon line as the image
feature. Instead of searching using the entire feature, we use a
subset of the feature and search the map for possible locations
where such features can be seen. In this search strategy, the
space of possible locations is first quantized to a discrete set.
Then we derive geometric constraints from the image feature
and the known camera geometry that need to be satisfied by
any of the hypothesized positions of the robot. Using these
constraints, large subspaces in this discretized search space of
the possible locations-are pruned as being impossible. Once
the search space is pruned to a manageable size, it is searched
exhaustively using the entire feature for the best estimate of
the robot location.
The DEM is a 3-D data base. It is a two-dimensional array of
uniformly spaced terrain elevation measurements. DEM's for
various areas in the US can be obtained from the United States
Geographical Survey (USGS). In this research, real terrain
data were used. The position estimation algorithms developed
were tested on DEM's of a 1:24000 scale, covering different
mountainous areas in Colorado and Texas. Typical DEM's
were 350 by 450 samples, with a spacing of 30 m between
samples. So they covered an area of typically of 140 km'. The
images recorded by the camera were 2-D intensity images.
The problem was to find common features to match the 2-D
images to the 3-D DEM.
The presented solution uses the DEM information and
structures the problem as a constrained search in the DEM
for the possible robot location. Since the robot is assumed. to
be located in the DEM, the DEM grid is used as a quantized
version of the entire space of possible robot locations. The
feature used to search the DEM is the shape and position of
the horizon line contour (HLC) in the image plane. From the
current robot position, images are taken in the four geographic
directions: N, S, E, and W. The contour of the HLC is
extracted from these images and coded. Using the height of
the contour line in the image plane and the known camera
geometry as input parameters, the entire DEM is searched for
possible camera locations so that the points in- the elevation
map project onto the image plane to form a contour of. the
shape and height we are searching for. Since searching the
DEM exhaustively for the exact shape of the horizon line is
a very computationally intensive process, we split;the search
into two stages. In stage 1, we search using the height of
the horizon line at the center of the image plane in all the
four images. Geometric constraints derived from the camera
geometry and the height of the HLC are used to prune large
subspaces of the search space; finally, the position is isolated
to a small set of possible locations. In stage 2, these locations
are then considered as the candidate robot positions, and the
actual image that would be seen at these points is generated
using computer graphics rendering techniques from the DEM.
The HLC's are also extracted from these images and then
compared with the original HLC's to arrive at a measure of
the new HLC's disparity. The robot location corresponding to
the lowest disparity is then considered as the best estimate of
the robot's position. As the results show, the approach is quite
effective and, in almost all cases, the position estimate is very
close to the actual position.
This approach is novel and has many advantages over
previous methods. It does not rely on the existence of environ-
mental landmarks and their detection. It does not necessitate
the building of a complicated world model from the sensor
observations. It is also a totally passive navigational technique
since the type of sensing assumed is a single visual camera.
The errors in positional estimation are not cumulative.
In generating the four geographic views, the camera is
assumed to have zero roll, and the optical axis of the camera
is assumed to pass through the image center. The camera's
tilt angle g, defined as the angle between the optical axis
and the horizontal plane, is adjusted until the horizon line
is clearly visible in the image. This angle is then measured
using an inclinometer. Commercial inclinometers that sense tilt
angles up to 0.005% resolution are available. The altitude of the
camera H is measured using an altimeter. This measurement
is adjusted such that the altimeter reading and the height in
the DEM are with respect to the same reference. Commercial
barometric altimeters with a resolution of up to a few tens
of feet are available. The HILC is then extracted from these
images using basic image processing techniques, and the
height of HLC at the center of the image plane in each of
these four images is measured. Let these heights be h; (i = N,
S, E, W). The reason for using the height of the HLC at the
center of the image plane is that the DEM is assumed to be
gridded along N-S and E-W axes. So, the points that project
onto the HLC at the center all lie along the same grid line in
the DEM. Using the height of the camera H, the tilt angle o;,
and the HLC height h, in one of the directions, say north, the
DEM is searched for the possible camera locations.
The algorithm Search is used to search the DEM in the
north direction. It is similar for the other directions except that
the search is carried out only in the reduced set of candidate
camera positions returned by the previous search process.
The direction and hence the limits of the search also vary
depending on the direction in which the image is taken. It uses
two pointers, one pointing to the current hypothesized camera
location, CAMERA, and the other pointing to a candidate grid
point, POINT. The idea is to assume that the CAMERA is at
a certain point (r,y). Search along the y line to see if any
POINT exists that will project at the desired height h, onto
the HLC. If so, mark this CAMERA as a possible camera
position. Repeat the procedure for all the CAMERA positions
along this y line and then for all y lines. The pseudo-code of
the algorithm is presented below.
The Algorithm Search Stage 1 of the search process for
searching in the north direction is described below.
Here, CAMERA.X and CAMERA.Y denote the z and y
grid values of the location CAMERA and POINT.X and
POINT.Y denote the values of the candidate point (see Fig. 1).
CAMERA.X is initialized to XMAX: POINT.X to XMIN;
and CAMERA.Y and POINT.Y o YMIN. Using the known
approximate camera height H and the tilt angle @y, the height
of the HLC, hN, is back projected to the POINT using the
geometry shown in Fig. 2. The elevation Zai, necessary for
this POINT to project on to the HLC is estimated using (1)
given below. The Appendix presents the derivation . The
actual height .eeuut (POINT.Z) is extracted from the DEM
data base. If aes1 is equal to u then CAMERA is
saved as a possible camera position. If aetus1 ls less than
Za, then the POINT is updated to a position closer to
the CAMERA (in this case, POINT.X is incremented). This
process is continued until POINT coincides with CAMERA.
Then POINT is reinitialized, and CAMERA is updated by
moving it closer to POINT (in this case, CAMERA.X is
decremented). If at some stage etua1 ts greater than Zca-
then CAMERA is not a possible location since if it were, the
elevation at POINT, Z.-us, would project a height higher than
hN. Therefore, if this CAMERA location was saved before, it
is discarded. We can also say that none of the points between
CAMERA and POINT are possible camera locations since at
all these, the elevation at POINT, ZZ.euai, would project a
height higher than h. This constraint proves very useful in
reducing the search space to a large extent for mountainous
terrains with many altitude variations, such as those considered
in the illustration. The search process is then repeated along
all the y lines in this direction, ie., for CAMERA.Y and
POINT.Y ranging from YMIN to YMAX. The possible camera
positions returned by this search process are then considered as
inputs for the next search, which searches among this set with
geometric constraints extracted from the image along another
direction. The process is continued by applying the constraints
in all the four directions, and the search refines the possible
locations to a small set usually clustered around the actual
location (see Fig. 14 below).
We give below an expression for the estimated elevation
ZZa, at a point in the elevation map for a hypothesized camera
location and for a known imaging geometry shown in Fig. 2.
where
f is the focal length of the camera, % is the perspective angle,
h is the HLC height at the center of the image, @ is the camera
tilt angle, H is the altitude of the camera, and z is the distance
of the point from the camera.
Another constraint used to reduce the search space is to use
the approximate altitude H of the camera. Only those points
of the DEM are considered as possible camera locations where
the elevation .e4iai lies close to H, ie., H - 8 aeuat
H 4 0y where 8y) is the resolution of the sensor used to
measure the camera's height.
Stage 2 of the search process further isolates the camera
location from the small set of possible camera locations
returned by the first stage. In this stage, each point of the
set is considered as a possible candidate, and the image that
would be seen in a particular direction if the camera were
located at that location is generated from the DEM using
computer graphics rendering techniques described in Section
VII. The HLC's from these images are then extracted and
compared with the actual image HLC in the same direction.
This is basically a 2-D curve matching problem. We have
a model curve (the HLC extracted from the image)- and
a set of candidate curves (the HLC's generated from the
candidate locations using the DEM). The objective is to find
the candidate curve that matches the model curve closest.
There is a substantial body of work in matching curves for
object recognition, [26], [27], [3)], and [31] to name a few. All
of these mainly concern matching an image curve to a model
and estimating the position and pose of the camera for the best
match. Our problem is simpler than these problems because
we use a search strategy to first isolate similar curves. Our
objective is only to isolate the curve that matches to the model
curve best. We use a least square technique to determine the
best match. Essentially, we compute the mean square disparity
between each of the candidate curves and the model curve. The
candidate curve that results in the lowest mean square error is
considered as the best estimate of the the robot's position.
If m;(i = 1, .,n) represents the model curve with n points
and the l candidate curves are represented by c,(k = 1,..),
each having n points cs,(i = 1,,n),then for each k(k = 1, .l)
we compute
The candidate location with the lowest by is considered
as the best estimate of the camera position. That means the
camera location corresponding to the HLC c, for which
is the best estimate of the camera location.
As the illustrations in Section VII show, the error measure
is quite sensitive, and a location about two grid points away
from the correct location results in a C of 48.3442, To reduce
the effects of noise on the HILC, the HLC's are first smoothed
using a Gaussian low pass filter. A zero mean Gaussian with
a o of 5 is used in the test runs. Due to quantization and other
noise effects, this search strategy does not always isolate the
position to the grid point nearest the exact location. However,
the search still isolates the location to a grid point that,
although not nearest the true location, is still within a small
neighborhood around the true location. By using the DEM and
generating the images that would be seen at different locations
within a small neighborhood around the location isolated by
the search, and using the curve matching strategy described in
the paper, the camera location was isolated to a point nearest
the true location.
This search strategy can be used in a bootstrap mode. That
is, once the robot isolates its position in the DEM, for the
subsequent navigation tasks this position estimate can be used
to search only ''near'' the current robot location and not the
entire DEM. When the location of the robot is known quite
accurately, then we can in fact eliminate stage l and only
use stage 2 of the search in a small neighborhood around the
current location.
In this section, we discuss the computational complexity of
stage l of the search strategy used to isolate the position of the
robot in the DEM. The most computationally expensive stage
of the whole search process is the search in the first direction
(N, S, E, or W). After searching along one of these directions,
we find that the number of possible robot locations falls to
a small number, and hence searching among this reduced
set using the constraints in other directions is considerably
cheaper. Typically in our test runs, shown in Table I, we find
that using a map of 164063 (359 s 457) grid points, after
searching in one of the directions, the number of possible robot
locations falls to a few hundred locations.
Note that deriving a general formula for the exact number
of computations required by the search process is not feasible,
since it is completely data dependent, i.e., on the nature of
the elevation values in the DEM. However, we can derive
bounds on the order of complexity of the search algorithm as
a function of the map size (using the probability distribution
function of the elevation data in the map) and verify these
with experimental results.
The algorithm Search described in the previous section
essentially searches the entire DEM size (M s N) using
the HLC height h and isolates a reduced set of possible
robot locations. Search searches along each of the N columns
iteratively for the robot location. Fig. 3 shows the search along
one of the columns. The r coordinate of the current camera
location, CAMERA.X, runs from M to 1. For each of the
CAMERA locations, we consider the POINT.X positions from
1 to (CAMERA.X - 1). And, for each set of CAMERA and
POINT positions, the main computation is performed by steps
4, 5, and 6, where, for each CAMERA and POINT location,
the estimated height Za,e is computed by back projecting h,
and this is compared with the actual height at POINT given in
the DEM, etuai Depending on the result of this comparison,
either the current camera location CAMERA is stored, or all
the locations between CAMERA and POINT are discarded.
Hence, the number of times steps 4, 5, and 6 are executed is
a measure of the amount of computation performed.
Consider the search along the ith column shown in Fig. 3.
To determine the average amount of computation performed,
we need to determine the grid point where the condition in
Step 6, (@aeeus1 7 .) is satisfied in each iteration. Let z)
be a random variable that denotes the grid point where the
condition (ZAeuua1(1) > a1)) is satisfied for the first
time, iie., when CAMERA.X =< M and POINT.X = 1, where
@aeusi(1) is the elevation in the DEM at the grid point r; and
Aa4I41) is the back-projected height at r) using te camera
geometry, ie., h, H,@,.
Now the expectation of r is given by
where p(r;), the probability of the condition in Step 6 being
satisfied at 1, is given by
Thus, p(1), the probability of the condition in Step 6 being
satisfied at z; = 1, is given by
and p(2) is given by the product of the probability that Step
6 is satisfied when z; = 2 and is not satisfied when z; = 1.
We have
Hence, the expectation of a) is given by
Thus, on the average we need to perform steps 4, 5, and 6
M4 times before we hit the first tall point, where the condition
in Step 6 is satisfied. In other words, we need to explore M4
POINT positions. The CAMERA location is now updated to
the grid point at M4 - 1, the POINT location is reset to 1, and
the search is continued. Let the location of the next grid point
where the condition in Step 6 is satisfied be denoted by My.
Then
Let this search procedure continue n times before it termi-
nates. Note that the limits of the summations are decreasing
each time, ie., M4 2 M 2 Ms,...,M,,, We thus have the
average amount of computation performed for the ith column
given by
On the average, this amount of computation is to be per-
formed for all the N columns. So, the total average computa-
tion for searching the entire map is given by NC. In the above
derivation the probabilities r, = p(i) = 1-Prob(seua1()
ZZa,4(i)) can be computed from the probability distribution
function of the DEM.
To derive an upper bound on the computatiomperformed by
the algorithm Search, consider the case in which' no pruning
takes place in steps 4, 5, and 6 for all the rowsi In this case,
M4 = M4 = M =-- = M,, = M, and n ='M. Hence, the
total number of POINT locations for which the steps 4, 5, and
6 are performed for each row is given by
If a similar situation occurs for all the N columns, we have
the upper bound on the complexity as O(MN).
To derive a lower bound, we can consider the more fortunate
circumstances in which steps 4, 5, and 6 perform the maximum
pruning. That is, the nature of the DEM is such that for
each of the (N - 1) columns, which do not contain the
true robot location, when CAMERA.X = M and POINT.X
= 1, we find that in Step 6, the condition Aaeus1 > a.. is
satisfied and hence all the POINT locations between POINT
and CAMERA are discarded. Thus, M) = 1 and n = 1 and
the amount of computation required is O(1) for each of these
N - 1 columns. However, for the column that contains the
true robot location, the minimum computation is required if:
1) the true camera location is at CAMERA.X = M and none
of the POINT.X positions from 1 to CAMERA.X satisfy the
condition in Step 6. Thus, M4 = M. 2) When CAMERA.X
M - 1 and POINT.X = 1, Step 6 is satisfied, and hence no
other CAMERA location need be considered in this column.
Thus, M = 1, and hence, n = 2,
The lower bound on the total amount of computation
required is given by
Note, however, that these upper and lower bounds are quite
loose. Since the data in the DEM usually represent a real
mountain range, it will be much more correlated and the
situations considered in deriving the bounds never happen.
Also, for a given H,@ and @, the the value a4c()) depends
on h, the height of the HLC. The average complexity of the
search is therefore dependent on h. Fig. 6 shows a plot of
the average number of POINT locations explored for different
values of h, and for a given H,o and 8. In this figure we used
a DEM that was 359 x 457, ie., M = 359 and H = 800 ft
o = 0%, 6 s+ 33%. The probability distribution function and
the probability density function of the elevation value in the
DEM used are shown in Figs. 4 and 5, respectively.
The probabilities r; and 4,, used in computing the average
amount of computation, are computed using Fig. 4. From Fig.
6 we can see that the average number of POINT locations
explored grows with h. This can be explained by the fact that,
as h increases, for each 1r1, Z4d41) increases, and hence the
probability Prob( .-nusti1) > Zaa44)) decreases. Hence,
we need to explore more POINT locations before the condition
in Step 6 is satisfied. So, the average amount of computation
performed increases with increasing h. This observation leads
us to devise a search strategy in which, of the four directions
N, S, E, and W that we desire to search, we search in the
direction with the lowest value of h first and then the next
higher h and so on. This has the effect of reducing the amount
of computation required in isolating the robot location. We also
observe from Fig. 6 that the average amount of computation
performed is O(M) for each column. For searching among all
the N rows, it should be on the order of O(MN).
In our experiments, in addition to computing the position
we also computed the amount of computation required by
counting the number of times steps 4, 5, and 6 are executed,
i.e., the number of POINT locations considered for back
projection. These are also shown in Table I. Note that the
number of POINT locations lies between the lower bound
O(M 4 N) and the upper bound O(MN). From the table
we see that these are is in O(MN) as predicted by Fig. 6.
The number of POINT locations considered are actually equal
to kM{N, where k is a constant and k e (10,20). Thus, the
constrained search strategy serves to reduce the complexity of
the search from O(MN) in the worst case to O(MN) on
the average, due to the pruning.
The search algorithm depends upon the errors in three
parameters: the error in altitude H of the camera, the HLC
height h; in the image plane, and the tilt angle of the camera
g;. These three parameters affect the estimated elevation Za4e
at a particular candidate point POINT for a given hypothesized
camera position CAMERA. The errors in Za,, directly reflect
as errors in the positional estimation, since in step 5 of the
algorithm, only when Za, is equal to eua do we consider
the current camera location as a possible candidate. In the rest
of this section, we analyze the effects of errors in these three
parameters on the computation of ZZa, and then discuss the
measures taken in the algorithm to account for the worst case
case errors.
The equation relating a4 to h,, , and H, derived in the
previous section, is given by (1) above. For a given size of
the image (?aaGa+) and angle of perspective projection
6, tan 8/laia ts a constant. Therefore, the variables are H,
h;, and g;,
Expanding the use of Taylor's series and ignoring the higher
order terms, we have
Equation (3) shows that the error in H is directly reflected
as an error in ZZg,. To account for these, a worst case error
in H is estimated from the sensor used to measure H. Let
this be bH. In the search algorithm, this can be accounted for
by considering the estimated elevation at the candidate point
(POINT) to be acceptable if Z.,, -&H 4eus a4eH&H.
The errors in h; are mainly due to image quantization error
and errors in the edge detector used to extract the horizon line.
Since z, the distance between POINT and CAMERA, occurs
as a multiplicative parameter in the second and third terms of
(1), the errors in h; are magnified by this and, hence, vary
depending on a. The worst error compensation should take
this into account. Fig. 7 shows a typical plot of ZZ,,, versus
a: for different 2h;. As can be seen, with increasing distance
the error in aU grows almost linearly. Fig. 8 shows a plot
of 2Za4, versus 2h; for different gcs and for fixed z and H.
Ideally, 2Z = 0 should occur at 2h; = 0. However, due
to image quantization errors, this occurs for 2h, 6 (-1,+1),
as can be seen for different values of @;. One way to account
for these errors in h; is to back-project a band of h; values,
(h;- 2h;,h,-2h,instead of a single h, in the estimation of
Z4u Thus, for each CAMERA and POINT location, we obtain
a range of acceptable elevations (., + 2a44 .-- 2Aa0
and we consider CAMERA as a possible camera location only
if Z.zaia at this POINT lies within this range.
Similar to h;, the effects of errors in ; are also magnified
by ar and, hence, depend on the distance between CAMERA
and POINT. The algorithm is very sensitive to errors in o;. As
Fig. 9 shows, an error of 0.5% in g; causes an error of about
50 m in Z,,,, for the given z, H, and h;. Therefore, @; has to
be measured accurately or at least represented by a good worst
case estimate so that, as before, Za,e can be compensated for.
In the actual implementation of the algorithm, errors in @; are
accounted for by considering the effect of these errors as errors
in h;. That is, the band h; it 2h; is made wider to account for
2@;. Hence, the range of acceptable ZZag, values is increased.
In the illustrations considered, we find that a 2h; = 3 pixels
accounts for a 0.5 error in o; and for a dtl pixel error in h;.
Another observation from Fig. 9 is that 2Za = 0 does not
always occur at 2; = 0. This is due to the roundoff errors
in the computation of Za,, from o;, H, h;, and z, which is a
reasonably complicated trigonometric expression.
The algorithms developed in this research have been tested
using real terrain data obtained from the USGS of various areas
in Colorado and Texas using synthetic images generated from
the DEM. This section details the results of a typical run using
a DEM of an area in Colorado. The elevation data is a uniform
square grid of 30-m resolution and has 359 s 457 grid points.
It covers an area of 148 km*. Synthetic images for an assumed
camera location are generated from the DEM using the AT&T
Pixel machine, to serve as the images taken by the robot.
The elevation data are tessellated into polygons, and surface
normals are calculated at each of the elevation points using the
four neighboring points. A light source position and direction
are assumed, and, for a given camera location and perspective
geometry, Gouraud-shaded polygons are drawn and projected
onto the image plane to generate perspective views of the
mountain range.
Figs. 10-16 illustrate a typical run of the algorithm. Fig.
10 shows a typical image used to test the algorithm. Fig. 11
shows the HLC extracted from this image using a gradient
operator. Fig. 12 shows a top view of the environment of
the robot. In this example, of the possible 164063 (359 s
457) possible locations, stage l of the search process using
hy and the associated camera geometry returns 473 points
as possible camera locations. Fig. 13 shows a top view of
these locations. These successively drop in the consecutive
searches that use h; in the other directions. Ultimately, we
have about eight possible camera locations. Fig. 14 shows a
top view of these locations. Figs. 15 and 16 show the results
of the second stage of the search process. The figures show the
HLC's of some of the images generated in the north direction
for the candidate camera locations returned by the first stage
of the search process. The figures also show a measure of
the difference between these and the actual image HLC in this
direction. The measure used is a mean square error between the
two contours, The candidate with the lowest error, in this case
48.3442 units, is then considered as the estimate of the camera
location. The actual position is (200.1,100.5) and the estimated
location is (201,102)). The four neighbors of this point are
then considered as candidates; images are also generated in
the north direction for these positions. The HLC's are then
extracted and compared against the original HLC, and the best
estimate of the location is isolated as the one with the least
mean square error.
The results of running the algorithm at various locations on
the DEM are shown in Table I. As can be seen, in most cases
the estimated position is quite close to the actual location. We
then add zero-mean Gaussian noise, with a standard deviation
of 5, to the horizon lines. This represents the noise in the image
formation process and the noise in the detection of the HLC.
The sweep frequency, oblique incidence sounder
(OIS) has become an integral part of modern adap-
tive HF systems. An oblique ionogram directly
characterizes propagation modes between a sepa-
rated transmitter and receiver, and, similarly to its
vertical incidence counterpart, contains informa-
tion about the ionosphere along the path. Extrac-
tion of this information for N transmitters and M
receivers, which are separated from each other in
an OIS network, for example, has the potential of
characterizing the ionosphere in the vicinity of NM
ionospheric reflection points. For the processing of
one-hop modes of an oblique ionogram the associ-
ated reflection point is normally located near the
sounder midpath point. In addition to the multiplic-
ity of reflection points at which ionospheric infor-
mation may be obtained, their location can be in
areas where vertical incidence data are denied.
Considerable attention has been devoted to the
development of sophisticated techniques for con-
verting vertical ionograms to electron density true
height profiles. For a recent review and discussion
see Titheridge [1988]. Less effort has been devoted
to the oblique case. All methods for oblique iono-
gram conversion assume a spherically symmetric
electron density and no magnetic field. Group path
delay is easily obtained in this case as an integral in
height, involving the electron density and other
factors, from the ground up to the reflection point.
Assumed forms for segments of the electron density
height distribution are used to evaluate the integral
at frequencies selected from the oblique ionogram,
and equations are set up which relate the measured
group path delays to these expressions. The param-
eters for each segment are obtained by inverting
these equations. The measured group path delays
along the trace for one-hop modes are successively
processed in this way from the smallest reflection
heights to the largest. The true height profile is thus
built from the ground up. At each step the unknown
segment parameters can be determined directly
from an equal number of measurements, or deter-
mined indirectly from a least squares fit with a
greater number of measurements, except that the
launch elevation angle for each frequency is an
additional unknown in the oblique conversion case.
Prior to 1971 all the methods had eliminated this
unknown by using a Breit-Tuve (BT) type of rela-
tion for group path length in terms of elevation
angle, range, and reflection height, based on an
approximation for ray path geometry. This class of
methods is referenced by Reilly [1985], who shows
that a new direct method of this type, based on an
improved BT relation, outperforms the techniques
developed prior to 1971.
Another significant advance occurred in the tech-
A numerical solution of (4) and (5) can proceed by
a straightforward application of the Newton-
Raphson method for two unknowns. Initial esti-
mates of the unknowns are needed for this process,
and, after some experimentation, we found the
following procedure to work quite well. One initial
estimate used is
where e is a very small number (e = I(0% in the
calculations). Since R,, is monotonically increasing
with n, the initial estimate is an underestimate. The
other initial estimate needed is Bg', and this is
obtained from the new BT relation of Reilly [1985]
by successive application of the following equa-
tions:
This turns out to be only a very slight overestimate
of fln,, hence assuring ray path penetration of all
the previously computed segments. The procedure
starts by evaluating F; and F with these initial
estimates. Then these calculations are repeated
with the pair of deviations, RP' - 2e, Bgf) and
RPP', gf! + 200e). These results are used to
approximate the partial derivatives of F; and Fy
withrespect to R,, and fa,, and it is then possible to
compute corrections to the initial estimates of the
unknowns. Next F;, Fy, and their partial deriva-
tives are computed in the same way with the new
estimates, and this process is continued until con-
Vergence is obtained. Only a few iterations are
necessary to obtain convergence is obtained. Only a
few iterations are necessary to obtain convergence
on R,, to within 0.le (about 0.7 m). Rao's [1973]
solution procedure differs from this.
A simple reentrant procedure is included in the
present method to handle the valley ambiguity. It is
the same as that used by Reilly [1985]. Subject to
the criteria mentioned later, a reentrant point r is
inserted after the point n - 1 when the calculated
slope B,,,; in (3) is more than 20% higher than the
slope B,,, The calculations are repeated for a se-
quence of reentrant point heights, which proceeds
upward from point n - 1 to point r along a segment
with slope -0.08 MHzfikm. This sequence is ter-
minated, with the help of a Newton-Raphson con-
vergence procedure, when the slope from r to n
matches the slope from n to n + 1. If there is no
convergence toward this condition, the reentrant
point is discarded. In some instances (for example,
some nighttime ionograms) the reentrant point
height increases take point r to the axis f+ = 0.
Further height increase is along this axis in the
convergence toward the slope matching condition.
Other criteria exist for discarding the reentrant point.
If the reentrant point is below the last point beneath
the valley cusp, it is discarded. It is also discarded if
there is an earlier reentrant point which is within a few
oblique ionogram points away from it.
Daytime ionospheres produce the most compli-
cated electron density profile structure, with the
presence of the E, F4, and Fy layers. The existence
of a valley between the E and F layers increases the
complexity. Lobb and Titheridge [1977] discuss the
E-F valley, as related to vertical incidence iono-
gram analysis. They summarize data taken from
daytime rocket profiles and incoherent scatter mea-
surements of the E-F valley. The average value of
minimum plasma frequency in the valley for typical
shallow valleys was found to be about 0.95f,,E,
where f,E is the E layer ordinary ray critical
frequency, and approximately 0.83f,E for the deep-
est valley. A typical valley width was about 10 km.
Five theoretical daytime ionospheric electron
density profiles were constructed by joining seg-
ments of the form
Different segments were used for the E, F;, and Fy
layers and for the transition between E and F;.
Oblique ionograms were computed for these pro-
files, and this was facilitated by the fact that the
integrals in (1) and (2) are easily evaluated in closed
form for the form of (9). The process used was to
search on elevation angle at each frequency to get
solutions for the correct sounder range in (2), with
the help of a Newton-Raphson homing procedure.
For each solution the group path delay in (1) was
in the 1989 method, obtained by eliminating the use
of a BT relation. One clear improvement is that
squared plasma frequency errors are virtually elim-
inated in the 1989 method. On closer inspection of
Figure 3 the reflection height errors show jumps
when crossing the slope discontinuities at the
boundaries of the linear segments. This arises be-
cause the segment connection (compare (3)) be-
tween points on each side of a slope discontinuity
cannot reproduce the sharp corner. The electron
density content is thus underestimated at the first
corner and overestimated at the second and third
corners. Reflection height errors occur in the oppo-
site sense, in order to reproduce the observed group
path delays. These error jumps tend to be damped
out for ensuing points in the solution. Reflection
height errors are observed to increase in Figure 3
for points near and in the high-ray portion of the F
layer. The reason for this is not dissimilar to the
reason for the error jump at the first slope discon-
tinuity. It is related to the inadequacy of the seg-
ment approximation of (3) for the given sampling of
oblique ionogram (OI) points. This assertion will be
supported in the calculations below. The reflection
height errors in the 1989 method can be diminished
by a denser sampling of OI points in the vicinity of
the cusps, in the vicinity of the Fy junction fre-
quency, and in the high-ray portion of the trace.
Given these considerations of numerical approxi-
mation type errors, it is clear that the 1989 method
is more accurate than the 1985 method, albeit
computationally slower.
Another example is the real height analysis for
the wide, shallow valley case, as shown by the solid
line in Figure 4. This case is simulated by four
quasiparabolic layers [Croft and Hoogasian, 1968],
as shown by the solid line profile. This is used to
calculate the one-hop oblique ionogram in Figure 5.
A sample set of data points from it is shown by the
crosses. The 1989 method applied to this OI data set
yields the cross mark results shown in Figure 4. The
reentrant procedure, which was discussed in the
previous section, was not included in this solution.
The result is that reflection height error jumps are
lem, a common technique is simply to discard the
associated point. The errors from doing this do not
appear to be serious. When the reentrant procedure
is applied to very deep valleys, large error is ex-
pected to result, but this situation seldom arises.
In practical situations one might, for example,
have an ionogram like Figure 7, except the actual
trace is thicker, noisier, and incomplete, particu-
larly in the high-ray portions for the various layers
and at low frequencies. Further, the visible high-ray
portion of the Fy layer trace will exhibit magnetic
field splitting into separate O and mode traces.
Measured ionograms often exhibit only relative
time delays. In this case our procedure is to esti-
mate the intersection of the E layer trace with the
vertical axis (zero frequency). Relative time delays
are converted to absolute time delays by guessing
the bottom height of the E layer at, for example, 90
km and using straight-line geometry. Points are
scaled from the centroid of visible portions of the
trace, with a density comparable to the crosses in
Figure 7. It is, however, frequently not possible to
go up too high on the cusp between high F; and low
Fy, because of noisiness and incompleteness. When
it comes to going up the Fy high-layer portion, a
trace intermediate between the splitting is selected.
Only about five points up this part of the trace are
selected, and these are the final points of the true
height profile solution. Not much more of the trace
is typically available for processing. More impor-
tantly, these five points are least squares fitted by a
parabola, in order to infer peak parameters of the Fy
layer, and this procedure works better if the five
points are not taken very far up into the high-ray
portion.
The method presented for obtaining true height
profiles from oblique ionograms, a variant of Rao's
method in which the equations for group path delay
and sounder range are placed on equal footing, is
the most accurate one available, within the approx-
imations of a spherically symmetric ionosphere and
no magnetic field. This was shown by comparing its
results with known model profiles and with the
results from a 1985 method, which was previously
indicated as most accurate of the methods which
invoke further approximations. These include a
Breit-Tuve type of relation for direct oblique iono-
gram conversion and Martyn's theorem for indirect
conversion. Improvements can still be made with
methods which are presently used for the real
height analysis of vertical ionograms [Titheridge,
1988]. These may include overlapping polynomial
proocedures for simultaneous solution of the range
and time delay equations, instead of the lamination
method used here, and a least squares procedure for
partial removal of the valley ambiguity, instead of
the reentrant procedure used here. Nevertheless,
this paper shows how the present method can be
used to obtain respectable accuracy, given the basic
approximations. It still remains to be shown in
detail how the results are to be interpreted for a
tilted ionosphere with irregularity structure. For
example, the question of how close the profile
obtained is to the actual midpath ionosphere under
various, realistic conditions needs to be answered.
Magnetic field effects for the larger transmitter-
receiver separation ranges manifest themselves
principally as a small O-X splitting near the junction
frequency and onto the high-ray portion of the F4
layer OI trace. The no-field trace is obtained by
simple averaging in this region of splitting. A more
detailed investigation of magnetic field effects is
desirable and of particular significance for smaller
sounder ranges.
Inherent in ionospheric skywave
propagation are time and frequency
dispersion and the time variability of
the propagation conditions. These effects
cause frequency selectivity, fading and
intersymbol interference. Thus, the
ionospheric channel tends to be employed
for narrow-band analogue comsunication or
for low data rates. In recent years,
wideband signalling over the HF
communication channel has grown in
importance. This is due to the increased
interest in spread spectrum communication
and to the advantages offered by such
wideband signals. Mainly, wideband
signalling will permit the resolution of
multipath, thereby reducing fading and
increasing the reliability of HP
circuits. It will also permit the use of
high data signalling rates (tens of
kilobits/second) or low data rates with
spread spectrum. Previous studies of the
characteristics of the ionospheric
channel over wide bandwidths have been
reported [Epstein, 1969: Lynch et al.,
Since different windows were to be used
for each mode, the analysis of the 60
sseeps contained in the 10-ain
observation interval sas carried out
rprately tor each wode (, an4 1E,) bs
filtering in the digital präcessing'
Narrow pulse distortion was estimated for
each mode as a function of frequency and
time in two different methods. Firstly,
the percentage of tie a particular 6-dB
width occurred for each frequency range
=wwat44 r4r sm= A, =n4 ,
ordinary and extraordinafy waves7 The
results are displayed in Figures 2, 3 and
4. Secondly, the slope of group tiae
delay was estimated by fitting straight
lines to the group time delay over 500
kHz sections and then calculating the
percentage of time a particular slope
occurred for each frequency range.
Figures 5, 6 and 7 display the slope of
group time delay as a function of
frequency for the $ and 1F, ordinary and
extraordinary waves, respectively.
A, lA $E- Analysis of the 1o-ain data
shoed that the g propagated wave was
ostly received oier the frequency range
4.88-6.38 MHz. The splitting of the
received echo into the two magnetoionic
Raves was not alsays apparent.
Therefore, the results are presented for
the strongest received magnetoionic
wave. From Figure 2 which displays the
percentage of time a particular 6-dB
width occurred as a function of
frequency, the follosing tso observations
can be made : (1) The 6-dB width varies
between 16 and 20 gs over ost of the
frequencies except at 5 MHz where it
varies between 20 and 24 gs. (2) The
Rinimum detected 6-dB width of 16 us
occurs for approximately 84 of the tiae
at 5.75 MHz.
As previously mentioned, the slope of
group tie delay was calculated by
fitting straight lines over 500 kHz
sections. The results which are displayed
in Figure 5 indicate that the slope of
group time delay decreases with frequency
from 32 and 40 gs/MHz to a value between
0 and 16 gs/MHz.
Since the estinated 6-dB width is
affected by the characteristics of the
weighting windos used in the spectral
analysis, the slope of group time delay
can be used to predict the width of
received pulses and to estimate the
channel bandwidth according to the
results presented in section 2. The
equations derived by Inston [1969]
provide reasonable results. Hence,
equations (3) and (4) were used in the
calculations of the ainium expected
pulse width and the corresponding channel
bandsidth and the results are presented
in Table 1. Moreover, equation (5) was
used to predict the 6-dB width of the
previously stated, this results from the
broadening property of FFT spectra and
the coherent addition of sidelobes. (4) A
100 kHz bandwidth can be transmitted
without the need for equalization over
the frequency ranges 5.88-6.88 MHz and
8.38-7.63 MHz via the ordinary and
extraordinary waves, respectively.
It is relevant to mention that in order
to avoid the effect of polarization
fading which results from the
interference between the two agnetoionic
waves, a modulation scheme capable of
resolving the two sagnetoionic waves and
discriminating against one of them has to
be used. In such a case, the transmission
bandwidth must be adequate to resolve the
two magnetoionic waves. With a 100 kHz
transmission bandwidth, the differential
tie delay between the two waves needs to
be greater than 10 gs. From the measured
time delay, the minimum separation
between the two waves was found to occur
at the crossover frequency which was in
the frequency range 7-7.25 MHg. However,
the two waves were ostly resolved over
this frequency range with the
differential time delay being greater
than or equal to 12 gs. Consequently, a
100 kHz bandwidth seems to be adequate to
resolve the two waves.
The degree of distortion suffered by
narrow pulses transmitted via the
ionosphere depends on the characteristics
of the transfer function of the channel.
When multipath is resolved, the amplitude
of the transfer function of the
ionospheric channel is detersmined by
polarization interference betseen the two
agnetoionic waves and the phase is
related to the group time delay
variations with frequency [Epstein,
1968]. Polarization interference causes
signal strength variations as a function
of frequency. The relation betsween
the same path in equinox and winter
[Salous and Shearman, 1986] shows that
real time measurements of the
characteristics of the channel are
essential to determine the least
dispersive frequency.
The contact-handled (CH) TRU waste stored and generated at Department of Energy
(DOE) sponsored facilities results from defense-related research and development ac-
tivities, as well as nuclear weapons production. The waste consists of materials such as
rubber gloves, clothing, laboratory glassware, discarded tools and equipment, and
process residues that are contaminated with TRU radionuclides (atomic number
greater than 92) in concentrations greater than 100 nCi/g. As defined in DOE Order
5820.2A, TRU waste includes
will produce TRU waste forms compatible with WIPP waste emplacement and isola-
tion requirements. The WAC only apply to CH and RH TRU waste forms, and to
waste emplaced in bedded rock salt. A summary of the WIPP W AC is presented at the
conclusion of this article.
The following sections summarize the waste management strategies at each of the ma-
jor DOE-managed waste generating and/or interim storage sites. These sections in-
newly-generated waste is not certifiable, it is placed into interim storage until it is
retrieved and processed in the Hanford processing facility.
The planned Waste Receiving and Processing Facility (WRAP) at Hanford will have
the capabilities to certify both stored and newly-generated CH TRU waste. When
this facility comes on line in 1999, it will assume all equipment and operations of
TRUSAF. WRAP will use both TRU assay and real-time radiography systems to ini-
tially sort the retrieved waste into TRU and LLW fractions. Waste packages shown to
comply with the WIPP WAC will be directly shipped to WIPP. The majority of those
waste packages that need processing for certification will be size-reduced (as needed),
shredded, and immobilized with grout. Packages not amenable to the shred and grout
process will be handled individually in a small-scale process glove box within the
facility.
Hanford plans to begin retrieval and certification of its stored RH waste inventories in
2006, and is scheduled to complete this operation by 2013. Waste will be retrieved
from the alpha caissons in a mobile caisson recovery building; it will then be certified
and shipped to WIPP. Any waste that requires processing for certification will be
shipped to ORNL for processing prior to delivery to the WIPP. Hanford will begin to
generate large quantities of RH TRU waste in 1995, as the result of the shear/leach
processing of Fast Flux Test Facility (FFTF) fuel. This waste will mainly consist of fuel
cladding hulls. Due to the expected high surface dose rates, this waste will be shipped
in canister casks.
Most of the waste that ORNL generates and puts into interim storage results from
glove box operations at the ORNL site. Table VII summarizes the ORNL stored waste
inventory.
Both stored and newly-generated CH TRU waste will be certified in the ORNL
Waste Examination and Assay Facility (WEAF). This NDA/NDE facility will include
a Neutron Assay System, a Segmented Gamma Scanner, and a Real-Time Radiogra-
phy (RTR) System. The WEAF has no processing capabilities, so those amounts of
waste that cannot be directly certified will be repackaged in an existing ORNIL hot cell.
It is expected, however, that only a few drums will require additional processing after
repackaging to be certified. These few drums will be placed in temporary storage and
processed in the Waste Handling Pilot Plant (WHPP). The WHPP is primarily an RH
processing facility, but will handle any CH waste from ORNL that requires
processing.
Currently, most of ORNL'g RH waste is contained in concrete casks which are neither
certifiable nor Type A containers. These casks will be retrieved, repackaged, and
scanned in the WHPP to determine if the waste is RH TRU or if it has decayed to CH
TRU waste or LLW. Any waste that is RH TRU will be processed, if necessary, and
certified for WIPP disposal. CH TRU waste will be certified in WEAF and LLW will
be disposed of in an appropriate facility.
The sludges in the ORNL Melton Valley Storage Tanks and other liquid collection
tanks will be sluiced out and solidified in 55-gallon drums prior to emplacement at the
WIPP. The solidification process will be accomplished in the WHPP,
The SRP produces waste that is placed into interim storage consisting of stacked waste
packages on above-ground concrete pads; the waste is covered with a moisture barrier,
with a four-foot-thick clay cover placed over the waste matris, This waste is a product
of glove box operations associated with the production of nuclear materials for use by
the nation's defense programs. The waste is packaged in S5-gallon drums, concrete
culverts, and metal boxes, The SRP stored waste inventory is shown in Table VIII.
NTS has no certification or processing facilities. All of the NTS waste has been
retrieved and certified using the mobile NDA/NDE system developed by LANIL. Any
waste that is uncertifiable will be used to demonstrate GCD technologies at that site.
The Nevada Test Site has no RH TRU waste.
The ten TRU waste storage and generator sites in the DOE system are in various
stages of the planning and implementation process for certifying waste to meet the
WIPP WAC, so that waste shipments are acceptable for geologic disposal in the
WIPP. Current plans will provide a steady stream of waste shipments to the WIPP,
while assuring compliance with all DOE regulations and orders pertaining to TRU
waste shipments.
CANDU nuclear power reactors use pressure tubes as the primary
heat-transport containment. The pressure tubes contain the fuel and
the hot, pressurized heavy-water coolant, and under normal condi-
tions operate at about 300 *C, with an internal pressure of about
10 MPa.' During some postulated loss-of-coolant accidents, the
the local transverse creep rate is
where T is the local temperature, o is the local transverse stress and t
is the time. (This is a time-hardening creep equation, but a
strain-hardening one could also be used.) The local transverse stress
is o = PrIw, where P is the pressure difference across the wall, r is
the tube radius and w is the local wall thickness.
With these assumptions, the local and average transverse creep
strains of an internally pressurized tube, having a circumferential
temperature variation, were calculated by dividing the circumference
into a number of small segments over which the temperature was
assumed to be constant. The creep equation was numerically
integrated for each segment. At the end of each time step, the
geometry was updated by calculating new segment lengths and wall
thicknesses using the increments in creep strain determined for each
segment. In the calculation of the wall thickness, it was assumed that
the volume did not change during creep and that the axial creep rate
was neglible. Thus Ne, = -8,, where e, and 3s, are the incre-
ments in radial and transverse creep strains during the time step. (If
the axial creep rate is not negligible, then the increment in axial
creep strain could be calculated for each segment and used in the
calculation of the wall thickness). The new tube radius was found by
adding all the segment lengths, to find the new circumference, and
then dividing by 27. This procedure was repeated until the tube
failed, or the test was complete.
At high temperatures, Zr-2-5% Nb pressure tubes are ductile, and
failure occurs by necking down to a chisel edge. To predict the time
to failure and the average transverse creep strain at failure, it is
necessary to know when the critical segment fails and the length of
this segment at failure. An upper bound to both can be obtained by
assuming that failure occurs when one of the segments is predicted to
creep to zero wall thickness. Just prior to failure, the creep strain
tends to be very localized, and the creep strain in the segment that
fails increases rapidly, with no significant increase in tube radius. The
increase in the transverse stress for this segment is due only to a
reduction in cross-sectional area, as is the case for a uniaxial creep
test under constant load. Thus, just before failure, the time required
for the critical segment to creep to zero wall thickness is the same as
taken into account in the model. A lower bound to the failure time
can be obtained by assuming that the maximum variation in the
initial wall thickness occurs in the critical segment. This region of
lower wall thickness will cause local necking and failure, before the
segment is predicted to creep to zero wall thickness. The transverse
creep strain at which this failure would occur has been evaluated' and
it is
where w is the initial nominal wall thickness and d is the maximum
possible depth of a local defect. This equation was used to determine
the lower bound for the failure.
The transverse creep equation for Zr-2-5% Nb pressure tubes, in
the temperature range from 450 *C to 850 'C, is'
where is the transverse creep rate in s'', o is the transverse stress in
MPa, T is the temperature in K, t is the time in s and t, is the time
when T 973 K. From this creep equation it is seen that the stress
exponent,
varies from 1-8 at low stresses to 9 at high stresses. Thus, at the
beginning of each time step the stress exponent was calculated for the
$egment with the maximum creep strain. This stress exponent was
used to continually update the lower and upper bounds for tube
failure, using eqns (1) and (3). However, eqns (1) and (3) were
developed assuming that the stress exponent was constant. If the
increase in the stress exponent caused by the increasing stress were
properly taken into account, the predicted local creep strain for the
of less than 0-013 mm. The specimens were arranged in the apparatus
shown in Fig. 2 so that a large region of uniform wall thickness would
be at the highest temperature during the test. The test vessel was
mounted on a hydraulic testing machine that allowed neglible end
loads to be maintained even during heating. The specimens were
stressed by internally pressurizing with high-purity argon gas.
Induction heating was used so that these rather large specimens
could be heated quickly. Also, the temperature distribution around
the tube circumference could be varied by changing the heating coil
design. A large coaxial lead was used between the induction unit and
the heating coil to reduce transmission losses. The temperature
distribution around the tube was measured using 10-14 thermo-
couples spot-welded around the circumference. The thermocouple
leads were arranged perpendicular to the windings of the heating coil
to minimize noise pick-up from the coil. With these precautions the
maximum error in temperature measurement was 35 %C,
A quartz thread was wrapped around the tube and attached to a
linear variable differential transformer (LVDT), as shown in Fig. 3.
This allowed the increase in the circumference of the tube, and
consequently the average transverse creep strain, to be measured
are not significantly different. These results show that there is good
agreement between the measured and predicted local transverse
creep strains up to the point of failure.
A special heating coil was designed to produce a narrow hot zone.
Tests with this coil were done with internal pressures from 1 MPa to
5 MPa and all specimens failed at average transverse creep strains
less than 20%. The results of a test with an internal pressure of
4<1 MPa are given in Figs 8 and 9. In Fig. 8 it is seen that there is
reasonable agreement between the measured and predicted average
transverse creep strain, but the predicted values are higher than the
measured ones. In a similar test that was stopped before failure, the
tube was found to bulge slightly in the region of high local
The major assumption in our model was that the cross-section of the
tube remains circular during ballooning even when there are varia-
tions in the local transverse creep strain around the circumference.
This assumption reduces the two-dimensional problem to a one-
dimensional one, greatly simplifying the analysis. In tests done with a
gradual circumferential temperature variation, between 30 and 7(0 'C,
and internal pressures between 1 MPa and 9 MPa, specimens did not
fail before test termination at an average transverse creep strain of
about 20%. At test end, even though there were large variations in
local transverse creep strain around the circumference, the cross-
sections of these tubes were still essentially circular. However, tubes
tested with a very narrow hot zone, causing the local transverse creep
strain to be concentrated in about 20% of the circumference, bulged
in the region of high local transverse creep strain. These results
indicate that, as the transverse creep strain becomes more localized,
the tube deviates further from a circular cross-section. However,
even when the transverse creep strain was concentrated in about 20%
of the circumference, there was still reasonable agreement between
the model and experiment.
The model was thoroughly tested using a wide range of circum-
ferential temperature distributions, which resulted in a wide range of
local transverse creep strain distributions. For all tests, the model
accurately predicted the average and local transverse creep strains to
failure. Also, the model was able to accurately predict the time to
failure. The difference between failure times predicted by the lower-
and upper-bound failure conditions was small, since the creep rate
was very rapid near failure. However, there was a significant
difference between the average transverse creep strains at failure as
predicted by the lower- and upper-bound failure conditions.
Nevertheless, these two predicted average transverse creep strains at
failure bounded the measured values for all tests that ended in
failure. Thus, the lower- and upper-bound failure conditions given by
eqns (3) and (1), respectively, accurately predict when failure occurs.
Also, the length of the failed segment predicted by eqns (3) and (2)
fr the lower- and upper-bound failure conditions, respectively, must
w=b s wrse wnsverse eeepsaii nsews
accurately predicted.
We thank E. T. C. Ho of Ontario Hydro Research for suggesting the
method of measuring the average transverse creep strain.
This work was funded jointly by Ontario Hydro and AECL under
the CANDEV development program.
This research, conduc-
ted at the USAF School of AAerospace
Medicine (USAFSAM), Brooks AFB,
Texas, was part of a comprehensive
study to assess the physiological and be-
havioral effects of using breathing gas
concentrations representative of those
produced by molecular sieve oxygen
generation systems (MSOGS). This paper
is confined to the discussion of subjec-
tive hypoxia symptoms resulting from
breathing various gas mixtures before
and after a rapid decompression to sim-
ulated high altitude.
In both the U.S. Navy (USN) and the U.S.
Air Force (USAF), interest is increasing in
MSOGS systems because of their logistic
and reliability advantages when com-
pared with liquid oxygen-supplied air-
craft breathing systems. The U.S. Navy is
interested in the Expeditionary Oxygen
Nitrogen System, or ''EONS'' arnd the'On
Board Oxygen Generating System, or
'OBOGS.'' The EONS is an air-trans-
portable, self-contained 'oxygen plant''
which produces both gaseous and liqui-
fied oxygen. The USAF is primarily inter-
ested in the On Board Oxygen Gener-
ating $ystem (OBOGS).
A limitation in the maximum oxygen
concentration attainable with MSOGS
has motivated USN and USAF devel-
opment communities to establish labor-
atory evidence of the acceptability of
using reduced oxygen breathing gas
throughout the envelope of current ox-
ygen systems. Our objective, therefore,
was to investigate the physiological pro-
tection of the current oxygen system
with various breathing gas mixtures
throughoutthe full altitude envelope.
On the basis of a fairly well
developed theory of respiratory gas ex-
change at altitude (1, 3, 5, 6, 7, 8, 9, 10,
12, 14), we concluded that there was no
reason to expect any adverse effects of
MSOGS gas at normal cabin pressures.
However, empirical data were limited
concerning the rapid loss of cabin pres-
sure while flying at the tactical aircraft
emergency ceiling (15). Therefore, we
decided to incorporate in our study a ra-
pid decompression (RD) from 20,000 to
50,000 f.
Four breathing gas concentrations were
used, under both the dilution and non-
dilution modes of the breathing regu-
lator. The breathing gas concenfrations
included 100, 93, 90 and 85% oxygen.
The balance of the gas mixtures was
composed primarily of argon. In the
breathing gas, nitrogen was kept to a
minimum to reduce the potential threat
and effect of decompression sickness. A
positive pressure breathing baseline
condition was conducted with 100% ox-
ygen, during a rapid decompression
from 8,000 to 22,500 ft. as a control, for
a total of nine conditions. Our subject
population was composed of chamber-
qualified active-duty male volunteers
from the USAFSAM Altitude Panel. This
paper reports the results of only those
subjects who had completed all treat-
ment breathing gas conditions. Hence,
only the data from 11 subjects were
available for 9 treatment conditions.
The measured variables and associated
instrumentation included: mask cavity
gases by mass spectrometer; arterial ox-
yhemoglobin by finger pulse oximetry
(BIOX Il); inspiratory flow pressure, and
temperature; heart rate (R-R interval) by
electrocardiogram; task performance by
a combined short-term memory and
tracking task; and subjective hypoxia as-
sessment by questionnaire. This paper
reports only the subjective hypoxia ques-
tionnaire data with respect to minimum
estimated alveolar POy's, and oxyhemo-
globin saturation percentages.
Shown in Figure l are the timeline and
altitude profile followed during the ex-
periment. The areas labeled' ''TEgT'
show when a dual short-term memory
and tracking task was performed. The
performance data are currently being
analyzed, and are not discussed in this
paper.
After ear and sinus and gut gas checks,
pre-decompression baseline recordings
and performance tasks were conducted
at 20,000 ft. At this altitude, about 3 to
5 minutes before the decompression,
the breathing gas mixture was switched
from 100% oxygen to the test mixture;
and the regulator was either placed in
the dilution mode or left in the non-
dilution mode. The subject was una-
ware of the mixture composition being
delivered, but could tell whether the
regulator was set on the dilution mode
or not. Approximately 20 sec before
rapid decompression, the subject was
cautioned to breathe normally. Then
the chamber was decompressed to a
simulated altitude of 50,000 ft. in ap-
proximately l sec. At this altitude, the
regulator delivered pure feed gas, either
100, 93, 90, or 85% oxygen. The subject
began the performance task 10 sec after
the RD. The total time of exposure to
the reduced pressure at 50,000 ft. was 1
min, whereupon the chamber altitude
was lowered to 40,000 ft. and held until
the subject completed the performance
task. A descent to 20,000 ft. was made
for another performance task session.
Then, at ground level, a subjective hy-
poxia questionnaire was completed con-
cerning only the symptoms experienced
during the 50,000 ft. trial.
Shown in Figure 2 is the Subjective
Symptoms Ouestionnaire used in the
study. Subjects were instructed to
indicate the presence or absence of each
of the 20 symptoms by circling a ''YES''
or ''NO'' response to each question. If
the symptom was present, the subject
circled the severity level experienced
from 1 (slight) to 7 (severe). Two
measures were derived from the q ues-
tionnaire, and are labeled at the bottom
of the form as: the ''Number of Symp-
toms,'' produced by totaling the number
of YES responses; and the ''Symptom
breathing gas condition. The minimum
SaO; values were generally found to
occur after 20 sec.
Notice the low PCO; values associated
with the minimum POy's. Hyperventila-
tion is a normal response to a fall in
PEO) below 55-60 mmHg (3,4,12). So,
this condition coexists with hypoxia, and
rmay be accentuated by the high positive
breathing pressure of 30 mmHg at
50,000 ft. The signs and symptoms of
hyperventilation were not separated in
the symptoms reports. This fact is most
evident in the symptom data of the 100E
condition, a low altitude rapid decom-
pression conducted as a PPB baseline.
Hypoxia would not occur in this condi-
tion, but as you can see, hyperventila-
tion does.
For the subjective symptoms, we ana-
lyzed two measures derived from the hy-
poxia symptoms questionnaire. The two
derived measures were used to estimate
the subjective level of hypoxia experi-
enced under each breathing gas con-
dition. An example in Figure 2 shows
the Number of Symptoms to be 10, and
the Symptom Score to be 29. Friedman's
Nonparametric Test was conducted with
each measure. The test is a nonparamet-
ric repeated measures-type procedure
based on rank comparisons.
The results of the test conducted with
the first measure, the Number of Symp-
toms, showed no significant differences
between breathing gas conditions. In
this study, there was a typical trend of
hhypoxia symptom reporting whereby
whereby individuals reported similar
symptoms from one exposure to the
next. What often changed was the de-
gree or severity of those symptoms ex-
perienced by the subjects relative to
their physiologic conditions.
Shown in Figure 5 is a bar graph of the
mean ranks of the second measure, the
Symptom Score. Borderline significant
differences (p = 0.1) were found. A
Duncan multiple range test showueu the
differences to be between the 100E and
the 93DIL, 93, 100DlL, and 85% condi-
tions and between the 100 and 85%
condition.
Following are a descriptive
discussion of symptom trends based
upon a preliminary overview of some of
the physiological data, and an explana-
tion of why some of the conditions
showing no differences were of par-
ticular interest to us. We wish to em-
phasize that these are only trends, but
we believe they are consistent with
previous research on hypoxia (19, 20).
These trends are not as easily seen in
Figure 5 as when the data are ranked
according to the physiological results.
Shown in Figure 6 are the raw mean
Symptom Scora data - ranked from the
highest average minimum PaO2, attain-
ed during the low altitude 100% con-
dition, to the lowest average minimum
PaO;, attained during the 85% dilution
condition.
We contend that subjects generally re-
port the severity of symptoms experi-
enced in a manner consistent with their
physiologic condition, up to a certain de-
gree of hypoxia. When hypoxia be-
comes more pronounced, however, we
believe the subject's perception of his
condition and well being (andlor his
memory process) is impaired. The bell-
shaped nature of this bar graph supports
this hypothesis. The average reported
Symptom score increased across condi-
tions with decreasing POy, as one would
expect with an increasing degree of hy-
poxia. With a further decrease of
alveolar PO;, however, a reduction in
reported symptom severity occurred, as
shown on the right half of Figure 6.
The last bar on this graph shows the
mean Symptom Score for the 85% dilu-
tion condition. An average minimum
POy, of 19.3 mmHg and an average mini-
mum oxyhemoglobin saturation percen-
tage of 78.6 occurred. The subject
should have been experiencing the
greatest hypoxic insult and, hence,
should have been reporting the highest
symptom scores. But we see in these
data that subjects reported symptom
scores comparable to those in conditions
producing less severe hypoxia.
We believe that this trend supports pre-
vious research concerning the incipient
nature of hypoxia (4, 16, 20). It has been
well documented that hypoxic impair-
ment of intellectual functioning is typ-
ically manifested by poor judgement
and memory capacity (2, 11, 13( 17, 18,
20). At some point, the subject is pre-
vented from making accurate assess-
ments concerning his condition or re-
membering them.
ln some instances, intellectual impair-
ment continues until total incapacita-
tion and unconsciousness occurs (3, 4, 7,
8, 15, 16, 20). This impairment can hap-
pen with little or no forewarning, a fact
which exemplifies the serious nature of
this degree of hypoxia. Although this
study did not proceed to total incapaci-
tation, trends of a reduction in the re-
ported symptom score may indicate a
change in the capacity of the individual,
as seen on the right side of Figure 6. As
for the subjective symptom data, we
believe that they, taken alone, do not
adequately describe the level of hypoxia
experienced by the subject. The sym-
ptom data really need to be viewed in
the context of the individual's physio-
logic state, and perhaps with an addi-
tional task performance measure - both
of which we are currently analyzing and
will report at a later date.
lf we conclude from our
data that subjective self-assessment of
hypoxia is not reliable in moderate to
severe levels, cognitive functioning is
believed to be affected. If cognitive
ability is compromised under such a
scenario as tested in this study, we might
assume that operational crew members
would also be affected when perform-
ing complex decision-making tasks un-
der a similar emergency condition. We
recommend that, if a reduction in the
oxygen content of aircrew breathing gas
is to be used with the standard oxygen
system equipment (i.e., 12-P mask and
CRU-73 regulator), the emergency flight
ceiling should be reduced to an altitude
at which better hypoxia protection is
evident in elevated PaO; 'S, particularly
if oxygen concentrations below 93% are
expected. If the flight ceiling remains
the same (or increases), then higher
breathing pressures are required for
better hypoxia protection (again, in
elevated PgO;).
This recommendation certainly is not
new; and it requires a change of equip-
ment to include an oxygen mask that
will maintain a face seal under positive
pressure breathing conditions greater
than 30 MMhg, and a chest counter-
pressure jerkin that will make possible
better personal tolerance of the higher
intrathoracic pressures generated.
Summarily, important trade-off deci-
sions need to be made relative to the
level of hypoxic impairment deemed ac-
ceptable. We recommend that only
those trade-offs with the least physio-
logical and psychological compromises
be seriously considered.
The voluntary informed consent of the
subjects used in this research was obtain-
ed in accordance with AFR 169-3. This
research was sponsored by the USAF
School of Aerospace Medicine, Human
Systems Division (SFSC), United States Air
Force, Brooks AFB, Texas 78235-5301.
ecognizing the true optimum
loading point of centrifugal
chillers can lead to chiller plant
efficiency increases of 20% or
more, many times with no investment
required. In the past seven years of energy
survey work for our industrial customers,
our energy engineering group has com-
pleted over 30 chiller studies.
By field monitoring centrifugal chil-
lers, we have noticed a trend that belies the
conventional wisdom for chiller operation.
Our large industrial customers operate
their multiple chiller plants based on the
widely accepted rule-of-thumb that the
optimum loading for centrifugal chillers is
60-80%% of full-load.
Typically, these plantsnever allow the
load on the lead chiller to reach full-load
before the lag chiller is started. We found
some plants operating multiple chillers on
acommon header and limiting each to less
than 50% load under the rationale of
reducing chiller power consumption.
Our chiller monitoringdoes not sup-
portthe rule-of-thumb that the optimum
chiller efficiency is 60-80% load. In fact,
mostcases show increasing efficiencies up
to 100%% load. Even ifa chiller's optimum
efficiency isat 8%% load, itisconsiderably
moreefficientto operateit byitself than to
operate two chillers at 40% load.
Investigating this discrepancy led us
through thehistory of partoad chiller effi-
ciency, typical manufacturers' chiller curves,
and specific plant operating procedures,
which will be discussed in this article.
The optimum loading of any chiller
depends on the specific combination of
compressor, drive motor, evaporator and
condenser, but most important, the con-
ditions under which the chiller was rated.
Misinterpretations of various chiller rating
conditions lead to these inefficiencies in
chiller plant operation.
It is only since the early 1980s that
part-load performance has significantly
influenced chiller design criteria. Prior to
that time, nominal chiller efficiencies
(expressed in kW per ton) ranged around
0.80 YW per ton and were only rated at
full-load.
There were no standardized testing
guidelines or certification procedures for
rating the chillers at part-load. Therefore,
thesechiller packagesmost likely havetheir
optimum efficiency at their rating points,
which is full-load.
Most centrifugal chillers manu-
factured today have full-load efficiencies
around 0.65 kW per ton. High efficiency
chillers have efficiencies to 0.60 kW per
ton,Thisrepresentsa 25% improvement in
the past l0 years.
Most improvements in chiller effi-
ciency come from increasing the water-to-
refrigerant heat transfer efficiency. This is
done by adding surface area in the evapo-
rator or condenser, or by improving their
overall heat transfer capabilities through
tube enhancements. Part-load efficiencies
haveincreased as well, mainly due to com-
pressor modifications such as aerody-
namic diffuser design and variable-speed
control.
In 1988, the Air-Conditioning &
Refrigeration Institute (ARI) developed
certification procedures for part-load per-
formanceof centrifugal chillers. This fur-
ther emphasized the importance of part-
load chiller performance. However, misin-
terpreting part-load performance data can
lead to inefficient operation.
Although many centrifugal chillers
manufactured today have improved part-
load efficiencies, itis important to distin-
guish the different conditions under which
the chiller is rated, particularly entering
condenser water temperature.
Our field monitoring of pre-1980
model centrifugal chillers supports the
theory that their optimum efficiency point
isat full-load. The kW per ton continues to
decline as the chiller approaches full-load.
This field monitoring is also consistent
with manufacturers' full- and part-load
ratings of new chillers.
Fgurme l shows field monitored data as
compared to onemanufacturer'sratings for
a450-ton singlecompressor hermeticfixed-
speed centrifugal water chiller. Our chiller
testing procedure requires collecting simul-
taneous electrical input and thermal output
data foraperiod of onetotwoweeks. Thdi-
tionally, this has presented problems for
the engineer, mostly due to the ability to
accurately monitor water flow rates.
Stephen B. Austin is a project engineer
with Burroughs Wellcome Company,
Greenville, North Carolina. He received
his BS. degree in civil engineering and
M.S. degree in mechanical engineering,
both from North Carolina State Uni-
versity. Austin isa member of the Ameri-
can Institute of Plant Engineers and
the IInstitute of Electrical and Electronics
Engineers.
We use a transit time ultrasonic liquid
flowmeter capable of non-intrusively meas-
uring and recording flow rates of relatively
pure liquids in full pipes. Chiller power con-
sumption is measured using a recording
demand analyzer. Iemperatures of the enter-
ing and leaving chilled water, entering and
leaving condenser water, and outdoor wet-
bulb temperature are recorded.
The simultaneous chiller dataislogged
and then entered into a computer where it is
analyzed in a spreadsheet program. Data
analysis has uncovered capacity and effi-
ciency problems and a variety of optimiza-
tion opportunities.
Figure 2 shows a manufacturer's chil-
ler efficiency curves for the same chiller
rated at different condenser water condi-
tions, These curves are expressed in kW per
ton versus percent refrigeration load. The
curves shown are for entering condenser
water temperature at ARI conditions and
constant 85*F (9'C).
ARI conditions are based on a 2.5'F
(1,4%C)condenser water temperature drop
for each 10%% load decrease starting with
85F(29%C)at 100% load and droppingto
60%F (16'C) at 0% load. The optimum
loadingpointis58% load at ARIcondenser
water conditions compared to 80%% load at
constant 85F (29'C)condenser water. The
constant 80%F (27%C)condenser water tem-
perature chiller curve has its optimum load-
ing point at 85%% load.
The shape of the chiller curve is dic-
tated by the combination of components
and rating conditions. The heat exchangers
(eaporator and condenser)at partload con-
ditions are effectively oversized, therefore the
water and refrigerant have closer approach
temperatures. This effect causes better YW
per ton values at partload and contributes to
the bowl shape of the chiller curve.
At lower condenser water tempera-
tures, the advantage of oversized heat
exchangers reaches a point of diminishing
returns. The result isa flatter shaped curve.
At full-load, heat exchanger limitations
account for the upward turn of the curve.
Reduced compressor and motor efficiency
atlow loads account for the upturn of the
curve below 40% load.
Other than the rating conditions, the
main factor in determining the optimum
loading point is the compressor. A com-
pressor map can be chosen to give an opti-
mum loading point over a range of loadings.
Multiple-stage compressors have sharp
efficiency curves and lead to a more well-
defined optimum loading point. Single-
stage compressors have flatter shaped
compressor maps and contribute to a
broader chiller efficiency curve.
The chiller efficiency curve most often
seen in literature israted at ARI conditions.
This is useful in calculating the integrated
partload value (IPILV)of different chillers.
The IPL is the average power consumption
per ton based on the number of cooling
hours at different loadings and the kW per
ton value at that loading. In this way, the
partload performance of different chillers
istaken into consideration.
ARIconditions may bea good way to
evaluate the annual performance of a
chiller, but they should not be used for
evaluating steady-state performance. The
ARI chiller curve overstates the part-load
efficiency for a constant condenser water
temperature.
For instance, at 60%% load under ARI
conditions, the entering condenser water
temperature is75F(24%C). Iftheentering
condenser water temperature is 85'
(29'C), the kW per ton value can be 15%
morethan thetypical ARl condition value
In some cases, the ARI curve may not
even be a good indicator of annual chiller
performance. The cooling load in a typical
industrial facility is driven primarily by
internal loads. A reduction in heat-
producing processes within the plant
would not necessarily be accompanied by
a reduction in condenser water tempera-
ture. Some plants cycle cooling tower fans
to maintain a minimum condenser water
temperature and may never get to 60%F
(16%C)condenser water.
Figure3 shows the average condenser
water temperature versus percent load
measured in an industrial facility com-
pared to ARI condenser water conditions.
Another common misapplication
that compounds the error of using the
ARI curve for steady state conditions is
to control the chiller by using the percent
amperage load as the percentrefrigeration
load. While this is a reasonable approxi-
mation at higher loads, at low loads itindi-
cates cooling loads considerably higher
than actual because of lower power factor
and chiller efficiency.
Figure 4 shows the measured per-
cent refrigeration load versus percent full-
load amps for a chiller compared to the
approximation. Note that at 60% full-load
amps, the refrigeration load is 55% of full-
load. At 30% amp loading, the percent
refrigeration load is only 20%.
The combination of operating a
chiller based on typical performance
curves and basing the percent refrigeration
load on percent amps can be very ineffi-
cient. Common practice in multiple chiller
plantsisto starta lag chiller when the lead
chiller exceeds its optimum loading point,
per ARI rating conditions. Then the per-
cent refrigeration load is based on percent
amperage of the chiller. We have seen cases
where the combination of these two prac-
tices was costing plants 20% or more in
operating efficiency losses.
Atwhatloading point onalead chiller
should a lag chiller be started? When two
chillers can operate more efficiently than
one. Using the chiller efficiency curve at
ARIconditions in Figure2,the kW per ton
rating at 82%%% load and 41%% load areequal.
Based on thiscurve, itwould be most
efficient to starta lag chiller when the lead
chiller reaches 82%% full-load. Therefore,
the plant is likely to start the lag chiller
when the lead chiller reaches 82% load,
based on amps. What is wrong with this
very typical operating scheme!
First, recall that the ARI chiller curve
is based on decreasing condenser water
temperature with decreasing load. In this
case, the condenser water temperature has
not changed; the plant cooling load was
just divided from one chiller to two.
Second, percent load ampsare not equal to
percent load tons.
How much does this cost the plant?
Eighty-two percent full-load amps equates
to 80% refrigeration load. If the entering
condenser water temperature is 85'R
(29%C), the rating at 80% load is0.6l0WW
per ton and 0.718kW per ton at 40% load.
Compare this to 0.576 kW per ton at
both load conditions based on the ARI
curve. Operating two 600-ton chillers at
40% load (instead of one at 80% load)will
consume an additional 52 kW, or 18%%
more power than necessary to provide the
exact same tonnage.
Operatethe lead chiller to 100%% load
before starting the lag chiller. At no point
on thischiller's efficiency curve can the kW
per ton rating be lowered by dividing the
load in half. Fxr some chillers at low con-
denser water temperatures, the lag chiller
may be started before the lead chiller
Reaches 100%%% load, however, this is rare.
Manufacturers' kW per ton ratings at
different loadings and condenser water
temperatures can be used to verify the max-
imum load point for the lead chiller.
When an older chiller is used, part-
load data will not be available, but chiller
log sheets can be used to determine opti-
mum scheduling at different loads. Be-
cause the older chiller curves are flatter
than the new chiller curves and the opti-
mum loading point is near full-load, it is
highly likely that the lead chiller should be
operated to full-load.
Once the lag chiller is started, how
should the cooling load be divided between
chillers? Ifthechillers arethesame size and
have the same part-load performance
curves, the load should be divided equally.
Combinations of different chillers at
different loads have to be evaluated in-
dividually, but be sure to use the proper rat-
ing conditions.
In selecting a new chiller, the opti-
mum loading point should be based on the
cooling load profile of the facility that the
chiller will serve. For instance, if internal
heat loads are high and the chiller will
spend a lot of time near full-load,select the
compressor, drive motor and tube bundle
combination sothe optimum percent load-
ing is near full-load.
Conversely, if the cooling load profile
isdriven primarily by outdoor conditions
and the chiller will operatethe majority of
the hours at off-design conditions, select
the chiller components so the optimum
loading point is at part-load.
Substantial electrical cost savings
are achievable by operating the most effi-
cient combination of lead and lag chil-
lers, In multiple chiller plants, optimum
scheduling can often be accomplished
with no investment and, at other times,
withadding a nominal amount of piping.
Projectswe haveworked on and have seen
implemented have had paybacks ranging
from 0 to 2years.
A lag chiller should be started only
when two chillers can operate more effi-
ciently than one. This should be based on
efficiency ratings at constant condenser
water temperature, not the ARI chiller
curve. In most situations, this point is at
full-load.
In plantswhere a chiller is never fully
loaded, a smaller, more efficient chiller
that would operate more hours at its opti-
mum loading point may be cost justified.
These projects have varied in the 3.5 to 6
year payback range. When purchasing a
new chiller, the facility's annual cooling
load profile should dictate the optimum
loading point.
Knowing the optimum loading point
ofaspecificchiller is critical in optimizing
operating efficiency. It is not always easyto
determine a chiller's part-load perform-
ancecurve for a specificset of conditions.
Sometimes field testing is necessary and in
every case, the right questions must be
asked. Nonetheless, the rewards-signifi-
cant increases in chiller plant operating
efficiency-often justify the effort.
At the time this article was first writ-
ten, the author was employed as a project
engineer in the Energy Engineering Unit
of Carolina Power & Light Company,
Raleigh, North Carolina. The findings in
this article were based on chiller studies
conducted for that company.
eaders from the air-conditioning and
refrigeration industries met near the
nationscapital to update developmentson
the environmental issues surrounding
today's common refrigerants. ASHRAE,
alongwiththeAir Conditioning & Refriger-
ation lnstitute(ARI), sponsoredthetwo-day
rmeeting.
ASHRAE and ARl invited represen-
tativesfrom major segments of the indus-
try, including equipment manufacturers,
producers of chemical refrigerants, con-
tractors and service groups, and end-
users (including building owners and
others involved with applications such as
food preservation). These organizations
sent their top volunteer officers and chief
staff representatives.
ThefirstASHRAEARI CFC Roundta-
ble was held three years ago as develop-
ments related to the ozone-depletion issue
began to accelerate and to become more
technically challenging. lt became evident
thatto meet these challenges, the industry
needed to establish a forum where infor-
mation and concerns could be shared
freely. Thus, the first roundtable was
created and,asthe issue has evolved and
the industry progressed, others followed.
On May 20-21, the fourth roundtable
was held. ASHRAE President Damon
Gowan chairedthe opening session, wel-
coming the industry representatives.
Each participating organization had an
opportunity tocomment on its view of the
issue, with some organizations describing
their own related internal and external
activities, and others providing assess-
rnents of impacts and particular concerns.
ASHRAE'EPresidentElect(now President)
Donald Rich summarized the Society's
activities in research, standards and
guidelines, technology transfer and edu-
cation.
The meeting focused on four key
technical components of the issue of par-
ticular current concern throughout the
industry: factors influencing choices of
refrigerants; refrigerant recovery, recy-
cling and reclamation; building code
recognition of CFC substitutes; and
accelerated CFC1HCFC reductions.
Each technical session featured an
overview sumrmary by a recognized ex-
pert in the area, followed by a discussion
period (or mini-roundtable) chaired by an
industry leader heavily involved in the dis-
cussion topic.
Asthe industry looks intothe next cen-
tury, several questions will need to be
resolved to make reasonable and cost-
effective choices on refrigerants. Among
the questions that two technical sessions
considered are:
State or local building codes in the
United States have provisions related to
refrigerant safety (generally through adop-
tion of one of the nation's three model
codes). Codes, in turn, rely on ASHRAE
standards as a resource.
The Society's Standard 15, ''Safety
Code for Mechanical RefrigerationC' and
Standard 34, ''Number Designation and
Safety Classification of Refrigerants''
address necessary safety considerations
in refrigerant systems. ASHRAE is cur-
rently updating both standards to reflect
changes necessary for the new alternative
refrigerants.
ASHRAE vice president Billy Man-
ning (who is director of engineering and
educationforthe Southern Building Code
Congress International) emphasized the
importance of updating the nation's local
codes as soon as possible to permit in-
stallation of equipment employing new
alternative refrigerants. With equipment
now available, he indicated that his code
Organization was processing an immedi-
ate change to add refrigerants R-123 and
R-134a, This action is interim in nature,
awaiting final approval of the ASHRAE
standards.
Roundtabie participants discussed at
some length the recovery, recycling and
reclaiming of used refrigerants. Refrigerant
conservation and how used refrigerants
can be recycled pose many unanswered
questions at this time. ARl standards
related to the performance (and certifica-
tion) of recycling equipment and the
acceptable purity of refrigerants were
addressed in the technical session.
The Clean Air Act (CAA) amend-
ments, passed bythe U.S. Congress in late
1990, mandate refrigerant conservation in
1992. The Environmental Protection
Agency expects to issue regulations later
thisyear for implementing the CAArecov
ery provision.
The meeting concluded with a gen-
eral discussion period chaired by Robert
Johnson, ARls Board Chairman. In the
wrap-up session, the importance of con-
trolling all refrigerant emissions (whether
CFCs, HCFCs, ammonia or mixtures) in
the manufacturing, testing, installation or
servicing of equipment could not be over-
stressed.
Herbert Gilkey (who chaired the
ASHRAE committee that developed the
SocietysGuideine 3,''Reducing Emission
of Fully Halogenated Chlorofluorocarbon
Refrigerants in Refrigeration and Air-
Conditioning Systems'')spoke for the par-
ticipants in stating that,''Controlling emis-
sions is not only being environrmentally
responsible but may make it possible to
sustain equipment for ts normal lifetime by
reusing refrigerants currently at work in
building systemstoday''
icrostructural analysis of
advanced ceramics and compos-
ites provides valuable product
control and reliability informa-
tion. However, correct interpre-
tation is dependent upon correct micro-
structural preparation procedures. A properly
prepared surface will exhibit true sample
integrity (i,e,, removal of all preparation-
induced damage).
For brittle materials, such as monolithic
ceramics and ceramic matrix composites
(CMCs), traditional cutting, grinding, and pol-
ishing operations generally are unable to pro-
duce true sample integrity. The major problem
lies in the production of unnecessary surface
and subsurface damage. Successful microstruc-
tural analysis requires the proper choice of
abrasive type, size, and bonding characteristics,
as well as correct machining speeds, relative
directions, and applied loads.
To properly prepare materials for microstruc-
tural analysis, all surface and subsurface dam-
age must be removed. For ceramics, surface
damage is characterized by either grain pullout
or scratches. Subsurface damage is harder to
identify, but usually takes the form of subsur-
face cracks or grain fracture.
Indentation studies by Lawn et al, '% reveal
that abrasive interaction with the surface is
dependent upon both the abrasive shape (sharp
or blunt indentation) and its bonding character-
istics (rigid or loose). For ceramics, a sharp
abrasive (indenter) tends to produce less struc-
tural damage than a dull abrasive (blunt indenter).
For this reason initial grinding should be conducted
with diamond instead of ceramic abrasives. For
example, diamond abrasives retain their sharp edges
during grinding while SiC abrasives fracture and form
blunt abrasives that tend to create excessive subsur-
face damage.
Ceramic sample preparation requires diamond
abrasives for initial cutting and grinding operations;
however, the most successful final polishes have uti-
lized mechanochemical polishing abrasives to remove
remaining damage.
Techniques for microstructural preparation of
ceramics can be categorized into two major areas.
These include: (1) structural ceramics and compos-
ites, and (2) electronic (components) ceramics.
Tables I and II show the general guidelines for these
procedures.
The initial stage of microstructural preparation is
performed by sectioning and mounting the specimen.
Sectioning a sample is required to obtain a workable
specimen size, as well as to locate specific areas ot
interest within the specimen. Proper sectioning to
the approximate area of interest will minimize the
requirement for coarse grinding. Note that excessive
coarse grinding can produce excessive damage to the
sample, which can be difficult, if not impossible, to
remove.
In order to minimize damage during sectioning, cut-
ting speed, applied load, blade type, lubrication, sam-
ple orientation, and sample fixturing must be consid-
ered in accordance with the properties of the ceram-
ic. Table III provides some specific guidelines.
The wafering blade grit size is an important consid-
eration for brittle materials. For extremely brittle
materials such as silicon and GaAs, a finer grit blade
is required to minimize damage. figure l shows a sili-
con wafer package cross-sectioned with a Series 10
and Series 15 diamond blade. The Series 10 blade
produced significantly less damage, thus reducing the
subsequent preparation requirements.
A similar comparison can be made for fiber com-
posites. Figure 2 compares a boron/graphite golf club
shaft cut with a Series 15 blade versus a Series 10
blade. Sectioning with a section 10 blade maintained
the integrity of the fibers; whereas, sectioning with
the larger diamond produced significant fiber frac-
ture. For tougher ceramic materials such as BjC,
SiLA1ON, ZrO, , or Si,N, brittle fracture is less catas-
trophic, therefore sectioning with a coarser grit blade
(Series 20) is appropriate.
In addition to diamond size, the wafering blade dia-
mond concentration is important. For materials such
as ceramics, glasses, and other brittle materials, a
low-concentration grit blade is recommended. A low-
concentration blade has a higher abrasive load, which
reduces sectioning time for brittle materials. For dia-
mond sectioning of MMCs or cermets, a high concen-
tration blade is recommended for more efficient
cutting of the metal component. For sectioning met-
als, applied load is a secondary consideration to the
size and number of abrasives particles.
Sectioning ceramics and composites can be accom-
plished with either a low-speed or high-speed dia-
mond saw. For ceramics, sectioning time is greatly
tatives from manufacturing, designers, consumers,
professional institutions, trade associations, indepen-
dent interests, and the parent body, with no single
predominant interest. A refractories and industrial
ceramics subsector of CICS was initiated in 1986.
This sector includes refractory manufacturers, raw
material suppliers, distributors, and refractory
installers and refurbishers. To date, 60 companies
(primarily in Europe) from refractory and associated
industrial companies have registered with (CICS with
32 certificates of approval awarded.
In order to become certified, a company must have
a documented quality system which complies with
the appropriate ISO 9000 standard. CICS conducts
an appraisal of the company's documented quality
program as well the practical outworking of this writ-
ten program. The assessors are registered by the
Institute of Quality Assurance. If the appraisal shows
to be favorable, a certificate of approval is granted.
Certification is maintained by announced follow-up
visits to the company at regular intervals.*
The use of monolithic refractories (often termed
unshaped refractories) such as castables, plastics,
gunning mixes, ramming mixes, sprayables and coat-
ings, and injection mixes have increased in populari-
ty. The principal applications of monolithics in the
steel-mill melt shop involve repair using gunning
mixes. There is currently a strong emphasis on the
improvement of more conventional castables, includ-
ing vibratables, for use as lining systems in ladles in
particular. In a recent survey, refractory manufactur-
ers said that an increase in the use of castables would
be the most significant trend in the near future. As a
group, these monolithics make up approximately
one-half the production tonnage of refractories today.
Since monolithics are used in such a wide variety
of applications, the proof of performance often relies
on full-scale field trials, according to R. E. Moore of
the University of Missouri-Rolla (Rolla, MO). The
fact that so many of these products are essentially
fired in situ and function in more or less steep ther-
mal gradients makes it difficult to define service con-
ditions. Control and simulation of installation condi-
tions are essential for utilitv of test information for
monolithics. Testing of monolithics at the present
time serves the purpose of product development,
quality assurance, field certification, and post-
In the development of new refractory products, the
The procedure for evaluating the HMOR for oxidiz-
ing trough castables involves placing a sample bar
into a test furnace, allowing it to soak at a tempera-
ture just long enough to reach a uniform tempera-
ture, and then breaking it. For castables with
non-oxidizing compositions, the sample is heated in
the furnace and soaked for a minimum of 4 h.
Researchers performed comparative HMOR tests
using oxidizing and non-oxidizing compositions at
short soak (15 min) and long soak (4-h minimum)
times in both oxidizing and non-oxidizing atmo-
spheres. Results showed that the HMOR data for the
short soak showed little relation to those with a
longer soak in either non-oxidizing or oxidizing con-
ditions. The longer soak data for the two furnace
atmospheres correlated reasonably well, however.
A slag test based on slag and iron in an induction
furnace was used in place of the rotary slag test nor-
mally used to measure slag resistance. Two major
weaknesses of the rotary slag test used as a research
and development tool for trough castables are
overemphasis of oxidation in leading to erroneous
conclusions that carbon additions have a negative
effect on slag resistance, and the fact that interface
effects such as the slagiron line cannot be evaluated.
In the induction furnace crucible test, the samples
form an inner crucible. The iron is melted bv induc-
tion, and a layer of slag is maintained during the
test. Erosion is measured at the slag/iron interface
after a specified time and temperature. Advantages
of this method over the rotary slag test are that oxi-
dation effects are reduced because of the continuous
slag and iron cover, and that interface effects are
definitely present.
The refinement of the tests used to evaluate HMOR
and slag resistance of trough castables has led
researchers at North American Refractories to a bet-
ter understanding of the wear mechanisms in blast
furnace troughs. The new methods, along with field
trials, have helped them in determining the best
directions for product development, leading to signif-
icant improvements in service life.
Researchers at Magnesita S.A. (Rio de Janeiro,
Brazil) and The University of Leeds, School of
Materials (Leeds, U.K.) have initiated a comparative
study of low-cement and traditional castables in
regard to thermomechanical properties. The refracto-
ry materials tested were traditional and low-cement
commercial fireclay and high-alumina castables.
Samples for testing were prepared following stan-
dard laboratory conditions for mixing, casting, cur-
ing, and drying. The sarmples were then fired in an
electric furnace for 5 h at 1200 C with a heating rate
of 6(0%C/h. After firing, the specimens were prepared
for testing by sawing and polishing.
Results of the tests showed that low-cement casta-
bles exhibited better properties relating to the frac-
ture initiation (including MOR, critical stress
intensity factor (K;,,), and fracture initiation energy
C7..)). The low-cement castables also had a higher
fracture energy accounting for the whole of the frac-
turing process and a lower degree of damage on a sin-
gle quench or multiple cooling cycles. The
low-cement castables, however, maintained the high-
est strength values in all thermal shock conditions.
As a result, when the absolute retained strength is
the parameter for selecting castables for applications
in which thermal shock takes place, the low-cement
castables seem to be a better choice.
While the testing and characterization procedures
for monolithics at the present time are not standard
in terms of strict adherence to prescribed tests and
procedures such as ASTM or other standards agen-
cies, there has been a good deal of attention given by
both manufacturers and users to the development of
standard tests for the physical and mechanical prop-
erties. On the other hand, very little has been done
in the area of standardizing test methods to gauge
installation properties of monolithics according to R.
E. Fisher of Plibrico Company (Chicago, IL). Below is
a review of the current use of installabilitv tests and
some of the current needs in this area in the areas of
plastic and castable refractories.
The workabilitv index has been the traditional
method of measuring the plasticity or rammability of
in position by two alumina bars that transmit the
load. Typical gases used during tests to prevent air
infiltration are Ar, CO, or CO,,
The DCB method was successfully applied to measure
the crack velocity propagation in MgO-C samples at
rates of the order of 10'% m/s. This method could even-
tually be adapted to measure work of fracture under
constant displacement. Attempts thus far to measure
the toughness and the crack velocity propagation in
MgO-C bonded bricks at 1500 C and higher have been
unsuccessful. Some of the problems encountered
include nonuniform heating of the sample and failure of
thermocouples in a CO atmosphere. Work is continu-
ing to overcome some of these limitations.
Researchers at Nippon Steel Corporation (Chiba,
Japan) have developed a diagnostic system that can
automatically and rapidly measure the residual thick-
ness of ladle refractories. The system is able to mea-
sure the ladle lining thickness up to 1000 C and has
an accuracy of 3t10 mm. Using two types of sensors
a high-frequency eddy current sensor and a differen-
tial transformer, the system can also detect metal
penetration into the lining.
The diagnostic system consists of a sensor box
(housing the two sensors), a drive unit, and a data
analyzer. The drive unit is comprised of an elevator
and a tranverser to move the sensor box vertically
and horizontally and a turntable for rotating the
ladle. A centering device to correctly place the ladle
is also installed on the turntable. The measured data
is instantaneously calculated, analyzed, and present-
ed on the display of a computer showing the develop-
ment, longitudinal section, cross section, and
progress of lining wear.
Using this system extends ladle life in a safe man-
ner, prevents molten steel leakage through early dis-
covery of metal penetration into the lining, and
reduces ladle inspection and measuring manpower
requirements by simplifying the lining thickness
measuring process.
In the continuous casting of steel through shrouds
and submerged nozzles, steep temperature gradients
often cause failure of the refractory. Because of this,
nozzles with a high resistance to cracking and snap-
ping and a method by which the spalling resistance of
nozzles can be evaluated, has been in high demand.
To fill this need, researchers at Harima Ceramic
Company, Ltd. (Hyogo, Japan), have developed a test
method called the ST Spalling Method.
In this method, a nozzle is heated to a temperature
between 600%-1500%C, with subsequent quenching of
the outside in water to cause a temperature differ-
ence between the inner and outer surfaces. The noz-
zle is then cut crosswise and checked for cracks.
Results of this test have correlated well with the
results of actual use. This test allows for the selection
of the appropriate nozzle material for the intended
casting conditions, as well as contributes to the
development of new nozzle materials.
Although linear fracture mechanics are used in the
area of advanced ceramics, investigation to see if this
is applicable to refractories has not been fully con-
ducted. Thus, researchers at NKK Corporation
(Fukuyama, Japan) have performed single-edge
notched beam (SENB) tests on MgO brick in order to
evaluate the bending strength of samples with differ-
ent notch lengths and to see if fracture mechanics is
valid to refractory brick.
Notches with depths between 0 to 42 mm were pre-
pared with a diamond saw at the center of 60 mm x
90 mm x 330 mm MgO bricks. The maximum grain
size of the brick was 3.5 mm. SENB tests were done
with a span of 240 mm. Because of the coarse grains
of most refractories, it was found that ordinary linear
fracture mechanics is not applicable. It was found,
however, that it is possible to apply fracture mechan-
ics to refractories if a grain-fracture model is incorpo-
rated. The grain-fracture model incorporates a
correction factor that is dependent of the size of the
material's microstructure. Using this method on MgO
brick, the ratio of the apparent fracture toughness to
the ideal fracture toughness was calculated and plot-
ted together with observed values. The observed val-
ues and the theoretical values were found to be in
good agreement suggesting that the application of
fracture mechanics to refractories is possible using
the grain-fracture model.
Editors note: Copies of Standardization and Deoelopment of
Testing of Steel-Plant Refractories, from the 28th Annual
Symposium on Refractories, sponsored by the St. Louis Section of
the American Ceramic Society, may be obtained for 815.00 from:
Patty Ruth, University of Missouri-Rolla, telephone 314/341--4430.
THE SPECIFITY of practical use of the Toms effect requires a transition from the study of
very diluted solutions of high-molecular polymers to that of concentrated ones in which,
by definition, intermolecular interactions are essential.
Various explanations of the effect of supermolecular structures of a polymer in solution
on drag reduction exist. According to one of the first hypotheses [1], the Toms effect is
caused by large particles (fibrils) made of a large number of polymer macromolecules and
also by the fluctuation net from macromolecules and fibrils.
Authors of the works [2, 3] consider that the condition of minimization of turbulent
friction is the existence of the fluctuation net of polyethyleneoxide-macromolecules (PEO),
but not of stable supermolecular formations of the ''particles'' type. Basically it is important
to estimate the validity of these hypotheses.
Dynamics of drag reduction under the conditions of PEO (WSR-301, WSR-701,
BADIMOL and not imported)- injection in a water flow has been studied in the experi-
ments described in [4-6] for developed turbulent flows in a pipe and between coaxial
cylinders (ICC). Measurements were carried out in a wide range of shear stresses, time
intervals t, polymer concentrations C (particular emphasis was put on the study of con-
centrated solutions). A typical example is given in Fig. 1: experiments were carried out
on the ICC-installation, the inner cylinder of which was immobile and the outer one, with
a diameter of 110 mm of the inner surface, rotated. Clearance between hydraulically smooth
surfaces of coaxial cylinders was 10 mm. Drag reduction was y = (1- C,/C;4). 100%4,
The extremal (maximum) character of dynamic curve of effectiveness is attributable
to the change of relation of two processes, occuring simultaneously under the conditions
of turbulence:
the break-down of supermolecular formations leading to the increase of the Toms
effect,
the degradation of macromolecules decreasing the effect.
Theoretical analysis of the mechanical destruction process shows that the main action
in the break-down of macromolecules is due to the low energy microstructural turbulence,
but generation of the largest number of the deformation sign changes and of the largest
amplitudes of stresses in the polymer particles is due to the highest pressure gradient.
Hence, the energy of deformation is directly proportional to the work of the friction
shear stresses, the wave number of pulsation and the third power of the polymer particle
length [8]. That is why the association of macromolecule strengthening with the PEO-con-
centration increase must accelerate their mechanical destruction: this fact explains the
increase of rate of the effect reduction, due to the C -- increase (Fig. 1).
In order to verify these conclusions, experimental study of drag reduction dynamics
was carried out together with a control of the polymer solution quality. The molecular
mass of polymer M was determined from the intrinsic viscosity of diluted solutions which
was measured by means of a capillary viscometer with diameter of capillary tube 1.1 mm
and length 2 m. Kinematic viscosity ; of concentrated solutions was measured using
standard capillary viscometers VPZh-2 and VPZh--4. The extinction coefficient (cloudness
of medium)a was determined from the Bouguer law: a = (lnJ,/J)/L, where J, is the inten-
sity of light passed through a layer of solvent with thickness L, J is the same value for the
solution. The value of light transmission J/J, was measured on a spectrophotometer
VSU-2 (''Carl Zeiss, Jena'')in the wave-length range i from 0.3 um to 1.0 um at L = 10 mm.
The experiment was carried out in the following way. Basic PEO-solution (WSR-301,
M 3.5- 10%) with mass concentration of 4 - 10%, self-restrained for three months, was
diluted to concentration 5 - 10% on impeller mixer during twenty four hours. Space between
the cylinders ''ICC'' was filled by the solution. The outer cylinder was untwisted to the
velocity of 17 m/s during -- 15 s. The installation ran in such regime for time t; then it
was stopped; the solution was taken out and its light transmission, kinematic and intrinsic
viscosity were measured. Thus, the solutions working in a flow for t = 0, 20, 150, 500,
1500 s were tested at the same hydrodynamic conditions. Results are given in the form
Cold is a very common problem in the food industry. The
demands set on modern food hygiene and maintaining the
quality of foodstuffs involve the handling and storing of
food at low temperatures. According to the regulations of
the European Community, the highest permissible temper-
ature of fresh meat during the cutting of carcasses is + 7'('
and that of the cutting room is + 10'C (Ministry of
Agriculture and Forestry, 1982).
Cold cutting rooms have increased thermal discomfort
and cold stress and strain among slaughterhouse workers.
Radiant asymmetry, cold draft, elevated air humidity and
low floor temperatures are also common complaints
(Ilmarinen et al, 1987; Nielsen, 1986; Nordström et al, 1976).
The lack of sufficient thermal insulation of the extremities
is a particular problem. Bare hands are usually in direct
contact with cold meat and the body fluids of the slaughtered
animals. Physical cooling is one reason for numbness and
stiffness in the hands and fingers, impairing manual dexterity
(Enander, 1988). Furthermore, whole-body cooling can
reduce muscle strength. Finger temperatures of 17.-2'(t
are common during meat cutting when working without
protective gloves (Ilmarinen et al, 1987). Intense hand
cooling may increase the risk of occupational accidents
involving the hands of meat-cutters (Meese et al, 1981). The
accident rate of meat-cutters is among the highest of all
occupational groups, and most of the accidents involve the
hands (Tammela and Korhonen, 1984; Kullman, 1986).
The feet of the workers are exposed to low floor temper-
atures during the entire workday. With decreasing floor
temperature below 20--22C, the complaints of local dis-
comfort increase (Fanger, 1977; Olesen, 1975); the floor
temperature in cutting rooms may be under + 5'('
Under the circumstances described, it is difficult to
maintain body heat balance in the work of meat-cutters.
Further stress comes from static work with a rather low
metabolic rate, 260 W on average, ranging from 170 to
400 W (Ilmarinen et al, 1987; Nordstrdm et al, 1976).
Clothing has great potential to minimise thermal dis-
comfort and unwanted effects of local cooling. However,
the clothing normally worn by meat-cutters is thermally
defective and garments with higher heat capacity should be
developed (Enander et al, 1979; Nielsen, 1986, 1987). There
is also a significant seasonal influence on the clothing used.
Workers use clothing with greater thermal insulation during
winter compared with summer, even though the temperature
at the working place is the same (Olesen and Mdrck, 1988).
The aim of the study was to design new functional work
clothing for meat-cutters, paying special attention to the
metabolic requirements of the work and the thermal and
general working conditions in slaughterhouses.
The pilot study comprised a review of the literature,
questionnaires and interviews concerning traditional clothing,
and workers' proposals and needs tor improved clothing,
the demands of maintenance of clothing, as well as analysis
of the work, and working and thermal conditions (Fig. 1).
The traditional clothing worn by meat-cutters consists of
a white cotton work coat, trousers, apron and metal mesh
safety apron (Figs. 2 and 3), provided by the employer;
underwear and all other attire must be paid for by the
workers themselves. Therefore, old worn-out articles oi
clothing which are especially thermally unsuitable are usually
worn by meat-cutters. The same applies to work shoes (Fig. 4).
According to the questionnaire, the most common complaints
were: local cooling of neck and shoulders, ankles, wrists and
lower back. The body fluids of the slaughtered animals wet
the clothing, particularly the stomach and hands and wrists,
decreasing the thermal insulation of the clothing and causing
extra discomfort.
On the basis of the pilot study, special demands were set
on the new clothing (Table 1), and three different sets of
clothing were designed by the students of the Helsinki
University of Industrial Arts. A few workers used these sets
for 3-4 weeks on the job. At the same time, the materials
were tested and the thermal insulation values of the clothing
ensembles were measured on a thermal manikin (Tammela
et a, 1983).
Further modification was based on the opinions of the
workers and on the test results. Two sets of clothing were
designed (Fig. 5) and used by five meat-cutters on the job
during physiological trials (Ilmarinen et al, 1987). All the
workers wore the same long-sleeved and long-legged under-
wear made of 50/50% cotton/polyester.
Physiological measurements consisted of continuous
registration of heart rate (Johne & Reilhofer, Olli 332),
measurement of oxygen consumption (Morgan Oxylog)
(Harrison et al, 1982) in different work phases (10-30 min),
body core (YS1-401) and six skin temperatures (YSI-427).
On the basis of mean metabolic rate (260 W), the required
thermal insulation of the work clothing (!y) was estimated
to be 13 clo (Burton and Edholm, 1955). Rectal and
different skin temperatures gave feedback of the thermal
insulating properties.
The skin temperatures, especially those of neck, lower
back and chest, were several degrees higher with the new
clothing (Fig. 6) than with the traditional clothing with
about the same thermal insulation. However, in the trad-
itional clothing the insulation was unevenly distributed over
the body, The thermal state of the body was unable to
increase the low skin temperatures of the fingers, which
were in contact with the cold meat.
During the wear trials the workers wore dirt- and moisture-
repellent winter boots with pile lining and thin socks of
woolfpolyamide. The warmth of the feet was determined by
measuring the temperatures of the toes and instep. The
comparative measurements with the workers' own shoes and
socks showed that in leather boots the feet were much
warmer than when wearing the usual combination of socks
and shoes (Fig. 7).
On the basis of the results of the physiological measure-
ments and of the workers' opinions, the modified model
was manufactured to be used by five meat-cutters over a
period of three months in normal work to get information
about prolonged use, repair and cleaning. The leather boots
worn during the measurements were chosen as footwear.
The workers found the new set of clothing warmer and
more protective against dirt and moisture than the traditional
clothing. Dressing and undressing was generally regarded as
easy. The ease of dressing was due to the small number of
garments. The workers wearing the new clothing, and their
workmates, felt that the appearance of the clothing was
satisfactory. The design of the collar and the sleeves was
successful, The new garments had improved thermal
properties and protected against the bone chips which,
during sawing of the animals, easily penetrate the clothing.
During the three-month wear trial, the workers found the
new leather boots better than the shoes usually worn. The
only negative property they mentioned was impermeability.
During the wear trial, the clothes were washed 20--30
times in the laundry of the slaughterhouse. The results were
good and the clothes maintained the requirements set for
the work clothing of slaughterhouses. The need for maint-
enance did not differ from that of normal polyester/cotton
blends, but it was less than that required by normal materials.
Later, the clothes were washed in a modern industrial laundry
and the results were good. The frequency of repair was similar
to that of normal polyester/cotton clothes. The ribbing in
the collars and sleeves lost some of its elasticity after 20
washes, but the polyester netting in the shoulders remained
unchanged. The materials used and the design of the clothing
were suitable for batch tunnel drying/finishing.
The final set of clothes consists of three pieces made of
polyester/cotton blend: an overall with braces, a work coat
and an apron (Fig. 8).
In the design stage, special attention was paid to protecting
the parts exposed to cold and moisture and to make the
collar and the sleeves functional. The clothes were designed
to be used with long-sleeved and long-legged underwear. Thus
the thermal insulation of the clothing ensemble (l,,; 1-3 clo)
is sufficient for the thermal conditions of the working place
(T 10'C, RH 60--85%, , 02 m/s)in relation to the
mean metabolic rate (260 W). The clothing ensemble with
the long underwear proved to be too warm for some of the
workers. In such cases the thermal insulation can be decreased
by using lighter underwear. The garments are also loose
enough to allow the use of additional insulating layers under
it, Thus the new types of work clothes can also be worn in
other tasks in the food industry - e g, in dairies and packing
departments, with colder environments and with lower
activity.
The best solution for meat-cutters' footwear is the use of
slip-resistant leather boots with no insulating lining. The
boots with a pile lining proved to be slightly too warm. The
boots are a good choice also from the hygienic point of view.
They are easy to clean by brushing and by wiping with a
damp cloth or sponge. Fat does not collect in the holes of
the bands, from which it is difficult to remove.
The project showed that the inconveniences of a cold
environment in meat-cutting work can be prevented by
functional work clothing and by increasing the thermal
comfort of the workers. The study indicated that the close
co-operation between workers, safety officers, employers,
research workers, designers and manufacturers is the only
way to succeed in designing and preparing functional work
clothing.
The authors express their appreciation to the project
group of the Helsinki University of Industrial Arts and to
the manufacturer of the clothes, Reima Oy. We are grateful
to the management of the LSO Food Forssa slaughterhouse
for their co-operation and especially to the volunteer meat-
cutters, who made the study possible.
Checkout systems in self-service shops have been
ergonomically studied in different countries beginning more
than ten years ago (see, for example, Ivergard, 1972; Ohara
et al, 1976; Bitsch and Peters, 1978; Köck et al, 1978;
Strasser et al, 1978; Elias et al, 1981). For German check-
stands there has been a first effort at analysis in the early
1980s, after the collection, review and filtering of the
knowledge present at that time (e g, review and general
proposals from Strasser and Miiller-Limmroth, 1983). From
these analyses a kind of minimum approval or standard
could be created (concerning ergonomics evaluation and
design), an approval which finally could even be incorporated
into a national agreement with regard to ''minimum require-
ments for cashier workstations'' (Anon, 1984a). Similar
proposals also based on human factors research exist for
other countries, too (e g, Grey et al, 1987).
Recently, however, new questions have arisen; this is not
surprising because technology has not stagnated, We have
now, for example, modular cashpoints or automatic reading
systems, called scanners, and the concomitant changes with
regard to the work itself as well as to cashiers' stress (see,
for example, Wilson and Grey, 1984; Buchberger et al, 1986;
Gros et al, 1986; Hinnen et al, 1987; Krueger et al, 1988).
Irrespective of discussion concerning the human and
economic aspects of laser scanners, twin-checkouts with or
without scanners have been designed and already put into
service in some self-service shops - these are of interest
both from an economic and an ergonomics point of view.
Twin-checkouts are advantageous because less area has to
be reserved for them in a shop (and space costs money) than
for two single (conventional) checkstands. In contrast with
the traditional technique of handling the goods with the
left arm from front to back of the checkout (and body), in
one part of the twin-checkout the cashier must work with
the left arm in the reverse direction. This unusual method
of handling the goods caused uncertainty concerning the
assessment of workload, even fears that these cash desks
involve extra risks for occupation-related health hazards.
These questions of potential operator injuries and health
problems were the focus of the study reported here.
Twin-checkouts without scanners in the self-service
shops of a retail company were analysed by time studies
(global registrations of the time needed for customers to
'pass through' in the two parts of the checkout), by photo-
graphic records of field observations, and by measurement
of the interior and exterior dimensions of the checkstands
from an anthropometric point of view. Using Sth-percentile
and 95th-percentile stencils of German females in con-
nection with true scaled side elevations and top view of the
checkout workstations, reach and clearance requirements
were evaluated, always essential for a rough ergonomics
assessment. This was necessary in order to prove whether
minimum demands and standard design recommendations
for the dimensions of cashier workplaces were satisfied.
In addition to this investigation of the somewhat more
static aspects of work, the dynamic demands of good handling
were examined. A general analysis of arm strength looked at
what was required in the two main (opposite) arm movement
directions employed at the twin-checkouts, Electromyo-
graphic investigations of local muscle strain in the hand-arm-
shoulder region, which is dependent on different movement
directions in a horizontal plane, produced arguments for an
ergonomically more-or-less correct job design (see Strasser
et al, 1989).
Electromyography yields data on electromyographic
activity (EA), the envelope values of the amplified, rectified
and smoothed myoelectrical signals picked up by bipolar
surface electrodes. As is well known, for methodological
reasons these cannot indicate directly the level of muscular
strain, Furthermore, inter-individ ual comparisons of EA
values are difficult and data from different muscle groups
cannot be compared without applying difficult normalisa-
tion procedures (cf Miller et al, 1989). A certain consistency
of results can be ensured, however, when tests on subjects
are done without changes in the electrodes' positions once
they are attached, and when only one test parameter ( e g,
the direction of movements of the hand-arm system) is
varied. In this way it was possible to interpret trends in
muscular load resulting from different working conditions
in the checkout investigations.
Finally, records of the operators' sitting postures, and of
them turning the head when trying to maintain visual
contact with the customers, have been used as criteria for
the assessment from an ergonomics point of view.
In twin-checkouts two cashiers sit face-to-face as demon-
strated in Fig. 1. One (on the left)works with the traditional
technique of material handling from front to back and the
other (on the right) works trom back to tront. It is obvious
that this arrangement has the economic advantage of an
optimum utilisation of space in self-service shops (see also
Fig. 2). Comparison of the dimensions in Fig. 1 with the
legally specitied workplace layout of Fig. 3 shows that the
twin-checkout design does meet minimum legal specifications.
It is concluded that anthropometric data have been taken as
the basis of inner and outer dimensions (clearances and reach
requirements), and that the workplaces 'fit' users from the
5th to the 95th percentile.
Indeed, detailed analyses (Strasser, 1987) showed that
most of the essential dimensions meet the minimum require-
ments for checkout systems obligatory on manufacturers in
Germany since 1984 (see Anon, 1984b). Yet seat-to-work
site relationship for the right hand keying in the prices on
a compact electronic cash register (which is about 25 cm
above the work counter) is not optimum, Considering a
preferred thigh clearance of 17 cm in both parts of the
twin-checkout, the optimum distance of 26 cm -- 30 cm
between seat height and work height for the right arm
cannot be produced (see Anon, 1984a). This would be
possible only when using a flat modular register.
Concerning the overall layout of the two parts of the
checkstands, there is only one circumstance which is really
unusual and that is the working direction from the rear to
the front on one side, How this has to be evaluated from
an ergonomics point of view, and what consequences for
the goods coming from the front - usually is inclined to
anticipate the speed of the conveyor and picks up the goods
as soon as they are in reach. This, though, generally induces
a bending forward of the body trunk, The three photos of
Fig. 10 indicate that in forward systems this even leads to
operators sitting on the leading edge of the seat without
utilising the backrest.
In the backward system the goods are stopped at least
140 mm in front of the work counter (lower belt of Fig.9).
This also necessitates head movements, but experience
shows in this working technique that the cashier -- waiting
for the goods coming from the rear -- will utilise a backward,
or at least an upright, sitting posture. This brings about a
relief of the intervertebral discs in the lumbar vertebrae
region which is greatly desirable from a physiological point
of view (see, for example, Grandjean and Hiinting, 1985;
Mandal, 1988). This also leads to improvements with regard
to visual contact. The sitting posture, somewhat bent back-
wards with an (always positive) increase of the angle between
the thigh and the trunk, may be demonstrated by Fig. 11.
Of course, in both parts of the twin-checkout the more-or-
less inconvenient sitting posture also results from the design
of the chairs, which is not optimum. However, the back-
ward system offered more possibilities for using the backrest
of even those chairs.
When considering all points of view, the inference can be
drawn about backward systems that they certainly cannot
be considered a step backwards with regard to humane job
design. Instead, the conclusion can be supported that back-
ward systems for seated work even allow a better agreement
with physiological laws and a rather good fit of the work to
the person. Of course there are also disadvantages; for
instance, the fact that the cashier has no visual contact with
the customers during the time when they are putting the
goods on the conveyor belt. It is improbable, though, that
backward checkouts will bear a long-term risk potential for
physical defects, provided that intra-personal based
potential for health hazards are separated from work specific
issues. Job rotation between the two parts of a twin-
checkout has to be regarded as not unimportant to reduce
physical load, by means of changing the kind of work and
related stress. There is also a better possibility of visual and
acoustic communication between the two cashiers sitting
face-to-face in the twin-checkouts. Such innovations should
be regarded as an enrichment in work variety of cash work-
stations. Social and psychological factors, which are of
interest to the supermarket management as well as to the
manufacturer, seem to be relatively well established in twin-
checkouts as maintaining the motivation of the cashiers.
Versatility and flexibility, variety and dynamics in the
working environment will also, from an ergonomics point
of view, always be more desirable than uniformity, especially
if this uniformity would probably become a tight corset
in accordance with a pattern preseribed by legislation.
minimization. Minimization, while possible, is unlikely, given the
constraints of the models, and uniqueness is not possible in our
circumstance for these inverse problems. In Figure 3, the proposed
repository block is sketched in for reference ; the repository would
lie 100 to 200 m above the local water table [Sinnock, 1984]. The
values for K corresponding to the heads in Figure 3 are listed in
the appendix. Areas of specific values are also sketched in a
following figure, Figure 6.
This particular configuration is the reference smooth
configuration referred to in the rest of this paper. Since the
inverse problem is nonunique, there exist many other configurations
somewha t different from this one that will give reasonable residuals
and fit well into the regional picture as well. The choice among
these configurations is subjective, but small differences will not
affect the conclusions that can be drawn from further studies based
on this reference configuration.
To get an idea of what transport might be like in this water
table aquifer, a source of unit concentration in the element was
outlined (O.25 unit at each of the four corners) at the water
table. No consideration was given here to how a source of
contamination might reach the water table; the source was simply
assumed to exist there at the start of the calculation. In
calculating transport, because of numerical dispersion, round- off,
and the peculiarities of parabolic equations, one often cannot
simply ask when a given node (or grid block) first sees a nonzero
value for the calculated concentration of contaminant; the answer,
while not physically possible, is usually that the contaminant
8ppears immediately. What one needs to know is when a certain
fraction of the source arrives at a given node or what is the speed
of advance of a ''front'' of contaminant at some level of contamina-
tion. In Figure 4 is displayed a contour of a contaminant level
6Tf af the source concentration, and discrete points at which the
concentration is calculated at 16'* or 16Tf are drawn in for a
time of 420 years. Calculations like these are typically plagued
from a liquid/vapor phase change. The last term is energy input by a volumetric
heat source. The term in brackets is the average heat capacity per unit volume of the
material as a whole, i, e,, rock matrix plus water plus gas, as defined by the following
equation:
where
s solid phase, i,e., the rock grain.
For example, p, is rock grain density.
In order to complete the governing Equations (1)-(4), relationships must be chosen
to specify how the fluxes depend on the unknowns. Similarly, the rate of vaporization
per unit volume, F,,, must be related to the unknowns. These relationships, which are
constitutive in nature and thus depend on physical mechanisms, are discussed in the
following two subsections.
The following transport mechanisms are accounted for in the mathematical model
used in NORIA:
solutions and would have been extremely difficult to test without the HYDR050IN
environment. The initial SRP solution for this case, shown in Figure 26,
s eemed very reasonable but was wrong. This solution is the kind that would
have been ohtained for a system with a very high molecular diffusion
l q. 26 vields the correct value of the weilbore
: :e ssure for a weil with an infinite-con ductivity
vertical fracrure, at carly and long times. It can
be assumed that it also yiclds the correct pressure
values during the transition period. A similar result
(y = 0.75) has becn obtaincd by MuskatV4 for a
well with partial penctration at steady state.
P',py) from Eq. 26 has been listed vs y iin
Table 1, and graphed in Figs, 8 and 9, where it
corresponds to the (xgL/) = =) curve.
W'hen a reservoir is in an early stage of depletion,
the pro ducuon of a particular weil is not perturbed
by the existence of other wells or by boundary
effects, ,After a while this is no longer true, and a
ne w solution must be devclopcd that consders the
re servoir boundaric s or the effect of the other wells,
Fig. 10 presents a schematic of a vertically
fracrnured w ell in a rectangular, closed drainage
system. Two types of fraccure will be considered
- uniform flux and infinite con ductivity - but, as
in the in finite-re servoir case, it 1S onlv necessary
to derive the dimension le ss pressure drop for the
uniform fiux fracture. This is obtained immediately
Several codes have been developed or specifically modified to use
for NNWSI performance assessment calculations. In COVE 1, five of
the se codes were benchmarked by generating four sets of solutions to
the problem published by Pickens et al.: SAGUARO, TRUST, FEMTRAN, a
modified version of FEMWASTE (Yeh and Ward, 1981), TRUMP, and TRACR 3D.
SA GUARO and TRUST are codes that solve the partial differential equa-
tion for fluid flow in unsaturated porous media based on Darcy' s law.
which results in Richard' s equation (Freeze and Cherry, 1979). GWVIP,
a reservoir engineering code, was also used to perform the hydrologic
calculations. In addition to Richard' s equation, GWVIP includes a
second momentum equa tion that accounts for air resistance to water
flow. No contaminant transport calculations were made for the flow
fields generated with GWVIP. FEMTRA N and TR UMP are mass-transport
codes and were used to predict contaminant movement for the flow
fields generated by SAGURO and TRUST, respectively. TRACR 3D is a
stand-alone code that calculates both fluid flow, based on Richard' s
equa tione and mass transport. The ma ss-transport codes, FEMTRA N,
TRUMP, and TRACR 3D, are based on the advection-diffusion equation and
con serva tion equations, with the inclusion of sorption as an addi-
tional term in the conservation equation. Sorption was modeled as an
equilibrium. linear-diffusion process in COVE 1YMa and CoVE 1YMb.
The cases solved by each code for COVE 1 are shown in Table 3-l.
The characteristics of the code s are given in Table s 3- 2 and 3- 3,
which summarize information described in the rest of this section and
in the cited references.
identical nodal arrangements were used. Considerable saving of
CPU time was achieved, however, with the present approach.
The second and third examples concerned transport in
blocky-fractured systems represented by systems of spheres.
These were chosen to demonstrate the utility of the computer
model in solving more complex practical problems for which
analytical solutions are usually intractable. In the second exam-
ple, longitudinal transport was assumed to occur in the frac-
ture, and radial diffusion was assumed to occur in the matrix.
The numerical simulation was obtained using initial and
boundary conditions identical to those in the first example. The
result indicates that the concentration front in the fracture of
the spherical system moves at a much slower rate than that in
the parallel-fractured system with a similar set of physical
properties. This is because a much higher degree of diffusion
occurs in the spherical matrix, which has a specific surface 3
times that of a prismatic slab and the same thickness as the
diameter of the sphere.
The third example was a practical problem involving migra-
tion of a radionuclide element ?'Np from a waste repository
buried in a fractured host rock represented by an equivalent
spherical-matrix fracture system. Two-dimensional transport
was assumed to occur in the fracture, and radial diffusion was
assumed to occur in the matrix. The model was used to simu-
late the breakthrough curve at the outlet. Because of the long
half-life of -'Np, it was necessary to run the model for 120
time steps to obtain a well-defined breakthrough curve. A
convergence study of the finite-element solution was also per-
formed to test the adequacy of the mesh used in obtaining the
outflow curve. It was found that concentration distributions
and computed values of outflow of the radionuclide obtained
by using the selected mesh were very close to those obtained by
using a substantially more refined mesh.
The computer model developed in this paper provides a
useful tool for analyzing contamination problems in fractured
aquifers and for evaluating potential migration of radionuclides
from subsurface waste repositories. This model has recently
been extended to deal with multispecies radionuclide transport
and chain reactions. The description of its extension and practi-
cal application will be the subject of the third paper in the
series. It should be emphasized that the computer model re-
quires a data base for the determination of the constitutive
transport coeflicients of the fractured medium. Field techniques
for obtaining these data, as well as procedures for the field data
evaluation, still need to be developed. In field applications
involving flow and transport in intricate fractured systems, one
must carefully interpret the simulation results of the present
dual-porosity model, keeping in mind the major assumptions
The bulk of calculations has been made for E4 B. Since require-
ment of accuracy and modelling were not precisely defined for
benchmarking and numerical implementation of b.c. is not always
exactly known a comparison of results on the percent level for
*6ay nd &as s impossible. In addition ay 4nd ay nave been
interpolated from results submitted by project teams. Here an
other uncertainty is introduced. Hence, the values might not ex-
actly coincide with those of the original maxima tables.
For the layered medium (case 2) in addition flux conservation and
concentration continuity have to be imposed at interlayer bounda-
riest
and
where X,4 is the coordinate of the interlayer boundary.
Possible discrepancies in calculational results might be induced
As mentioned in Chapter 2 a number gf parameter variations were
defined for case 1. In table 3.3 a summary of the parameter va-
riations calculated with the various codes is presented. As can
be seen most of the codes have calculated the basic set whereas
the number of codes that have been applied to the other parame-
ter variations varies strongly. The code GARD2S does not treat
dispersion and has thus only been applied to cases with the P4
variant of the Peclet number.
The discussion below is carried through only for the basic set.
The other parameter variations are discussed in section 3.4 and
the results are available on microfiche.
The constituent dispersion tensor, 5 is taken in the present model as
dependent upon the hydrodynamic dispersivity. This dispersivity is a function
of the local fluid velocity.
The general expression for the constituent dispersion, , in terms of P,
(molecular diffusivity), as well as hydrodynamic dispersivity 2, can be
written ast
where is the identity matrix and where the elements of ] . assuming the
medium to be isotropic, are given by:
where the 4 and 4. refer to longitudinal and transverse dispersivities,
respectively.
A linear equilibrium adsorption isotherm is used to give the retardation
factor R, (Equation 1-4) in terms of the distribution coefficient
54; 4=
Available through Rockwell International, Basalt Waste Isolation
Project.
The empirical distribution function in Figure 19 provides an esti-
mate of the population distribution function of the output. lhe sample
mean of the 10 values of Y provides an estimate of the population mean,
and other sample values provide estimates of their corresponding popu-
lation values in the usual manncr for random samples.
In order to see how well these s.mples function as the basis for
population estimates, five random samples of size 10 each were obtained
using the Latin Hypercube Sampling Program, and were entered into the
black box model to obtain outputs. The five empirical distribution
functions are given in Figure 20, while Figure 21 presents a picture
of the mean of all five e.d.f.'s together. In the background of Figures
20 and 21 is an accurate estimate of the true distribution function of
the output, obtained by using a random sample of size N = 1000.
The mean of all five e.d.f.'s, averaged in the vertical direction,
is plotted again in Figure 22. This is the same e.1.f. one would
obtain if al1 50 sample observations were treated as a random sample
of size N = 50, wnich it actually is. Above and bel ow the mean curve
in Figure 22 are curves that represent one standard deviation distance,
where the standard deviation is computed vertically from the five
curves in Figure 220, and smoothed using a three point movi.g average.
The standard deviation is presented to give some idea of the accuracy
involved in each individual random sample of size 10.
Slug tests were originally developed for estimating the flow parameters
of shallow aquifers, which are often well approximated as homogeneous
porous media. They have also been widely used to estimate the flow parame-
ters of fractured rocks, which are often highly heterogeneous, The attractive-
ness of siug tests is that they are inexpensive and easy to operate and require
a relatively short time to complete. However, available analysis methods for
slug tests are limited to a few ideal cases, In this chapter, we will attempt to
develop solutions to various models of slug tests that may be applicable in
analyzing the results of such tests where existing solutions are inadequate.
WVe then present case studies to demonstrate the use of the new solutions.
Cooper, Bredehoeft and Papadopulos (1967) presented a solution for the
change in water level in a finite radius well subjected to a slug test. They
obtained the solution from the analogous heat transfer problem in Carslaw
and Jaeger (1959). The transient water level h,(rp .,) at any point in an
aquifer normalized to the initial level in the well is:
w here
with boundary and initial conditions:
The constant A in Equation 4.2.2 describes the area open to flow. The
above equations in dimensionless forms are:
where the dimensionless terms are defined as:
After applying the Laplace transformation with respect to time, Eqs, 4.2,7
through 4.2.12 become:
T he general solution for Equation 4.2.17 is
where C; and Cg are constants.
By applying the boundary condition Equation 4.2.18,
Using Equation 4.2.17 and solving for C ;,
Therefore, the solution in the Laplace domain becomes:
Equation 4.3.8 is numerically inverted back to the real space. The normal-
ized pressure responses at the injecting well with a radial constant head
boundary at various dimensionless distances (r,) for u==I0* and u=108 are
shown in Figures 4.8a and 4.8b, respectively, As can be seen from these
figures, the boundary is undetectable if u is larger than 108 and the dimen-
sionless distance to the boundary (r,) is larger than 500. However, for a
much smaller value of u, (10f), the boundary effect can be felt at as far as
r, == 10. Unfortunately, the curves are similar to each other so that it may
be difficult to obtain a unique match. If the normalized pressure response is
plotted in log-log scale, the late time behavior of a system with a radial con-
stant head boundary can be distinguished from that of an infinite system
because the curve for the radial boundary does not follow a straight line of a
negative unit slope as can be seen in Figure 4.9.
Numerical simulations of well tests in two dimentional fracture systems
have been conducted using the numerical model discussed above. First, sys-
tems with two orthogonal sets of continuous fractures with constant spacing
were investigated. Snow (1950) showed that such a system behaves like a
porous medium under regional flow but he did not consider well test condi-
tions in his study, Since the pressure gradient along a fracture is not con-
stant or even uniform under well test conditions, it is not obvious that such a
system always behaves like a porous medium. Next, well tests were per-
formed in discontionuous fracture systems with distributed lengths and orien-
tations. Such systems should be closer representations of actual fracture sys-
tems, The study is litmited to two-dimensional systems but should provide
useful insight especially where vertical fractures are dominant.
Figure 5.5a shows the fracture mesh used in Case-1. The fractures have
a constant spacing of 1m and a constant aperture of 5 ym . The storage
coefficient of a fracture is set to 1.0x 10 1/m, which essentially assumes the
fracture is rigid. The theoretical value of the permeability of this system as
predicted by Snow's technique, is 1.02x 10: W /s. The storage coefficient is
1.00x 10*P p/m, which can be obtained by volumetrically averaging the value
for the fracture over a unit volume of rock. The pumping well is located at
the center and the pressure transient is monitored at observation wells
where
At the exterior cylindrical boundary, the condition is, for an infinite
region:
and, for a finite region, either no flow,
where
or specified pressure:
Solutions to the dimensionless form of equations 2.5.4.2.1-2.5.4.2.4 are
given in Van Everdingen and Hurst (1949) and were derived using Laplace
transform techniques. For exaample, the flow-rate response to a unit change in
pressure boundary condition (eq. 2.5.4.2,3a) and the pressure response to a
unit withdrawal flow-rate boundary condition (eq. 2,5.4.2.3b) for an infinite
outer-aquifer region are :
In the case of an unconfined aquifer with a well screened through the free
surface, the screened length, by 4s adjusted as the saturated thickness
varies in time.
where
and where
Equations 3.3.1.1b and 3.3.1.1c are valid for the z-coordinate directed
vertically upward.
For wells drilled at an angle f, to the vertical:
The following order generally has been observed for data input: (1)
Fundamental and dimensioning information, (2) spatial geometry and mesh
information, (3) fluid properties, (4) porous medium properties, (5) source
information, (6) boundary condition information, (7) initial condition
information, (8) calculation parameters, (9) output specifications. Items 5,
6, 8 and 9 have transient data associated with them, and data are input in the
item-order given. The static data are read only once while the transient data
are read at each time a change in the data is to occur. Only the data that
are being changed need to be entered, because any unmodified data will remain
the same over the next time interval of simulation. Each input record number
identifies a particular record in the input-data form listed in table 5.1.
The following data are invariant throughout the simulation.
where
The slugs in the flow path and the source term at the repository are
adjusted for radioactive decay in each time step by solving the Bateman
equations. A five-member chain of equations is used in computation of
radionuclide quantities as a function of time. For the decay chains with
very rapidly decaying nuclides, each of the short-lived radionuclides,
i.e., Pu-241, Ra-225, Cm-242, Pb-210, and Np-239, is assumed to remain in
secular equilibrium with its immediate precursor. No branching ratios
are considered in the decay chains.
The curies released to the accessible environment at 2 m from the
repository, , j. is the sum of slugs transported to the spatial coor-
dinates corresponding to the boundary of the accessible environment,
i.e- ,t; ; 2 227. cumulative curies released to the accessable envi-
ronment ror ee 'f' eaaionuc1i4e alons tne ;''' yatn, 4( , . re the
curies in all slugs reaching the boundary integrated from time 0 to t.
for gt; ; 2 27 wnere k is the index for time steps, and K is the
number of time steps.
Though the capability is not illustrated in the test problems,
SPARTAN can compare the performance of a site, i.e., the cumulative
curies released to the accessible environment, with the EPA release
requirements (EPA, 1984). The measure of performance is simply the
''EPA release ratio'' (ER),
sample and further illustrated the marked differences be-
tween the fluid flow behavior of this random fracture net-
work and that of a homogeneous porous medium.
To see how the density of fractures affected the hydraulic
behavior, the following three examples were analyzed. All
three examples consisted of two fracture sets with the
uniform characteristics given in Table 4. Fracture centers
were random as described above.
Figures 9a-9c show the three fracture meshes studied.
The difference between Figures 9a, 9b, and 9c is that the
same fractures have been squeezed into successively smaller
areas. Figure 9a is 40 x 40 cm, Figure 9b is 30 x 30 cm, and
Figure 9c is 25 x 25 cm. Thus the number of fracture
where C7 is the initial repository inventory for the m-th nuclide. The
Dirichlet condition is given directly by (10). The Cauchy condition is
given by the vector product,
Approximate solutions to the transport equations (44) sub)ect to ap-
propriate initial and boundary conditions (5), 6), (T), (8) are obtained by
applying the method of weighted residuals (MWR). A detailed description of
the method has been given by Heubner and Thorntonf, among others. Additional
descriptions can be found in reports by Yeh and Ward' and Duguid and
Reeves.'
To apply the MWR, the computational domain is subdivided into an as-
semblage of smaller sub-domains referred to as finite elements. The
dependent variable, c'', 1s approximated ty g''' (Y each element with,
where N, are the trial (basis) functions approximating the spatial variation
or c''' n the element, and C(t) are the unknown time-dependent nodal values
of concentration for the m-th nuclide. Bilinear basis functions defined on
quadrilaterals are used for the N;. Substitution of (11) into the governing
equations results in a set of equations with some residual error, w-ggg' The
MWR chooses the unknown f unctions C2 such that the residual vanishes over
the element domain in some weighted average sense. The weighted residual is
set to zero by requiring
The ''breakthrough'' activity history at the 500 m observation point for the
three members is shown in Figure 9. Also shown is the solution generated
with the Swedish code RAN CH; 5 RANCH is based on a classical one-
dimensional advective-dispersive radionuclide model for saturated porous
media. The initial inventories defined in the INTRACOIN study are in ar-
bitrary units of activity ratio as are the output results. FEMTRAN computes
in units of mass concentration. The following relation was used to convert
activity ratios to mass concentration ratios,
where w'' denotes atomic weight and the remaining symbols are defined as
before. The comparison between the RANCH and FEMTRAN solutions is good over
the entire range of activities varying over six powers of ten. The mass
balance results for the computation are given in Figure 10. As before, most
error occurs early in the simulation reflecting the numerical approximation
to the initially discontinuous boundary condition. The band release bound-
ary condition is also discontinuous when t = yeaoy 8ere the source
concentrations are stepped down to zero. The err6 +,,,. '; (ncreases from
the leach time on as the nuclide is transported out of the region. The
least error is computed ra <4;y uhich is strongly sorbed resulting in an
effective transport velocity given by,
The error increases tor the taster running nucliaeg *'; g4 %%qg uitnn ef-
fective transport velocities of 1/60, 1/20 m/ yr, respectively. Improved
mass balance results could be obtained at the expense of taking more time
steps.
The head difference 3,j,Y AiSt next be expressed in terms of specific
head values which are related to the head values used to calcu'ate flows into and
out of the cell. 2n the hydrograph for cell i,),k (fig, 6), two values of time,
and m-1. are noted on the horizontal axis; the corresponding head values,
ni,;V ana n( ;, are naicstea on te vertical ais; tre slobe o' the cottea
1ine is an4 ; 14%w. n the method of comoutation utilized here, the flow
terms of equation 23 are evaluated at the more advanced time, %, while the
bydrograph s'ope, '/,;. 5 evaluated as
Thus the hydrograph slope, or time derivative, is approximated using the
change in head at the node over a time interval which precedes, and ends with,
the time at which flow is evaluated. This is termed a backwarc-difference
aporoach, in tat ''/,, 5 calculated over a time interval wnich extends
backward in time from 9, the time at which the flow terms are evaluated.
There are other ways in wnicn ''/,, 4ould be approximated; for example,
we could approximate it over a time interval which begins at the time of
flow evaluation and extends to some later time or over a time interval which
is centered at the time of f1ow evaluation extending both forward and backward
from it. However, there can be problems of numerical instability using
these alternatives. Numerical instability means that if heads are calculated
at successive times, and if for any reason errors enter the calculation at
a particular time, these errors will increase at each succeeding time as
the calculation progresses until finally they completely dominate the result.
By contrast, the backward-difference approach is always numerically stable--
Module 8A51AL allocates space for data arrays used by the 8A5 Package.
5nace s allocatea tor HNEW, HOLD, I80UND, CR, CC, CV, HCOF, RHS, DELR, DEuC,
aro 1OFLG. 5oace is allocated for the 5TRT array if the user intends to
calculate drawdown. Space is also allocated for an array called 8UFT ER,
whhich is used to accumulate various data arrays such as drawdown and cell-
by-cell flow terms when they are being calculated prior to output. To
conserve space, the user may specify that arrays 8UFFER annd RHS should
oCCupy the same space.
The number of spaces allocated for each of the arrays--OLD, 1B0UND, CR,
CC, CV, HCDF, RHS, 5TRT, and BUFFER is equal to the number of cells in the
grid. Twice that number of spaces is reserved for HNEW because it is double
precision. DELR and DELC are allocated a number of spaces equal to the
number of rows and columns, respectively. I0FLG (an array of flags used by
Output Control) is allocated a number of spaces equal to four times the
numbe r of layers,
Module 8AS1AL performs its functions in the following order:
Calculaticn of vertical conductance is conceptually similar to ca'culation
of horizcntal conductance, The finite-difference flow equation requires t^e
conductance between two vertically adjacent nodes. C$ ,j, 1/2 1s the
conductance between nodes 1,), and i,),k=1 in layers k and +1. Applying
ecuation 31 between two vertically adjacent model nodes (fig. 26) gives
w he re
K ,j, 112 's the hydraulic conductivity between nodes ',), and
'.J,=1 (Lt-); and
DELV,j,+1/2 is the distance between nodes i,),s and i,j,1 (L).
rather than specifying both vertical hydraulic conductivity and vertical
gr:d spacing, a single term ''Vcont'' is specified. Vcont between nodes 1,J,4
and ',,=1 is given by
, ne program requires that Vcont between nodes be entered as input data
rather tan calculating it in the program.
Several methods can be used to calculate Vcont depending on the way
that the aquifer system is discretized vertically. It is often desira5le
to use more than one method for calculating Vcont within the same simulation,
Many of the methods of calculation could have been includei in the model
progran, but the complexity of specifying where the various methods were
to be applied and eeping track of data requirements would make the program
This module calculates rates and volumes added to the aquifer by
areally distributed recharge.
: nan tte Ri ver or Drainn Rack ages, each of unich deal withh two linear
relatiOrships, In the third case, the 2HB ?acage does not attempt to
account feor change in storage in the aquifer material between the boundary
hhead and the simulated area.
Data describing each GHB, which is stored in a list, is soecified by
the user for each stress period. Input for each boundary consists of the
location of the boundary cell--layer, row, and column--the boundary head,
and the constant of proportionality. During the formulation phase of each
iteration, the term -(*H8 is added to the accumulator HS and the term -C
is added to the accumulator HCOF,
The 51ice-Successive 2verrelaxation Pack age (55%1) consists of tree
rima ry mudules and one submodule. They are:
PRREEQE is designed to perform a sequence of simulacions in a
single computer run. Each simalation consists of two separate problems:
Many pathways for a simulation are accessable with a card input data deck;
that is, no program modification should be necessary. Required input
begins with a title card followed by an option card. Depending on the
options selected, additional data are supplied using various 'Keyword''
data blocks. A data block consists of a Keyword card followed by appro-
priate data. The Keyword informs the program of the type and format of
he data to follow. ELEHENIS and SPECIES, if they are used, should be
the first two data blocks while the other keyword blocks may follow in
any order. The keyword END denotes the end of the inxut data and is
required once for each simalation. After the calcula tions f or one sixmu-
lar.on are completed, the program starts the data input process aga4n,
beginning with a new title and option card.
The general types of reactions that can be simlated are as follcws:
In the reconstructed data base we now have nitrogen speciation for
NH, and N04 considered separately, with reactions writen in terms of
rhe 'new' maste species, NH,. Although the thermodynamic data listed
here are realistic, they have been compiled from various sources and are
not documented. These data are given for example purposes only and
should be checked before application to specific problems.
IMn order to add a new element to the data base, the the rmodynamic
data base for the aqueous model of that element must be comp ile d. This
is obviously no simp le task and requires a critical evaluation of the
existing the rmochemical literature for the particular elemen t. The
uranium data listed below were compiled by F.J. Pearson, Jr. (personal
communication) large ly from Langmuir (L978) and Baes and Mesmer (1776),
but have not been docume nted in detail. The aqueous data for uranium
are given below for example purposes and may not be comple te enough for
applicazion to seawater or solutions containing significant amounts of
dissolved phosphate and flouride.
In adding uranium to the data base, ;'' as chosen as the maste r
specie s (number 24). The (arbitrarily defined) species numbers, reac-
tions, thermodynamic data for reactions, Debye-Hückel activity coefficient
P4ameers, an4 estimates of species concribucions to total alkalinity,
as used in the updated daca base are listed in table ).
then the folloing message is printed, ''I HAD TROUELE ADDING EINERALS
so THAT AA THE NECESSARY ELEHENTS HAD FOSI5IvE cONCENIRATIONS.
FLEASE ADJUST INITIAL cONCENRATIONs To soE SMALL FosITIvE qUANTIrY.:
Subroutine YCEP calculates the ass ociation constants for the
s ue ous species and the equilibrtum constants for the mineral phases.
Either an analytical expression or a Van't Hoff expressionn is used.
Subroutine 5TEP recalculates the uotal concentration of elements
in solution for a reacion step. In addition, any tempera ture change
for the reaction step is set in this subroutine.
Subroutine SET makes initial estimates of the activities of th:
ma ster species.
Subroutine HODEL iteratively calls other subroutines which provide
approximations to the solution of the problem. The program exits from
HODEL when the calculations have converged within specified limits.
Subroutine GAMHA calculates the activity of water, the icnic
strengt h of the solution, and the activity coefficients f or each species
using one of the following formulas:
Subroutine AOHOD is one the most critical subroutines of the program.
It was written with an emphasis on convergence at the expense of cotmputing
speed. The scheme employed in this subroutine is as follow s:
The subroutine uses the following me thod to revise master species'
activities:
If there is a large fraction of an element in a species involving the
square or higher power of the master species (e-g. 8s when N04T is the
master species) then a quadratic formulation is used,
where,
Subroutine CHE CK tests for convergence to the soluuion of the
equations. If the errors are less than specified tolerences (less than
l in the sixth significant digit) the problem is solved. If the soiuticn
has not converged then another iteration is begun. If mass balance is
not satisfied within 20 percent (a smaller tolerence i; substituted as
the ice rations increase) or the ionic strength is changing rapidly then
the atrix is not solved and only mass balance is considered for the
rexr iteration.
If the number of iterations for a single simulation exceeds 200
then the calculacions are terminated and the following message is printed,
'CALGuuATIOSs TERHISSATED AT 200 ITERATIONs.'
The numbers used in the convergence criteria are contained in a data
stateme nt in the code for this subroutine. These numbers seem to work
well. Relaxing the criteria may make the program run faste r, however,
experience has shown thar the last few orders of magnitude in accuracy
require few iterations. This indicates any time savings would be margginal.
Subroutine SOLVE copies the matrix coefficients frcm array AR to
array AS. Only the rows and columns of the necessary equations are
copied. Finally, the matrix solving routine SLNQ is called.
respectively.
The variance of unsaturated hydraulic conductivity for the
three-dimensional model, using (26) and (31) is
When a is a deterministic constant, from (12), the mean
specific discharge of a one-dimensional flow case (x; = 2,
; = 4. J, = J, Jg J4 = 0. h/&x, = &h/&x4 = 0) becomes
where K.= K, esp (-aH), In K,, = E[In K,]. and
J = 1 + dH/dZ is the mean hydraulic gradient. The first ex-
pected value term on the right hand side of (34) is simply the
variance of the logarithm of the unsaturated hydraulic con-
ductivity, 4,2f. The second expected value in (34) can be
determined by noting E[hdh/d:] = 0 for any stationary pro-
cess and by using
for any input spectrum S,,. Thus
for the exponential and the hole function covariances, respec-
tively.
The right-hand side of (35), KL, multiplied by J, the mean
hydraulic gradient, produces the mean moisture flux or specif-
ic discharge. Note K, generally depends on the mean hy-
draulic gradient, ie., the mean Darcy equation is nonlinear.
When J = 1, (5 = a(2J - 1) = x, (35) implies a relationship
between the mean infiltration rate and the mean capillary
pressure. Since K.= K, exp (- aaH) and K,, = exp Eln K,].
the geometric mean of the saturated hydraulic conductivity,
the effective conductivity of the unsaturated flow reduces to
Using the relationships in (32a) with J = 1, this can be ex-
pressed in terms of o),c; as
Equation (36b) represents the mean or effective conductivity-
capillary pressure relationship for steady vertical infiltration
through a perlectly stratified heterogeneous soil of unbounded
vertical extent.
Note that if o,'[2(1 + mi] > 1, K. becomes negative. This
unreasonable result occurs because higher order terms are ne-
glected in (12) and (7). For the saturated flow case, Gutjahr et
al., [1978] found an 18% error in this type of first order
approximation for one-dimensional fiow perpendicular to lay-
ering with a lognormally distributed K and o,' 1. For the
unsaturated flow case, the error due to the approximation
depends on a,', a, and i for J= 1. The magnitude of a,'/[2(1
-+ mi)] is likely to be greater than t for some soils. One
possible way to extrapolate to these relationships to large o, is
to consider the quantities in brackets in (35) and (36) as the
first two terms of the Taylor series expansion for ef as pro-
posed by Gelhar and Axness [1983]; then K. becomes
The more general formula (35) in which J is not restricted to
be 1, then can be expressed as
sion characterizing the Midconarcrt region, While included
here as part of the Midcontine. s 64s province, the Blue
Ridge and Valley and Ridge provuuces are characterized by
scattered and locally contradictory stress orientations (Plates 1
and 2). The scatter may reflect the inherent uncertainties of
the different methods used to determine stress or may repre-
sent a broad zone of transition in which the horizontal stresses
are approximately equal in magnitude and the apparent stress
field is more controlled by local inhomogeneities within the
crus.
The relative magnitudes of the principal stresses were con-
strained on the basis of information from the measured mag-
nitudes of in situ stress and from the current style of deforma-
tion within each region. In the western United States the high
level of seismicity and broad zone of active faulting indicate
generally large stress differences. In the eastern United States
the seismicity is more locally concentrated, and whether the
regions of high seismicity occur within localized zones of
weakness or result from some mechanism producing local
stress concentrations is unknown.
Two main styles of deformation can be broadly distin-
guished on the basis of the relative magnitudes of the princi-
pal stresses: (1) a predominantly extensional mode in which
the least principal stress is near horizontal and remains in-
variant and (2) a predominantly compressional mode in
which the greatest principal stress is horizontal and remains
invariant. One end-member of the extensional mode is pure
normal faulting. whereas pure thrust faulting is the corre-
sponding end-member of the compressional mode. Pure strike
slip faulting, such as occurs along much of the San Andreas
fault (with the exception of the Big Bend area), represents the
other end-member in both modes.
The available stress data often indicate a mixed style of
faulting within a given stress province (Plate 2, Table 2). Re-
gions characterized by extensional tectonics (normal and
strike slip faulting) are found in the western United States and
include: the Sierra Nevada, the Basin and Range-Rio Grande
rift province, the northern Rocky Mountains, the Snake River
plain, and local regions along the San Andreas fault near
right-stepping en echelon offsets, Normal, growth faulting
within the sedimentary section in the G ulf Coastal Plain prob-
ably results from a purely extensional stress regime. Regions
of compressional tectonics (strike slip and thrust faulting) are
located in the eastern and central as well as the western
United States and include the Pacific Northwest, the Big Bend
area of the San Andreas, the Colorado Plateau interior, the
Atlantic Coast province, and the Midcontinent provinces.
The occurrence of a mixed style of faulting within a stress
province with consistently oriented stress axes can be inter-
preted in several ways, Two of the principal stresses may be
approximately equal in magnitude and minor regional varia-
tions in their relative values might determine the style of fault-
ug; this is suggested by earthquakes along much of the west-
ern margin of the northern Basin and Range [e.g., Hamilton
and Healy, 1969]. Alternately, while comparison of shallow
freologic data and in situ stress measurements) and deep
(focal mechanisms) stress indicators within a stress province
Eenerally suggests that the orientations of the principal stress
axes may remain unchanged throughout the upper crust, it is
Tossible that the relative magnitudes of the principal stresses
may vary markedly with depth. Crosson [1972] attributed the
wcurrence of shallow thrust events, deeper strike slip events,
and very deep (222260-km depth) normal faults with similarily
oriented stress axes in the Puget Sound area, Washington, to a
more rapid increase with depth of the vertical stress relative to
the horizontal stresses.
Figure 6 shows the map of least principal horizontal stress
directions overlying the heat flow map of the United States by
Lachenbruch and Sass [1977]. The map illustrates that the
general level of heat flow is much higher in the West than in
the East, that in the West the pattern of heat flow is much
more complex and marked by large regional variations (the
areas of both highest and lowest heat flow in the United States
are in the West). and that abrupt transitions sometimes occur
between the major heat flow provinces (cf. Lachenbruch and
Sass [1977, 1978] and Blackwell [1978] for discussion of the
heat flow provinces in the western United States). As men-
tioned above, similar features (a complex pattern, major re-
gional variations, and abrupt transitions) also characterize the
modern stress field in the western United States. In particular,
a generally good correlation exists between the stress direc-
tions and heat flow data in the actively extending Basin and
Range-Rio G rande Rift province and along its margins. This
broad area of crustal rifting is characterized by a mean heat
flow of 88 mW/m' (2.1 HFU), well above the continental
mean of 63 mW/m (1.5 HFU) [Lachenbruch and Sass, 1978].
A fairly good correlation exists between regions of high
heat flow and the lateral extent of crustal extension, most no-
tably along the Colorado Plateau margins, where the high
heat flow and associated recent faulting and Quaternary vol-
canism extend well inward of the plateau physiographic
boundary [Thompson and Zoback, 1979]. An area of the Colo-
rado Plateau interior of relatively uniform average heat flow
(63-67 mW/m'; 1.5-1.6 HFU) can be defined that resembles
the Colorado Plateau interior defined on the basis of stress
data [Reiter et al., 1979; Thompson and Zoback, 1979]. The
correlation breaks down along the Rio G rande rift-southern
Great Plains boundary in northern New Mexico, where a 9(0?
change in orientation stress transition is well controlled by
data from Pliocene to Quaternary volcanic fields in both the
Rio Grande rift and the southern G reat Plains. As might be
expected from the recent volcanism on the G reat Plains, the
region of high heat flow extends through that area, and the ac-
tual heat flow transition lies to the east. However, the avail-
able stress data suggest an extensional stress regime in the
southern Great Plains with -NNE-SSW direction of exten-
sion, as opposed to the much more active WNW-ESE exten-
sion in the Rio Grande rift.
New data on heat flow in the Mojave block (San Andreas
stress province) are quite uniform and reveal a mean heat
flow of 67mW/m' (1.6 HFU) [Lachenbruch et al., 1978]. The
heat flow rises sharply to the east along a north-northwest
trending boundary that coincides with the eastern limit of ac-
tive seismicity and a change from predominantly strike slip to
normal faulting within the Basin and Range province.
Lachenbruch and Sass [1978] have developed thermo-
mechanical models compatible with observed extension rates
to explain the high surface heat flow in the Basin and Range
province. These models, which require convection (either
solid state or by magmatic intrusion) in the crust and up-
permost mantle, are consistent with gravity data which re-
quire that an influx of mass must accompany the horizontal
extension in this province [Thompson and Burke, 1974]. Such
shallow level thermal sources are consistent with abrupt heat
Describe the mechanical properties of discontinua (fractures, joints, bed-
ding planes, inclusions, voids) present in the rock units, Provide site-specific
data as well as available generic data fron sisilar rock units and environments.
If the information is available, the discussion should include the coefficient
of friction, the compressibility of fractures and filling saterials, and the
effect of heating and changes of pore pressure on the sechanical properties of
the joints, fractures, bedding planes, and other discontinua. Discuss the
effects of the discontinua on the aechanical properties of the rock mass (e . 9.,
strength and deformation characteristics).
Present the results of laboratory studies of the thermal properties of
the rock units, Provide available site-specific data as well as generic data
from similar rock units.
Include discussions on the thermal conductivity, heat capacity, and
coefficient of thermal expansion of the rock units.
Present the stress field data, if' available, and list the assumptions used
to infer stress from field observation. Also present applicable stress measure-
ments that have been made in the candidate area or at the site. Include a
discussion of the expected direction and magnitude of the principal stresses
as a function of depth.
Describe any special thermal, mechanical, thermomechanical, or other
properties of the rock units that were considered in developing the conceptual
design of a repository appropriate to the site (e. g., brine migration, thermal
decrepitation, thermal dewatering). Provide available site-specific data as
well as generic data from similar rock units.
Describe excavation investigations that have been conducted within the
candidate area, and discuss pertinent excavation experience in similar rock
types under siilar conditions using various techniques such as controlled
blasting and echanical nonblasting. This discussion should include information
on how the investigations were sonitored, analyzed, and applied to the conceptual
design of a repository appropriate to the site. The discussion should also
include an assessment of the potential damages produced by the various techniques
and appropriate mmethods for avoiding or aitigating such damages.
ince has been studied by gravity surveys. Figure 3 also shows the succeeding
ash-flow tuffs, including the Timber Mountain Tuff, which has an age of about
11 m.y, and was extruded from the Timber Mountain caldera, and the Thirsty
Canyon Tuff, which was extruded from the Black Mountain caldera. Both
calderas have been mapped in detail.
Mapping in the test site and in the ranges to the east has disclosed several
thrust fault systems of Mesozoic age. The best understood of these has been
named the CP thrust after the Control Point Hills south of Yucca Flat, This
thrust places upper Precambrian and lower Paleozoic rocks over middle and
upper Paleozoic strata. Figure 4 shows the approximate location of the root
zone of the CP thrust and the east edge of the CP outliers, which corresponds
Recognition of the influence of clastic strata upon movement of ground
water in the miogeosynclinal carbonate rocks is important for many purposes.
First, and most critical, it may permit the partial assignment of hydraulic
boundaries needed for hhe construction of an initial ''working'' flow net of
an interbasin aquifer system. A flow net, however crude, is essential for aca-
demically oriented studies, such as those involving the dating or the geo-
chemistry of ground water, because the major heterogeneities introduced into
the flow system by the hydraulic barriers must be carefully considered during
selection and evaluation of sampling points. Knowledge of the distribution
and extent of the clastic rocks is a prerequisite in a hydrologic analysis of the
volume of a reservoir or of the interference between a well field and a promi-
nent spring discharge area. In prospecting for oil within the miogeosyncline,
exploration of basins surrounded by clastic strata appears to offer greater
potential for entrapment than those groups of basins connected hydraulically
by the regional movement of ground water through the carbonate aquifers.
Finally, in problems involving radiological safety, drilling of wells for the
monitoring of the movement of radionuclides from a reactor site, or from
the site of an underground nuclear detonation, clearly requires knowledge
of the position of all clastic aquitards, even the relatively thin ones. Geologic
study of the areal distribution of the thick clastic rock sequences and major
shear zones thus appears to be an impottant first step for semiquantitative
study of the movement of ground water within the complex carbonate aquifers
of the miogeosyncline.
overlain by an equally widespread rhyolitic to quartz latitic ash-flow tuff that
correlates with the Fraction Breccia at Tonopah (Nolan, 1930).
The lavas are generally more resistant to erosion than adjacent strata and
tend to form ragged dark-colored cliffs and ridges. Topography developed
on the intrusive nasses is generally more subdued, Where unaltered, the lavas
are mostly reddish-gray, brown, or medium gray and, where vitrophyric, they
are black. The lavas are commonly purple where slightly altered, dark green-
ish-gray where propylitized, and variegated pastel colors where intensely
altered as at Goldfield, Tonopah, and other smaller mining areas. In these
areas they are the chief hosts for precious-metal deposits. The rocks are
generally flow layered, commonly flow folded, and uncommonly flow brec-
ciated, Essentially all of the rocks are porphyritic and typically contain about
25 percent phenocrysts consisting mostly of plagioclase. In order of decreasing
abundance and requency the mafic phenocrysts are biotite, hornblende,
augite, hypersthene, and rare olivine. Ouartz is a common but minor consistent
in many rocks. The rocks range from basaltic andesite and leucogabbro to
quartz latite and quartz latite porphyry, Individual rock types, dacites for
esample, are similar chemically and petrographically throughout the area. In
some lava piles and intrusive complexes there is a trend to more siliceous types
with decreasing age.
Thickness of the lavas is as much as 1000 feet in the Belted Range east of
Kawich Valley (Fig. 1), about 500 feet in the southern F awich Range west
of Kawich Valley, possibly as much as 3000 feet in .ae central Kawich
Range, and about 1500 feet in the northern Kawich Range east of Cactus
Flat, Thicknesses are generally less than 500 fcet in the western part of the
area north of Gold Flat, on the flanks of the Cactus Range, and in the Gold-
field and Tonopah mining districts. In the Cactus Range intrusive rocks are
more abundant than lavas. They form masses up to 20 square miles in area.
Each lava pile is inferred to have been erupted from nearby or subjacent
vents. This feature, together with the widespread occurrence of intrusive rocks
of similar lithology and age, is interpreted as evidence for the existence of a
substratum of coeval middle Miocene magma of equal or greater extent.
The maximum range in age of the lavas is indicated by potassium-argon
ages on the underlying and overlying tuffs of 22.3 and 17.8 m.y., respectively
(H. H. Mehnert, 1966, written commun,; R. W, Kistler, 1965, written
commun. ) A date of 18 m.y, was obtained on one of the lavas north of Gold
Flat (H. H. Mehnert, 1966, written commun.). Thus, the igneous activity
producing rocks of predominantly intermediate composition occurred within
a period of about 5 m.y, during the middle Miocene.
Chemical variation among selected samples of lavas from each of the major
mountain ranges is shown in Figure 2. The curves are best fitted visually to
long axis (Fig, 3). Although more complex in detail, this pattern closely
resembles those of the central domes of the Valles caldera, New Mexico
(Smith and others, 1961) and the Creede caldera, Colorado (T. A. Steven
and J. C, Ratte, 1964, oral commun.). The over-all structural pattern of
Timber Mountain may be discussed conveniently in terms of three fault types:
an outer arcuate fault zone, radial and longitudinal faults, and an apical graben
0Fig. 3).
An arcuate fault zone (point 1 of Fig. 3) is exposed at the southeast margin
of the Timber Mountain dome and is concentric with it. In an earlier struc-
tural description of this part of the dome (Carr, 1964), the arcuate fault
zone was called the ''inner ring fracture.'' The zone is shown in detail on two
geologic quadrangle maps (Carr and Ouinlivan, 1966; Byers and others,
1966).
Numerous normal strike faults in this zone generally parallel the dome
margin along an arc 3 mi long on the southeastern side of Timber Mountain
(Fig. 3). The zone probably extends farther to the northeast beneath a cover
of younger rocks. A persistent ring fault (Carr, 1964, Fig. 2, p. B18) of
about 200 ft displacement consistently uplits the center of the dome relative
to the flank and has been intruded by probably syntectonic, discontinuous,
brecciated rhyolite dikes. This ring fault divides the arcuate fault zone into
inner and outer parts: faults in the inner part dip outward parallel to the
ring fault, omit parts of the section, individually have several hundred feet
of throw, and displace mainly the moderately dipping (25' to 308) lower
part of the Cat Canyon. In the outer part numerous faults of small individual
displacement are mainly antithetic to the ring fault and repeat the steeply
tilted (35* to 65%) upper part of the Cat Canyon, The repetition of the Cat
Canyon section continues outward for at least a mile from the ring fault; a
few erosional windows reveal parts of the Cat Canyon that must be repeated
by faulting if dips are about constant.
Additional features of the arcuate fault zone include a syntectonic granite
porphyry dike, intruded into the lower part of the Cat Canyon on the south-
east flank of the dome, and later basaltic dikes and silicic tuff dikes on its
northeastern and eastern slides (Fig. 3). These intrusive elements are con-
centric with the arcuate fault zone, and the granite approximately coincides
with the exposed inner edge of the arcuate fault zone. A rhyodacite lava flow
(Fig. 3) located between the tuff dikes and the granite also vented in the
arcuate fault zone.
The arcuate fault zone and its accompanying rhyolite and granite porphyry
dikes are displaced (point 2 of Fig. 3) and therefore postdated by large radial
and longitudinal faults. Apparent horizontal offset of the ring fault is several
thousand feet in the area of the granite porphyry body. Some faults probably
related to the arcuate zone do not displace the stratigraphically highest Cat
end of fuly but decreased somewhat in early August,
and the ratio CO,:SO4 declined markedly on August
Just after noon on August 7, harmonic tremor
began, and it continued to increase in amplitude for
several hours. The combination of changing gas
fluxes, harmonic tremor, and occasional small earth-
quakes suggested an impending eruption, and ash
emission started at 1623, growing rapidly to full erup-
tion by 1627. This first burst produced an ash-laden
eruptive column that rose to an elevation of more
than 13 km. A small pumiceous ash flow swept the
area below the breach on the north side of the am-
phitheater (fig. 15). The How reached part way to
Spirit Lake, leaving a thin lobate deposit. Smaller
eruptions continued through the late afternoon and
evening, with one major sequence around 1930. A
culmiaating burst, nearly as large as the first one,
began at 2232. The intensity of harmonic tremor
decreased and small deep earthquakes occurred dur-
ing the period following this last eruptive burst,
which ended before midnight.
A dome began to rise in the vent crater on the
morning of August 8, filling it to about half its former
depth of 90 m by the end of the day and stopping
growth just below crater-rim level by August 10. For
several weeks following, the volcano remained rela-
tively quiet; there were few earthquakes and no ma-
jor eruptions. Some incandescence could still be seen
in the walls of the vent crater and in cracks on the sur-
face of the new lava dome. Gas emissions fluctuated
from moderate to low levels.
Following the August 7 eruption, the volcano
generally remained very quiet for more than 2 mo
(fig. 16), and seismicity dropped to its lowest levels
since before the reawakening of,the volcano in
March. A slow outward movement of the unstable
-sector on the north flank, however, indicated that
Tilt data received in approximately 10-min inter-
vals during the 24 hr spanning the May 25 eruption
are shown in figure 105. An abrupt deflationary
change in tilt that coincided with the beginning of the
major eruptive phase was recorded at the Ape Cave
North station (fig. 105A). A similar though much
smaller deflationary change in tilt may also have been
simultaneously recorded by the Ape Cave station
(fig. 105B); however, the magnitude of this change is
not convincingly above the noise level of this station,
which is about 0.75 urad. A maximum value of
0.75 urad tilt change is indicated by the record
shown in figure 105B.
The tilt change recorded on May 25 at the Ape
Cave North station and the maximum value esti-
mated for the change at the more distant Ape Cave
station allow the rough estimation of the maximum
depth of the source giving rise to these possible tilt
changes (fig. 106). Assuming a small spherical or
point source in an elastic half-space, the volume
change of the volcano as a function of depth may be
computed from the observed and the bounded values
of changes in tilt. Assuming that this point source is
directly below the center of the volcano, a tilt change
of 2 urad at the distance of the Ape Cave North tilt-
meter would result in a volume transported from
various depths to the surface as indicated by the solid
curve. An upper bound on the change in tilt recorded
by the Ape Cave tiltmeter of 0.75 urad requires all
possible volume changes to lie below the hachured
line in figure 106. The intersection of these two curves
indicates that the maximum source depth for this
eruption, based on this model, is less than 7 km.
None of the eruptions later in the summer at Mount
St. Helens produced immediate tilt changes larger
than 0.5 urad at the Ape Cave North station (fig.
107) or at other stations (fig. 102).
Electronic tiltmeters are well suited to measure
rapid, small changes in tilt that accompany eruptive
activity of short duration; however, these instru-
ments do not allow dependable determination of
long-term tilt changes of small magnitude because of
drift of the electronic components, the short base of
the sensor, and the site installation.
At Mount St, Helens, the electronic tiltmeter net-
work has met with modest success: changes associ-
ated with eruptions produced small changes in tilt.
These instruments recorded the gradual inflation of
the region surrounding the volcano, which continued
until the appearance of the first lava dome in mid-
June. Since then, additional tiltmeters, together with
those that survived the May 18 eruption, have indi-
cated no net change or at most a very minor deflation
or subsidence of the region immediately surrounding
the volcano from mid-[une to early October. Rapid
changes in tilt of a few microradians were recorded at
the onset of activity during the May 18 and May 25
eruptions. No immediate tilt changes larger than
O.5 urad accompanied surface activity during subse-
suent major eruptions of Mount St. Helens, which
occurred on June 12, July 22, and August 7.
Precise gravity measurements are a rapid and inex-
pensive means for monitoring deformation and mass
movement associated with volcanic activity. Assum-
ing that the effects of solid-earth tides, ocean tides, and
mass movements in the atmosphere have been taken
into account, gravity measured at a point fixed to the
Earth's surface will vary with time in response to ver-
tical displacement of the observation point and to
variations of mass distributions near the observation
point. Temporal gravity changes associated with
volcanic activity typically range from a few microGals
to a few hundred microGals (1 uGal = 1x 10-* cm/s')
and generally display a good correlation with elevation
changes.
Recent advances in the design of gravimeters and
refinements in observational techniques permit grav-
ity differences between widely spaced points to be
of the bulge near The Boot and north peaks 1 and 2,
compared to observations of May 2. One possible ex-
planation is that as bulging progressed on the north
slope, crack opening from tension-related slumping in
and beneath the Forsyth Glacier changed to crack
closing from compression-related bulging.
Two more infrared surveys were obtained before
the maior eruption of May 18 (Rosenfeld. 1980).
These predawn observations by the OANG show
thermal patterns similar to those on May 16, and also
the development of two small thermal features in the
upper Wishbone Glacier at approximately the loca-
tion of the thermal anomaly seen from helicopter on
April 11. There was apparently no major change in
the pattern of thermal emission from Mount St.
Helens during the last 50 hr prior to the major
eruption that would have signaled its immediate
occurrence.
Summing the excess radiant energy of each of the
infrared anomalies identified in figure 158, and using
the terrain immediately surrounding each anomaly as
the natural radiation level, yields a total emitted ex-
cess radiant power from the summit of Mount St.
Helens of approximately 1 MW (table 23). This pro-
cedure ignores the large areas only slightly above am-
bient temperature. A more inclusive procedure is to
consider the crater and upper bulge areas as two
larger regions, adapting background temperatures for
each based on locations away from volcanic thermal
sources. Using the two larger regions outlined in
figure 158, with background temperatures of -10.76
and -11,17 C for the crater and bulge, respectively,
the excess radiation is 2.3 and 0.34 MW, respectively,
in these areas. All of the above calculations ignore the
opacity of the atmosphere and presume graybody
emission. Thus, a reasonable estimate of the excess
radiant exitance from the summit area of Mount St.
Helens on May 16 is 3 MW. The total geothermal heat
How is an unknown factor, probably large, greater
than the excess radiance, as convective fumarolic
flux, subglacial melting, and advective hheat loss were
likely major processes. The total power for these
The video and microwave systems were fabricated
by Reynolds Electrical and Engineering Co., Inc.
(REECO) and Sandia National Laboratories under
authority given by the U.S. Department of Energy.
The systems were installed at Mount St. Helens by
REECO. Sandia Labs, USFS and USGS personnel.
System layout and components are described at the
end of this report.
The camera is located directly north of the volcano
at a site that affords a clear view of the volcano, a
view of the interior of the crater, and a 360' view of
the surrounding area. The camera can pan 355?, can
tilt over a wide vertical range, and has a zoom ratio of
10 to 1; and both focus and iris diaphragm can be ad-
justed. These functions can all be controlled from the
monitor console (fig. 194) in Vancouver. Power is
supplied by batteries that are recharged by propane
thermoelectric generators. The system can be turrned
on and off remotely each day and during poor
weather conditions, to save electrical power.
The video system provides a sharp, live color pic-
ture of the mountain and has proven to be a valuable
monitoring tool for hazards response and in planning
daily operations. During periods of good weather
in fuly and August, the system was operated for
as much as 13 hr/day. Electrical power is adequate
for about 65 min of pan, tilt, and zoom functions
per day, which permits viewers in Vancouver to
investigate the state of the volcano each morning
and look for changes that may have occurred over-
night. During late summer and fall, 1980, the video-
In many sections a lower, coarser massive bed A2a
is overlain by a finer, vaguely to conspicuously
laminated bed A2b (fig. 258 and cross section A-A',
fig. 259). The contact between the beds is generally
sharp, suggesting that they were emplaced by
somewhat different processes of sedimentation. The
lower massive bed A2a commonly is gradational with
the top of layer A1, suggesting that bed A2a was
emplaced by the same general processes as layer A1.
Immediately beyond the outer limit of layer Al, bed
A2a forms the base of the density-flow deposit; far-
ther out, bed A2a disappears and bed A2b forms the
base. In some intermediate areas where A2a is miss-
ing, the boundary between layer Al and bed A2b is
very sharp.
The three-bed sequence is common but not univer-
sal. Where layer A2 is thick within the northwest-
through-northeast sector, three beds constitute the
normally graded layer A2: laminäted medium sand
(A2b), laminated fine sand (A2b), and massive very
fine sand (A2c). The contacts between the beds as
well as the basal and upper contacts of layer A2 are
very sharp. Bed A2b locally comprises a fine sand bed
sandwiched between two medium to coarse sand
beds. In relatively distal areas where layer A2 forms
the base of the deposit but where bed A2a is missing,
the entire layer A2-beds A2b and A2c-is vaguely
laminated. Although bed A2c is texturally transi-
tional between bed A2b and layer A3, its upper con-
tact with the darker colored layer A3 is generally
sharp. In some areas where beds A2b and A2c are
missing altogether, layer A3 sharply overlies granule
gravel or coarse sand of layer A1 or bed A2a. Bed
A2c did not accumulate toward the lateral margins of
the flow in the east-northeast and west-northwest
sectors.
The surface of layer A2 is in many places quite
wavy. The massive bed A2a where distinguished
from layer A1 by a sharp contact may vary in thick-
ness by as much as 20 cm, causing the wavy surface.
The overlying laminated bed A2b forms ripples or
dunes that embellish the broader relief at the surface
of bed A2a. Some dunes are developed in bed A2a,
embellished by bed A2b, and draped by bed A2c.
Where bed A2c is missing, the nearly planar top of
bed A2a is capped by bed A2b only at dune crests.
Where only beds A2b and A2c are present, bed A2b
forms the dune and trough topography. while bed
A2c subdues the dunes, being thicker in troughs than
on crests.
The laminated bed A2b, which is thickest in the
northwest-through-northeast sector 10-20 km from
the crater, shows plane laminae, dune-foreset
laminae, and antidune-backset laminae. In some
places the dune or antidune internal structure. cor-
responds to surficial waves, most of whose crests are
normal to the inferred direction of flow (fig. 260).
In layer A2, as in Al, the juvenile gray dacite con-
stitutes 10-50 percent of the clasts, and lithic
fragments most of the rest. Along the lateral margin
of the deposit just east of the volcano, layer Al does
not contain gray vesicular dacite; but farther out on
this margin, in an area swept by an arm of the density
How from a more northern radial, layer A2 contains
the gray dacite. Some areas in the north-through-
such as roads, the lateral edges of the highest lahar
deposits on both the North Fork Toutle River and the
lower Muddy River were marked by abrupt flow
fronts at least 3 cm high. Flows of various consisten-
cies supported cobbles and boulders at or near their
surfaces. Fragile boulder-size clasts of soil and col-
luvium occur perfectly preserved in coarse gravel
bars. These clasts and the platy structure of the thin
basal layer of mudflow units suggest that vertical
shear was concentrated at the base of the flow and
that ''plug' flow occurred at least locally. The gradual
reduction in runup heights on trees with distance
from the axis of maximum velocity, however, in-
dicates a less concentrated distribution of shear in the
horizontal plane.
Fiow velocities ranged from less than 1.5 m/s in
downstream reaches on the North Fork Toutle River
to more than 40 m/s for highly mobile flows on and
near the cone. Apparent discrepancies between
velocities at individual points and the lower rates at
which the peak stage was propagated downvalley are
explained by local variations in channel configuration
and slope, loss of volume through progressive deposi-
tion, and the expectable difference between rates of
movement of the entire mass of a mudflow and flow
near the axis of maximum velocity.
Fow characteristics of the catastrophically induced
lahars, especially those in the South Fork Toutle River
and Muddy River watersheds, changed strikingly
downstream. Air-mobilized lithic avalanches and
flows were transformed to and replaced by mudflows
downvalley. In the South Fork Toutle River farther
downstream, the mudflow was changed to and re-
placed by debris flow and normal streamflow. This
contrasts with the massive lahar that originated on the
North Fork Toutle River, the peak deposits of which
were remarkably similar throughout the entire course
of the flow to the Columbia River. Downstream
changes did occur in the recessional character of this
How, particularly in the amounts and relative propor-
tions of debris flow and normal streamflow.
The lahar deposits are generally thin compared
with the depths of the initiating flows. Nonetheless,
impressive amounts of fill were deposited locally,
particularly by mudflows. For example, over 4 m of
channel thalweg fill occurred locally on the South
Fork Toutle River. Even where lahar-induced channel
deposition was slight or where scour occurred in
channels, overbank deposits are commonly more
than l m thick. At some sites on the North Fork Tou-
tle River, 3-4 m of fill occurred on the flood plain.
However, the postlahar channels are commonly in-
cised through the May 18 deposits and into older
alluvium or lahar deposits.
The present channels are unstable and are adjusting
rapidly to changes in geomorphic and hydrologic
conditions. Continued erosion will release much sedi-
ment from previously stable pre-May 18 channel and
flood-plain deposits, as well as from sediment
deposited on May 1ß and subsequently.
posed. They may be thicker elsewhere in the northern
part of the pumice plain. The deposits thin southward
toward the base of the stairsteps. The thickness is
generally less than several meters in the amphitheater
and on the northern flank. The estimated volume of all
pyroclastic-flow deposits of May 18 is at least
O.12 km', collectively an ''intermediate''-sized deposit
according to Aramaki (1961) and Sheridan (1979).
Darkness and poor weather prevented observa-
tions of the pyroclastic flows of May 25; they prob-
ably formed during the opening minutes (0232) of the
eruption, when activity was at its peak. Most of the
pyroclastic-flow deposits of May 25 extended north
from the vent for about 4 km, where they ended in a
deposit represents magma that had crystallized in the
edifice of Mount St. Helens before the May 18 erup-
tions (Lipman, Norton, and others, this volume).
Individual deposits at Mount St. Helens show
significant variation in particle size, and thus many
size analyses are necessary to characterize both in-
dividual deposits and all deposits of a single eruption.
The particle size varies greatly even in single
pyroclastic-flow units. Vertical sections through
deposits of May 18, for instance, show reverse
grading (fine to coarse upward) of light pumice lapilli
and normal grading (coarse to fine upward) of heavy
lithic fragments. Additionally, large pumice lapilli
and blocks are concentrated relative to the fine ash
matrix at the margins of levees, channels, and lobes of
pyroclastic-flow deposits of all ages. Exposed vertical
sections are rare in the pyroclastic-flow deposits.
Multiple samples of each Hlow unit were collected,
however, from vertical sections in phreatic pits in the
deposits of May 18 and from slump scarps in the
deposits of June 12 and May 18. Inasmuch as holes in
the post-fune 12 deposits do not maintain vertical
walls, it was difficult to sample more than 0.5 m
below the surface. For post-June 12 deposits, the up-
per 10-20 cm of the surface was scraped away to
remove air-fall deposits and the surficial layer in
which pumice lapilli and blocks are concentrated, and
bulk samples that typically weighed 4-7 kg were col-
lected. More representative samples that reflect ver-
tical and horizontal variation in particle size may be
obtained later when younger deposits become more
consolidated and have been dissected by streams.
Samples were dried, weighed, and then split into
particle-size fractions of greater and less than 32 mm.
The diameters of pumice fragments greater than
32 mm were measured by hand, and the samples in
each phi size class (Krumbein, 1934) were weighed on
a pan balance. The fraction less than 32 mm was split
with a sample splitter to obtain a subsample of
500-700 g. The subsample was then sieved gently by
hand for about 5 min in a set of sieves with a size
range of 16 to ] mm at 1-phi intervals. The C } -mm
fraction was transferred into a 3 to ,2 mm sieve set
(1-phi intervals), which was placed on a sieve
vibrator and vibrated at the lowest possible frequen-
cy and intensity and for the minimum time required
for particle separation (5 min or less) so as to
minimize particle abrasion. Fractions in each size
plass were then weighed on a pan balance. Weight
fractions in size classes less than ,; mm were deter-
mined for ash-cloud deposits by a settling-velocity
hydrophotometer method. Estimates of the volume
percent of pumice, lithic clasts, crystals, and glass
shards were made for each size class by visual ex-
amination with a binocular microscope.
Walker (1971) classified the components of
pyroclastic deposits into five groups: lithic fragments,
pumice fragments, dense glass fragments, crystals,
and vitric ash shards. All these components form
significant proportions of Mount St. Helens deposits,
except glass fragments.
Lithic fragments are dark-gray, black, deep-
reddish-brown, and brown foreign rock particles
derived mainly from the conduit walls and expelled
during eruptions. They are typically about 10 to
30 mm in diameter, but holocrystalline clots of
crystals as small as 1 mm are also found in thin sec-
tions, Most megascopic lithic fragments are andesite
and basalt, and microscopic xenocrysts and xenoliths
samples are bimodal, with the major modes falling in
the coarsest size fractions, from 32 to greater than
64 mm (table 52). This distribution reflects the abun-
dance and relatively high density of coarse pumice
fragments in the June 12 pyroclastic-flow deposits.
July 22.-Pyroclastic-flow deposits of July 22 have
intermediate median grain sizes but the greatest
ranges of all the pyroclastic-flow deposits, spanning
about S phi units (fig. 305, tables 50 and 53). One
sample (80R-186a) shows the coarsest median grain
size (about 12 mm) of all the samples of pyroclastic-
flow deposits (table 53). The large range in median
grain sizes probably reflects the broad range in grain
sizes present in the prominent levees, lobes, and chan-
Equilibrium thermodynamics provides an addi-
tional method for examining mineral stability and
water chemistry. Figure 387 is a plot of stability rela-
tions in the gibbsite-kaolinite-amorphous silica-
smectite system. The compositions of water from
ponds on the pyroclastic flows, from the inflow to
Spirit Lake, and from Spirit Lake plot in the kaolinite
stability field, but the data suggest ponds in the debris
avalanche are saturated with calcium smectite. The
stability field drawn for calcium smectite should be
regarded as approximate because of uncertainties in
extrapolating thermodynamic data to the composi-
tion of specific smectites. Metastable reaction prod-
ucts may also persist at relatively low temperatures.
We have sketched lines on the stability diagram to
show the progressive change in chemistry from rain-
water to the surface waters. If data for dissolved
sodium and sodium smectite are used (Tardy, 1971),
the debris-avalanche pond waters still plot in the
smectite field, and the other waters remain in the
kaolinite field. Stability fields for chlorite, mag-
nesium smectite, or other minerals could be plotted
by using additional axes. However, figure 387 is the
simplest diagram that includes the principal iissolved
cations and silica.
Stability diagrams and data from the literature for
soil-forming processes and hydrothermal alteration
suggest that kaolinite (or halloysite) and smectite
should be forming in the new deposits at Mount St.
Helens. We have not positively identified neoformed
smectite or kaolinite: the high levels of sulfur, iron,
calcium, and sodium in encrustations at fumaroles
and the presence of zeolites in the warm pyroclastic
flows provide the strongest evidence for neoformed
minerals. Mixed-layer chlorite-smectite and chlorite
that are present in some thermal waters may be
neoformed, in part.
However, the chemistry of surface waters, leaching
experiments, and the appearance of glass and mineral
surfaces under high magnification suggest that
dissolution and alteration are occurring at present.
The lack of evidence for crystalline aluminosilicates
such as kaolinite suggests the presence of amorphous
or nearly amorphous material, but we have not
chemically analyzed this phase. Many investigators
(see Ugolini and Zasoski, 1979) report rapid neofor-
mation of minerals in ash-rich deposits. The absence
of neoformed crystalline silicates in deposits from
Mount St. Helens after 150 days of alteration may
reflect either slow rates of dissolution or sluggish
recrystallization of amorphous material, perhaps in-
hibited by the abundance of sulfate, chloride, and
cations.
Despite the ambiguities in interpreting the leachate
and surface-water chemistry in terms of mineral
weathering, it is apparent that two interrelated proc-
tions, which have made this report possible. Some of
the samples and (or) sample data were provided by
S. H. Wood (Boise State Univ.), J. O. Davis (Desert
Research Inst., Univ. Nevada), Thomas Bateridge
(Geoservices West, Missoula. Mont.). James Beget
(Univ. Washington), and P. R. Hooper (Washington
State Univ.). Instrumental neutron activation
analyses (table 82) were performed by Frank Asaro
and Helen Michael (Lawrence Berkeley Laboratory).
Those within the USGS who provided samples or
information about Mount St. Helens tephra include
D. R. Mullineaux, R. B. Waitt, Jr., Daniel Dzurisin,
M. P. Doukas, R. P. Hoblitt, and F. K. Mller.
Tephra analyses were made by V. G. Mossotti, T. W.
Holmes, S. D. Ramage, J. F. Carr, T. L. Fries, and
Chris Heropoulos, all USGS. Robert Oscarson made
SEM photos of the ash, and Jose Rivera made density
separations. Susan Shipley helped with preparation
of the manuscript, and Jane Pike and Wendell Duf-
Held improved the manuscript through many helpful
suggestions.
The major air-fall ash units within the proximal
area (units A through D; Waitt and Dzurisin, this
volume, figs. 354, 357, and 359) differ from each
other in petrography and bulk chemical composition.
Samples of layer A3 and units B and D (stratigraphic
terminology of Waitt and Dzurisin, this volume,
figs, 384, 357, and 359) were examined under binocu-
lar and petrographic microscopes to determine the
proportions of their components. We had difficulty in
making modal counts because of the large size varia-
tion in each unit. Small particles, for instance, would
adhere to larger ones, making grain identifications
diffcult or impossible. Samples were sieved through a
40-um mesh, and the coarser fraction was separated
in liquids with densities of 2.5 and 2.8 g/cm'. The
resulting three separates of the coarser fraction were
River and 7V m (25 f) at the mouth of the Lewis
River. The deposition from the Lewis River would ex-
tend downstream in the Columbia River for about
8 km (5 mi), and thus would not affect the nuclear
plant. Because the post-May 18 channel in front of
the plant's water intake structure has a minimum
depth of about 10V m (35 H), the additional deposi-
tion from the Kalama River would not reach the in-
take structure, which is at a depth of 3 m (10 ft).
The May 18 eruption of Mount St. Helens ejected
some 3.67x 10% t (metric tons)(4.05 x 10 short tons) of
tephra, nearly all consisting of ash which, within a
few hours, fell in a broad band across eastern Wash-
ington, northern Idaho, and western Montana
(Sarna-Wojcicki and others, 1980). This heavy ash
fall, which had an estimated volume of about
O.73 krm' (0.18 mi'), greatly affected civil works and
operations in the areas of deposition. The bulk of the
ash fell in Yakima, Grant, Adams, Whitman, and
Spokane Counties of eastern Washington, where
depths reached as much as 10 cm (4 in.); however,
depths sufficient to require ash removal by mech-
anical equipment or jetting also occurred in northern
Idaho and western Montana.
The ash fall immediately paralyzed transportation
in the area, Within hours of the eruption, 2,900 km
(1,800 mi) of State highways were closed in eastern
Washington (Anderson, 1980) due to ash accumula-
tion. Interstate 90. which crosses the State from
Seattle to Spokane. was closed for a week. Many
thousands of kilometers of county roads, municipal
Significant aspects of these results include large positive amplitude anom-
alies at several Yucca Mountain surface sites, subsurface amplitudes sub-
stantially lower than observed and expected at the surface, and site-
specific standard deviations (best resolved for Station 14) much smaller
than for the Vortman (1984a) regressions (2.04, 2.13, and 2,33, respec-
tively, for vertical, radial, and transverse components for a randomly
chosen site).
The amplitude factors listed in Table 8.1 are similar to results obtained
by Vortman (1984b) from analysis of unscaled signals. The present results
are subject to a slight bias in that the yield-scaling exponent at high
frequencies, 0.53, employed in the Murphy (1977) algorithm, exceeds the
values obtained by Vortman (0.518, 0.454, and 0.426, respectively, for peak
ve rtical, radial, and transverse acceleration). The results are indicative
of a broad area affected by anomalously high amplitudes for Pahute Mesa
events, with maximum amplitude factors near 2 at the northern limit of the
repository site. At Station 12, near the southern limit of the repository
site, peak accelerations were slightly below average, and near-average
accelerations were recorded at Stations 15 and 16, located approximate ly 20
km to the north-northeast of Station 12 (Vortman, 1984b), These observa-
tions limit the extent of the anomaly in the radial (approximate ly north-
south) direction. Current investigations to determine the cause of the
amplitude anomalies observed at Yucca Mountain, and to the east at Calico
Hills, indicate a significant azimuthal dependence, with distinct differ-
ences in amplitude for events in the eastern and western parts of Pahute
Mesa (Vortman, 1984b). These observations suggest that propagation phenom-
ena near the source region are responsible in part for the observed ampli-
tude anomalies and leave open the question of expected amplitudes for
events at Buckboard Mesa.
For present purposes, the amplitude anomalies were treated as site-related,
i.e, independent of event azimuth and distance. Accordingly, site-
s pecific adjustments to the Vortman (1984a) attenuation functions were for-
mulated from the foregoing results for use in the probabilistic calcula-
tions described be low. Calculations were performed for site amplification
factors of 1.5 and 2.0 for surface motion. The site-specific geometric
standard deviation was taken to be 1.46, as determined for vertical and
radial components at Station 14.
because it provides a rationale for the apparent local absence of extensional
stram in basement rocks beneath isolated exposures of the sheared transition
zone. Thus, discontinuous stretching or elongation of the basement may be
spatially separated in an area of obvious extension from limited surface ex-
posures of the detachment surface (as in the Whipple Mountains of southeast-
ern California). The Whipples are associated with an elongate, north-trending
zone of high gravity values interpretable as an expression of a zone of shallow
densification of the crust caused by the injection of broad, sheeted swarms of
basaltic dikes (see Eaton et al 1978, P1. 3-1).
Figure 6d simulates the effect of a slow strain rate imposed on a crust that
had earlier been deformed more rapidly. This is the basic two-fold sequence
of extension recognized in the Basin and Range province. The model is little
more than suggestive, for it is based on an instantaneous change in strain rate
and the rapid thermal-seeking of a new level for brittle-ductile transition.
Given a pronounced thermal lag, one would anticipate a gradual deepening of
the transition zone and attendant effects.
As the boundary separating brittle and ductile behavior moves up or down
in the crust, with varying strain rate and temperature, it may encounter abrupt,
subhorizontal boundaries or zones of inherited mechanical contrast. Stress
concentrations that develop at flaws (such as zones of former compressional
thrusting, stratigraphic layering between rocks of, unlike mechanical proper-
ties, or major nonconformities such as the top of a massive crystalline base-
ment) may give rise to the development of surfaces of detachment. This may
explain the geographic coincidence between regions of sequential com-
pressional and extensional structures, e.g. the Basin and Range province and
the Newark province of Late Triassic-Early Jurassic rifting in eastern North
America (Figure 7).
The Basin and Range province and Newark rift province are both products of
extensional rifitng that followed intense folding, thrusting, and mountain
building. Similarities between these two regions include the following: (a ) the
width of these extended regions (when one includes the Mesozoic rift basins
of the northwest margin of Africa with those of North America); (b) their
location within the compressional orogens; (c) the faithful reflection of the
salients and recesses of the orogen by chains of younger extensional structures;
(d) the distribution and size of the basins produced; (e) their coarse, locally
fanglomeratic, cross-bedded sandstones and fine-grained lacustrine facies; (f)
the local presence of evaporites; (g ) the related flows, dikes, and sills of mafic
be underlain by youngcr oceanic lithosphere. The
youngcr overlying rocks also clearly show the mark
of these oceanic cnvironments by their depositional
patterns and fossils.
The stage is now set. Thhe crystalline foundation is
completed, and rifting has carved a new western
continental borderland. The Great Basin is to be
center stage for some exciting screcnplay. If you are
looking for geologic action, it is most exciting along
the cdges of continents.
sarily have had active volcanism. Some of the early
Paleozoic deep-water western assemblage clastics
may have been eroded off this continental landmass
rather than an island arc. The siliceous sediments
would have swcpt into the oceanic basin lying be-
tween the approaching fragment and the continent.
By Devonian time, however, most geologists be-
licve thcre is sufficient evidence to establish a land-
mass lying offshore from the contincnt. Most
probably, this was an island arc.
For three hundred million years, the continental
margin was passive and the general pattern of shelf
and slope sedimentation remained cssentially un-
changed. Then, suddenly, in Late Devonian the
pattern of the Cordilleran miogcocline came to an
abrupt and dramatic close. The passive continental
margin suddenly became involved in large-scale
compression and mountain building. Where there
had been a shallow sea with a gently sloping un-
deformed sea floor, the Antler Mountains would
rise.
Large-scale episodes of tectonic movement that
create great uplifts, warp up mountain rangcs, or
generally rearrange picces of the crust are called
orogenics by geologists, The Antler orogeny differs
from other orogcnies. The forces that cause moun-
tain building usually gencrate great pressures and
heat. Orogcnics are usually accompanicd by meta-
morphism of rock and by igneous intrusions into
the contorted rocks. The Antler has no known meta-
morphism or intrusion.
The siliccous and volcanic rocks of the western as-
semblage, which were derived from the erosion of
the continent and perhaps partly shed from an off-
shore landmass, had blanketed the decp-water basin
that lay to the west of the continent. Squeezed east-
erly by great compressive forces, these western rocks
yiclded in sheetlike faults, forcing young deep-water
rocks over old slope or shelf sediments. The young
rocks somctimes squcezed between the layers of the
old rocks. The stack of previously almost horizontal
and carefully layered oceanic rocks were shuffled
like a deck of cards. The sorting out of the suits has
hardly been accomplished by geologists diligently
working on the problem for over fifty years, let alone
discrimination between the aces and eights.
The fault slices, riding over the top of one an-
other, were thrust eastward at a rate of about one-
often anb1guous, there are a number of cases now known in which n1ddle
lertiary granites, little older than the detachment faults that cut them,
d1splay severe mylonitization and even mylonitic gneissification.
When further extension can no longer be accommod ated on the undulating
faults, they are broken by steep, brittle faults, related to new ductile
faults at depth. It is obvious in the Death Valley region, however, that
gently dipping structures resain active at quite shallow depths.
This model appear s to be applicable in the Death Valley regian.
Rny middle-crust rocks, irom slate to kyanite ssigeatite, can occur
beneath detachment faults, whereas unsetamorphosed supr acrustal rocks occur,
w1th uncosmon excep tions, only above the faults. (1 have discussed elsewher e-
-Hamilton, 1981--criteria for determining depth of origin in granitic
assemb1ages. ) hiddle-crust crystalline rocks beneath detachment faults are
widely terned ''core complexes'', but 1 do not. use the tern because those who do
oiten incorpor ate the misconception that detachments are necessarily related
to local thermal an omalies. Sone detachments may, indeed, be related to
synextensianal magmatism, but many probably most, are not.
hxceptionally, 1ittle-metamorphosed Faleozoic strata lie beneath 6r eat
Nasin detachmen t faults, as in the Mior on Mountains cf southern Nevada
(Wernicke $ 8 1Y85) and the Sevier Uesert and Lanyon Kange of central
Utah. 1n the latter case, a detachmen t fault dipping gently west can be
tracked in excellent reflection recor ds from its outcrop to a depth of abaut
12 km (R11endinger $ E.- . 1Y83), although as 1 read the reflection prof1le
the iault passes beneath a higher crustal-extension lens at a depth of 7 km,
n the profile as presented by Rllaendinger et 4], (1983, their iig. 2), I see
reflect1ons H, F, and, shallower than about 7 km, R as representing the
compos1te top of such lenses, and the C reflections and the deeper part of A
as repr esent1n g shear zones between len ses. 1 have presented elsewher e
The farthest east of the detachment faults of the Death Valley region are
those of Bare Mountain and the Bul1rog Hills. These structures, like the
Funer al Mountains detachment fault, are now inactive or nearly s.
The lower plate of the Bare Mountain systes consists of Casbrian and
upper Proter ozoic strata, setamorphosed at upper green schist to lowest
amphibolite facies and broken into lenses by normal faults of aostly gentle
dips (Lornwall and Kleinhaspl, 1Y61, their Tertiary thrust fault; Cornwall,
1972; Monsen, 1982). Unsetamor phosed Casbrian strata are truncated downward
against the gently dipping detachment fault at the south end of the ountain
(fig. 24). Miocene volcanic rocks older than about 11,5 Ma are truncated
against the detachnent in the north, whereas strata younger than that lap anto
it are aerely tilted (P. P. Urkild, oral cannun., 1985 4[- Zarr, 1784). East
and west f1an ks of svnmetrical Bare Mountain are much eroded fr os the
stripped, domiform detachment fault, and are overlapped by alluvium of late
Huatern ary age.
lhe surface defined by crests of the lateral ridges of deeply eroded Bare
Mountain dip gently under the alluvium on both sides of the range, and 1 in+er
this crestal topogr aphy to be but moderately eroded beneath the stripped
domifor m detachment fault. R steep range-front fault sas inferred by Lornwa!l
(1972) and Lornwall and Kleinhampl (1961) to bound the east side of Bare'
Mountain, although no downdropped material was recognized. Mferith keheis
(this symposium ) found evidence for local disruption of the alluvium near the
bedr ock contact, and inferred that steep faulting has occurred within Holocene
time. I regard the lack of even much-degraded srarps in either bedr ock or
surficial materials f1g. 24) as, strong evidence against her conclusion.
Detailed gravity traverses across the range iront (Snyder and Carr, 1982) show
and Quaternary alluvium on the south [Gar-
side, 1982a, b; Stewart et al., 1982;
Dohrenwend, 1982a, b) and, in other
places, cuts Quaternary alluvium. East of
the Excelsior Mountains, it fo rms the
northern boundary (Plate 1, location 7) of
outc rops of Cenozoic volcanic rocks and
Quaternary and (or) Tertiary gravel depos-
its IGarside, 1982a; Dohrenwend, 1982a,
b]. Farther east (Plate 1, location 8) it
cuts Cenozoic sedimentary deposits and
Ouaternary or Tertiary gravels south of
the Pilot Mountains (J. H. Stewart, unpub-
lished field mapping, 1983). In the
northwest pa rt of the Monte Cristo Range a
few faults are included in the Excelsior
fault zone (Plate 1, location 9), but
elsewhere in the range the zone cannot be
traced on the basis of surface faults.
Within the Monte Cristo Range, however,
a diffuse band of Late Cenozoic silicic
extrusive and shallow intrusive rocks and
a conspicuous outcrop of basalt and basal-
tic andesite (Plate 1, location 10) both
are elongate east-southeast and are on
line with the Excelsior fault zone. These
are here considered to be intrusions and
flows that were erupted along the fault
zone. Many individual intrusive masses
(not separated on Plate 1) within the
silicic igneous band are also elongated
east-southeast, further suggesting that
intrusion was along a zone having that
trend .
The distribution of several ot her
Cenozoic units in the Monte Cristo Range
is also in accord with an infe rred major
structural zone that cros ses the northern
part of the range. For example, andesitic
rocks are wides pre ad in the southern part
of the range but generally terminate along
the inferred Excelsior fault zone. Sedi-
mentary rocks, on the other hand, are thin
in the southern part of the range and
thicken abruptly across the inferred fault
sone. Finally, sedimentary rocks in the
basalt and ba saltic andesite unit are cut
in at least one place by syndepositional
faults that appear to be on line with the
Excelsior fault zone.
Continua tion of the Excelsior fault
zone east of the Monte Cristo Range is
uncertain. I propose here that it extends
southeastwa rd through the northernmost
part of the Lone Mountain area and into
the San Antonio Mountains. Such a contin-
uation is conpatab le with the distribution
of Paleozoic rocks discussed below, and it
is also suggested by continua tion of the
band of silicic extrusive and shallow in-
trusive rocks from the Monte Cristo Range
to the northernmost part of the Lone Moun-
tain area (Plate l, locations l1 and 12)
and to the San Antonio Mountains (Plate 1,
location 13) [Bonham and Garside, 1979].
Dikes of silicic rocks in the Lone Moun-
tain area trend east-southeast near local-
ity ll and south-southeast near locality
12. These trends suggest that a diffuse
band of intrusive rocks is continuous in
the subsurface from the Monte Cristo Range
southeastwa rd to the San An tonio Moun-
tains, although this zone widens and is
poorly defined in the San Antonio Moun-
tains.
If the Excelsior fault zone trends
southeastwa rd in the San Antonio Moun-
tains, it may continue al ong this trend
and join a cons picuous zone of southeast
trending fault s and abundant silicic in-
trusive and extrusive rocks in the Cactus
Range [Ekren et al., 1971], 40 km farther
southeast. If, on the other hand, the
trend of the Excelsior fault zone in the
San Antonio Range is more nearly east
trending, the fault zone could join with
the Warm Springs lineament [Ekren et al.,
1976], an east trending feature marked by
topographic, structural, and stratigraphic
discontinuities in central Nevada.
The western extent of the Excelsior
fault sone is also uncertain. The fault
zone does not appear to cut Cretaceous
granitic rocks in the western pa rt of the
Exceleior Mountains [Stewart et al., 1982;
John, 1983], but as described below (see
''Age of offset''), the granitic rocks may
post date the main movement on the fault
aone. If so, the Excelsior fault sone may
have originally extended farther west and
may be part of a zone of west to west
northest fault s, linear fe atures, and
hydrothermally altered rocks extending
west from the Excelsior Mountains into
easternmost California (the Swee twater
Mountains-Garfield Flat lineament of Rowan
and Purdy [1984]).
The se eming ly abrupt change in char-
acter of pre-Tertiary rocks across both
the Coaldale and Excelsior fault zones is
8uggestive of major strike-slip movement
that has juxtaposed rocks once more widely
separated. The distribution of upper Pro-
terozoic, Paleozoic, and Mesozoic rocks
suggests from 60. to 80 km of right-lateral
Coaldale fault zones (Fig. 4). Right-lateral of fset of 45 to 55 km on the
Excelsior fault sone and 60 to 80 km on the Coaldale fault zone is indicated
by apparent disruption (Stewa rt, 1985) of the trends of Precamb rian through
Mesosoic rocks and of the line of initia1 g,1g+ = 0.706 (R. W. Kistler,
written commm ., 1986).
The right-lateral disruption of facies trends may extend into the Sierra
Nevada. ere, 9pper Triassic and Lower Jurassic rocks of the King sequence
(Saleeby et al., 1978) may be displaced fran litho logically similar and
generally age-equivalent sequences in western Nevada (Fig. 5). The apparent
disruption of these rocks can be accounted for by right-slip on the Excelsior
and Coaldale fault zones and on a system of northwest-trending faults in the
eas tern Sierra Nevada proposed by Kistler et al. (1980) to account for
apparent disruption in the line of iattia1 gg/g+ = 0.706 (Fig. 5).
Offset on the Excela ior and Coaldale fault zones appears to be la te
Mesosoic in age, because the Lower Jurassic to Cretaceous( ?) Dunlap Fo rmation
appears to be offset and mid-Cretaceous and younger plutons appe ar to intrude
the faults (Stewart, 1985). The faults locally cut Cenosoic rocks, but
lateral displacements, if amy, are small compared to Mesozoic displacements.
Oligocene and Miocene tuff units may have been displaced 20 to 25 km left-
laterally (Stewart, 1985, Fig. 3) along the Coaldale fault zone, indicating a
reversal in the sense of movement from Mesosoic to Cenozoic. Speed and
Cogbill (1979) also indicate a coaponent of left-lateral offset on the
Cenosoic Candelaria fault zone that lies between and subparallel to the
Excels ior and Coaldale fault zones, and a component of left-lateral movement
Was also reported on surface rupture near the Exce lsior fault zone during the
6.3 magnitude Excels ior Moun tains earthq uake in 1934 (Cal laghan and Giannel la,
1935).
1985). Such a change in the stress field seems likely and would undoubted ly
create even more complex s t ructures than those accounted for by the re lative ly
s imp le mo de l prese nt ed here.
A mo de l of repe ated ch anges fr om an extension mo de to a strike-sl1p mo de
accounts for the presence in the same region of structures re lated to
ext ens ion, such as basin-range blocks and major gr abe ns of Owens and De ath
Valley, and of structures re lated to shear, such as major strike-slip
f au lt s. In this scheme, one set of structures aay have been act ive during the
extension mode and anothe r set during the strike-slip dao de. In addition, some
fault s, or parts of some fault s, that originated as eithe r dip-slip or strike-
slip may have been reactivated as strike-slip or dip-slip, respe ctively, after
a pe rmutation in 8; T. Such a change in fault ing style may account for the
lateral variability along individual faults from segments of dip-slip, fo raed
during a tiame of ext ens ion, to segme nts of strike-slip, fo rmed during a time
of shear. Furthe rmore, the Pine Nut, Yerington, and Wassuk faults that hhave
trends simila r to closely right-lateral fault s and are predoainantly dip- or
obliq ue-slip may have originated as strike-slip fau lts and changed to dip- or
obliq ue-slip normal faults during ext ens ion.
In the model presented here, some areas of la rge-scale exte nsion (Fig. 8)
formed as a conseq ue nce of extens ional stres s (F ig. 12A), and ot her areas
fo rmed as a conseq uence of shear stress (Fig. 12B). In the latter
iaterpretation, shear stress produced the 40 to 100 km of right-lateral
disp lacement on the northwest-trending Furnace Creek f au lt zone, as well as
t he asso ciated la rge-scale northest ext ens ion on de tachme nt faults (Ama rgos a
and re lated systems) in the Death Valley area and the corresponding southwest
ext ens ion of a simila r magnitude on detachme nt fault s (Mineral Rid ge-Weepah
and re lated systems) in the Goldfield se ction (Fig. 128). Re lating extens ion
T'he theory presented below is a variation of
theory published in petroleum engineering literature
[3, 5, 19, 20]*, It is realized that most irrigation
and drainage engineers do not have easy access to
this literature. This synopsis is included here, there-
fore, for the convenience of the readers.
The assumption is made that fluid compressi-
bility and inertia are not significant factors affecting
the flow so that the equation of Navier-Stokes reduces
to a statement that the gradient of piezometric pres-
sure P= is balanced by the viscous drag. For a
Newtonian viscous fluid, this relationship is given by
where V is the total velocity vector, Equation (1) can
be solved for particular boundary conditions, e. E-.
1-dimensional flow through straight tubes, flow be-
tween infinite flat parallel plates, and film flow over
an infinite flat plate, The solution of equation (1) for
each of these particular boundary conditions will be
presented to show how the solutions vary with the
shape of the boundary.
Flow through a small diameter tube is con-
sidered first, For this geometry it is more conven-
ient to express the quantity v'V in cylindrical coordi-
dinates, i, e.,
where r is measured from the centerline of the tube
toward the boundary. If the flow is steady, there is
no variation in V along the tube in the direction x. The
last term on the right of equation (2) is, therefore,
$0. Because of symmetry, the term involving
+-- ---++-
here u is the component of velocity along the tube.
Pecause the flow is laminar, there is no component of
E perpendicular to x .
Since there is no flow perpendicular to the
*-==9, +Y+Yise=is++sGä'4+6++äi ++äa-
r w- ie wouna=rs, his ioaes 4e aee4i66-
ntial lines are planes perpendicular to the as1s ot
'% we, re erasGi'9+T6is+ere. ases so
+e< + 4aiie +e444<eSisi.'YGG
*8iables, u and r , are separable and after
+er=ie ee.'d issäiiß
' 9btained, In order for equation (4) to be valid at the
^FDterline where r = 0, the value of C is zero since
8-------- -4--
After separating the variables, integrating and using
the condition unat $E is finite (which implies the
velocity of the fluid approaches the velocity of the
boundary at the boundary) then
where r, is the radius of the tube.
Integrating the discharge over the entire cross-
sectional area of a tube of radius r, and dividing by
the crossnsectional area gives the equation of
Poiseuille for the mean velocity, 1. e.,
Solving equation (1) for 1-dimensional flow in
a flow in a film over a flat plate gives
where d is the thickness of the film. Similarly,
for 1-dimensional flow between parallel flat plates
where b is the spacing of the plates.
It will be noted that eqyations (7), (8), and (9)
are of a similar form in that u is proportional to
Pr, and to the square of a length dimension charac-
terizing the size of the flow se ction. The volume flux
is inversely proportional to viscosity and a numerical
constant,
By induction, then, an analagous equation
applying to straight tubes of uniform cross section of
any shape is
where R is a length characterizing the size of the
tube, and k is a numerical constant depending on
the shape of the cross section and the dimension cho-
sen to characterize the size.
In applying equation (10) to tubes of irregular
shape, the question arises as to what length dimension
should be used to characterize the size of the tubes,
One possibility is the bESEEElSE9E. R . I is
defined as the cross-Sectional area divided by the
wetted perim eter.
If equations (7), (8), and (9) are rewritten in
terms of R , only the numerical constant is changed.
Poiseuille 's equation becomes
and equations (8) and (9) become
After drilling and logging test borehole USW U2-1 (Whitfield, 1985 ;
and Palaz, 1985, this proceedings), 15 depth zones were selected for
installation of pressure transducers, TP, and access tubes. These depth
locations were designated by instrument stations (IS) 1 through 15. At
e ach IS, sensors were housed in one of three well screens (fig. 1), and
designated A, B, and C from top to bottom. The top well screen (A)
contained a TP and was connected to the land surface by an access tube for
gas sampling. The middle well screen (B) housed a TP and a pressure
transduce r. The bottom well screen (C) contained a pressure transducer and
was connected to an access tube for gas sampling, and for checking
calibration and prope r functioning of the pressure transducer.
In add1tion to these IS, 18 different depths were selected for instal-
lation of HDP, 5 of which were accompanied by TP. These HDP were desig-
nated as HDP-A or HDP-B depending on whether they were located above
(HDP-A) or below (HDP-B) the nearby IS. The TP associated with these HDP
we re designated as HDP-TP,
Prior to installation, the sensors were calibrated with cable lengths
cut for the predesignated depths of installation in the borehole. After
calibration, all cables with attached sensors were laid out, and
instruments were inserted into the well screens and secured. The H)P were
protected with cups filled with saturated silica flour, but they were not
housed in well screens. The well screens and the HDP were adjusted, so
that they would be located in predesignated-depth intervals. The entire
assemb ly was transported to the test borehole site as a bundle.
To emplace the filler material in the borehole, two tremie pipes we re
lowe red into the borehole prior to installation of the bundle. After
tremie-pipe 1nstallation, a television camera was lowered in to the bore-
hole to inspect it for obstructions. The bundle then was lowered into the
borehole w1th a 2.38-in. (6.0-cm) outside-diameter fiberglass access tube
for geophys cal logging. The wires and tubing were encased in
polyurethane -foam isolation plugs to prevent gas flow between stations
along the wires. The isolation plugs were situated so that they would be
surrounded by grout during stemming. Standoffs also were installed on the
fiberglass tube near each IS to prevent damage to the instruments by
collision with the borehole wall during installation.
When the assembly was lowered into the borehole, dry materials (silica
flour, sand, and bentonite) we re poured through one tremie pipe, and wet
mate rials (cement and water) were poured through the other tremie pipe to
s tem the hole. Final configurat ion of the stemmed borehole and the
locat ion of the IS and HDP are shown in figure 2. Actual location of the
sensors and the contacts between different materials were dete rmined by
geophysical logs run inside the fiberglass tube.
Long-term temperature measurements within this borehole indicated that
the geothermal gradient was slightly convex upward within the welded unit.
Deviation from the straight line correlated with frequency of fracture
traces observed in the borehole. Assuming a uniform the rmal conductivity,
calculated air fluxes range from -49 to -98 in./yr (-1,250 to -2,500
mm/yr). The quantity of water that could be transported upward by this
flux in vapor form was estimated to be from -1 x 10% to 2 x 10 in.fyr
(-0.025 to -0.05 mm/yr). This air flux probably occurred through the
fractures of the welded tuff.
A noticeable increase in precipitation was recorded at the Beatty
weather station following 1972 when the station was moved to a location
76 meters higher than the previous site. The increase in both cumulative
precipitation and cumulative number of events of 0.6 cm or less after 1972
is shown in figures 10 and ll. The probable effect of moving the station to
a higher altitude is suggested by the data shown on both graphs.
The seasonal distribution of daily precipitation events of several dif-
ferent magnitudes that are less than or equal to 2.5 cm at Beatty is shown
in figures 12 and 13. These storms typically account for 70 to 90 percent
of the annual rainfall. Exceptions to this arise when a large storm occurs
during a year of low annual rainfall such as 1972. A greater frequency of
occurrence for small events (less than or equal to 0.b cm) in July and
&ugust as compared with June, September, and October (figure 1l) probably
reflects increased convective-storm activity during these two aonths.
Detailed studies of meteorological data and soilmoisture movement in the
natural stratigraphic sequence at the waste-burial site demonstrated that deep
percolation can occur, given the required antecedent conditions. The depth of
downward moisture movement observed from February 1979 to February 1980 was
controlled largely by the coarse-grained layer from about 0.75 to about
2.5 meters. This layer served as a natural capillary barrier to unsaturated
flow (Corey and Horton, 1969; Frind and others, 1976; Rancon, 1980). A
capillary barrier is formed when unsaturated fine-grained sediments overlie
unsaturated coarse-grained sediments. The downward movement of soil moisture
is retarded at the contact between the two layers. Movement into the coarse
sediments does not occur until the saturation level in the overlying fine-
grained sediments becomes such that gravitational forces exceed interstitial
tension forces. Such a barrier does not exist in the backfill material
encompassing the radioactive waste containers in the waste-burial trenches.
The trench backfill is a heterogeneous mixture of the sediments removed during
trench construction. It still is considered a stoney soil, but the hydraulic
characteristics are a composite of the characteristics given in tables 12 and
13. Regardless of the exact character of the backfill material, no capillary
barrier overlies the existing trenches and nothing is present to retard
moisture movement to depths greater than the, 2. meters observed during this
study.
The hydraulic properties of the trench backfill are expected to vary from
place to place, but they probably are not too different from the properties
obtained for the samples listed in tables 12 and 13 or the properties reported
by Mehuys and others (1975) for the Rock Valley stoney soil. A reasonable
range of unsaturated hydraulic conductivity fgr matric potentials in the range
of -5 t9 -50 bars might be from about 1 x 10n% cm/d to perhaps as little as
1 x 10 cm/d. Volumetric water content might be somewhat greater at depth in
the trench backfill; in the absence of field or laboratory data, an estimated
range of water content of 5 to 12 percent might be reasonable. Soil-water
potential in the backfill is not known, but, on the basis of the measurements
shown in figure 29, it is expected to be in the range of -5 to -25 bars.
Conditions of steady-state unsaturated flow are unlikely in the trench
backfill material. Large potential gradients are likely near the wetting
fronts of successive deep percolation events. Such fronts are not expected
to occur more frequently in the trench backfill than in the undisturbed
sediments, but the depth of percolation likely will be greater because of
the absence of a natural capillary barrier. (Even this infrequent deep
percolation could be reduced by constructing capillary barriers over the waste
trenches.) Under the circumstances, an estimation of unsaturated flow rates
in the trench backfill is speculative at best. The rate might be as high as
10 cm/d near wetting fronts, where potential gradients are steep, but such
rates would not continue to significant depths, nor for signifi cant lengths of
time.
2. For steady, unsaturated flow in a statistically isotropic medium,
with unit hydraulic gradient, a 3-D analysis results in a smaller water
head variance as compared to a 1-D analysis. However, this difference
disappears as the correlation length increases.
If hydraulic conductivity is approximated by
where 4 is pressure head in meters and &, 1s saturated conductivity,
then a 3-D analysis is required if akc1 where k is ,he correlation
length (Yeh, Gelhar, and Gut)ahr, 1985a).
For Topopah Springs tuff, K vs. 4 curves have been generated (Peters
et al., 1984). If only matriz flow is assumed, then a0.01 g'', In
this case for kO100 m, a 3-D analysis is required to give good es-
timates of variances; 1-D analyses will over-estimate the variance in
water head. If fracture flow is important, then a:0.2-0.5 and kc2-5 m
requires 3-D analyses. For a fractured medium, k is likely to be
small. In either case (fractured or not), a 3-D analysis will be
required to provide good estimates of variance.
3. For steady, unsaturated flow in a statistically anisotropic medium
(i.e., k depends on direction), for which Eq. (5) is reasonable, the
results become more complicated. First, assume anisotropy involves
only the vertical (k,) and horizontal (k) directions, and assume unit
vertical gradient flow. Then, the variance in water head relative to
the isotropic case (k=k,) is less if kk, and greater for k>k,, and
varies roughly as the square of k/k, (Yeh, Gelhar, and Gutjahr, 1985b).
The appropriate value of k/k, depends upon how the soil or rock was
formed. For many soils, one would expect k),, but what value to use
for Yucca Mountain is unclear.
connections from the vertical fractures into the matrix. Each matrix block is
partitioned into 10 nested, equal-volume continua, idealized with the multiple
interacting continuum (MINC) mesh generator (Pruess and Narasimhan, 1985).
Vertically, the matrix continua of neighboring nodes are directly connected. For
the vertical infiltration problem, we take into account only two orthogonal verti-
cal fracture sets, Earlier studies, with both vertical and horizontal fractures
explicitly modeled, indicated that horizontal fractures did not significantly affect
the vertical flow rate (Wang and Narasimhan, 1985). For the Paintbrush
nonwelded unit, the fracture spacings are much larger than the welded units,
For most steady-state and transient-pulse cases, the fluld flow field in the
Paintbrush nonwelded unit is dominated by matrix flow. We can either neglect
the fractures in the nonwelded unit or treat the nonwelded unit as a composite
medium. These two treatments yield essentially the same results, The fracture
continua and the matrix continua of the welded units are connected to the grid
blocks of the nonwelded unit.
profiles with lateral flows are not significantly different from the profiles with no
lateral fiow (Figs. 17a, b, and c).
The fixed-gradient approximation can be used to estimate the amount of
lateral flow if the flow magnitudes are small, If the lateral-flow magnitudes are
large, significant lateral variations of saturation and pressure can occur, For
example, if laterally flowing water accumulates toward the east, it will result in
decreasing saturations toward the west, The corresponding capillary pressure
gradient will act against the eastward component of the gravity gradient and
decrease the lateral flow magnitudes. The fised-gradient approximation does not
take this effect into account, although the effects of a saturation gradient could
be significant. In the discussion of the differences between O.1 mm/yr and 0.5
mm/yr vertical steady-state infiltration rates through the Paintbrush nonwelded
unit, we noted that the vertical saturation gradient can greatly increase or
suppress the vertical gravity gradient. Similiar coupling between lateral satura-
tion and gravity gradients is expected in the lateral direction. The fixed-gradient
approximation should be used judiciously. If It is applied indiscriminately,
misleading results could be produced. Using the fixed-gradient approximation for
the 0.5 mm/yr case, the conductivity in Paintbrush nonwelded unit is so large
(Fig. 8d) that the explicitly estimated magnitude of the lateral flow exceeds the
infiltration rate, Unrealistic flow reversals occur for the nodes below the
Psintbrush nonwelded unit. These unphysical results are caused by overestima-
tion of the lateral flow due solely to the fixed-gradient approximation, Account-
ing for lateral saturation gradients would more realistically predict the magnitude
of lateral flow. Thus, the fixed-gradient approximation breaks down for the
high-flux cases. We need to go beyond the single vertical column model to more
accurately estimate the effects of lateral flow. To properly model lateral flow in
cases with high fluxes, we need to simulate the lateral variations of the saturation
At test well USW H-4, the following assumptions were made in the concep-
tual model for the flow system at this test well site:
Thus, the conceptual model for this well site is based on the mathemati-
cal model of a single vertical fracture located in a region in which the outer
boundaries are maintained at a constant pressure and fluid crosses the outer
boundaries as a uniform flux (Raghavan and Hadinoto, 1978). Because there are
several vertical or very steep fractures penetrated by this well, this con-
ceptual model was only used to explain the linear-flow period during pumping
tests in which the vertical fractures controlled flow. This period was char-
acterized by a straight line with a slope equal to 0.5 on a loglog graph,
referred to as the one-half slope line.
The authors believe that the key phenomenon that controls the pumping
test is the linear-flow period in which most of the water is derived from
fracture storage. The pseudoradial-flow period during both drawdown and
recovery of water level is indicated by the straight- line graphs of water
level on semilogarithmic paper.
To minimize the invasion and plugging of fracture and matrix porosity
while drilling the well, bentonite and polymer-base drilling muds were not
used. Instead, air foam, consisting of a small volume of detergent and water
Cambrian shales and alaskite; and that the
lower levels ''were essentially contours of the
Tertiary surface.''
Discovery was delayed because free gold
occurred not in jaspery outcrops but in adja-
cent earthy mixtures of kaolin, alunite, gyP-
sum, iron oxides, and fragments of porous
quartz. Only a small portion of the ore filled
fractures, although the deposits are intimately
related to fissuring. Instead, the deposits are
''irregular masses of altered and mineralized
rock traversed by multitudes of small irregu-
larly intersecting fractures, such fracturing
passing in many places into thorough breccia-
tion'' (2). The brecciated rock may either be
loose and associated with interstitial limonite
and kaolin or cemented by quartz, limonite,
or hematite. These brecciated or altered de-
posits contain the ore shoots. AIthough the
two are equally irregular and are structurally
indistinguishable, they are not coextensive.
Moreover, the ore shoots have assay walls,
although the change from high grade to waste
may occur over short distances.
Unoxidized ore consists of native gold,
goldfieldite, pyrite, bismuthinite, arsenical
famatinite (3CuiS-SbS-), and minor sphal-
erite within a flinty quartz gangue associ-
ated with mixtures of alunite and kaolinite.
In depth, the ores contained abundant famatin-
ite, and Searls (22) reports ore-grade figures
from the Tertiary, pre-Tertiary contact horizon
Tolman and Ambrose (11) recognized mar-
casite, wurtzite, tennantite, sylvanite, hessite,
petzite, yellow gold carrying silver, and red
gold low in silver. Their studies indicate that
alunitization and probably kaolinitization were
completed prior to the introduction of the ore
minerals.
A minor tin content in the copper-rich
ores,' although it was not recognized during
mining, was reported by the U.S. Geological
Survey in February, 1941,
The ore-bearing andesite series is partially
overlain by lake beds, and these, in turn, by
basalts and rhyolites. Searls (22) states in re.
gard to age of mineralization there can ''E4
no question about the fact that the Goldfiej;
veins were formed, partially oxidized, and au
tacked by erosion before there was substantis;
wWsumulation of sediments or tuffs in the uppe
Miocene lake.''
The source of the ore is not known, but
Searls considers ''that whatever the origina;
source of the gold in this bonanza camp--
perhaps a rhyolite vent or plug or breccis
pipe-its distribution was accomplished by the
Columbia Mountain fault,'' the most promi.
nent mineralized feature in the district.
GoLp AcREs The Gold Acres open pit gold
property somewhat resembles the Lynn Creek
deposit, In 1939, Vanderburg (16, p. 39-46)
described the principal ore body as brecciated
iron-stained, silicified limestone, dipping flatly
southwest between a porphyry hanging wall
and a blue limestone foot wall. Brecciation
is attributed to the Roberts Mountains thrust
(46, p. 92), As at Lynn Creek, thrust planes
are exposed in the walls of the pit, and the
gold, irregularly distributed, is so fine-grained
it cannot be recovered by panning.
Gold Acres, operating from 1936 to 1961,
produced some 120,000 ounces of gold and
6500 ounces of silvert from ore containing
as little as $4.00 per ton in gold (16, p. 39).
PLAcER GoLD At the present time, there is
no formal placer mining within the Great
Basin, but formerly it was important. Bingham
Canyon, prior to the development of its lode
mines, may have yielded $1,000,000 in placer
gold from 1868 to 1871 (Rubright and Hart,
this volume). Osceola is said to have produced
$2,000,000 to $5,000,000 in gold prior to
1907, 90 per cent from placers. The source
of the Osceola placers appears to have been
shattered masses of Cambrian quartzite (5,
p. 253).
More recently, Manhattan, Round Moun-
tain, and Battle Mountain have witnessed
dredging operations based on bajada-type
placers typical of the arid West, Only the
dredging operation at Manhattan has proved
successful. Between 1938 and 1946 the Man-
hattan Gold Dredging Company produced
$4,596,427 from 21-cent gravel (24, p
113-131), and Bergendahl (46) credits the
district with a production of 206,000 ounces
of placer gold.
In 1946, the Manhattan' dredge was moved
and pressure. There is no evidence that these beds were
ever affected by hydrothermal activity or metamorphism.
Beds of volcanic tuff that are only slightly or partly con-
verted to zeolites probably were deposited in lakes that
were less alkaline and saline.
The host for the zeolites was silicic vitric ash, Volcanic
glass is readily soluble in alkaline solutions, especially when
the solution has a pH of 9 or more. The process is aided by
high permeability, abundance of small particles, and large
surface area of the ash. The reaction with the glass adds
alkali ions to the interstitial fluid, making it more alkaline
and facilitating further dissolution of glass. The silicic glass
and the materials originally present in the connate water
furnished the constituents necessary to form the zeolites
and other silicates. The development of authigenic silicates
involves the solution of materials from the glass surface and
precipitation of various minerals; it is not simply a process
of internal devitrification (Deffeyes, 1959b, p. 607).
The chemical composition both of the rock and inter-
stitial fluid determines which zeolites are formed. Siliceous
and alkali-rich zeolites occur in altered silicic tuffs, but
aluminous and alkaline earth-rich zeolites occur in mafic
lavas (Sheppard and Gude, 1969a, p. 885). In addition, the
zeolites formed in a low-temperature sedimentary environ-
ment generally are more silica- and alkali-rich than the same
zeolites occurring in a basic igneous rock (Mumpton, 1960,
p. 365). Of the zeolites formed in silicic tuffs, erionite
and phillipsite apparently require saline water but clino-
ptilolite and mordenite can form in either saline or fresh
water (Sheppard and Gude, 1969b, p. 27).
The pH and composition of the waters that formed
these zeolite deposits must be inferred from laboratory or
field study. Abundant laboratory data show that zeolites
form only in an alkaline environment, and the alteration of
silicic ash to zeolites by saline, alkaline lake waters has been
reported in numerous areas. In a careful study of tuff beds
deposited in Pleistocene Lake Tecopa, Calif., Sheppard
and Gude (1968) have shown that the vitric tuffs are un-
altered where deposited in relatively fresh water near the
lake edge, are mostly altered to phillipsite, clinoptilolite,
and erionite in moderately saline water, and are mostly
altered to potassium feldspar and searlesite (NaBSigOw
H40) in highly saline water in the central part of the lake.
Variations in the mineralogy of the altered tuffs is
attributed to differences in salinity and pH of the lake
water trapped in the rocks during deposition.
The effect of solution composition on authigenic silicate
mineralogy in playa lake deposits in Nevada and California
has been studied by Hay (1966, p. 32-36). At Teels Marsh,
Mineral County, Nev., rhyolite ash is presently altering to
zeolites wherever salinity conditions are favorable; a near-
surface ash is altered to phillipsite only where the playa sur-
face is covered by a trona-rich crust. At China Lake, Calif.,
where the water is moderately saline, zeolites are abundant
but potassium feldspar is absent in the altered tuffs. At
Searles Lake, Calif., where a concentrated brine is present,
potassium feldspar is the dominant authigenic silicate
mineral.
In the deposits described in this report, a zeolite-rich
bed commonly contains several layers of differing mineral-
ogy. Because the host rock probably was similar in the
several layers, the chemical composition of the entrapped
waters may have been a critical factor in determining which
authigenic minerals formed. Sheppard and Gude (1968,
p. 34) suggested that the tuffs at Lake Tecopa, which are
interbedded with relatively impermeable mudstones, acted
as closed systems, and that the (Na'+K) H' activity
ratio, the activity of Si0g, the activity of H4 0, and the K:
(Na+Ca+Mg) ratio were responsible for the differing authi-
genic mineralogy. The development of montmorillonite,
generally one of the first authigenic silicates to form,
required a relatively low (Na'+K'):H' activity ratio
(Hemley, 1962, p. 196). This reaction caused an increase in
the pH and in the concentrations of alkali ions and silica in
the solution, and created an environment more favorable to
zeolites, A relatively low activity of Si0g favored the
development of zeolites low in silica, such as phillipsite and
chabazite (table 1; fig. 2). Precipitation of these- zeolites
increased the activity of Si0g and created conditions favor-
able to the development of high-silica zeolites, such as
erionite, mordenite, and clinoptilolite. On the other hand,
a high K':H' activity ratio along with a high activity of
Si0g promoted the precipitation of potassium feldspar
(Sheppard and Gude, 1968, p. 3436).
The activity of H40 also influenced which kind of zeo-
lite was formed; a dilute solution promoted the develop-
ment of the more hydrous zeolites. The amount of alkali
and alkaline earth ions in solution determined which of the
zeolites with similar silica content were formed and, in
addition, affected the amounts of the various exchangeable
cations in the zeolite. Thus, a high K:(Na+Ca+Mg) ratio
favored the formation of phillipsite but a lower ratio to-
gether with a high activity of Si0g favored erionite rather
than phillipsite.
In a closed system, the continued dissolution of volcanic
glass and precipitation of minerals cause a continuous
change in the composition and pH of the pore fluid, and a
variety of minerals are formed. In most cases the silicates
formed earlier remain stable under later changed condi-
tions. However, Sheppard and Gude found that potassium
feldspar and searlesite formed, at least in part, from zeo-
lites at Lake Tecopa (1968, p. 35), and that analcime re-
placed other zeolites and was in turn replaced by potassium
feldspar near Barstow, Calif. (1969b, p. 29-30) in a highly
saline environment.
The Eastgate zeolite deposit i 55 miles east of Fallon
and 3 miles east of the junction of U. S. Highway 50 and
Nevada Highway 2, in Churchill County (fig. 1). The
settlement of Eastgate is 3.0 miles to the east. Most of the
zeolite-rich beds crop out between the diverging highways,
principally in sec. 28, T. 17 N., R. 36 E. Some exploration
work has been done on thin beds, however, for 1M miles
south of Highway 2, in sec. 33 of the same township and
sec. 4, T. 16 N., R. 36 E. (pl. 1, location map).
reducing ceondit iuns, nd Lhere is nu evide 1ee lot pjreu inble diss,olut ion
in oxygen -deple ted ground wute rs. Oxdiizinpt condit iomn at or nne ar the
g rouund shrface or as:uc in ted wl th pyr ite deeonmos it iun he iow e w.t er
t nbie normn lly rendur the mine ru ls solubie, 'The re nultant urayl ions
reac t in various w.ys with thu geol og ica l environm-nt.
In the prcsence of vanadium ions or ionic comp lexes, stable urnnyl vnnnndnte s
such as carnotite and tyuyamnite s re likely to form, occnsionally in rich
but local concentra tions such as the well-known ca rnotite log deposits of
the Colorado P1atesu. On the other hand the uranyl ion may re -enter the
reducing environment in which hyd rogcn sulfide formed bacteriogenica l ly
or as the product of disproportionetionnn of sulfur-ion spec ies may re -reduce
thw g'f gy gy* snd cause uranium precipitation and renewed accumula tion.
The roll ores of Wyoesing a re formed essentially by this proccss,
In the sbsence of suitsble reductants, or absorbe nts such a s coalified
wood, uranium may move through the ground -wa ter system to be dissipa ted
in some way that is of geochemical rather than economic importance. The
fate of urantum in waters in deep ba sins is not known but shculd be of
interest becsuse of the potential for reacting with the reducing components
of otl and ga s, the reby chemica lly restricting its furthor movement and
possibly yield ing additional accumulation s.
Sandstone s conta ining cos lified plant remsins, hume te substances, pyrite,
or hydrogen sulfide mnke suitable host rocks for uranium ore deposits.
The potential for reduction of uranium moving in to these sed iments in
ground wn ters is conside rable . Other elements with transitional valenee
states are also affccted, includ ing vanad ium, iron, molybdenum, coppur,
sul fur, and se lenium, and thcir reconcentra tion can also be accounted for
largely by chemical rcduction.
B1ack unoxidized ores containing uraninite and coffinite are norma lly
conside red to be primnry deposit s in relation to the yellow oxidized
uranium ores. Dranium readily fluctuates be tween its oxidized wa ter-
soluble state and its reduced insoluble state, and thus conside rable
trans formation from one to the other goes on. The te rms primary and
sccondary thus tend to lose significnnt meaning. Episcdes of oxidauion-
reduc tion are evidently frequenrly repea ted in many ore dietricts,
e specially in the absence of vnnadium, and these cyelical event s lead to
a constant or discontinuous mobiliza tion of ore with gradual movemcnt of
ore awsy from the direction of most intense or persistent oxidat ion. Ln
other words, there is a tendency for ore to ''rol1'' away from advancing
oxidation brought about by oxygenatcd ground waters. Rccycl inng responsible
for formatton of the important black ore -roll deposits so prominent in
the Wyoming ba sins may beg in by the uranyl ion be ing rcduccd and precipi-
ta ted tn the presence of organic ma tter, decomposing pyrite, or hydrogen
sulfide. Oxygenated ground wa ter re-oxidizes and solubilizes the uranium
uhile simultnneously causing oxidation of pyrite and HsS and contributing
to oxidative degradation of the plant ma tter, The uranyl ion re leased on
oxidation re-enters the reducing environment either by gro nd -water move -
ment or by diffusion and is ag in reprecipitated. Ore (ormeed in this
manncr involves sn sccretionary process, rclying on in flux of urnnium ions
from some remote or exte rnal source or on an abundance cf ions relea sed
nearby in the sediments.
Recycling may also contribute to the forma tion of tabular and lenticular
deposits. Young (2) and Malan (3) hnve expressed the opinion that the
low-venadiumm ores found in paleochsnnels in Trisssic (Shina rump Formaticn)
to 60'' to the south, and varies in width from six inches to
one foot.
Cinnabar occurs mostly as an earthy ocherous variety in the
gouge zone of the fault and in nearly verrtical joints trending
S. 29 E. away from the fault, but the earthy cinnabar also fills
small vugs in the cemented fault breccia. A few small veinlets
of crystalline cinnabar, associated with chalcedony and some
pyrite, were found in the fault breccia, and lesser amounts occur
as isolated rhombohedral crystals,
The underground workings have not fully explored the ore
zone, and further development along its strike and down its dip
can be expected to yield additional ore.
Former names... World Exploration Company, Red Hope.
,ocation.------8ec. 2, T. 1 S., R. 33 E.
Ownership.. - J. P, Cleary, Jack Chiatovich, L. H. Shirley.
P)roduction. ---... .None.
Geologic type. .. Volcanic.
The Red Cloud property is at an altitude of 8,000 feet about
two miles up Trail Canyon in the southwestern part of the dis-
trict. The property was first located by the World Exploration
Company in 1928 but was abandoned in 1929. In the latter part
of 1929 it was restaked by J. P, Cleary and L. H. Shirley who did
a little development and held the property until 1935. It was
then abandoned until 1940 when it was relocated by the present
OWTiers.
1)evelopment work includes several trenches and a l5-foot shaft
exploring a mineralized zone about 200 feet long. The rocks are
flat-lying andesite agglomerate and tuff of Tertiary age. Cinna-
bar with chalcedony is exposed in a trench and shaft in a narrow,
veinlet that trends N. 5* E. and dips 65' to the east.
Former names....Chrysler.
Location.---- Sec. 18, T.1S., R. 34 E.
Owmership. . . . . Walter F. Dunnigan.
Discovery.----,..Located for silver, 1914; for quicksilver, 1927,
by George Chrysler.
Prodction.---...1,734 flasks to the end of 1943.
Geologic type. . . Metamorphic.
The Red Rock mine lies on a group of four claims in the south-
ern end of the district and about 11 miles west of the Chiatovitch
The western group of ore bodies was localized along an apex
between a vertical dike and a heavy layer of clay or gouge which
dips at a low angle to the east. The structural control is further
complicated by large disconnected fragments of dike within the
apex. The ore in these bodies was of very high grade and almost
completely replaced the diabase. It consisted of cinnabar, clay
minerals, and scant pyrite.
Location. ------... Sec. 29 (?), T. 41 N., R. 82 E.
Oonership. . . .Clyde Franklin of Sod House and Dr. Fred
Keeney.
Discovery.-----....1940 by Franklin
Pmtoduction.. .---...None,
Geologic tvpe. ...Opalite (?).
The Franklin-Keeney property of six claims is about two miles
east of Happy Creek and about three miles northwest of the
central part of Bottle Creek district. It is on the north slope of
Buff Peak across the ridge west of most of the mines of the dis-
trict, and it is accessible by road only from the Happy Creek
ranch. The property has been intermittently developed since its
discovery, and no attempt has been made to recover any quick-
silver from the ore.
The principal work was done in a creek bottom and was caved
when the property was visited in August 1943. Reportedly it
consisted of a 50-foot shaft with a 25-foot drift to the east and
a shorter drift to the west on the 30-foot level. The shaft is said
.to have followed a fault gouge which is vertical near the surface
but dips at a low angle to the north near the base of the shaft.
The country rocks are rhyolitic and andesitic tuffs and flows.
Location. ------...Sec. 1, T. 40 N., R. 32 E.
Oonership. .-,,...William J. Hagan and James Hegan.
Discover).-------.,..1940 by Hagan and Hegan.
Production. . --....None.
Geologic type. ...Opalite.
The Hagan-Hegan property is in the northwestern part of the
district just southwest of the Niebuhr mine.
The principal working, a branching adit less than 400 feet long
with several inclines and raises, explores altered and iron-stained
pre-Tertiary flows and tuffs, No ore has been stoped from the
workings, and they do not appear to contain rmore than a few
scattered crystals of cinnabar.
. Location. -----..T. 9 N., R. 44 E.
Ownership. . ..,.Homer Williams and John Connolly of Tono-
pah.
Discovery..------....1928 by J. Humphrey.
Production. - --.,. 728 flasks to the end of 1943.
GGeologic type. . . Metamorphic.
The Van Ness mine, on a group of six patented claims, is at an
elevation of 8,600 feet on the east slope of the Toquima Range
and about six miles northwest of Belmont. In the year of its
discovery the property was sold to Raymond Van Ness. He pro-
duced a small amount of quicksilver in 1928-1929, and after
installing a 30-ton Cottrell furnace he recovered about 500 flasks
of quicksilver in 1930-1931. This operation was apparently
unprofitable, for in the latter part of 1931 the property was sold
under judgment to Homer Williams and John Connolly. Since
1931 the owners have leased the property to various men who
have annually produced from retorts about 25 flasks of quick-
silver. The property was idle when visited in 1943, but the rotary
furnace appears usable.
The underground workings explore parts of a single easterly
trending zone for a distance of 800 feet, and to a depth of 145
feet; however, the productive workings all lie in the western end
of the zone, These workings consist of two glory holes tapped
by a haulage adit, and several stopes from which ore was raised
up a 135-foot inclined shaft. A 900-foot adit was driven to
explore the ore zone at depth, but little ore has been removed
from it. (See Plate 14.)
The rocks of the area are slightly metamorphosed sedimentary
rocks of Ordovician (7) age occurring as a roof pendant in gran-
ite. Most of the metamorphosed sediments are phyllites, but one
15-foot bed of mildly metamorphosed chert contains all the ore.
The ore bodies are tabular and contain irregular veinlets of
purplish-red cinnabar locally as much as a quarter of an inch
thick. They occur adjacent to dike-like bodies of granite and
also in the more broken parts of the meta-chert, The ore bodies
in the chert were narrow and elongate parallel to the bedding
which strikes N, 70* W. and dips about 45' southward.
Location. ------. Sec. 1, T. 9 N., R. 44 E. (7).
Ownership. --... E. A. Michal.
Discovery. ----....1939 by Michal.
Production.. . . 3,259 flasks to the end of 1943.
Geologic typpe. . Limestone.
The Juniper property was first located in 1923 by E. M. Bene-
dict who shortly thereafter put down an inclined shaft which
passed within a few feet of the rich ore body he struck by lateral
exploration several years later. In 1927 the property was bought
by L. A. Friedman who organized the Nevada uicksilver Com-
pany and developed the property to such an extent that it became
one of the leading producers in the country during 1928-1929. A
40-ton rotary furnace, installed in September 1928 and operated
until 1930 on ore from the Juniper mine, yielded nearly 3,000
flasks of quicksilver. In 1931 Joe Huntley and Joe Forbes leased
the property and recovered small amounts of quicksilver from the
margin of the old stopes, but the property remained essentially
inactive until Basil Prescott obtained a lease in 1941 and, after
unwatering the mine, did extensive diamond drilling. However,
he apparently found no ore for the lease was dropped in 1942,
Late in the same year the present owners acquired the property
and installed a 20-ton rotary furnace, but they made no effort to
develop the mine in 1943.
The principal workings of the mine connect with a two-
compartment 45'' inclined shaft with a reported vertical depth
of about 335 feet, but all workings below 250 feet were flooded
when the mine was examined. Levels extend from the shaft at
vertical depths of 95, 161, 193, and 229 feet, and published maps
indicate other levels at depths of approximately 260, 300, and
330 feet. The total length of drifts and crosscuts is reportedly
more than a mile, but only about 2,500 feet were accessible in
1943. The ore came mostly from two stopes now considerably
caved, The larger stope nearly reached the surface north of
the collar of the northeasterly trending shaft and from there
ppitched easterly at about 45'', crossing the shaft just above the
95-foot level, and tapering out above the 161-foot level east of
the shaft. A smaller horizontal stope lies approximately normal
to the shaft between the 161- and 103-foot levels. (See Plates 17
and 18.)
The workings are in beds of Upper Triassic limestone con-
glomerate and overlying shale which strike approximately N. 35
W. and dip about 45* northeastward, The limestone-shale con-
tact that was followed in the search for ore is in most places more
or less faulted, but locally it is apparently a normal contact. A
prominent easterly trending vertical fault which has offset the
limestone shale contact was of importance in the localization of
and sthr ucturaliy hlgher rubble rock breccla
event of similar size and shape as the ear lier
felsite-gneous breccla e vent. The abundance
or ab sence of sedimentary and crystallIne rock
breccia fragments withIn each success Ive ly
younger breccia pIpe event attests to the
str uctur a l leve l to which each p Ipe was able to
stope. The profusely carbonate-fragment-Iaden,
late phase, rubb le rock breccia pipe clear ly
stoped to a hlgher level than the dominantly
basement-rock-laden, felslte-igneous breccla
pipes.
Gold mlneralization occur s as dlssem inatlons
w Ithin both breccia events of the p ipe comp lex
and Is closely assoc iated with and deposlted as
f ]11 Ings In fractured pyr lte. The replacement
of carbonate brecc ia fragments wlthin the
r ubble p Ipe by pyr Ite has greatly Increased the
sulfide content of thIs rock body withfn the
p Ipe complex. ThIs higher pyr ite content has
also served to concentrate the gold minera l-
Izatlon, resulting In the rubble breccla plpe
be lng the richest gold-mIneralized rock body
w1thin the plpe comp lex.
The gold zones are overprinted on the copper
and zInc mineralized zones, whlch range In
value from +100 ppm Cu and +500 ppm Zn, whlle
lead occur s as a +100 ppm core to the copper-
zInc zone.
The gold Is dlstr ibuted within branch-shaped
zones which appear to foi low the arcuate con-
tacts of the breccia p lpes In the near surface
outcropp ings. Deeper In the subsur face these
2ones coa lesce Into tr unk channe l s of minera i-
lzatlon. The overall geolog ic tonnage and
grade of the gold mineralization within the
p lpe comp lex Is estlmated to be approx imately
15 mi11Ion tons at .05 ounces gold per ton.
lLas Vegas Is the largest city in Nevada with a
population In 1982 of 183,184; It and the rest of
CIark County have a population of 515,021 or about
575 of the total state popuiation. Las Vegas
means 'The Meadows'' In Spanish. FlowIng spr Ings
In the area were first used by white men about
1830 and became a stopp Ing place on the Old Span-
Ish Trail from Santa Fe to southern Ca liforn ia.
After a brief attempt by the Mormons to colonize
the valley In 1855, the area was operated for many
years as a large ranch. Las Vegas was forma lly
estab lished In 1905 when WI11Iam A. Clark (Montana
copper mii Iona ire and former senator ) buiit the
San Pedro, Los Ange les and Salt Lake Ra ilroad (now
the Un ion Paclflc) through the va lley. Las Vegas
got a boost In population when Hoover Dam was
bui it In the 1930s, but in 1942 If was stti1
mostly a railroad commun Ity with a population of
10,000. After 1946 Iarge caslnos and hotels were
built, and the tour ist Industry became Impor tant.
alent, and boilIng occurred at hlgher levels
(Fahley, 1981).
The orebod les In the Tonopah dlstrlct occur In
a zone approx Imately 65O feet thlck, which Is
e lon gated In an east-west direction. ThIs zone
reaches its highest altltude In the central part
of the district, whlch Is the only place that
orebod les are exposed at the surface. The zone Is
control led by several sets of pre-ore faults,
which are both high and low angle. The exposed
orebod les are oxldized to an average depth of
about 100 feet, although some deeper oxldatlon
occur s loca lly along faults which cut the ore.
Supergene enr ichnent was not a slgnificant factor
In the Tonopah distr lct. S1lver-gold ratlos In
oxIdIzed ore were essentia lly the same as those In
unoxidized ore, which Indicates that relatively
1 Ittie super gene mlgration of silver or gold
occurred. Most of the siiver In the oxidized ore
occurred as sllver haloids.
The age of the bonanza sliver-gold minerallza-
tlon de scr lbed above Is we il defined by K-Ar age
dating of pre- and post-ore rocks and hydrotherma l
vein minerals (Bonham and Garslde, 1979, p. 111-
112) as 18-19 m.y.
ln addltion to the sllver-gold deposits des-
cr Ibed above, younger low--grade gold-silver miner-
alizatIOn hydrotherma l alteration occur In Intru-
slve rhyollte (Oddle Rhyolite) and tuffaceous
sediments (S Iebert Formation). These mIneralized
units are younger than the maln-stage Tonopah
minera iization. Very f[ttie ore has been produced
to date In the distric from rocks mlneralized In
this later per lod of hydrothermal activity. In
genera i, the minerallzatlon Is dlstinctly lower
grade than In the ear iier phase and has a hlgher
gold/sllver ratio. MInerallzed ve ins of this
later per lod occur on Moount Oddle and Aarat Moun-
tain. DIsseminated, iow-grade, gold-silver mlner-
allzatlon also occurs In silicified, tuffaceous
sed iments of the Sleber t Formation and In Oddle
RhyolIte at the Three HI1Is property 1/2 mlle
northwe st of SIeber t Mountain.
Beca use unaltered rocks of the Brougher Rhyo-
1Ite uncon formab ly over l le hydrothermally altered
rocks of the Siebert Formation, the age of the
Broug her Rhyoiite places a tower 1Imit on thls
later per lod of mInera iizatlon In the Tonopah
district. The 8rougher Rhyo llte In the Tonopah
dlstrict has K-Ar ages of 16.1 and 16.5 m.y. The
avallab le evldence Indicates that thIs second
per lod of mlnerallzatlon In the Tonopah district
Is the same age as the mineral lzation In the near
by DIvide district and Is probably genetlcally
related to It, as adularla and adular ized whole
rock from minerallzed todes In the Divide distr ict
were dated by the K-Ar method at 16.5 and 16.4
m.y.
Anoma ious go 1d, silver, arsen Ic, antimony,
zInc, bar ium, manganese and other metals are
repor ted from ve in and wall-rock samp les In the
Tonopah dlstrlct. The details of the geochemical
anomai les are repor ted ln Boonham and Garside
(1982).
EO OF DAY 2
The Jamestown mine lIes along the hlstor ic
Mother Lode gold belt about one mlle west of the
commun Ity of Jamestown, Tuo lumne County. Here,
the Mother Lode belt lies along the trace of the
Me lones fault zone. Gold mineralization at the
site has occurred In disseminated form In assoc ia-
tlon with altered Mesozolc volcan ic and sed imen-
tsry rocks which have been metamorphosed to the
lower greenschist facles. $tratlgraphlc unlts
exposed on the property Include serpentin Ite,
cherty sed lmentary rocks, carbonaceous slates,
meta tuffaceous rocks, and quar tz-carbonate rocks,
Includ ing mar lposlte. In addition a masslve bull
quartz vein Is exposed along the promlnent r idge
lIne (see geolog ic map and cross sectlon). The
volcan lc and sed lmentary rocks are character ized
by the presence of chlor ite and ser lcite. The
units form a layered sequence which str ikes nor th/
nor thwe st and dips steeply to the east. The
serpentin Ite body exposed a long the western margin
of the ore zone Is thought to form the base of the
SequefCe.
Exploration drllling conducted by New )ersey
ZInc In the eariy 1980s as well as ear ller work on
the property, forma lly known as the Harvard mine,
has outlined 14 mililon tons of ore averagIng
O.072 ounces of gold per ton. Gold-bear Ing ore Is
often encountered wlthin, but not confined to the
zones of pronounced a lteration. Computer pro ]ec-
tions from the dr 1illng data Indlcate that some
nine distinct mineralized zones are spaced a long
The L.8.M. (Littie Bald Mountain) mine is
located in White PIne County, Nevada, north of
Eureka and south of E! ko. Dynasty is operator of
the mine and owns 42,5$ of the oroperty,
Within Zone t, a geolog ic reserve of aporox i-
mateiy one mlllion tons averaging 0.100 ounces per
ton gold was outiined by a 1984 qritttng program
toraiiing 15,000 feet.
Open pit mtning dur inq 1985 and 1986 has ex-
tracted 140,500 tons ot ore grading about 0.080
ounces per ton,
Underground deve lopment work is now ln pro-
gress to access Zone 1 ore be iow current 5it
leve is. Over the term 1987- 1989, an initial
275, 000 tons ot ore grad ing 0.15 ounces per ton
wiii be mined by a buik underqround min inq method.
An ongoing proqram ot exp ioration dri i] ing is
svstemat ical iy testing and eva iuat lng nuemerous
oother geolog ical and geochemicai tarqet areas on
the L.8.M. oroperty.
The L.3.M, property is underiain by a sequence
of Pleozoic carbonate rocks with qentle easter|y
d 1ps. Structure ls complicated by ma jor north-
northeast trending norma faults (1.a. East and
West Fauits), with northwest and northeast
trending normal fauits of smai ler displacements.
The oranqe weather ing quartz-eve oorphyry dlkes
are orobabiy reiated to the Batd Mountain stock.
The stratigraphic column froam to6 to bottos
inciudes the upper Ordovic ian Laketown Dolomite
comor ising llghht to grey weather ing sandy dolo-
m1feS exposed on the down -dropped D lock on the
eastern s ide of the East Fau!t zone,
The Eureka Quartzite, a light frey to white
orthoq uartzite, ls exposed on!y in the disturbed
area aionq the East Fauit zone.
The Antelope Vailey Formstlon forms the d1p-
sIope between the East and Mest Fauits on the
eastern fl ank of L]ttle 8ald Mountain and consi sts
of three members. The uppermost, lnformaily
called Blue Sponge, ls not exDosed on the
property. The Rsgged Yeilow member, host ing Zone
1 mineral ization, consists of tan to yel!ow,
med ium to thick bedded q'6y 1 lmestone with inter -
bedded calcareous sha ie. The Basal Doiomite
member is predominant! y massive, bedded, fetid,
fosst! iferous l imestone up to 250 feet tN ick.
The Nlnemi ie Format ion ls beige to brown
weather ing, ptaty, sandy doiomite and is under lain
by the grey tO dark qrey weather inq Goodwi n Lime-
stone, The Goodwin ls predom inantiy a flaggy
l imestone with ] iqht qrey siit interbeds and a
very smel 1 shate comoonent,
The indfal1 Format ion has been divided into
three membxers. Oniy the upper two member s are
Seen on the property, The Trans ition Beds, imme-
diately below the Goodwi n L imestone, are nodular
Steamboat Spr Ings geothermal area l Ies approx l-
tmate ly 16 km south of down town Re no, Neva da.
DI scharge of thermal waters and gases from an area
of of about 5 km' over the past 5 m.y. are now
mostly restr Icted to the Main Terrace ] ust west of
U.S. HIghway 395 (FIg. 1) and the Low Terrace to
the southe st. Of the active geothermal systems
stud ied In the wor id, Steamboat Spr ings has the
longest and one ot the most comp lex geologlc hls-
tor ies (F Ig. 2; White and others, 1964; SIIberman
and others, 1979),
The hot spr ing water s have been used In local
spas and heating since the ear iy 1900s. Several
attempts were made In the 1950s and 6Os to exp lore
for geotherma l steam for electr ica l power genera-
tlon. BegInning In 1975, PhI11Ips Petroleum Co.
dr1iled nunerous temperature grad ient holes as
well as production we lls ranging from 100 to near-
ly 2000 m deep Indicating a reser voir temperature
of 225* C (Ph11lips Petroleum Co., unpub. dete,
1981), with the hlghest temperatures encountered
about 2.5 km southeast of the Main Terrace and
1HHle surtace evldence of hydrothermal activity
directly above. Currentiy, Geothermal Deve lopmsnt
Associates Is developing a 5 mw geothermal elec-
trlc piant about ) km nor thwest of the Maln
Terrace.
intermittent sma ll-sca le mining has occurred In
the area. Probably less than 100 flasks of mer-
cury have been produced from the distr [2t from
severa l smail mines In leached granodi ir Ite and
basaitlc andeslte, and slnter (Bonham, 1969).
Some sllica, shlpped as glass sand, was produced
from the SIIIca P1 (FIg. 1), and a small amount
of kaolin was mined for br lck manufacture In Reno
from the Clay (Faith) pit (Papke, 1969).
The Steamboat Spr ings area Is underlain by
pendants of ear ly Mesozolc metavolcanIc and meta-
sed Imentar y rocks In iate Mesozo lc granodior lte.
Erosional remnants of the ear ly Miocene Al ta For-
mation loca lly lie unconformab ly over the Mesozolc
rocks. A few dlkes of ear ly mlddle Mlocene Kate
Peak Formation occur In the area. in the vicinity
of 5Inter HIi1 (FIg. 1), the basement recks are
unconformably over la in by approx imate iy 3 m.y.-old
slnter which Is overiain by basaitic andesite
erupted from a vent near the cre st 4f the Steam-
boat HIls which has ylelded a K-Ar age of 2,5 +
O.11 Ma (S1lberman and others, 1979). The thermal
area occur s approx imately on a northwester ly
strJkIng lIne of four known r hyollte domes. About
5 km southwest of the Mein Terrace Is the largest
of the domes which y lelded a K-Ar age of 1.14 r
O.04 Ma. One and one-half to 5 km to the north-
east of the Main Terrace are three domes y lelding
ages of 1.2 to 3.0 Ma (S Iberman and others,
The purpose of this paper is to describe the close spa-
tial and genetic association of uranium mineralization with
a structural trap and an oAidation-reduction boundary
associated with an ancicnt ground-water level, This associa-
tion has not been reported by previous workers at the Apex
Uranium Mine.
The Apex Uranium Mine, also referred to as the Early
Day or Rundberg Mine, is near the geographic center of
Nevada about three miles (4.8 km) south of Austin, County
Seat of Lander County (Fig 1). Uranium was discovered in
the summer of 1953 by Austin residents. From 1954 into
1966. about 21.000 tons of ore were mined and shipped,
with an average grade of 0.25% UaOs and yielding about
106,000 lbs of L xOs. About half of the uranium production
from Nevada has come from this mine. Four working levels
totalling several thousand feet were developed: Main Shaft,
6,200 feet (1890 m); Adit = 1,6,325 feet (1928 m), Adit =2,
6,386 feet (1946 m); and The Rundberg,6.500 feet ( 1980 m)
The geology of the Toiyabe Range in the area near the
Apex Mine is shown on Figure 2. Lower Paleozoic sedi-
ments of two contrasting facies were deposited in central
Nevada. The siliceous-volcanic, or Western Assemblage.
has been thrust eastward over the carbonate and Transi-
tional Assemblage during the Late Devonian to Early
Mississippian Antler orogeny. Horizontal. eastward dis-
placement of the siliceous and volcanic. deeper marine
rocks of the Western Assemblage mav have been 90 miles
(150 km) in this area along the Roberts Mountains Thrust
(Stewart and McKee, 1977). Further tectonism took place
Tatlock, and Silberling, 1960; Wallace, Tatlock, Sil-
berling, and Irwin, 1969). Since then, the region
has been undergoing upliit and erosion. ln late
MIesozoic time, block-fauiting, accompanied by igne-
ous intrusions and volcanic activity, ushered in a
new regime which culminated in the development o
basin-and-range structure. The cycle began with
the emplacement of intrusive bodies during late
Eocene and early Oligocene time, iollowed soon after
by widespread volcanism throughout Oligocene, MIio-
cene and Pliocene time. Block-faulting, which gave
rise to the present topography, was most intense in
Pliocene and early Pleistocene time.
Nevada may be divided into two major metallo-
genic provinces: a western one, characterized by
gold, silver, tungsten, mercury, and antimony de-
posits; and an eastern province, characterized by
lead and zinc deposits with minor silver and gold %
(Figs. 1, 5) (Ferguson, 1920; Bateman, 1950;
Roberts, 1966). The boundary between the prov-
inces is gradational and roughly bisects north-central
Nevada. Copper, tungsten, and molybdenum de-
posits occur in both provinces.
The ore deposits of the western or precious metal
province occur mostly in eugeosynclinal Paleozoic
and MIesozoic rocks (shale, chert, graywacke, vol-
canic rocks, and minor limestone) and in overlying
Tertiary rocks, The eugeosynclinal rocks were de-
posited on simatic oceanic crust 5-10 km thick. The
ore deposits of the eastern or base-m1etals province
occur mainly in miogeosynclinal carbonate rocks
(limestone, dolomite, and minor shale ) that were
deposited on sialic crust.
The boundary between the two provinces coincides
broadly with the boundary between major geosyn-
clinal trends and with the frontal zone of the Roberts
MIountains thrust fault. The nature of these rela-
tionships is not clear, but processes related to geo-
synclinal sedimentation and subsequent orogeny re-
sulted in conversion of the upper mantle under the
geosyncline to continental crust with consequent
magmatism and volcanism during several epochs
(Bateman, 1950; Coats et al., 1965; Roberts, 1968).
In north-central Nevada (Fig. 3) there are ap-
proximately 50 plutons of coarse-grained equigran-
ular to porphyritic quartz-monzonite to granodiorite.
These bodies range from about 130 to less than 1
square kilometer in outcrop area, Although many
bbodies appear to be distributed randomly in the re-
gion, others may be structurally controlled. Thirty-
one of these plutons have been dated by the
K-Ar method, and their ages are considered repre-
sentative of the times of m1ajor plutonism.
The grouping of ages indicates that plutonism
occurred during five periods in the MIesozoic and
Lenozoic (Fig. 4). The oldest group of dates in
Jurassic (168 to 143 m.y, old), then follow two
Cretaceous groups (105 to S7 and 71 to 6s m.y, old,
respectively ), the nest group is early Tertiary -40
to 30 m.y, old) and the youngest group is late
Tertiary (16 to 10 m.y, old),
Widespread tectonic activity including the em-
placement of large granitic bodies west of central
Nevada in the Sierra Nevada is reflected by the plu-
tons of the two older age groups in north-central
Nevada, The oldest group of north-central Nevada
plutons are the same age as parts of the Inyo and
Yosemite intrusive epochs of the Sierra Nevada
batholith as defined by Evernden and Kistler ( 1970;
Fig. 4). The older of the Cretaceous periods of
plutonism in north-central Nevada is about midway
in age between the Huntington Lake and Cathedral
Range Sierran intrusive epochs of Evernden and
Kistler (1970; Fig. 4), and a number of plutonic
rocks in the Sierra Nevada batholith have been dated
at about 100 m.y. as well. The younger Cretaceous
intrusive rocks in north-central Nevada (70 m.y.
old) correspond in age to early Laramide, as defined
by Damon and Mauger (1966). The early Tertiary
(40 to 30 m.y, old) plutons in north-central Nevada
are the same age as the start of Tertiary igneous
activity in the Great Basin (McKee and Silberman,
1970a, b). Plutonic rocks of Laramide and middle
Tertiary age are not found in the Sierra Nevada,
uggesting that the two younger periods of plutonism
(70 m.y and 40 to 30 m.y.) in north-central Nevada
are not related to Sierran intrusion but are related
to geologic events in the eastern Great Basin.
Large bodies oi plutonic rocks younger than about
30 m.y, old are not known in north-central Nevada,
but widespread volcanic rocks younger than about 16
m.y. old probabiy have deep-seated plutonic equiv-
alents not exposed at the present level of erosion.
This younger group (16 to 10 m.y, old) shown in
Table 2 was defined by MIcKee and Silberman
(1970a) from occurrences in the Sheep Creek MIoun-
tain and northern Shoshone Ranges, and is repre-
sented by basaltic andesite to rhyolite flows and dikes.
A swarm of these dikes and flows also occurs in the
Cortez Mountains and Roberts Mountains (Fig. 3).
Roberts and Coats consider that the five intrusive
epochs in north-central Nevada also represent dis-
tinct metallogenic epochs, and have so designated
them in Table 2. The replacement deposits were
Ground-water chemical data can help define ground-
water flow systems and evaluate the relative importance
of ground-water sources and pathways when combined
with hydraulic data. Many attempts have been made to
use ground-water chemical data in this manner, with
limited success, primarily because of lack of understand-
ing of the mechanisms responsible for dissol ved constitu-
ents in ground-water samples and difficulty in obtaining
samples that are representative of a definable part of the
hydrologic environment.
Advances have been made during the last decade toward
understanding the role of various mineral phases in disso-
lution and precipitation reactions, primarily in the con-
text of thermodynamic equilibrium. However, this concept
is limited for two reasons (1) Accuracy of the thermody-
namic data for naturally-occurring minerals is suspect;
and (2) use of equilibrium concepts to describe systems
that are generally in disequilibrium is questionable.
In this report, geochemical concepts that recognize
the chemically dynamic nature of ground water are applied
to determine the sources and pathways of ground water
in the west-central Amargosa Desert in southern Nevada.
These concepts help evaluate migration potential for
radioactive waste resulting from nuclear weapons test-
ing or migration potential for radioactive wastes stored
on the Nevada Test Site. The study area encompasses
one of the possible migration paths.
This research was supported in part by the U.S. Depart-
ment of Energy under Interagency Agreement DE-AI0
8-76DP00474A between the U.S. Geological Survey and
the U.S. Department of Energy.
The first comprehensive study made of water resourc-
es of the study area was by Walker and Eakin (1963);
they discussed the climate, geologic history, lithology.
hydrology, and some of the water-quality characteristics
of the Amargosa Desert, particularly as they relate to
water use and development potential. The primary study
area in this report is shown in figure 1.
The surface-drainage area of the Amargosa Desert is
about 6,700 krm (Walker and Eakin, 1963); a part is
shown in figure 1. Elevations in the study area range
from about 670 to 2,100 m above mean sea level, although
a few higher peak elevations occur at about 2,400 m.
Mean annual precipitation corresponding to these eleva-
tions is 50 to 380 mm (millimeters); mean annual temper-
atures range from 6% to 16 C.
The type of vegetation depends on degree of slope,
slope aspect, and precipitation. Steep slopes, especially
south- or west-facing, have little to no vegetation. lLower
elevation, gently sloping alluvial fans support desert
scrub; higher density cover exists at elevations near
1,500 m and lower density cover exists at lower elevations.
Scattered juniper and sage are the dominant woody
plants from about 1,500 m to about 1,800 to 2,100 m,
where pinyon pines begin to dominate.
Principal fluxes specified in the model are the distributed areal fluxes
at Franklin Lake playa, occurring as evapotranspiration; at Fortymile Canyon,
occurring as infiltration; and as linearly distributed flux at Furnace Creek
Ranch, occurring as seeps and springs. The evapotranspiration flux estimate
of 1.3 = 10 m/s (1.12 10% m/d), applied throughout an area of 31,7 km% at
Franklin Lake playa (0.42 m/s, or 3.63 x 10% m /d, throughout total area),
was obtained by allowing the model to optimize on this flux as the only model
parameter. Significant correlation of this flux parameter with upgradient
transmissivity parameters prevented convergence to a solution, hence the need
to solve for this parameter individually. By holding all other parameters
constant, this procedure produced only a local-minimum error variance. This
model-estimated flux was used in subsequent simulations. Although not
obtained in the most optimal manner, this estimated value of flux is in agree-
ment with the value for evapotranspiration of 0.39 m /s (3.37 x 10% a/d)
estimated by Walker and Eakin (1963), which they noted was ''crude,'' Because
of the importance of this flux to the regional-scale model developed by
Waddell (1982), analysis of the sensitivity of the present model to changes in
this and other flux parameters was performed and is discussed in a subsequent
section of this report.
Measurements are being made throughout the year and at various locations
at Franklin Lake playa to evaluate evapotranspiration rates and to refine this
flux. Initial measurements of evapotranspiration at Franklin Lake playa
produced rates of about 3 x 108 m/d during June 1983 and 1 x 10 m/d during
January 1984 (D.I. Stannard, U.S. Geological Survey, written commun.,
1983-84).
The flux occurring as infiltration at Fortymile Canyon was set as a
parameter; however, setting that flux as a parameter did not allow model
convergence, because of significant correlation with parameter 3. Estimates
of this flux were varied for individual runs until a minimum error variance
was achieved. As for the case of estimating the evapotranspiration flux at
F ranklin Lake playa, this produced only a local-minimum error variance.
Standard errors in parameters were estimated by the parameter-estimation
procedure (Cooley, 1979, p. 606 ; R. L. Cooley, U.S. Geological Survey, written
commun ., 1981). As stated by Cooley (1979, p. 606), ''Standard errors = = are
measures of the ranges over which the respective parameters may be varied and
produce a similar solution for the head distribution as that obtained by using
ä [the value of the parameter].'' Uncerttainty in the parameters normally is
larger than that indicated by the estimated standard errors.
Standard errors range from 0.3 to 2.4 percent of the associated parameter
value (table 1). This fit was obtained after many simulations and recombina-
tions of parameters. Additional uncertainty occurs because of uncertainty in
flux terms that were assumed to be known exactly.
Two of the primary responsibilities of the U .S. Geological Survey are to
assess the Nation's water supply and to develop the understanding necessary to
predict the environmental consequences of alternative means of developing and
managing water resources, To carry out these responsibilities the G eological Survey
conducts studies, many in cooperation with State and local agencies, to determine
the quantity and quality of the Nation's water resources and the response of hydro-
logic systems to both natural and manmade stresses. The results of the studies are
made available in numerous ways, including published reports, written and oral
responses to specific requests, and presentations at scientific and public meetings.
A lthough most reports are designed to meet the technical needs of those
engaged in the development, management, and protection of water supplies, the
U.S. Geological Survey has long recognized the need to present the results of its
studies in a form that is also understandable to those who are affected by and who
benefit from water developments. To better meet this need, the Water Resources
Division of the Geological Survey expanded the preparation of general-interest
reports in 1980. The reports planned as a part of this program deal both with spe-
cific water-related problems, such as abrupt land subsidence that results in sink-
holes and ''water logging'' of the land in urban areas due to a rising water table,
and with general topics of broad public interest, such as this report which describes
the ground-water resources of the N ation.
G round water occurs in the rocks that form the Earth's crust and thus is in
the domain of geology. Because the geology of the country is complex, the occur-
rence of ground water, in detail, is extremely complex. This complexity makes it
difficult for many people to develop an understanding of ground-water occurrence
and availability and has resulted in problems of ground-water depletion and
ground-water pollution whose correction will be both difficult and expensive. For-
tunately, such problems are not yet widespread and can, with intelligent application
of existing ground-water knowledge, be avoided in most other areas. However, to
realize this goal, those engaged in water-resources development and management
and the general public need to become better informed on the Nation's ground-
water resources. The purpose of this report is to help meet this need.
The report consists of sections that deal concisely with discrete parts of the
overall subject. The sections are arranged in a sequence that begins with a discus-
sion of the general aspects of geology and rocks and proceeds to a description of
the ground-water systems in the 15 ground-water regions into which the U nited
States, Puerto R ico, and the Virgin Islands are divided. An attempt has been made
to illustrate most of the important concepts and topics covered in the discussions.
It should be noted that the block diagrams used for illustration in the regional dis-
cussions are intended to show the major features of the ground-water system in the
region rather than a specific part of a region.
The section entitled ''Ground Water Regions of the United States'' also war-
rants special mention. It includes maps that show the boundaries of the regions and
tables that summarize the physical and hydrologic characteristics of the regions.
As is the case in the Nonglaciated Central region,
mineralized water occurs at relatively shallow depth in
the bedrock in large parts of this region (fig. 35). Because
the principal constituent in the mineralized water is
sodium chloride (common salt), the water is commonly
referred to as saline or salty. The thickness of the fresh-
water zone in the bedrock depends on the vertical hydrau-
lic conductivity of both the bedrock and the glacial de-
posits and on the effectiveness of the hydraulic connec-
tion between them. Both the freshwater and the underly-
ing saline water move toward the valleys of perennial
streams to discharge. As a result, the depthto saline water
is less under valleys than under uplands, both because of
lower altitudes and because of the upward movement of
the saline water to discharge. In those parts of the region
underlain by saline water, the concentration of dissolved
solids increases with depth. At depths of 500 to 1,000 m in
much of the region, the mineral content of the water
approaches that of seawater (about 35,000 mg/ L). At
greater depths, the mineral content may reach concentra-
tions several times that of seawater.
Because the G laciated Central region resembles in
certain aspects both the Nonglaciated Central region
(region 6) to the south and the Northwest and Superior
Uplands region(region 9)to the north, it may be useful to
comment on the principal differences among these three
regions, First, and as is already apparent, the bedrock in
the Glaciated Central and the Nonglaciated Central
regions is similar in composition and structure. The dif-
ference in these two regions is in the composition and
other characteristics of the overlying unconsolidated
material. In the Nonglaciated Central region this mate-
rial consists of a relatively thin layer that is derived from
weathering of the underlying bedrock and that in any
particular area is of relatively uniform composition. In
the Glaciated Central region, on the other hand, the
unconsolidated material consists of a layer, ranging in
thickness from a few meters to several hundred meters, of
diverse composition deposited either directly from glacial
ice (till) or by meltwater streams (glaciofluvial deposits).
From a hydrologic standpoint, the unconsolidated mate-
rial in the Nonglaciated Cetntral region is of minor impor-
tance both as a source of water and as a reservoir for
storage of water for the bedrock. In contrast,the glacial
deposits in the Glaciated Central region serve both as a
source of ground water and as an important storage
A total of 39 tortoise sign, or 0.6 sign per transect mile, was
observed including: the five tortoises, 15 burrows, 16 scats, and three
skeletal remains (Table 5). Evidence of tortoise was f ound in each of
the six areas although no tortoises and very little evidence of recent
tortoise a ctivity was found in Area 4, the largest area.
Evidence was f ound of two mammals not previously o bserved. A kit
fox (Vulpes macrotia) was observed briefly as it entered a den in
Area 2, but no other kit fox sign was found. Several bobcat (Lyna
rufiue) scats were found in rocky areas on hills in Areas l and 6.
C2-S1. Exeept for application on the poorly drained
land along Carson Slough, this water is of moderately
good quality.
With few exceptions the water sources in Ash
Meadows do not exceed concentrations recommended
by the U.S. Publie Health Serviee (1962) for any
chemical constituent other than fluoride. All samples
reported from Ash Meadows exceed the recommended
limit for fluoride (0.8 mg/l under the existing temp-
erature) by a factor of 2 or more. Dental fluorosis
and discoloration may occur in the teeth of children
raised here. Recommended maximum concentrations
of sulfate and chloride are exceeded in wells 1 and 2
and in the 1970 sampling of Jack Rabbit Spring.
The effects of pumping were monitored during 1971
and 1972 by water-level recorders in Devils Hole, in
several observation wells, and in flumes and weirs
installed in most of the major springs. (See table 2.)
Moderately detailed records of pumpage from the
production wells in 19T1 were reconstructed from
electrical-meter readings, discharge measurements,
and frequent observations of whether or not they
were pumping.
Because gaps in the hydrograph of Devils Hole had
oceurred before, well 17S/50-36dd was instrumented
in January 19T1 as a precaution against lost record.
An excellent correlation was established between the
depths to water in this well and in Devils Hole, which
is about 900 ft (275 m) west of the well,
During the aquifer tests of the individual production
wells, no effects on the water level in Devils Hole were
discernable. Daily fluctuations of the pool (as much as
0.5 ft or 15 cm) would easily have masked any changes
due to pumping.
The correlation between gross pumpage from the
well field and the water level in Devils Hole, however,
Phreatophiytes are plants that obtain their water supply primarily from
ground water. Because their roots normally must reach the capillary fringe,
they are found in areas of relatively shhallow ground water, generally along
water courses and thie adjacent valley areas. Most phreatophiytes lave little
economic value and the water they use provides relatively little benefit to
man, Many species are prevalent in the southwestern states, although som1e
may not be recognized as phreatophytes, Among the most common are salt-
cedar, arrowweed, cottonwood, saltbush, willow, and mesquite. Iobinson%
lists more than seventy species that are found in western United States.
Closely associated with the phreatophytes are the hydrophytes-plants that
live with their roots wholly or partly submerged in water. Tule, cattail, and
various swamp grasses classed as amphibious hydrophytes are common in
areas containing offstream ponds or meander channels. This swamp-type
vegetation often is contiguous with phreatophytes and contributes importantly
to evapotranspiration losses from some western rivers,
Estimates of the amount of water used by phreatophytes in the arid western
regions are strikingly impressive, Phreatophytes covering some 16 million
acres in seventeen western states may use from 20 to 25 million acre-ft of
water annually,% n six states alone-Arizona, California, Colorado, Nevada,
New Mexico, and Utah-the area of phreatophytes was estimated by the Senate
little water geting into the soil, The lighter the precipitation thhe
greater the percentage which was intercepted. During light storms,
hurdly any ruin was cnught in gges under heavy brush. l)ring
heavy storms, we suw leuf drip but no stem flow on the sugebrush.
Witlin each henvy sagebrush plot, 3) percent of the rain gages
were under hheavy brush, 4) percent under medium brush, and 3) per-
cent in the open. (iages in the open cuuglit over twice the rain
cnught by gges under henvy brush. However, averaging all gnges
on the heavy sagelrush1 plots, henvy brush intercepted 31 percent
of the rain at Holbrook and 3) percent at 'Twin Fuulls, when com-
pared to gages on the brush-free plots. Sngebbrush on the henvy
This report was prepared as an account of work sponsored
by an agency of the United States Government. Neither
the United States Government nor any agency thereof, nor
any of their employees, makes any warranty, express or
implied, or assumes any legal liability or responsibility
for the accuracy, completeness, or usefulness of any in-
formation, apparatus, product, or process disclosed, or
represents that its use would not infringe privately-owned
rights. Reference herein to any specific comercial
product, process, or service by trade name, trademark,
manufacturer, or otherwise, does not necessarily consti-
tute or imply its endorsement, recommendation, or favoring
by the United States Government or any agency thereof.
The views and opinions of authors expressed herein do not
necessarily state or reflect those of the United States
Government or any agency thereof.
Printed in the United States of America
Available from
National Technical Information Service
U.S. Department of Commerce
5285 Port Royal Road
Springfie1d, VA 22161
NTIS price codes
Printed Copy 993
Microfiche Copy: A01
The site characterization project covered a 27.5-sq-mi area located on
Yucca Mountain in the southwestern portion of NTS and adjacent portions of the
U.S. Air Force Nellis Bombing Range, and Bureau of Land Management (BLM) lands
in eastern Crater Flat, Nye County, Nevada (Figure 1). The area in which
studies took place was defined to encompass all areas which might be impacted
by near-term site characterization, and potential future repository construction,
operation, and decommissioning activities associated with NNWSI.
The major topographical features of the project area included the long
north- south aligned ridge of Yucca Mountain which drops off sharply on the west
salt beds. In addition, and largely because of the different mode of forma-
tion, the following differences between the two types of salt rock are
noteworthy:
Some of the most important of the above factors affecting waste isolation
at salt sites are related to the chemical composition and configuration of the
host rock. All salt sites would rely primarily on the extremely low perme-
ability of the salt and the isolation of the host rock from surrounding
aquifers. One significant potential failure mechanism in salt that can affect
ground-water flow is the dissolution of the salt in ground water, whether
initiated by inadvertent human intrusion or by unexpected salt deformation.
The nature and the relative importance of this failure mechanism differ sig-
nificantly for bedded and dome salt in their respective geohydrologic environ-
ments. For example, at salt domes dissolution would occur along the flanks by
ground water from surrounding sedimentary strata. The dissolution of bedded
salt could be induced by laterally migrating dissolution fronts, inter-salt-
bed sedimentary aquifers, or vertically circulating water in fault zones.
Finally, although the Paradox Basin in Utah and the Permian Basin in
Texas are both bedded-salt settings, they also have significant differences
that warrant considering them as separate and distinct geohydrologic set-
tings. The bedded-salt sites in Swisher and Deaf Smith counties, Texas, are
located in the Bigh Plains setting as defined by the USGS. This setting is
underlain by relatively horizontal bedded sedimentary rocks that are capped by
the partially unconsolidated sands, gravels, and clays of the Ogallala Forma-
tion. The geohydrologic system is dominated by the High Plains aquifer (the
Ogallala Formation). Other aquifers, such as the Triassic Dockum Group, occur
in deeper strata, but they produce poor-quality water in comparison with the
Ogallala.
The bedded-salt sites of Davis Canyon and Lavender Canyon, Utah, on the
other hand, are located in the Paradox Basin, which is a subsetting of the
Colorado Plateau and the Wyoming Basin and is characterized by a broad
uplifted plateau consisting of gently folded sedimentary sandstones, shales,
carbonates, and evaporites. The stratigraphic sequence includes a few low-
yield aquifers that generally contain poor-quality water. Ground water
generally flows toward drainage systems in deeply dissected canyons of the
region. ther specific differences include the following:
the parameters used in the screening calculations and provides a detailed
discussion of the screening results. The last three provide detailed back-
ground material about the performance objectives (Sinnock and Fernandez,
1984), physical attributes and associated quantitative criteria (Sinnock et
al., 1984), and computer programs (Sharp, 1984) for rating alternative
locations.
Many assumptions were quantified during the screening study, and the
validity of the results and conclusions clearly depended and continues to
depend on the reasonableness of these assumptions. The information in the
referenced screening reports allows each assumption or set of assumptions to
be traced to its effects on the results and conclusions. The remainder of
this section contains an overview of the data and analyses contained in these
reports.
The formal screening analysis (Sinnock and Fernandez, 1982) was applied
to an area on and near the southwestern portion of the NTS (Figure 2-8). The
analysis consisted of four basic elements.
The perforaance objectives were organized into a three-level hierarchi-
cal tree (Table 2-1), which allowed site-specific objectives of the lowest
level of the tree to be clearly tied to the broad goals of waste management
(DOE, 1980) represented by the uppermost level of the tree (Sinnock and
Fernandez, 1984). Each objective was correlated with existing criteria of
the DOE and the Nuclear Regulatory Commission to ensure that no relevant sit-
ing factors were overlooked. Table 2-2 shows this correlation and also shows
the correlation with the DOE siting guidelines (10 CFR Part 960, 1984), which
did not exist at the time of screening. A weight, or percentage describing
relative importance, was assigned to each objective at each level of the tree
to account for priorities within each level (see figures 2-9a and 2-9b). The
weights were obtained from a poll of technical experts (Sinnock and
Fernandez, 1984).
The physical attributes that form the second basic element of the formal
screening analysis are shown in Table 2-3. Each of the 31 attributes repre-
sents a physical condition that both (1) varies throughout the screening area
and (2) might influence repository behavior (Sinnock et al., 1984). As
Table 2-3 indicates, the attributes fall into two general categories, geo-
graphical (attributes 1 through 23) and host rock (attributes 24 through 31).
Tingley (1983). Boreholes have been drilled in and around Yucca Mountain for
the Nevada Nuclear Waste Storage Investigations Project (Maldonado and
Koether, 1983; Spengler et al., 1981), and core samples and drill cuttings
have been routinely analyzed by geochemical methods. Field exploration and
geologic mapping has been conducted by the U.S. Geological Survey
(Christiansen and Lipman, 1965; Lipman and McKay, 1965; Scott and Bonk,
1984). From all of the above investigations, it can be concluded that the
overall potential for development of mineral or energy resources at Yucca
Mountain is low.
There is no evidence that Yucca Mountain contains any commercially
attractive geothermal, uranium, hydrocarbon, oil shale, or coal resources
(Bell and Larson, 1982). None of the drill holes at or near Yucca Mountain
have shown evidence of hydrocarbons. The geology of the area suggests that
the existence of fossil fuel resources at depth is highly unlikely (Bell and
Larson, 1982).
There are no warm springs at Yucca Mountain. The area around Yucca
Mountain is well known in terms of heat flow. More than 60 drill holes (some
as deep as 1,829 meters (6,000 feet)) have been drilled and analyzed.
Surface and subsurface evidence near Yucca Mountain indicates a potential for
low to moderate geothermal energy at depths less than 1 kilometer (3,300
feet) (Bell and Larson, 1982). However, the geothermal gradient measured in
several drill holes at Yucca Mountain (Sass and Lachenbruch, 1982) indicates
that it is unlikely that high-temperature waters could be present at depths
that are economically attractive. Water temperatures measured in wells east
of Yucca Mountain range from 21 to 65*C (70 to 149'F) (Bell and Larson,
1982). With present technologY, this temperature range is insufficient for
commercial power generation, which requires temperatures of at least 180*2
(350'F) (White, 1973).
Minor amounts of uranium have been reported west of the site at Bare
Mountain, but no uranium mines or prospects have been developed. Under
current economic conditions, the uranium resources identified in the Bare
Mountain area are not attractive targets for development (Bell and Larson,
1982).
Table 3-2 identifies the status, number, and types of exploratory and
mining operations for base and precious metals in the Yucca Mountain area,
and Figure 3-10 shows the location of these deposits. Historically, Nevada's
metallic industry centered around the mining of precious metals in the
Comstock district in west-central Nevada and in the Tonopah and Goldfield
districts more than 150 kilometers (95 miles) northwest of the site.
Although there are numerous small mining districts throughout the southern
Great Basin, the only active silver and gold mine in the region is the
Stirling-Panama mine near Bare Mountain. Reserves have not been reported by
s a duck paddles along the edge of
a pond, it nips at the tops of under-
water vegetation. When one nip catches
a shoot of Chara, a relative of the green
algae, it sends a spectacular system into
action. The force of the duck's bite trig-
gers an electrical mechanism in the
plant, and ionic current rushes across
the membrane of the nibbled cell. Then
the fluid inside the cell, the protoplasm.
stops its normal flow around the pe-
riphery. The protoplasm quickly jells,
preventing any leakage that could arise
from the duck's attack.
Chara is hardly the only plant that re-
sponds to external stimuli, All plants re-
spond to gravity as they grow, and
plants can have various responses to
light. Some follow a 24-hour cycle, ad-
justing the orientation of their leaves for
the maximum absorption of light dur-
ing the day. Some plants respond with
movement when they are touched by
predators.
What may be less obvious is how
plants respond to stimuli, Although
most people know that electrical signals
mediate the responses of an animal's
nervous system, it is less widely known
that plant behavior, too, is governed by
complex electrical mechanisms. Plant
cells, in fact, are hotbeds of electrical ac-
tivity, and plant studies have provided
much of the foundation of what is
known generally about electrical activi-
ty in cells. Chara has been important in
those studies and continues to be.
Physiological studies of electrical ac-
tivity began in the 19th century, and
since then animal and plant physiolo-
gists have worked in parallel. In order
to study the activity where it happens,
at the cellular level, investigators had to
find organisms in which the activity
could be studied in isolation from the
whole plant or animal. They also need-
ed to find cells large enough that they
could be probed with electrodes. In ani-
mal studies, the search led to the long
nerve cells of squids, in which axons,
the fibers carrying messages from the
cell body, are so large that they were
originally thought to be blood vessels.
Plant physiologists, on the other hand,
selected species of algae that have large
cells, such as the characean algae Chara
and Nitella.
In 1898 Georg Hörmann, a German
physiologist, observed that big differ-
ences in voltage measurements could
develop across cell membranes of
Nitella. When such differences are re-
generative they are called action poten-
tials, because the regeneration implies
action-the passing of an impulse. By
the 1930s, characean algal cells were so
well known that many investigators
studied them. For example, K. S. Cole
and Howard Curtis of the National In-
stitutes of Health, who later became
known as pioneers in the electrical ex-
citability of squid neurons, began
studying excitability in Nitella. These in-
vestigations showed that an action po-
tential in Nitella is accompanied by a
200-fold increase in the cell membrane's
conductance, as measured by the num-
ber of ions crossing the membrane.
They concluded that ions carry the cur-
rents that create the action potential.
Although plants are no longer the
leading organisms used in research on
the basis of electrical excitability, a num-
ber of investigators have significantly
advanced our knowledge of both the
mechanisms and the effects of electricity
in plants. Modern techniques common
to neurophysiology have been applied
to a variety of plants, and the results
show that electrical physiology in plants
is as complex as the systems found in
animals. Moreover, a variety of plants
use electricity to initiate action; exam-
ples are the closing of the leaves of a
Venus flytrap and the touch-driven
drooping of the leaves of some Mimosa
species. Nevertheless, the most detailed
information exists for characean algal
cells, which I shall examine here. The
electrical activity in these algae is worth
examining not only for its importance in
plant biology, but also because studies
of plant excitability may help us under-
stand the evolution of the human ner-
VOus system.
Characean algae have been used in
much of the work on plant excitability.
They are stoneworts, with a fossil
record stretching back to the Devonian
period, which began about 400 million
years ago, and they are the ancestors of
all higher plants. Extant stoneworts be-
long to a single family, Characeae,
which is composed of six genera in-
cluding Chara and Nitella. The majority
of the extant species inhabit the bottom
of clear freshwater ponds, where they
live entirely submerged.
As I have noted, the primary attrac-
tion of characean algae as an object of
study is the size of their cells. In Chara,
the plant body is composed of long in-
ternodal cells separated by smaller
nodal cells. A single internode may be
six centimeters long and half a millime-
ter wide, or about half as long as a
toothpick and half as wide. The inter-
nal structure of an internodal cell is un-
like that of an animal cell. Like all plant
cells, the external border is a cell wall,
which is composed of cellulose fibers
that provide rigidity to the cell but are
permeable to the extracellular fluid. Just
beneath the cell wall is a semipermeable
plasma membrane, which is composed
of two layers of lipids that are inter-
spersed with proteins. Beneath the plas-
ma membrane there is a layer of chloro-
plasts, the sites of photosynthetic
processes. Most of the interior of the cell
isa vacuole,a sac filled largely with wa-
ter and bounded by another membrane.
The area between the vacuolar mem-
brane and the plasma membrane is
filled with protoplasm; here are found
the cell nucleus and the cytoplasm, a
viscous fluid that contains the cell's or-
ganelles such as mitochondria and ri-
bosomes.
The protoplasm of characean cells
moves constantly around the periphery
of the cell, just beneath the chloroplasts.
The rotating belt of protoplasm travels
at a speed of about 100 microns per sec-
ond. Its movement, visible through a
microscope, is called protoplasmic
streaming or cyclosis. The streaming
process is driven by the same interac-
tions between actin and myosin that
create contraction in muscles. The
movement of the protoplasm mixes and
transports molecules through the cell,
which would take too long in such large
cells if diffusion were the only mecha-
nism available.
A fundamental concept in electrical
physiology is defined by the term po-
tential. A potential is a voltage across a
membrane, which is created by the sep-
aration of positive charges from nega-
tive charges. In biology, charges are car-
ried by ions. Positive charges are carried
by cations such as sodium, and nega-
tive charges are carried by anions such
as chloride. If one side of a membrane
has more positively charged ions and
the other side has more negatively
charged ions, then there is a potential,
or voltage, across the membrane.
Here I shall discuss four potentials:
membrane potential, resting potential,
receptor potential and action potential.
A membrane potential is the voltage
across a membrane, Or a measurement
of the distribution of ions. The resting
potential is the membrane potential
when the cell is not being stimulated.
Both a receptor potential and an action
potential change the membrane poten-
tial. A receptor potential arises when a
receptor in a membrane, such as a mol-
ecular mechanoreceptor, is stimulated.
The stimulation generates an ionic cur-
rent that changes the membrane po-
tential, but the receptor potential de-
creases in magnitude with distance
from the stimulated receptor An action
potential is a large, transient change in
the membrane potential that is self-
perpetuating, or regenerative, and it
can travel the length of the cell without
decreasing in magnitude.
Characean algae generate action po-
tentials when subjected to a variety of
stimuli, including a sudden change in
temperature, ultraviolet radiation, odor-
ants and mechanical action. These stim-
uli first cause the plant to produce a re-
ceptor potential. For example, a small
mechanical stimulus is converted by a
receptor into electrical energy that is
proportional to the magnitude of the
stimulus. In a resting characean cell,
there is a negative voltage inside the
plasma membrane relative to the out-
side of the cell. In other words, there are
more negatively charged ions inside the
membrane and more positively charged
ions outside the membrane. The recep-
tor potential generates depolarization,
a decrease in the voltage difference be-
tween the inside and the outside of the
cell. This potential generally lasts as
long as the stimulus is present, and it is
essentially an electrical replica of the
stimulus. If the stimulus depolarizes the
cell to a specific threshold level, an ac-
tion potential is generated.
An action potential in one area of a
characean cell causes protoplasmic
streaming to stop throughout the cell.
As I shall explain below, the action po-
tential causes external calcium to move
into the protoplasm. The increased cal-
cium concentration activates a protein
kinase that adds a phosphorus group to
myosin and thereby inhibits its interac-
tion with actin, which stops the driving
force behind protoplasmic streaming.
membrane and the vacuolar mem-
brane) and the vacuolar fluid (the fluid
inside the vacuole). The concentrations
of ions can be given in the ratio of extra-
cellular concentration to protoplasmic
concentration to vacuolar concentra-
tion, because only the relative values are
significant to this discussion. The aver-
age ionic-concentration ratios are
100:1:12,000 for calcium, 1:55:405 for
chloride, 1:50:340 for sodium and
1:1,100:1,030 for potassium. In other
words, the concentration of calcium is
low in the protoplasm and high in the
vacuole; the chloride concentration is
higher in the protoplasm than in the ex-
tracellular fluid, and higher still in the
vacuole; the distribution of sodium is
similar to that of chloride; and potas-
sium has a higher concentration in both
the protoplasm and the vacuole.
The location of an ion is determined
by a chemical force and an electrical
force (Figure 8). In response to the chem-
ical force, an ion tends to go from an
area of higher concentration to an area
of lower concentration. The electrical
force pulls an ion toward an area of op-
posite charge, so that a cation, or posi
tively charged ion, is drawn toward a
negative area. Consider a potassium ion
in the protoplasm. Potassium is more
concentrated in the protoplasm than in
the extracellular fluid, and thus the
chemical force tends to pull potassium
out of the cell. The resting potential of
the plasma membrane, however, is neg-
ative in the protoplasm relative to the
extracellular fluid. This creates an elec-
trical force that pulls potassium, a
cation, from the extracellular fluid into
the protoplasm. At equilibrium, the
chemical and electrical forces balance,
and there is no net movement of ions,
or charge. Therefore, an uneven distri-
bution of ions can create a stable mem-
brane potential.
The membrane acts like a capacitor,a
component that separates electrical
charge. By knowing the difference in an
ion's concentration across a membrane,
itis possible to calculate the voltage dif-
ference, or potential, at which the chem-
ical and electrical forces will be bal-
anced for that ion. This potential is
called the equilibrium potential or the
Nernst potential, after Walter Nernst,
the German physical chemist who de-
rived it, and the equation follows:
In this equation, E is the Nernst poten-
tial, R is the universal gas constant (8.31
joules per mole per degree in Kelvin), T
is temperature in degrees Kelvin, z is
the ion's valence, F is the Faraday con-
stant (9.65 s 10Coulombs per mole), C,,
is the ion's concentration on the outside
of the membrane and C, is the ion's con-
centration on the inside of the mem-
brane. By assuming a temperature of 20
degrees Celsius or 293 degrees Kelvin,
which is approximately room tempera-
ture, the equation can be simplified to:
This equation gives E in millivolts. Con-
sidering the Nernst potential for sodi-
um across the plasma membrane, the
equation would be:
(For sodium, z = 1.) This means that
sodium would be in equilibrium across
the plasma membrane at a potential of
-985 millivolts.
Each ion has a Nernst potential. In
characean cells, the average Nernst po-
tentials for the major ions across the
plasma membrane are 59 millivolts for
calcium, 103 millivolts for chloride, -100
millivolts for sodium and -180 milli-
volts for potassium.
The resting membrane potential aris-
es from the combined equilibrium po-
tentials of all of the ions. You may have
noticed, however, that both the resting
potential of the characean plasma mem-
brane and the Nernst potential for
potassium are -180 millivolts. This is
not merely a coincidence. It has been
shown that in resting characean cells, as
well as in most resting animal nerves,
the membrane is largely impermeable
to calcium, chloride and sodium, but it
is readily permeable to potassium. This
means that the resting potential is large-
ly determined by the passive diffusion
of potassium. During an action poten-
tial, the membrane's permeability to
specific ions changes.
Ionic movement generates action po-
tentials in animal, plant and fungal
cells. In 1949 Alan Hodgkin and
Bernard Katz, both then at Cambridge
University, showed that external sodi-
um is necessary for an action potential
in a squid nerve. Through a series of ex-
periments, they developed the sodium
hypothesis, which states that the mas-
sive depolarization of an action poten-
tial results from sodium rushing into a
cell. It was later shown that tetrodotoxin
(the deadly poison found in the [apan-
ese puffer fish and removed before the
fish is eaten as sashimi) prevents an ac-
providing a secure trap. Then nearby
secretory cells exude enzymes, forming
a little stomach that digests the insect.
One of the best-known examples of
plant behavior comes from Mimosa ptu-
dica, often called the sensitive plant.
When the leaves of the plant are
touched, they bend over and appear
dead. The drooping arises from a me-
chanically driven action potential. More-
over, an action potential propagates
from the stimulated region throughout
the plant. This causes drooping in the
rest of the plant, a defense mechanism
apparently designed to make the whole
plant look unappealing.
Not all plant action potentials, how-
ever, cause obvious responses. In
Luffa-the plant whose gourd or fruit is
used for ''loofah'' sponges-action po-
tentials cause a transient inhibition of
growth. And in a variety of flowers,
pollen landing on the stigma generates
an action potential, which may be in-
volved in subsequent pollination or the
maturation process. In tomato seed-
lings, a mechanical wound induces
electrical activity that causes the accu-
mulation of proteins that limit further
damage to the plant.
Electrical phenomena control many
responses in plants. In a characean alga,
we understand many of the details of
the mechanism that leads from a duck's
nip on the plant to the cessation of pro-
toplasmic streaming. But we are just be-
ginning to address the similarities be-
tween the electrical excitability in
characean algae and higher plants, let
alone animals. In any case, it is apparent
that plants can perform long-distance
communication through electrical sig-
nals, such as the passing of information
from a mechanical stimulus from one
Mimosa stem to another. Many biolo-
gists continue to describe electrical ex-
citability as part of the animal world. In
the future, we should think of plants as
excitable too.
Thanks to Drs. Atsushi Furuno, Owen
Hamill, Roger Spanswick, Mark Staves,
Robert Turgeon and Scott Wayne for
their comments on this manuscript.
stant and one component, e.g. water vapour m ', can be determined diagnost-
ically using
for given values of m', mf, m'. We study the warm rain process with the
following transitions between water vapour, cloud and rain drops: activation
of cloud drops ACT, condensation on cloud drops CON, evaporation of cloud
drops E VC, autoconversion of cloud drops AUT, accretion ACC and evapo-
ration of raindrops E VR (cf. Fig. 1). The parameterization of each transition
as a function of mass fractions, temperature, pressure, and the two additional
parameters cloud droplet concentration at cloud base N,; and spectral width
CC, is based on Höller (1986). It is described in the appendix. Diffusional
growth of raindrops is ignored and cloud drops are assumed to be suspended
in the flow with zero terminal velocity. Both assumptions are explicit parts of
the physical distinction between cloud drops and raindrops, Note that satu-
ration is not assumed. The three ordinary differential equations describing
the system are
In the following, these equations are referred to as Cloud Microphysical Sys-
tem CMS. Transitions between water vapour and liquid water involve tem-
perature changes which are described by the latent heat of condensation l;, (T),
a given function of temperature (cf. Eq. 34). The transformation rates are
continuous functions of their respective variables within the range of physi-
D; are zero. The ratio of rain production to rain fall out is given by the coa-
lescence-Damkohler number D,. For large values of D;, D;; and D,-the fluxes
can be ignored and the system is approximately closed. Values near 1 describe
critical situations, while for small values the fluxes dominate the transforma-
tton terms.
To compare with the closed system, trajectories are shown in the same 3-D
space spanned by the coordinates (m ',T,mf) which now, however, is a sub-
space of the four-dimensional phase space of Eq. 19. Thus two trajectories
might go through one and the same point in the (m ', T.m:*)-subspace.
Similar to the closed system two groups of initial values with the same
equivalent temperature are studied. In the first set of experiments T. =0 is
assumed so that the compensating flow of dry air does not change the temper-
ature of the system; consequently the heat exchange term vanishes and D;; = oo.
Three cases have to be distinguished according to the magnitude of D,.
(il ]D] I; fall out eevaporation or condensation. For this case, closed and open
system solutions agree in so far as first a strong motion towards the saturation
curve is observed, approximately in the plane of a constant equivalent tem-
perature. Then slow and isothermal motion occurs in the saturated state (cf.
Fig. 5). The equivalent temperature plane, however, is left now with devia-
tions of the order of 0.01 K. For t-oo trajectories approach fixed points char-
acterized by:
Cloud and rain drops vanish completely with t--oo. All trajectories reaching
saturation are very close with slightly but significantly different temperature
values. The temperature difference, however, is much smaller than the initial
temperature difference. The final values m} and T, depend in a nontrivial
way on the initial values. Analytical relations were not obtained, except for
saturation:
but for undersaturated final states m} mf' holds, Compared with saturated
states, undersaturated ones show higher final temperatures. The fixed points
appear to be stable. Asymptotic stability may be possible for limited regions
of the phase space but a more detailed stability analysis was not performed.
(iil ]D;] = I; fall out= evaporation or condensation. If fall out of raindrops
approximately equals evaporation, trajectories first are heading the plane
m'=0 and then approaching the fixed point. Saturation may be reached for
m'0. Again, changes do not longer occur in the constant equivalent tem-
perature plane.
(iii) ]DA] e I; fall outs evaporation or condensation. If falling of raindrops
dominates evaporation, within a second or less all raindrops have left the sys-
tem and trajectories are found in the plane m '0. Again, if saturation could
be achieved, the fixed points are significantly different but very close, as com-
pared with the initial distance between the respective trajectories.
The motion in the saturation state needs further explanation. The main
cept of constant equivalent temperature, the saturation state and in the case
of an open system by three Damköhler-numbers. Cloud microphysical pa-
rameters do not affect the stability behaviour of the solutions. As the results
are governed by the basic characteristics of the parameterization scheme and
even do only slightly depend on the involved parameters, they might be trans-
ferred to similar parameterization schemes. Thus it is likely that the complex
structure of the presented scheme can be significantly simplified without loss
of accuracy. This, however, was not intended in this study. The inclusion of
ice processes will complicate the stability characteristics. Modelers observe a
strong sensitivity of the precipitation development on timing, location and
formulation of the ice initiation process (WMO, 1986). This paper, like the
one of Chaumerliac et al. (1987), demonstrates how the constraint of the
saturation assumption can be avoided and suggests a convenient numerical
method. Studies of this type are strongly recommended for any cloud micro-
physical scheme. Understanding of the underlying basic mathematics of cloud
microphysical transformations is helpful for understanding cloud develop-
ment and in interaction with the dynamics.
H6ller ( 1986) derived parameterization schemes for cloud microphysical transformations
for use in a mesoscale model. One of these schemes and which is based on a saturation assump-
tion has been used successfully in numerical cloud studies (Schumann et al., 1987). In this
paper the scheme is extended to account for non-saturated states. For discussion, the reader is
referred to Holler (1982, 1983, 1984). Chaumerliac et al. (1987) developed a similar parame-
terization scheme with explicitly calculated supersaturation. In contrast to this scheme, the in-
tegration method was modified in order to increase the time constant of the condensation resp.
evaporation process. This was necessary to avoid numerical problems when the cloud micro-
physical equations were solved together with the dynamical equations. Transformation rates
are given as functions of partial densities p', cloud droplet concentration at cloud base N,4, and
spectral width C,, If mass fractions m' are required, p' may simply be replaced by p'=pm' with
p determined by
Cloud drops are assumed to follow a non-normalized log-normal density distribution;
with
m - drop mass
N,,4 - total number concentration of cloud drops in m'
a; - variance of f,(In m)
Based on detailed spectral calculations Höller ( 1984) derived a parameterized approxima-
tion of the form
with
and (!4= 1 kgm s' is a reference conversion rate for the autoconversion rate. The collection
kernel of Berry and Reinhardt ( 1974) was uscd for the detailed calculations which are de-
scribed in H6ller (1982). The autoconversion formula is valid within the range (G....P..)
where pG., =gm? is the maximum value and where pG,, is determined as the minimum
value from either N,;(p')> 500 cmor
Below the minimum value autoconversion is a linear function in pf, whereas above the maxi-
mum value pa4, autoconversion is constant. The autoconversion vanishes for vanishing cloud
drops only: p%0.
Following Manton and Cotton (1977) raindrops grow at a rate ACC given by
where p,= 1.28 kg m? is a reference density value and a,=4.71 m' kg' 41. The accretion
vanishes if either cloud drops or rain drops vanish, that is for p*--0 or p's-0.
Rainfall in subtropical southern Africa is strongly seasonal, with a well-defined summer (December-March)
maximum over most of the subcontinental interior (Nicholson et al., 1988; Lindesay, 1993). Relatively small
areas along the eastern and southern coasts receive year-round rainfall, and the south-western tip of the
subcontinent has a winter (June-September) rainfall maximum. Most of the interior is semi-arid to arid, and
a marked rainfall gradient exists from the wetter east coast to the hyper-arid west coast (Tyson, 1986;
Lindesay, 1993). Important features affecting atmospheric moisture over the region are the high potential
evapotranspiration, exceeding 2000 mm year''' over the west-central interior in summer due to generally
clear skies and high insolation, and low levels of available surface moisture from the arid, sparsely vegetated
continental surface (Henning, 1989; Lindesay, 1993). Most of the moisture that contributes to precipitation
over southern Africa therefore must be imported over the subcontinent from source regions elsewhere.
Despite the importance of water vapour transport to rainfall over southern Africa, relatively few analyses
of the availability of atmospheric moisture have been undertaken for any part of the region. On a hemi-
spheric scale James and Anderson (1984) have shown that tropical-mid-latitude transport of water vapour
increases the growth rate and vigour of mid-latitude baroclinic systems. In the southern Africa region, mean
atmospheric water vapour content and vapour fluxes have been investigated over South Africa (McGee,
1971, 1972, 1975, 1986), as has the interannual variability in water vapour (McGee, 1978). The relationship
between precipitable water and rainfall has also received attention (Harrison, 1988). Whereas regional
studies of atmospheric moisture have been undertaken for North Africa (Flohn et al., 1965), West Africa
(Adedokun, 1978; Anyadike, 1979), South America (Rathor et al., 1989), North America (Benton and
Estoque, 1954; Hastenrath, 1966; Rasmusson, 1967) and Australasia (Hutchings, 1961), the content, intra-
and interannual variability, and transport of atmospheric moisture over the southern African region as
winds and dew-point temperatures at 850, 700, 600, 500, 400, and 300 hPa, have been obtained from Monthly
Climatic Data for the World (US Department of Commerce, various dates) for those stations in the southern
African region with the best data availability for the period (Figure 1). Stations with the shortest data series
are Douala, Kinshasa, and Grootfontein; the location of these stations, coupled with the lack of other
adequate radiosonde observations over tropical Africa, makes the interpretation of flow fields north of 1(0'S
difficult. Wind vectors have been resolved into zonal and meridional components, with westerly zonal
components positive and northerly (poleward) meridional components positive. Monthly means of outgoing
longwave radiation (OLR) on a 5' s 5' grid, from the NOAA AVHRR, were available for the 10-year period
1975-1984 and have been used to identify areas of convective activity in the mean and for anomalously
wet/dry months within this period. Although the circulation and rainfall data have been analysed over
a 20-year period, it is possible to make qualitative comparisons between the longer term meteorological
results and the shorter term OLR changes between wet and dry months.
Water vapour transports in the zonal and meridional planes are calculated from the equations
(after Hutchings, 1961), where g is gravitational acceleration, 4 is specific humidity, and u and v the zonal and
meridional wind components respectively. Specific humidity has been obtained using the method described
in McGee (1971) to derive saturation vapour pressure (E,) from dew-point temperature (T,):
and specific humidity from E,:
Although the relationship between j and T,; is non-linear the underestimation of j by this method is
sufficiently small, particularly in lower latitudes, to make the estimation acceptable (Gaffen et al., 1991). The
vertically integrated zonal (W,,) and meridional (W,,) vapour fluxes have been calculated by integrating
equations (1) and (2) between 850 hPa and 300 hPa using the trapezoidal rule:
where the total flux is u =iüj + u' (after Rathor et al., 1989)L
The vertically integrated vapour flux has been calculated from the equation
(after Hastenrath, 1966). The use of monthly mean data makes it impossible to calculate eddy and hence also
total vapour fluxes. The term 'vapour fluxes' as used here, therefore, refers to circulation or advective fluxes of
vapour. The vector mean wind V has been obtained from the equations (after Giles, 1963):
for magnitude, and
for direction, where V,, is the zonal component and V, the meridional component of the wind.
The total precipitable water in the atmosphere across southern Africa in October and January is illustrated in
Figure 2 to show the changes in the actual distribution of moisture between the wet and dry early and late
summer months. An increase in moisture content with the progress from early to late summer is evident, with
maximum wet-month precipitable water reaching only 24 mm in October (Figure 2(a)) and exceeding 30 mm
in January (Figure 2(c)). During dry months of January the air contains more moisture than in wet months of
October. In all cases maxima are found over the central subcontinent at 3 15''S, and in the dry months of
October and January there are additional maxima over Madagascar (Figure 2(b and d)). This coincides with
the eastward shift in the preferred locations of tropical-temperature troughs and cloud bands between wet
and dry Januaries (Harrison, 1986a, b). Minimum values of precipitable water (less than 10 mm) are found
over the south-western parts of the subcontinent, particularly, and as is expected, in October (Figure 2(a and
b)). Although these moisture fields allow speculation on some aspects of the circulation associated with
moisture changes over southern Africa, it is not possible to formulate a precise picture of the sources of
moisture over the subcontinent without analysing the fluxes of vapour over the region. The presentation and
analysis of vapour fluxes across southern Africa forms the focus of this paper, with a view to identifying
important source regions, circulation controls, and interseasonal and interannual variability as these
contribute to rainfall changes.
Analysis of vertically integrated water vapour fluxes for mean October conditions (Figure 3(a)) reveals
maximum fluxes of 1400 g cm ''4'' over the western Zaire Basin in tropical Africa, and secondary maxima
in excess of 1000 g cm ''4' over Madagascar to the east, and over central South Africa at about 30S,
Minimum moisture fluxes occur in the subtropics across Namibia, Botswana, and southern Zimbabwe at
approximately 20'S, Fluxes over Madagascar are north-easterly, but further north over the tropical Indian
Ocean, where October atmospheric moisture levels are relatively low (Hastenrath and Lamb, 1979,
southerly fluxes prevail. Across the east coast of the subcontinent north of about 20''S vapour fluxes are
easterly to south-easterly, becoming north-easterly further to the west. North-easterly fluxes extend over
northern Namibia, but south of 25'S all fluxes are westerly to west-north-westerly (Figure 3a)).
fluxes in wet Januaries. Vapour fluxes further north are largely unchanged from the wet-January pattern,
with north-easterly fluxes in both cases (Figure 5(b and c)).
Vapour fluxes on the zonal component of airflow are weakly westerly in the tropical regions in wet months
of January (Figure 6(a)); easterly fluxes extend over the subtropics, with a maximum across the subcontinent
at 20S, and there are westerlies further south over South Africa. Westerly fluxes also occur between the
Equator and about 15''S in dry Januaries (Figure 6(b)), but are stronger and more extensive than in the wet
months. Fluxes over the subtropics are easterly, with a maximum concentrated over Zimbabwe at 20'S.
Although maximum easterly fluxes over the subtropical regions are not as strong in wet as in dry Januaries,
the area in which fluxes exceed 600 g cm ''4'' is larger in the wet months, as is the overall area of easterly
fluxes. Comparison of zonal vapour fluxes for anomalously wet and dry months of January thus indicates
a northward displacement of the mid-latitude westerly and subtropical easterly fluxes between wet and dry
months (Figure 6(a and b)), and a simultaneous northward movement and strengthening of the tropical
westerly fluxes which extend across the subcontinent from the Atlantic to the Indian Ocean in dry Januaries
summer, when rainfall-producing systems are more baroclinic in structure, vapour fluxes on the zonal
westerly airflow are dominant. This is particularly true in wet months of October, when vapour flux
convergence occurs over the west-central subcontinent and divergence further east. An important influx of
vapour in these months comes from the north-west, from the southern tropical Atlantic Ocean, This flux is
absent in dry Octobers, when south-easterly fluxes dominate the vertically integrated vapour fluxes.
Convergence over the subcontinent is reduced in these months, and only continentally modified drier air is
advected southward over the summer rainfall region.
During January the pattern of vapour fluxes across southern Africa is quite different, reflecting the more
important role of the tropical circulation and tropical-subtropical interactions in late summer rainfall-
producing systems. Easterly zonal fluxes across subtropical southern Africa are stronger in January, but it is
the changes in the northerly meridional flux over the central subcontinent that are important for rainfall
variations. Wet months of January are characterized by a strong north-easterly flux over the eastern
subcontinent, advecting moist tropical Indian Ocean air into a convergence area along the ITC and ZAB.
There moist north-westerly fluxes from the tropical Atlantic Ocean provide further input to the strong
convective activity over central southern Africa, verification of which is provided by the negative OLR
anomalies in these months. Poleward fluxes of moisture continue southward from the convection area across
South Africa, raising moisture levels above those critical for effective rainfall. In dry Januaries the importance
of the tropical circulation declines; fluxes are less meridional, with stronger westerly fluxes across South
Africa, and south-easterly fluxes across the east coast indicating a strengthened anticyclone over the
south-west Indian Ocean.
Adjustments in the circulation and vapour fluxes over southern Africa between months and years are
clearly important in influencing rainfall over the subcontinent, with moist tropical air from both the Atlantic
and Indian oceans contributing to rainfall-producing systems. Changes in the areas of vapour flux conver-
gence and divergence are equally important, however, and the January situation with convergence and
convection in a disturbance over central southern Africa at about 20'S, coupled with poleward vapour fluxes
across South Africa, seems to be the most favourable for rainfall in the summer rainfall area. Although it has
been possible to identify some of the main features of the vapour fluxes and their interannual variability over
southern Africa, more detailed work is needed to confirm these findings for the data-sparse regions of central
southern Africa. Additional analysis using more spatially comprehensive data would also contribute to the
identification of vapour source regions and the changes that take place in those regions over time. Only with
improved understanding of factors such as vapour flux variations will more complete explanations of rainfall
variability in southern Africa become possible.
Helpful comments by Professor P. D. Tyson and an anonymous referee on previous versions of this paper are
gratefully acknowledged. Mrs W. Job and Mr P. Stickler drew the diagrams. The South African Weather
Bureau, Pretoria, are thanked for supplying the rainfall data, and Dr P. Aceituno for providing the the OLR
data. The research forms part of the Special Programme on Climatic Change: Analysis, Interpretation and
Modelling (SACCAIM), funded by the Foundation for Research Development.
The first recipe for synthetic nutrient medium for
Tetrahymena thermophila is more than 40 years old
(Kidder and Dewey 1951). This type of medium has
been used for gaining insight into nutrient requirements
(Hol 1973), metabolic pathways (Kidder 1967), uptake
mechanisms (Dunham and Kropp 1973) and reactions
between inorganic and organic molecules in the medium
(Hutner 1972). In spite of the fact that it was possible
to maintain doubling times and final cell densities
comparable to those obtained in the best complex
medium, the synthetic medium are deficient in at least
one respect: single cells in 1 ml cultures die shortly after
being transferred to such a medium (Christensen et al.
1992).
We have recently shown that TI thermophila grown
under nutritional stress situations need various com-
pounds not required at initial densities of more than
1,000 cells per ml. The stress conditions consist of
growth at low population density (Schousboe et al. 1992)
or growth at low ambient nutrient concentrations (Chris-
tensen et al. 1992). These results agree with the notion
that the cells produce and release compounds with
effects like those of growth factors (Ghiladi et al. 1992).
Here we present further detailed evidence for these
ViewS.
Cells: The following cell lines of Tetrahymena thermophila were
used: wildtype inbred strain B 1868-III (Orias and Bruns 1976); the
mutant strains MS-1, secreting lysosomal enzymes at low rates
(Hiünseler et al. 1987); II8G, defective in food vacuole formation
(Tiedtke et al. 1988); and SB 281, releasing no mucocysts (Orias et
al. 1983).
Nutrient medium: Cells were grown either in a complex
medium, PP}S, or in a standard synthetic nutrient medium, SSM.
PPYS is a solution of 0.75% proteose peptone (Difco Laboratories,
Detroit, Michigan, USA) enriched with 0.75% yeast extract (Difco)
and-salts, SSM consists of 19 amino acids, 4 nucleosides, glucose, 7
vitamins, salts, and citrate (Szablewski et al. 1991). In both cloning
and mass culture experiments SSM was diluted in the ratio of 1:1 with
TRIS/HC buffer (pH 7.5). When testing the presence of stimulatory
compounds in extracellular fluid, we replaced the buffer with ex-
tracellular fluid, keeping the concentrations of the nutrients in the
medium constant. Cells requiring special conditions for growth and
multiplication were grown in media modified to fulfill these condi-
tions: mutant strain II8G was grown in medium enriched with iron
and copper salts and folic acid as prescribed for this mutant (Tiedtke
et al. 1988). Chemicals were from Sigma Chemical Co., St Louis,
Missouri.
Cultures: Stock cultures were grown in 2 ml portions of standard
synthetic medium, SSM. Experimental cells were grown in 10 ml
portions of SSM in conical flasks for 20 h, transferred to 10 mM
TRIS/HC1 buffer (pH 7.5), centrifuged for 3 min at 800xg and
resuspended in the buffer. This procedure was repeated three times
and resulted in more than a l(0'-fold dilution of the extracellular fluid.
The cells were then divided into three batches and used as follows: i)
for preparation of extracellular fluid, ii) for multiplication analysis in
10 ml portions of SSM where effects of initial cell density and
presence of hemin, phospholipids, proteins or extracellular fluid were
studied, and iii) for cloning analysis in various volumes of either
PPYS or SSM or SSM enriched with cither hcmin, phospholipids or
proteins. The cells were grown at 37%C.
Preparation of cell-free extracellular fluid: Cells were starved
in 10 mlTRIS/HCI buffer (pH 7.5) in conical flasks for 0-5 min and
5 h at a density of 50,000 cells per ml. After that,a sample of a culture
was transferred to a centrifuge tube and placed on top of 2 ml of a
solution of 10% Ficoll and precipitated for 10 min at 3000xg. The
Ficoll solution was sterilized by filtration and used at 4'C. The
cell-free supernatant, containing no cells, was removed and used in
the experiments. The Ficoll has a high specific weight and high
viscosity and prevents the cells from swimming back up. To certify
that no cells were left in the extracellular fluid a sample was placed
in a small Petri dish and checked under a stereo microscope.
Cloning procedures: After centrifugation and resuspension in
the buffer single cells were transferred to two different sizes of
cultures, 1 ml or 10'' ml (lul). One ml cultures: single cells were
transferred to either PP)S, SSM or SSM enriched with either hemin,
phospholipids or proteins with a fine pipette (see Schousboe et al.
1992). One pl cultures: single cells were suspended in either SSM or
PPYS in test tubes at a density of 1,000 cells per ml. By means of a
10 ul Hamilton pipette droplets of lul from the test tube were placed
on the top of paraffin oil placed in a glass Petridish. The droplets sank
to the bottom of the dish and the paraffin oil prevented evaporation
of the medium. The dish had a diameter of 7 cm, the paraffin oil was
autoclaved at 121%C for 45 min and reached a height of 2 mm in it.
Only droplets containing a single initial cell were kept under observa-
tion.
Cell densities in test tubes and droplets were recorded every day.
In most cases many cells (1,000) were present already after the first
24h. In all cases droplets and tubes were kept under observation until
day 4.
Compounds added to SSM: Hemin and asolectin, a crude
preparation of phospholipids, were used at final concentrations of
7.5 uM and 50 ug per ml, respectively (see Schousboe et al. 1992 and
Christensen et al. 1992). Bovine serum albumin,egg albumin, trypsin
and soy bean trypsin inhibitor were used as sources of proteins. They
were dissolved in redistilled water, sterilized by filtration and used at
final concentrations of 50 yug per ml.
Cell counting: Culture samples were enumerated in an electronic
particle counter (see Christensen et al, 1992).
All experiments were repeated more than 10 times with similar
results.
Initial cell density is an important parameter in cell
multiplication of Tetrahymena in synthetic medium.
Figure 1 shows the number of cell doubling of T her-
mophila, wildtype, as a function of time and number of
cells transferred into SSM. In cultures having 750 or
more initial cells per ml the cells thrive, whereas 250
and 500 cells per ml die within 20 h. One thousand or
more cells per ml have a short lag phase (about 1 h);
750 cells per ml have a lag phase lasting about 10 h.
Extracellular fluid is also an important parameter for
cell proliferation in synthetic medium. Figure 2 shows
the number of cell doubling of T thermophila, wildtype,
as a function of time and concentration of added
extracellular medium where cells were starved for 5 h.
In all cases the initial cell densities were 250 cells per
ml. Without any addition of extracellular fluid the cells
do not multiply. If extracellular fluid is added in the ratio
of 1 portion of SSM to 0.02 portion of extracellular fluid
(to achieve 2% of extracellular fluid in the medium) the
cells multiply with a lag phase of about 15 h. If thc
extracellular fluid amounts to 10% of the volume, the
cells have a lag phase lasting about 10 h before multi-
plying, and where extracellular fluid is 50% of the
media's volume the cells have a short lag phase, repeat-
ing the growth curve representing 2,500 cells per ml.
Hemin or phospholipids or proteins also initiate cell
proliferation. Figure 3 shows the effects of these com-
pounds on cell multiplication in the assay system used
for testing effects of extracellular fluid. Cells in SSM
enriched with these compounds multiply after a short
lag phase (0-1 h), repeating the growth curve repre-
senting 2,500 cells per ml.
It should be noted, that if the cells survive and start
multiplication then they multiply at the same doubling
times (about 2 h) and reach the same final cell densities
irrespective of the duration of the lag phase.
Results of cloning experiments in various volumes of
PPYS, SSM and SSM enriched with either hemin, phos-
pholipids or proteins, are listed in Table 1. TI ther-
mophila, wildtype, in 1 ml portions do not form clones
in SSM unless it is supplemented with hemin, phos-
pholipids or the proteins bovine serum albumin, 4gg
albumin, soy bean trypsin inhibitor or trypsin or grown
in PPYS. In 1 pl portions of SSM, however, the cells
have a high probability to form clones. Thus the cloning
efficiencies of T. thermophila in SSM are culture volume
dependent. The presence of paraffin oil in test tubes with
one ml of either SSM or PP)S does not affect the cloning
efficiencies (data not shown).
Many workers have shown that a wide variety of
compounds are released from Tetrahymena. These in-
clude hydrolytic enzymes (Tiedtke et al. 1992), mating
pheromones (Adair et al. 1978), transcription inhibiting
factors (Andersen et al. 1980), mucocysts and accom-
panying compounds (Maihle and Satir 1986) and
proteins without ascribed functions (Suhr-Jessen 1987).
Our results suggest that Tetrahymena also release un-
known compounds with effects on cell proliferation.
Also known groups of compounds (tetrapyrroles, phos-
pholipids and proteins) have similar effects on cell
proliferation (Schousboe et al. 1992 and Christensen et
al. 1992). The cell-produced factors appear to be dif-
ferent from those added by us in that they are active at
lower concentrations.
Our experiments show that cell-free media samples
of a TRIS/HC buffer, in which cells have been starved,
improve cell proliferation and reduce lag phases in a
concentration dependent fashion, Figure 2. As controls,
we have also made experiments with cell-free media
samples, where cells have been exposed to starvation
conditions for a period of 0 to 5 min. In these cases we
did not see any stimulatory effect on cell multiplication.
Furthermore, we have shown that single cells in 1 ul of
standard synthetic medium (corresponding to an initial
cell density of 1,000 cells per ml) form clones with high
probability. These observations indicate that com-
ponents from dead cells - or physical contacts between
the cells - are not responsible for the stimulation.
Therefore, it seems that a small volume of growth
medium allows the single cell to build up sufficient
concentrations of one or more molecules stimulating its
own growth and multiplication. In larger volumes these
molecules may become too diluted to be effective.
The ideas presented here may explain why initial cell
densities are decisive for the fate of TI thermophila in
standard synthetic medium. The amounts of growth
stimulating factors released at low initial cell densities
may be insufficient for stimulation of the cells. However,
at higher cell densities the amounts of these molecules
may be sufficient to support cell proliferation. This idea
may also explain why cells exposed to starvation con-
ditions for 4 h can multiply when inoculated at 250 cells
per ml in synthetic medium as previously reported
(Christensen et al. 1992): the inoculation volume needed
to transfer cells to the growth medium may contain
sufficient amounts of growth stimulating compounds
which had been released during the period of starvation.
We have previously shown that certain porphyrins,
including hemin, as well as phospholipids, improve
proliferation under nutritional stress and conditions of
expected low cloning efficiencies in standard synthetic
medium (Schousboe et al. 1992 and Christensen et al
1992). Here we show that these compounds, in addition
to certain other proteins, stimulate cell growth and
multiplication in mass culture experiments where the
cells are inoculated immediately after the extracellular
medium had been diluted more than 10'-fold by
centrifugation and resuspension in a TRIS/HC1 buffer.
These data, combined with results obtained with ex-
tracellular fluid and ''micro-cloning'', seem to indicate:
i) that Tetrahymena thermophila produce and release
growth stimulating compounds into the growth medium;
ii) that the compounds are responsible for cell prolifera-
tion in standard synthetic nutrient medium; iii) at low
initial cell densities we can see that exogenous com-
pounds as hemin, phospholipids and certain proteins can
substitute for cell-produced stimulatory compounds
which may be present, but at ineffective concentrations;
iv) stimulation is expressed neither in changes of dou-
bling times, nor in attainment of higher final cell
densities, but in the rate with which the cells leave the
lag phase and start growth and multiplication. We choose
to name these stimulatory compounds growth factors,
and point out that they appear to represent a group of
compounds different from the well-known nutritionally
required components.
PPYS supports cell proliferation at low initial cell
densities. This may mean that the complex medium
contains growth factors or growth factor-substituting
molecules, e.g. proteins. In SSM these compounds are
missing and the cells are forced to adapt or ''condition''
the medium by releasing their own growth factors. Thus,
survival, growth and multiplication are dependent on
cell-produced growth factors in synthetic medium.
There are several possible mechanisms behind
release of compounds from Tetrahymena. Molecules
leave the cells either via lysosomes (Tiedtke et al. 1992),
mucocysts (Maihle and Satir 1986), or egestion of food
vacuoles to mention a few. To investigate possible
connections between some of these mechanisms and the
release of growth factors, we have made experiments
with the mutantcell strains II8G (lacking food vacuoles),
MS-1 (releasing lysosomal enzymes at low rates) and
SB 281 (releasing no mucocysts). The observed patterns
of growth and multiplication in both cloning and mass
culture experiments were similar to those presented in
Figures 1, 2 and 3 and Table 1. This seems to exclude
the possibility that growth factors are released together
with lysosomal enzymes and compounds from
mucocysts, Furthermore, food vacuoles appear to play
no part in either utilization or release of the growth
factors.
Tanabe et al. (1990) have reported on a growth factor
from a mutant Paramecium tetraurelia, When this factor
is added to the mutant, it will recover the multiplication
rates characteristic of the wildtype. In our case the
growth factor is so far unknown, but it is apparently
secreted by wildtype cells. Cells in synthetic medium
seem to require the presence of a growth factor.
Previously, Kidder and Dewey (1951) reported that
it was necessary to inoculate Tetrahymena cultures in
synthetic medium with not less than 1% of the new
culture volume. This concurs with the idea that the cells
somehow change their medium making it fit for cell
survival and multiplication. Lilly (1967) and Lilly and
Stillwell (1965) have reported on the effects of growth
factors on multiplication of T. pyriformis in conditioned
medium. Their results indicate that such compounds
may be present, but the reported difference between
control and experimental cultures was small. In view of
our results it is obvious, that their initial cell densities
(20.000 cells per ml) were too high to reveal any
significant effect.
The experiments presented here indicate that a variety
of compounds, including hemin, phospholipids etc., can
initiate cell multiplication in synthetic medium. These
compounds may act as signals for cell multiplication.
They may do so by acting on the cell surface, in the
cytoplasm or inside the cell nucleus. Furthermore, the
cells themselves appear to release these or other factors,
by what looks like an autocrin process, to stimulate their
own cell division.
Acknowledgements, We thank the NOVO Foundation, Copen-
hagen, Denmark, for support; Peter Schousboe for stimulating dis-
cussions; Helle Eis and Esther J. Laursen for the results obtained with
the strains II8G, SB 281 and MS-1; and Lene Jergensen and Ruth
Lenstrup for technical assistance. Flemming Bryde Hansen advised
us in establishing micro-cloning procedures.
case of those who oppose the prac-
tice and creates conflicts among
potential allies. Are the long-term
interests of the scientific commu-
nity best served by AAU, for ex-
ample, taking the side of the ex-
ecutive branch versus that of
Congress, the side of authorizers
versus that of appropriators, and
condoning earmarkers on the agri-
culture appropriations subcom-
mittee? Only the clearest and most
inclusive definition of earmarking
truly supports the goal of achiev-
ing the highest-caliber science, not
only within the academy but
throughout the federal govern-
ment. This definition should be
adopted by organizations such as
the National Academy of Sci-
ences, the American Association
for the Advancement of Science,
and AAU to help them put a lid
on the academic pork barrel.
If you're out of town and need
cash, an automatic teller machine
can instantly determine if you have
money in your account and let you
withdraw what you need. If you
apply for a loan, the lender can
quickly get access to your credit
history to help determine your re-
liability. If the Internal Revenue Ser-
vice is reviewing your tax return
and needs to know how much was
withheld from your salary during
the year, the information is readily
available. But if you need emer-
gency medical care and the physi-
cian needs information on your
medical history, it is highly unlikely
that it can be found and made avail-
able promptly. In fact, much of the
information necessary to understand
the workings of the U.S. health care
system is not to be found no mat-
ter how long one is willing to wait.
And without better access to medi-
cal data, the health care system will
be hampered in its efforts to pro-
vide better care to individuals and to
enhance the overall effectiveness
of the entire system.
The information-management
challenge experienced by health
care professionals and institutions
is growing daily. At least three fac-
tors contribute to this growth. First,
health care practitioners must mas-
ter and track an ever-increasing
base of medical knowledge. MED-
LINE, the computer data base of
biomedical literature, grows by ap-
proximately 360.000 new articles
per year. Second, patient records
include more data as patients live
longer, experience more chronic
disease, undergo a greater variety
of tests, and have more encounters
with health care providers. Third,
the demand for patient data is in-
creasing. In addition to supporting
the diagnostic and therapeutic
work of clinicians, patient data are
used to document patient risk fac-
tors, expectations, and satisfaction
with treatment; to perform quality
assurance, risk management, cost
monitoring, and utilization review;
to identify emerging public health
problems; to track adverse reactions
to pharmaceuticals; to document
services provided for billing and
legal purposes; and to assess the
effectiveness of new technologies
and procedures.
The users of patient data in-
clude not only physicians, nurses,
and other health care practitioners
but also virtually everyone associ-
ated with the health care delivery
system. Patients themselves are in-
creasingly likely to be interested
in their records as they become
more informed consumers of
health care services. Administra-
tors of health care institutions seek
data to manage the quality and
costs of services provided as well
as to project staff, budget, and fa-
cility needs and identify opportu-
nities for new programs. Insurance
companies, other third party pay-
ers, and employers who pay for
health benefits seek patient data to
monitor the frequency, cost, and
quality of health care services pro-
vided to their subscribers or em-
ployees. Health services researchers
seek access to aggregated patient
data to study patient outcomes,
variations in practice patterns, or
appropriateness of alternative treat-
ments for a particular condition.
Policymakers seek data to monitor
the performance of health care in-
stitutions, to evaluate coverage de-
cisions for federal and state insur-
ance programs, and to evaluate the
availability of health resources to
meet current and future needs.
Patient records are a linchpin
of information management in
health care, but traditional medi-
cal records have not kept pace with
the changes in health care and can-
not satisfy many of the new de-
mands placed on them. Despite the
broad diffusion of computer tech-
nology, most patient records today
exist only on paper and are often
inaccessible, inaccurate, incom-
plete, illegible, disorganized, not
secure, and not integrated into the
various settings of care. Comput-
erizing current paper records
would help, but it would not meet
all current and future user needs.
Existing patient files do not have
a standard form, do not integrate
data from multiple care settings,
and do not include all the types of
data needed to enhance patient care
and better manage the system.
Given the broad array of users and
uses of patient records and the new
technologies available to support
them, a new concept of the patient
record is needed.
In 1989, the Institute of Medicine
(IOM) convened a Committee on
Improving the Patient Record that
articulated a vision for ''an elec-
tronic patient record specifically de-
signed to support users by providing
access to complete and accurate
data, alerts, reminders, clinical de-
cision support systems, and other
aids.'' As a first step, the commit-
tee described what an ideal com-
puter-based patient record (CPR)
system would entail: ease of opera-
tion, convenient locations of work-
stations in the patient care setting,
24-hour availability, rapid response
time, and simultaneous use of a
given record by multiple users.
Security is a critical require-
ment of CPR systems and depends
on technology and user behavior.
Systems must track when users log
on and off the system, lock out at-
tempted log-ons after failed at-
tempts, require users to update
their passwords on a regular basis,
and be able to generate secondary
records that exclude patient iden-
tifiers and contain only those data
needed by nonclinical data users.
The ability to connect the com-
puter systems within and beyond
an institution is another essential
component of CPR systems. For
example, physicians would be able
to request laboratory tests, order
prescriptions, refer patients for con-
sultation, or admit patients to the
local hospital from the CPR work-
stations in their offices. Informa-
tion would also flow into the CPR
system from other sources. Labo-
ratory test results, consultation
notes, and discharge summaries
would be sent electronically to the
physician's office and filed auto-
matically in the patient's record.
Similarly, bills could be generated
automatically at the end of each pa-
tient visit and sent electronically
each day to third-party payers. Rel-
evant data could be automatically
reported to the Centers for Disease
Control, the Food and Drug Ad-
ministration, or tumor registries,
rather than requiring practitioners
to complete forms manually.
CPRs should offer users assis-
tance with routine tasks, thereby in-
creasing the time physicians and
other health professionals can spend
with patients. For example, users
would be able to generate with the
stroke of a key routine forms such
as school or insurance examinations
and patient instructions for a range
of illnesses or treatments. Perhaps
the most significant feature of the
CPR environment would be the
availability of clinical decision sup-
ports, Repeated laboratory test re-
sults could easily be transformed
into a graph, thus facilitating recog-
nition of a pattern. Decision algo-
rithms and clinical practice guide-
lines would be available to assist
in diagnostic and treatment deci-
sions. Access to current medical
knowledge would be facilitated by
linkage with MEDLINE and other
literature and bibliographic data
bases. On-line, clinical reminders
would support preventive medicine
by informing practitioners or pa-
tients of needed vaccinations or
tests. Clinical alerts, identified by
subroutines embedded in the com-
puter's program, would prompt
practitioners if a patient's lab results
revealed a dangerous trend or if in-
compatible drugs were prescribed.
In addition to improving the
quality of care by providing bet-
ter information to physicians,
CPRs should also contribute to the
moderation of health care costs in
several ways. Direct entry of lab-
oratory test results should reduce
the frequency of redundant test-
ing that occurs when previous test
results cannot be found. Produc-
tivity is likely to be enhanced as
time need not be spent tracking
down missing records or missing
data or waiting for records that are
in use elsewhere. Since data need
be recorded only once in the com-
puter record, redundant data entry
can be eliminated.
Finally, CPR systems will sup-
port the advancement of medical
knowledge by making improved
patient care data available for clin-
ical and health services research.
Data that are maintained in CPR
systems are likely to be more easily
and less expensively collected and
aggregated since data will no longer
need to be manually abstracted
from records and entered into re-
search data bases. And CPRs offer a
means of bringing research results
directly to practitioners.
Although health care lags behind
other industries in applying com-
puter technology for data storage
and retrieval, some activity in this
arena has begun. Automated pa-
tient records can be found in vari-
ous stages of development in some
health maintenance organizations,
outpatient clinics, hospitals, and
multihospital systems. In addition,
some physicians are using clinical
decision support systems that pro-
vide guidance in areas such as gen-
eral medical diagnosis, drug ther-
apy decisions, and the management
of chemotherapy for patients par-
ticipating in formal clinical trials.
But nothing currently in use pos-
sesses the scope and scale of the
envisioned CPR, How can we
move from the present inconsis-
tent and frequently archaic infor-
mation-management practices and
technology toward widespread and
compatible CPR systems?
Developing a comprehensive
CPR system represents a signifi-
cant, but not insurmountable, tech-
nological challenge. Progress is
needed in four major areas: Facile
user interfaces must be developed
so that practitioners will not find
it cumbersome to use CPRs; sys-
tem security technology and pro-
tocols must be enhanced to protect
the accuracy and confidentiality of
patient data; local, regional, and
national networking capabilities
must be built so that linkages
among CPR systems can be set
and data standards must be estab-
lished so that data can be shared
between CPR systems and used for
various purposes.
Equally important, though per-
haps more difficult to overcome,
are the nontechnological impedi-
ments to CPR development: the
lack of a clearly articulated and
widely agreed-upon definition of
what a CPR is and what the per-
formance expectations of its users
are for vendors; high research and
development costs and an uncer-
tain market; an inadequate num-
ber of experts trained in medical
informatics; the public's concern
about protecting confidentiality of
patient data; the issue of patient
data ownership; and ambiguity in
and inconsistencies among state
laws related to patient records.
Organized or overt resistance
to CPRs is unlikely, but subtle re-
sistance is likely on several fronts.
Individuals who believe that their
jobs are threatened by the change
and health care workers who are
reluctant to learn new skills may
be unwilling participants. Among
those who stand to benefit from
CPR implementation, competing
and sometimes conflicting inter-
ests must be addressed. Vendors
who must play a key role in the
success of CPR development must
strike a balance among cooperat-
ing to facilitate development,
avoiding antitrust violations, and
pursuing profits. Finally, individ-
ual institutions may be hesitant to
invest in a CPR system due to high
costs and as yet unquantified ben-
efits. Overcoming these barriers
will require coordination among
the many organizations and indi-
viduals interested in CPRs and a
decisionmaking process that will
be accepted throughout the health
care system. For this reason, the
IOM patient record committee's
major recommendation was the es-
tablishment of a Computer-based
Patient Record Institute (CPRI) to
promote and facilitate develop-
ment, implementation, and dis-
semination of the CPR,
In the spring of 1991, the
American Health Information
Management Association (for-
merly the American Medical
Record Association), American
Hospital Association, American
Medical Association, American
Nursing Association, and U.S.
Chamber of Commerce formed a
coalition for establishment of the
CPRIL The CPRI was incorporated
in January 1992 and held its first
annual meeting in July 1992. The
CPRI currently has 22 organiza-
tional members representing the
health care professions, insurers,
payers (for example, employers),
information systems and service
vendors, and government, as well
as a data base of interested groups
that includes over 7(00 organiza-
tions. Aware of the major barriers
to implementation, the CPRI has
established four workgroups: CPR
demonstration projects; confiden-
tiality, privacy, and legislation;
codes and structure; and education.
The CPRI is not alone in its
efforts to advance CPRs. The fed-
eral government is demonstrating
increasing commitment to im-
proving information management
in health care. In early 1991, the
General Accounting Office (GAO)
issued a report on the benefits of
automating medical report systems.
Since the GAO and IOM reports
were released, at least two bills in-
troduced in Congress have explic-
itly addressed the automation of
patient data systems by requiring
hospitals participating in Medicare
to be able to submit their claims
electronically and by authorizing
funds to develop model systems
''to facilitate gathering of health
care cost, quality, and outcome
data,'' Several federal agencies--
particularly the Agency for Health
Care Policy Research, the Health
Care Financing Administration, the
National Library of Medicine, and
the Department of Veterans Af-
fairs-were actively involved in
the IOM patient record study and
continue to support improved in-
formation management and CPR
development through involvement
of their staff in CPRI workgroups.
by funding research related to or
directly associated with CPRs, and
by disseminating information about
the value of and ways to accom-
plish information management in
health care settings.
In addition, in November 1991,
Secretary of Health and Human
Services (HHS) Louis Sullivan con-
vened national health care leaders
to discuss the challenges of reduc-
ing administrative costs in the U.S.
health care system. At the forum,
three health care industry-led work-
groups were created--the Work-
group for Electronic Data Inter-
change (WEDI), the Task Force on
Patient Information, and the Work-
group on Administrative Costs and
Benefits. In its July 1992 report,
WEDI presented a vision and rec-
ommendations that are consistent
with the efforts of CPRI. WEDI
will continue in existence as a col-
laborative effort among health care
industry participants and will report
to the secretary of HHS each year
on industry progress. The other two
workgroups are still conducting
their deliberations. The potentially
complementary efforts of these
three workgroups and of the CPRI
must be coordinated by their re-
spective leadership to avoid re-
dundancy and possible conflict.
Despite the extensive attention
that CPRs have been receiving, we
will not have them in place and
ready for use fast enough. Health
care could benefit from the use of
CPRs today, and certainly any re-
formed health care system will rely
heavily on the information-man-
agement capabilities that CPRs
offer.
President Clinton's new tech-
nology initiative includes increased
investment in high-performance
computing and networking appli-
cations to improve the provision
of health care by ''furnishing health
care providers and their patients
with better, more accurate, and
more timely information,'' This
initiative may provide a signifi-
cant boost to CPR development
efforts by acknowledging the im-
portance of building an informa-
tion-management infrastructure to
support health care and by pro-
viding the level of funding that is
needed to support large-scale CPR
demonstration projects.
But just as technology alone
cannot overcome the challenges
involved in improving patient
records, money alone will not get
us to the CPR. The many federal
agencies and private-sector orga-
nizations that are involved in CPR
issues-particularly open discus-
sions regarding standards-must
coordinate their efforts. In the short
term, the newly appointed presi-
dent of the CPRI may be well ad-
vised to convene a ''CPR summit.''
In the long term, if CPRI is to ful-
fill this coordinating role, it must
increase its visibility and credibil-
ity by strengthening its financial
base and producing tangible results
that move us toward the ultimate
goal of CPRs.
BEGIN THIS PAPER with an immedi-
ateconfession of bias.Ithink entomol-
ogy is a superior discipline-from a
child's point of view. Entomology offers the
chance to run about outside, splash around
in creeks, holler, shriek, put things in jars,
and see wonderous adaptations and bizarre
creatures. In short,it'sjust plain fun-ifone
isnot fearful of insects, if one is not discour-
aged from the interest by socialized gender
roles, or if one's interaction with insects is
not limited to inner-city roaches.
A complex interaction of gender,
ethnicity,race,and socioeconomicclass dis-
advantages historically has prevented full
participation of some groups in
entomology-specifically women and
minority groups: African-Americans,
Hispanics, Native-Americans, Asian-
Americans, and others, Of the 7,506
Entomological Society of America (ESA)
members in 1990, only 3% were not white,
and 6%werewomen (ESA National Office).
While significant improvements have been
made, much more must be done to achieve
full integration of women and minorities
into entomology in the coming decades.
The problem of recruiting students from
these underrepresented groups is one
component of the larger task of educating
the American public about entomology.
Nearly 95% of the American population is
classified as scientifically illiterate
(Goodstein 1990). An informed electorate
is essential in determining how best to
respond to challenging issues such as
biotechnology,sustainable agriculture, and
environmental degradation. In short, the
population mustmove beyondthe mentality
of 'the only good bug is a dead bug.'
In the past three years I have taught in
a university outreach program in elemen-
tary schools and in a 'gifted' student
program. My time with these children has
demonstrated clearly that young students
view science as a boring field that is limited
to 'geniuses,' Additionally,I observed that
the willingness of children to handle insects
varies greatly by age, gender, and race.
These observations inspired me to investi-
gate what could be done to increase the
numbers of students from underrepresented
groups entering (and understanding) ento-
mology.
I therefore conducted a survey to iden-
tify how current graduate students became
interested in entomology, with the purpose
offinding commonalities that could be used
to design outreach programs to interest
these underrepresented groups in entomol-
ogy. A previous survey (Wrensch 1986) of
seventy professional entomologists found
that teachers were the greatest influence on
choice of entomology as a career.I wished
to quantify what these teachers or other
individuals were doing that successfully
persuaded students to become entomolo-
gists, Additional questions on the graduate
experience were included;responses tothese
questions are reported in Pearson (1992)
(see pp. 103-114).
The questionnaire was prepared, tested,
andmailed under thesame procedures listed
in Pearson (1992). In total, 470 surveys
were returned for a 44.5% response rate.
The demographic profile of the respondent
population is shown in Pearson (1992).
Several cautions are made about the inter-
pretation of data from these surveys in
Pearson (1992); these warnings also apply
to the data reported here.
Responsestoclosed questionswere coded
and examined with F-tests and chi-square
analyses (SAS Institute 1985). Occasional
fragments of students' responses will be
quoted, but this is done only when the
response is representative of a number of
responses and cannot be identified to any
individual student.
Students were asked to name the age at
which they became interested in science and
entomology (Table 1). White, or majority,
Americans appear to develop an interest in
science an average of two to four years
earlier than international and minority stu-
dents, respectively. Significant differences
existed between genders in the average age
of choosing entomology as a career. While
men chose entomology at age twenty-one,
women did not untilagetwenty-three (Table
1). American minority students also ap-
peared to choose entomology later in their
careers at age twenty-three. The ranges of
agesnamed for these groups also show large
differences.
The next question asked students to
describe their childhood attitudes toward
insects(Table2).Significant differenceswere
seen between men and women (= 18.7, df
= 4, P 0.005), majority and minority
Americans ('15.7, df4,P0.01),and
Americans and international students ('=
17.8, df4,P+0.007).
Respondents were asked who was re-
sponsible for their interest in science and
entomology. High school and junior high
school teachers and parents were named as
most influential on students' developing
interest in science (Table 3). Some other
influencesmentioned were county extension
agents, Marlin Perkins, Mr, Wizard, and
Mother Nature.
University professors were cited by 60%
of respondents as influential in making the
decision to become an entomologist (Table
3). These results are quite similar to
Wrensch's(1986), which showed thatteach-
ers influenced 50% ofher sample to become
entomologists, Other influences mentioned
were beekeepers, a National Science Foun-
dation (NSF) Female Mentor Program,and
[. H. Comstock (presumably not in person).
Toquantifyhow these teachers and other
individuals influenced students toward en-
tomology careers, survey takers were given
a list of choices of possible actions of the
person (Table 4). 'Enthusiasm and excite-
ment ofthe individualfor entomology'' was
ranked first by 35% of the respondents.
While neither rankings or first choice sig-
nificantly differed by gender, race, or na-
tionality, the comments of the respondents
were quiterevealing. Ofthose studentscom-
menting on this question, the majority said
some person had taken a personal interest,
orhadencouraged them in some way. Eighty
percent of the studentsmakingthis observa-
tion were female. Some representative com-
ments were:
Inever seriously thought about a career in
science-this individual broke through the
mental block I had about math, etc. The
realization Iwas capable changed my outlook
and gave me more self-confidence.
[He] was very encouraging and demand-
ing. This feeling of acceptance was lacking in
other scientific areas and prompted me to
change majors.
This person felt I had the ability to be
successful at the graduate level and encour-
aged me to pursue these goals. [He] gave me
a great deal of positive feedback for the work
I did, enhancing my self-esteem.
Several studieshave reported thatwomen
in nontraditional fields may suffer from a
lack of self-confidence about their abilities
(Ehrhart &c Sandler 1987, Berg & Ferber
1983). For the students in this survey, an
advisor's, employer's, or teacher's show of
beliefin their abilities apparently motivated
them to set and achieve higher goals.
Students were asked to rank a list of
elements of entomology that they found
most appealing (Table 4). Their responses
may perhaps indicate the best 'selling
points' to emphasize when dealing with
nonentomologists. 'Wonderous diversity
of insect life' was the definitive first choice
(46%). Of twenty-six comments on this
question, nearly all were some sort of dis-
paragiing comment written in next to the
choice of 'Job opportunities and pay.'
Perhaps the best comment was 'What?
Where??3))*
The survey results illustrate that some
differences do exist between the paths to
entomology traveled by majority, minority,
male, and female students. A glance at
sociological and educationalliterature shows
that this is not surprising, Harty &c Beall
(1984) and Simpson & Oliver (1985) both
reported more positive attitudes toward
science in boys than girls. Young girls and
minorities also have less exposure to science
extracurricular activities and thus less
opportunityto develop an interest in science
(Anderson 1989;Kahleet al.1985, Jones &
Wheatley 1988).
Perhaps the most dramatic report on the
way in which children view science and, by
association, entomology, is described in
Chambers (1983). He used the 'Draw-A-
Scientist-Test'' to chart the development of
children's science imagery. Of 4,708
drawings made by children, only twenty-
eight women scientists were drawn, even
though halfofChambers's research subjects
were female. Seven children drew images of
scientists as naturalists; the rest portrayed
scientists indoors in laboratories. Children
from low-income families were slow to
develop an image of science; in fact, while
upper-andmiddle-classchildren had a well-
developed image of scientists in the first and
second grades,low-income children did not
until the fifth grade, and even then their
drawings were less detailed. Laboratory
equipment frequently dwarfed scientists in
these drawings, 4ggesting feelings of
overpowerment and disconnectednessfrom
the scientific process, While Chambers did
not analyze his data by race, it is an
unfortunate fact that minorities make up a
large portion of low-income households in
the United States. Studies on older students
(Anderson 1989) found arritudes of
alienation from science common among
African-American students.
Discussion of youth programshas begun
in the ESA, and programs for youth should
be a priority in the effort to involve under-
represented groups in entomology. Girls
are so strongly socialized to be pretty in-
stead of smart, and there is such a lack of
positive scientificcareer images for African-
Americans and other minorities, that the
majority of these future entomologists are
probably lost by the fourth grade.
The survey revealed that women and
people of color are more likely than other
groups to say they were frightened or re-
pulsed by insects as children. It is important
to reach these studentsat an early age before
societal concepts of what is and is not
appropriate for a group have been formed.
My ownexperiencesteachingchildren show
that while at age four most girls and Afri-
can-Americans will readily handle insects
and are quite curious, by age six they begin
to hang back and watch, and by age eight
there is squealing and giggling, and some-
timesfear, White boys remain equally inter-
ested in handling insects at allages, and will
sometimes push other groups out of the
way,dominatingclassroom demonstrations.
These behavioralobservations parallelthose
made in several general science education
studies (fones & Wheatley 1988, Kahle et
al, 1985). My own observations on children
are echoed in the comments of an African-
American survey respondent:
Ibelieve thatan interestin insects (and life
in general) is something that has to be culti-
vated at an early age. By exposing minority
children, especially city youth, to nature ...
many more of these individuals will not grow
up thinking that ''black people don't do that
kind of thing.''
It is also particularly important to reach
the children of those groups for whom
agriculture is not a positive career image. If
people from one's own group are most
likely to be laboring in the fields harvesting
crops,one isnot likely to view agriculture as
an enticing profession.
Studies have shown that African-Ameri-
can high school students are interested in
science but feel that it lacks applied use
(Anderson 1989). Research has also dem-
onstrated that women are more likely to
show interest in social and ethical applica-
tions of science (Rosser 1990). By stressing
the potential benefits to agriculture and
global ecology, we may successfully recruit
these and other students into entomology.
The effort to begin youth programs in
the ESA and elsewhere is admirable, and we
should make sure that outreach to under-
represented groups is an integral part of
each program. Women and minority role
models, portrayal of girls and children of
color in visual aids, and inclusive language
are all essential parts of this process.
I have occasionally given a 'Draw-An-
Entomologist-Test' to children. Of sixteen
drawings I unearthed in a search through
my files, from twelve girls and four boys,
there are twelve female entomologists and
four male entomologists. For me, this is a
clear example of the power of a visit by a
same-sex role model. My use of women in
visual aids resulted in an excited young
entomology convertnamed Anna, who was
fascinated by a picture of Anna Comstock.
In envisioning themselves as entomolo-
gists,it is easier for children to feel they will
be one of many and not a statistical oddity.
For children to develop a concept of science
as a process and insects as something more
than unpleasant things in their home, they
must find something connected to their
experiences in the world.Iwould argue that
only then can they see themselves as partici-
pants in the process.
High school and undergraduate mem-
bers of underrepresented populations might
be effectively reached by visits or research
partnerships with traditionally African-
American and women's colleges and reser-
vation schools.Land-grantuniversities often
exist near colleges with predominantly
African-American or female student bod-
ies, but this resource of potential students
remains largely untapped. Guest lectures,
arranging for independent-study students,
participation in career days, and seeking
student technicians to participate in field
work are all examples of ways in which
thesestudentscould be exposed toentomol-
ogy. These same techniques could also be
effective at a department's own university.
Istrongly recommend the Association of
American Collegespublication Looking For
More Than A Few Good Wonmen In Tradi-
tionally Male Fields (Ehrhart & Sandler
1987) as an aid in developing departmental
outreach programs. It contains many useful
recommendations for graduate recruitment
and retention strategies that are also easily
adapted to minority students, Hannah
(1990) and Stikes (1990) also discuss mi-
nority retention and recruitment.
The importance of teachers' influence
on students' interest in science and in
entomology should provide increased moti-
vation for the development of excellence in
teaching. The way in which students are
taught science is an area in which increased
ESA and member participation could
provide benefits. Congress, NSF, American
Association forthe Advancement ofScience,
and many other organizations have already
recognized the need for reform. Attitudes
toward science (and entomology, as shown
in this survey) are formed early in life. By
third grade,50% of allelementary students
dislike science-and by eighth grade, 80%
dislike it, Nobel laureate Leon Lederman
described this phenomenon as the way in
which schoolstake 'naturally curious,natu-
ral scientists and manage to beat that curi-
osity right out of them.' As long as science
is stilltaught by lecture and memorization,
this trend will presumably continue.
'Cover less, uncover more'' is the theme
of many educational reforms that stress
observation, experimentation, and discus-
sion. Some examples of improvements in
general education through entomologist
participation are: elementary and second-
ary school visits, involvement in teacher
training programs, and providing lesson
plans in entomology which emphasize stu-
dent discovery.
Undergraduate teaching is as much in
need of reform as that of secondary schools.
Astudyexamining science enrollment found
most science students change their majors
after their first-year science course (Tobias
1990). Even those who did not change
majors had negative comments about the
classroom climate, describing their fellow
students as unfriendly and highly competi-
tive. With a few exceptions, lecturing to
classes of more than one hundred students
followed by memorization of the facts pre-
sented is the predominant method of teach-
ing introductory biology. Is this the best
manner in which to challenge students?
Doesn't it weed out not only those less able,
but also those with a low tolerance for
boredom, those who wish for more interac-
tion with their instructor and peers, and
those who find a passive dictation process
intellectually stagnatingh Are students in-
terested in what they memorize for an exam,
or are they interested in memorizing the
exam? Evidence supports a strong correla-
tion between process and product and sug-
gestswomen and minoritystudents are likely
to be those most affected by teaching style
NSECTS HAVE LONG BEEN DEPICTED IN
a variety of art forms (Hogue, C. L, 1987.
Cultural entomology. Annu, Rev. Entomol.
32: 181-199 ). Especially interesting are art
forms in which the insects themselves are the medium for
the creation of art, The phrase Victorian scientificart is used
by many antique dealers to describe art displays using
natural objects such as insects,flowers, or seed pods created
during the Victorian era (1837-1901).
Smaller displays of the era, such as framed pressed
flowers and table trays showing colorful butterfly wings,
continue to be popular and are readily collectible. Larger,
framed arrangements of insects as art that originated in this
period, however, are rare (Richard Schachner, personal
communication).
Entomologists are aware of the fragile nature of dried
insects and the need to protect them from breaking and
from destructive pests, Unfortunately, lack of awareness on
the part of most owners of Victorian scientific art over the
years has left few examples of this highly esoteric art.
One surviving example of Victorian scientific art is
Joseph A. Kaplan's ''fly case'' (left). The large,framed insect
display, which dates from 1891, previously was featured in
an antique shop in Berkeley Springs, West Virginia, and
now resides in the living room of its new owners from
Washington, D.C.
thought by Edna Engel to have been
created in the 1920s, it does not actu-
ally qualify as Victorian scientific art.
Any connection between [oseph
Kaplan and Henry Engel is specula-
tive. Engel was born in Germany and
moved to Pittsburgh in 1888 at the
age of 15.Engel firstbought a farm at
Library, Pennsylvania, and later the
farm at Finleyville, both near Pitts-
burgh. He operated Engel's Flower
Farm and later Engel's Farm Market,
growing many varieties of flowers,
fruits, and vegetables for sale and for
family use.
Engel expanded his earlier interest
in Lepidoptera by planting wildflower
gardens and other flowers especially
attractive to butterflies and by light-
ing for moths on a hillside overlook-
ing his farm. He corresponded with
other lepidopterists and exchanged specimens, Engel collected with local hobby
lepidopterists including his brother-in-law Fred Friday and his wife, among others,
A photograph of this ''butterfly club'' remains at the Finleyville farm. Engel kept his
extensive Lepidoptera collection in his workroom in the dahlia cellar on the flower
farm. After his death in 1943 at the age of seventy, Engel's collection was sold to
the Carnegie Museum in Pittsburgh where it remains,
When Joseph Kaplan created his fly case in 1891, Henry Engel was 18 years old
and had been in the United States only three years, We have no evidence that Engel
was collecting insects or making displays that early in his life. Thus, Engel probably
was not an influence in Kaplan's artistic creation,
Another example of Victorian scientific art resides in the shop of a notary and book
collector at Allison Park, Pennsylvania, north of Pittsburgh. This display, which
measures 71 cm by 101.6 cm, is a mixture of pinned native, western U.S., and exotic
insects, Included are representatives of many families of Lepidoptera and Coleoptera,
aswell assomeOdonata,Homoptera,Orthoptera,Hemiptera,Hymenoptera,Diptera,
Chilopoda, and Araneae. Several pinned salamanders are also present.
Don Johnston, the shop owner, was given the display by an unidentified man who
explained that he was cleaning the attic; he said the display could either be sold or
thrown away, The man told Johnston that his great-grandfather made the display.
The Allison Park display is smaller and less well preserved than the Kaplan
display. It definitely qualifies as Victorian scientific art because of its obvious age
and the nearly symmetrical arrangement of insects on either side of the central line
of luna and cecropia moths, A symmetrical arrangementappears to be typical of the
large Victorian scientific art displays.
We may wonder how many other examples of this art form are in family attics
waiting to be discovered or discarded. Because of its age, its degrec of preservation,
and its known history, the [oseph A. Kaplan fly case remains one of the best
examples of this type of Victorian scientific art.
For assistance and cooperation,I acknowledge F. IL, Kaplan, David and Irene Addlestone,
Jeffrey Eling, Edna Engel, Cheryl Engel, Don Johnston, C. E. Mecca, John Rawlins, Richard
Schachner, and the staff of Youngblood's Antique Shop.
N 1975, several
children from Lyme,
Connecticut, were
diagnosed as having
juvenile rheumatoid
arthritis, However, the
rural setting and presence
of a unique rash led several
nvestigators to suspect a
different etiology. Subse-
quently, Willy Burgdorfer
and coworkers at the
Rocky Mountain Laborato-
ries in Hamilton, Montana,
isolated a spirochete, which
was eventually named
Borrelia burgdorferi, from
Ixodes dammini ticks, In
1983, this spirochete was
isolated from the blood of
patients with what is now
called Lyme disease. Since
that time, Lyme disease has
become the most frequently
reported tick-associated
illness in the United States.
may be initiated when a mature
female releases a pheromone that
the sexually mature male senses
via receptors on his palps.
During copulation, the male
positions himself on the ventral
surface of the female so that he
can insert his hypostome and
chelicerae into the female genital
aperture (Fig.1). It has been
hypothesized that this allows the
male to detect specific
aphrodisiac-type pheromones
that, when emitted from the
conspecific partner, signal the
male to form a spermatophore.
The spermatophore forms
outside the male gonopore. Since
male I, dammini do not have
copulatory organs, they use
subtle body contortions to push
the spermatophore to the female
genital aperture. The male then
uses its palps and chelicerae to
insert the tip of the
spermatophore into the genital
aperture, where the enclosed
sperm are released into the
female genital tract.
After copulation, the female
I, dammini tick rapidly ingests a
large amount of blood. The
epicuticle has been observed to
expand up to nine times during
this bloodfeeding phase (Fig. 2).
The female tick disengages from
the host after full engorgement,
which has been shown to take
eight to eleven days under some
laboratory conditions, and the
blood is digested. The digestion
products are then utilized for
vitellogenesis and egg
maturation, Investigators have
observed a direct correlation
between the volume of the final
blood meal and the number of
eggs produced.
In our laboratory, eggs were
deposited seven to fourteen days
after engorged adult female ticks
were physically removed or had
dropped from the host (Fig. 3).
Eggs were deposited via an
ovipositor (Fig. 4) which forms
when the vestibular vagina
prolapses through the genital
aperture. During egg deposition,
the female's Gene's organ and
porous areas excreted an
aqueous solution which may be
a waterproof, fungicidal agent to
coat and protect the eggs from
deterioration (Fig. S). The female
I, dammini ticks died shortly
after the egg masses, each
containing approximately 2,500
eggs, were deposited.
I, dammini eggs hatched after
twenty-nine to thirty-six days of
incubation at conditions described
above. When hatching, the egg
split mediolaterally and the larva
emerged mouthparts first (Fig. 6).
The recently emerged six-legged,
teneral larvae were opaque (Fig. 7).
during the final blood meal
before oogenesis (Figs. 15 and 16).
I, dammini are highly
developed parasites that
complete their life cycle by
obtaining blood meals from mice
and deer, Mice and other small
animals are efficient B.
burgdorferi reservoirs, and
immature I, dammini often
become infected while obtaining
a blood meal. The abundance of
these hosts and their role as a
reservoir for B. burgdorferi has
allowed B. burgdorferi-infected
I, dammini to disperse
throughout many regions of the
United States and increase the
threat of Lyme disease.
Oil palms (Elaeis gtuineensis) are now one
of the world's most important plantation
crops (figure l) and in Malaysia palm oil is
one of the country's major economic prod-
ucts. In parts of South East Asia, oil palms
are estimated to outnumber the population
by almost two to one. Whereas plantatlon
management and the growing of oil palm,
particularly of high-yielding material, has
reached a high standard of efficiencv, har-
vesting of the ripe fruit and its conveyance
to the factory still involves losses that lead
to unacceptable reductions in overall yield.
The numbers of fruit produced by a palm
(the promise of oil on the palm) is not sat-
isfactorily reflected in the amount of oil re-
covered from the harvested bunches.
The cause of the shortfall is the non-
synchronous ripening of the fruit on the
different spikelets on any one bunch and
the shedding and loss of the ripest fruit of
the bunch before it reaches the factory. A
recent and new understanding of the
processes of ripening and shedding in the
oil palm fruit offers a biotechnological ap-
proach to overcoming these losses. It com-
bines the advances of clonal propagation
programmes with the understanding of the
unique physiological and biochemical con-
trols that determine ripening and the tim-
ing of fruit-fall from the palm.
After fertilisation the fruit enlarges, reach-
ing maximum size and fresh weight of
both mesocarp and kernel by 120 days or
so. Then, at a signal not yet understood in
chemical terms, the cells of the mesocarp
initiate a new range of gene expressions
which include the synthesis of carotene
and lipid and the production of at least two
enzymes, a lipase and a cellulase. This is
the start of ripening and for the next 30 or
40 days the intensity of the orange colour
of the mesocarp increases as the carotene
level rises; the commercially valuable
palmitic, oleic and linoleic triglycerides in
the flesh accumulate and the activity of
the lipase reaches a maximum [!]. These
are all concurrent events and they continue
until another signal initiates a further set of
gene expressions that herald the onset of
fruit shedding. From the point of view of
high quality lipid, the fruit should reach
the processing plant just before, or at the
time, that the shedding processes are initi-
ated, for subsequently the lipase progres-
sively functions in a hydrolytic role and
the lipid levels start to fall. This results in
the release from the triglycerides of the
free fatty acids (again, mainly palmitic,
oleic and linoleic) which, as they accumu-
late, increasingly spoil the quality of th:
oil extractable from the fruits. For the
highest oil yield, though, harvesting should
be delayed until a considerable number of
the fruit have been shed, since lipid syn-
thesis continues in the fruit that are still at-
tached. At present, some loss of oil quality
is accepted to maximise the yield, and
where fallen fruit are collected, there is
also a considerable additional cost. An al-
ternative solution to the problem, there-
fore, is the elimination of the shedding
process altogether, Current research
Suggests that suitable genetic manipu-
lation of the oil palm could offer this pos-
sibility.
The anatomy of abscission of the oil palm
differs from that of most commercial
fruits. Instead of a synchronous series of
cell separations across a plane of cells be-
tween the fruit and the stalk, resulting in
immediate shedding of the fruit above, the
oil palm undergoes abscission in two dis-
tinct stages with a time lag of 1-2 days be-
tween the two [2].
The flower of the oil palm is hermaph-
rodite, but normally only one of the sexes
will complete development in any one
bunch so that sequential bunches will carry
only male or female spikelets. In the fe-
male, the staminal ring aborts before an-
thesis and is left as a circlet of tissue, the
rudimentary androecium, surrounding the
base of the ovary and immediately adja-
cent to the inner whorl of papery tepals
(figure 2a, b). (Tepals are elements in the
perianth of a flower lacking differentiated
sepals and petals.)
As he fertilised fruit enlarges, the cells
of this rudimentary androecium and the
tepal bases continue to divide, keeping
pace with the increase in diameter of the
base of the fruit. This pattern of differenti-
ation results in the fruit being attached to
the spikelet at a junction with three distinct
tissue types. The first (position 1) is the
base of the fruit itself (figure 3a, c) and
here, even before anthesis, the sites of cell
separation are clearly defined by a line of
small cells with dense cytoplasmic con-
tents. This is the destined site of the first
stage of the abscission process and separa-
tion at this position occurs only when the
fruit is fully ripe. Even when cell separa-
tion at position 1 is complete, the fruit is
not shed, for the cells of the circlet of the
rudimentarv androecium and the bases of
the tepals still adhere closely to one an-
other (positions 2 and 3, figure 3b and c).
The fruit is, however, loosened and at this
stage can, in the wild, be readily plucked
from the bunch by primates, parrots or
other fruit feeders.
The second stage of abscission, which
results in the fall of the fruit to the ground,
occurs only after the first stage is complete
and involves a special role for the cells of
the rudimentary androecium. Usually, the
cells of the circlet immediately adjacent to
the fruit base (position 2), though fre-
quently some of those adjacent to the tepal
bases (position 3), will undergo separation
(figure 3c) so that the fruit falls free from
the enclosing tepals. The upper parts of the
tepals are by then brown and dry. The shed
fruits are therefore either naked of any
basal attachments (figure 4a) or may have
fragments of the rudimentary androecium
still adhering.
A naked fruit leaves a complete circlet
of its rudimentary androecium still at-
tached to the tepal bases and partly to the
pad of stalk tissue at position 1 (figure 4b),
but where some of the androecial ring
clings to the fruit, the remaining tissue of
the circlet stays with the tepals on the
spike.
In exceptional circumstances, when
spikes of fruit have been removed from a
bunch before they are fully ripe, fruit shed-
ding will eventually occur across the bases
of the tepals (positions 4 and 5). Even in
these conditions, separation always occurs
first at position 1, to be followed by the
second stage at either position 4 or 5. Such
fruit are therefore shed still enclosed in the
ring of tepals (figure 4c) and normal sepa-
ration at positions 2 and 3 at the margins
of the rudimentary androecium is by-
passed.
On the plantations, palms are checked
every few days for bunches at the appro-
priate stage of ripeness for cutting.
Conventionally, the falling of a few ripe
fruit to the ground is taken as the harvest
signal, and for the taller palms this affords
proof of ripeness of a bunch that is other-
wise difficult to see among the bases of the
fronds. But before the bunch reaches the
factory many more fruit are shed and so a
yield loss is established
Long ago, occasional abnormal fruit devel-
opment was reported to occur in certain oil
palms [3]. In these, the rudimentary an-
droecium did not remain as a circlet of tis-
sue but instead enlarged and developed
into six (usually) additional seedless lobes
of female mesocarp (supplementary
carpels) surrounding the central fruit (fig-
ure Sa, b). These parthenocarpic lobes syn-
thesised carotene and lipid and ripened in
concert with the kernel-containing fertile
ovary. This additional lipid-rich mesocarp
offered a potential for high yields, and cer-
tain seedlings and at least one genetic line
of oil palm was lound which routinely pro-
uduced such fruit. The promise of high
vields from these so-ealled 'mantled' fruit
was not fulfilled, however, perhaps be-
cause, although the fruit ripened, it was not
shed. In the absence of the usual signal of
the first few ripe fruits that fall to the
ground, bunches on the mantled palms
were left unheeded and the fruit were
quick to rot on their spikelets.
In the 198Os. great efforts were made to
upgrade the yields ot lipid by the introduc-
tion of clonal plant material raised by
tissue culture from root or shoot ftragments
taken trom elite, high quality, high lipid-
producing palms. Many thousands of these
clonally propagated individuals are now
bearing fruit in plantation trials around the
world and improved yields have resulted
trom these plantings. Certain of the tissue
culture procedures, involving the use of
plant hormones in the media have, how-
ever, also led to a proportion of the palms
showing sexual abnormalities that resem-
ble the naturally occurring mantled fruit
[4]. While the rudimentary androecium
may form very well-developed lobes of
supplementary carpels that extend the
whole circlet of the androecial ring, some-
times, only otne or twO small lobes may
arise while the remainder of the ring may
be normal. In such fruit. abscission occurs
normally at position 1, but at positions 2
and 3, cell separation takes place only
where the rudimentary androecial ring has
remained as aborted staminal tissue.
Where the ring has differentiated into
mmesocarp tissue, the fruit remains attached
to the bases of the tepals (figure 6a, b).
Control of this second stage of fruit ab-
scission, and hence of fruit shedding, can
therefore be manipulated by altering the
developmental programme of the cells of
the rudimentary androecium early in dif-
ferentiation and before anthesis. Evidence
from clonal propagation biotechnology
now indicates that the levels of hormones
used in tissue culture can determine the de-
gree of mantling expressed by a palm sev-
eral vears later when it starts to flower.
Because the condition does not show con-
slant expression it may have an epigenetic
origin within these highly specialised cells,
but RFLP (restriction fragment length
polymorphism) analysis for certain of the
clones expressing mantling indicate that
DNA genomic changes may also have oc-
curred between the parent palm and the tis-
sue culture progeny [5]. Although the
mechanism by which mantling is directed
remains unresolved, the techniques by
which it can be induced are now known,
attording new possibilities for manipulat-
ing abscission.
In those crops where fruit shedding has
been closely studied, the signal for abscis-
sion can be directly linked to critical levels
of ethylene produced by the ripening fruit
and perceived by the ethylene-responsive
target cells of the abscission zone. In ap-
ples, oranges or tomatoes, for example, the
synthesis of ethylene increases as ripening
proceeds, and at full ripeness the levels
reach a threshold that initiates in the zone
cells an expression of new cell wall modi-
tying enzymes (glucanhydrolases) that
loosen adhesion at the zone cell interface
with neighbour cells. Because the line of
separation is so precise (despite the fact
that the secreted enzymes may migrate
considerable distances through the cell
walls of adjoining tissues), it is evident
that substrate specificity exists between the
secreted enzymes and the walls of a lim-
ited number of cells which are restricted to
the immediate vicinity of the zone [6]. In
this way, only certain cells become sepa-
rated from their neighbours by the en-
zymes that are induced in the zone. In
these dicotyledonous fruits, once the ab-
scission cascade of enzymes is produced
(diagnostically, this usually includes a spe-
cific 9.5 pl isozyme of 5-1, 4-glucanhydro-
lase) separation is initiated across all the
cells of the fruit stalk and the fruit is shed.
In the oil palm, the situation is different.
Throughout ripening, during the rise in
lipid and carotene, the levels of ethylene
produced remain insignificant. It is not
until the fruit is deep orange in colour and
near maximum lipid content that ethylene
synthesis starts to rise. This occurs first at
the apical end of the fruit and quickly pro-
gresses towards the base. In spikelets that
are removed from the bunches, ethylene
production by a fruit reaches a maximum
within 24 hr of the initial rise and there-
after declines. Ethylene production and the
onset of cell separation at position 1 indi-
cate that these events are very closely
linked. Time-course experiments have
shown that only those fruit that exhibit the
rise in ethylene synthesis initiate separa-
tion at the fruit-pedicel junction (position
1) figure 7.
It is not mesocarp tissue, however, that
abuts directly onto zone cells. Instead, sev-
eral layers of cells that synthesise neither
carotene nor storage lipid provide a narrow
barrier between the mesocarp and the cells
of the zone. It is at the junction of these
'barrier' cells and the zone cells that the
first wall separations take place, The
second stage of separation at positions 2 or
3 appears to be dependent upon the
achievement of the first stage of separation
at position 1, and not directly upon the sig-
nal of ethylene. Another signal, generated
by separating cells at position 1 (and possi-
bly an oligosaccharide) would appear to
pass to, and be perceived by, the cells that
constitute positions 2 and 3. During this
second stage of abscission, cells of the
rudimentary androecium dissociate from
each other and from other cells adjacent to
them. Since these other cells include those
at the outer edge of the base of the fruit,
the pedicel and the bases of the tepals, sev-
eral distinct cell types are intimately in-
volved. When the rudimentary androecium
develops as mesocarp, as in the formation
of the mantled condition, cells are differ-
entiated that no longer have the compe-
tence to participate in cell separation
responses to abscission-inducing signals,
and the intimate complex of inter-tissue
signalling fails.
In dicotyledonous fruits and leaves, abscis-
sion is linked to the induced expression of
the 9.5 pI isozyme of B-1, 4-glucanhydro-
lase (cellulase) by the separating cells of
the zone. This is not so in the oil palm
fruit. AIthough ripening mesocarp tissue
produces a cellulase, the zone does not.
Furthermore, the cells of the mesocarp do
not separate despite the production of cel-
lulase isozymes. Instead, at position 1 of
the oil palm fruit, an active polygalactur-
onase is produced at abscission, indicating
break-down of uronide linkages in the
zone region, while at positions 2 and 3
high levels of a 5-1, 3-glucanhydrolase are
induced. Although these enzyme activities
may in part reflect the overall differences
between the cell wall composition of di-
cotyledons and monocotyledons, they also
serve to illustrate the differences between
the major enzymes induced in the two dis-
tinct stages of abscission in the oil palm
fruit 7].
A number of ways are now open for the
genetic manipulation of the oil palm: these
should reduce the losses that result from
unwanted shedding. Each could be engi-
neered during the clonal stages of propaga-
tion in tissue culture.
Firstly, the production of ethylene could
be blocked in the fully ripe fruit by the in-
corporation of antisense genes to the im-
portant enzyme ACC-synthase. This
enzyme controls the synthesis of the ethyl-
ene precursor ACC. Antisense mRNA pro-
duction has already been achieved for the
tomato, thereby preventing translation of
the normally produced sense mRNA and
inhibiting the rise in ethylene formation by
mature fruit, so delaying ripening [8]. It
would be important, however, that the anti-
sense gene be expressed only in the ripe
fruit and not in all other cells of the palm,
for enough is not yet known about ethyl-
ene control of other aspects of palm devel-
opment. This control may be achieved
through the use of gene promoters specific
to fruit tissue. Alternatively anti-sensing
ethylene-response genes may give greater
specificity. In this case a promoter specific
to abscission zone cells would be required.
Secondly antisense could be introduced
for the polygalacturonase gene of position
1 or for the 5-1, 3-glucanhydrolase gene of-
positions 2 and 3, again under the control
of the appropriate promoters. Since it is
likely that specific isozymes of the en-
zymes are synthesised only at these separa-
tion positions, such options offer a precise
developmental control of gene expression
that is restricted to the abscission zone
alone.
Thirdly, biotechnology could be further
advanced so that the tissue cultures from
which elite palms are propagated could be
so precisely manipulated hormonally that
the rudimentary androecium would consis-
tently develop on the female inflorescences
as a narrow circlet of mesocarp cells. This
differentiation would then permit fruit loos-
ening at position l, but preclude the possi-
bility of the second stage of the fruit
abscission process at positions 2 or 3.
Whenever any one of these options be-
comes a reality - and there is no doubt that
each would increase the total harvestable
yield of fruit - they will raise a serious
practical problem for the men who work
on the plantation. If the fruit are no longer
shed, so that none falls upon the ground,
how will the cutters know exactly when
each bunch has fruit that are fully ripe and
so be able to judge exactly when each
bunch should be harvested'? To resolve
this, new practices for monitoring ripeness
in the field must now be developed.
Overall, the importance of these re-
searches is the promise they now offer for
operating more regulated fruit shedding
and harvesting regimes not only for the oil
palm, but for other major palm plantation
crops of high value including dates and co-
conuts.
The BungerHills are located near the coastof EastAntarctica
at about 100E longitude (Fig. 1). They cover an area of
about 300 km' and consist of low rocky hills and glacially
deepened valleys, with many lakes (Fig. 2). Together with
adjacent islands and the Obruchev Hills, to the south-west,
the Bunger Hills form a well-exposed section of the Pre-
cambrian EastAntarctic Shield in a region ofgenerally poor
outcrop. The firstgeological studies ofthe Bunger Hillswere
made in 1956-57 by Soviet Antarctic Expedition scientists,
a detailed, mainly petrological, account being given by
Ravich et al. (1968).
Thispaper,based onfieldworkcarried outduringthe 1986
summerseason,summarizesthe geology ofthe Bunger Hills
areaandpresentsresults ofthe firstdetailed geochemicaland
geochronological study of the area. It also attempts to
correlate the geological history with that of once contiguous
parts of Gondwana. A detailed account of the geology will
be published in a forthcoming AGSO Bulletin (Sheraton
et al. in press). The results of specialized structural and
petrological studies of the main Bunger Hills outcrops are
given by St6we & Powell (1989), Stüwe & Wilson (1990)
and Ding & James (1991).
The Bunger Hills area consists of granulite-facies
metamorphic rocks with a variety of compositions (of both
igneous and sedimentary origin), intruded by voluminous
plutonicrocks ranging from gabbro to granite, and a variety
ofmaficdykes (Fig. 1). The field relations,petrography,and
geochemistry of these rocks are described in the following
sections.
Felsic orthogneiss is widespread in the Bunger Hills and
adjacent islands, and makes up most of the Obruchev Hills
(Fig. 1). It also occurs as a relatively minor component of
layered metasedimentary rocks. Major constituents are
orthopyroxene(upto 15%),quartz(mostly 15-40%),feldspar
(mostly 50-65%) and biotite (up to 3%); minor amounts of
clinopyroxene, hornblende, or garnet may also be present.
Opaqueminerals,apatite,andzircon are ubiquitous accessory
phases, whereas monazite is less common and allanite and
rutile are rare. Textures are commonly granoblastic
inequigranular,andmostgneisses have only aweakfoliation
defined by elongated mineral aggregates, biotite grains, or
lenticular quartz aggregates. In many gneisses the only
feldspar is plagioclase (commonly antiperthitic andesine,
rarely calcic oligoclase or sodic labradorite), and such
tonalitic (using the nomenclature of Streckeisen 1976)
orthogneiss predominates in the south-eastern Bunger Hills
andObruchevHills. Elsewhere,morepotassic(granodioritic
to granitic) orthogneiss occurs interlayered with tonalitic
orthogneiss and metasedimentary rocks.
Chemical data (Sheraton et al. in press) and field
relationships indicate an igneous origin for the vastmajority
ofthese gneisses. Massive, poorly layered orthogneiss,such
as thatin the Obruchev Hills,is probably of intrusive origin,
whereas at least some of that interlayered with supracrustal
rocks may well be metamorphosed extrusive rocks. Most
orthogneisses aremetaluminous oronlyslightlyperaluminous
and are thus equivalent to the I-type (derived by partial
melting of igneous precursors) granitoids of Chappell &
White (1974). Like similar orthogneisses elsewhere in the
East Antarctic Shield (Sheraton et al. 1987b, Sheraton &
Collerson 1984),mostare depleted in Y and heavy rare-earth
elements. Such Y-depleted orthogneisses are thought to
representnew continental crustderived by partialmelting of
a hornblende t garnet-bearing, but feldspar-poor, mafic
source(possiblysubductedhydrated oceaniccrust)(Sheraton
& Black 1983). Only a few orthogneisses belong to the
Y-undepleted suite of Sheraton & Black (1983), which are
thought to represent partial melts at relatively low PR,o of
predominantly felsic crustal rocks, their relatively low Sr
being due to partialmeltingwithmajor residual plagioclase.
An Y-depleted granodioritic orthogneiss from south-
western Bunger Hills has given an ion-microprobe U-Pb
zircon age of 1700 fMa, whereas a tonalitic orthogneiss
fromthe ObruchevHills yieldedalateArchaean conventional
U-Pb zircon age of 26413; Ma (Sheraton et al. 1992); both
are interpreted as emplacement ages. Granodioritic
orthogneissfrom Thomas Islandwas emplaced 152129Ma
ago and underwent high-grade metamorphism at 1190 E2
15 Ma (ion-microprobe data: Sheraton et al. 1992).
Mafic granulite is commonly interlayered with both felsic
orthogneiss and paragneiss, although individual layers are
rarelymore than afew metres thick and some are boudinaged.
Most probably represents metamorphosed mafic intrusives,
buttexturesare granoblastic. Typical granulite layerscontain
plagioclase (?4g- 50-60%), clinopyroxene (10-20%),
orthopyroxene (10-20%), opaque minerals (up to 5%), and
minor apatite. Greenish-brown hornblende (up to 30%),
biotite (up to 5%), or minor quartz may also be present.
Ultramaficlayersand,morecommonly, podsarewidespread,
butvolumetrically insignificant. The mostcommon varieties
are hornblende pyroxenite and pyroxene hornblendite,
containingvarious amountsofclinopyroxene, orthopyroxene
and pale brown hornblende. Other bodies contain abundant
biotite or phlogopite, and olivine or plagioclase may also be
present. Minor phases include opaque minerals and dark
green spinel.
Layered, garnet-bearing felsic gneiss occurs throughout the
Bunger Hills area, but is rare in the Obruchev Hills.
Almandine-pyropegarnet(2-15%),biotite(up to 6%),quartz
(30-50%), perthite (up to 50%), plagioclase (commonly
A%wip to 50%) and minor opaque minerals and zircon
occur in most rocks, and small amounts of apatite, rutile,
monazite, corundum, spinel, and sillimanite may also be
present; orthopyroxene occurs in some layers. Much garnet-
bearing gneiss is of relatively potassic composition, with
K-feldspar being the dominant feldspar. It is commonly
associated with aluminous metasedimentary rocks, and is
probably mostly of sedimentary origin. However, some
massive leucogneiss of granitic composition, with minor
garnet, is clearly intrusive.
Aluminous metasedimentary rocks (metapelites) crop out
with garnet-quartz-feldspar gneiss in much of the Bunger
Hills and nearby islands. Garnet (commonly 5-25%),
sillimanite (up to 10%), cordierite (up to25%),biotite (up to
10%), quartz (25-50%), perthite (up to 45%), commonly
antiperthitic oligoclase-andesine (up to 25%), opaque
minerals (magnetite + ilmenite; up to 3%), and minor dark
green spinel are prominentconstituents. Accessoryminerals
comprise zircon,rutile, corundum,monazite,and rare apatite
and ?chevkinite. Orthopyroxene (generally rather altered)
occurs in some layers, and cordierite is partly replaced by
pinite and/or biotite. Corundum occurs in association with
opaque oxides and spinel.
Typical assemblages are:
The assemblage
The predominant mesoscopic structures in the Bunger Hills
area are open to tight or isoclinal, commonly asymmetric
folds (F,, Fig. 4b) which probably formed during a major
shortening event (Stüwe & Wilson 1990), under granulite-
facies conditions (M,). The small separations of D,boudins
and minor flattening component suggest a much smaller
finite strain than during D,. F, folds have highly variable
orientations, largely due to the effects of D,; they commonly
plunge to the WSW or to the E in southern Bunger Hills and
in more northerly or southerly directions near the Fishtail
Gulf pluton. D, axial planar foliation (S,) and mineral
elongation lineation (L,) are only weakly developed.
The regional strike in most of the Bunger Hills area is the
result of a third major ductile deformation (D,), still under
granulite-facies conditions (M,), which is thought to have
been accompanied by emplacement of the Lake Figure and
Fishtail Gulf plutons. F, folds are major upright,commonly
asymmetric structures with shallow E- or W-plunging axes
(Fig.4c). They clearly refold F, and F,structures, which are
thensteeply plunging(Fig.4d). F,foldsin south-westernand
south-eastern Bunger Hills have axes plunging at low angles
(0-30') to the WSW (Ding & James 1991), whereas major
F, folds adjacent to the Fishtail Gulf pluton trend N-S to
NNW-SSE (Fig. 1). This variable orientation of F, folds is
thought to be due to deformation having been
contemporaneouswithemplacementofthe two Bunger Hills
plutons. Except for those adjacent to the Fishtail Gulfbody
(which hasthe strongestmarginalfoliation),plotsoffoliation
orientations define gitdles consistent with folding about
WSW-trending axes having been the main factor in
determining theregional strike. Stüwe & Wilson (1990)also
explained the variable orientation of F, (theit F,)folding by
theeffectsofthe Fishtail Gulfpluton,althoughtheyconsidered
thisbody to have been emplaced during D,. In contrast, Ding
& James (1991) interpreted the N-S folds as a separate
generation (F,), on the basis of superimposed folding
relationships with the WSW-trending F, folds. However,
such relationships could be explained if the plutons were
emplaced late in D,, after the WSW folds had started to form.
Much of the Charnockite Peninsula pluton (the unfoliated
quartz monzodioritic and probably the granitic rocks) is
about 20 Ma younger than the Bunger Hills plutons and
clearly cross-cuts D, structures (e.g. at eastern Thomas
Island). Metamorphosed dykes within the pluton include
probable syn-plutonic dykes which have some chemical
features (e.g. high Nb) in common with the quartz
monzodioritic rocks. Group 1 dolerites may be of similar
age. However, subconcordant, folded and metamorphosed
mafic dykes which cut the country rocks are clearly of pre-
D, (and possibly pre-D,) 4ge.
A complex history of more localized brittle and brittle-
low to medium grade; alternatively, the two areas may not
have been juxtaposed at that time (Black et al. 1992).
Althoughgranulite-faciesmetamorphism apparently occurred
in the Archaean, the time of the most extensive new zircon
growth in the tonalite, T;model ages of other rocks in the
area (intermediate to felsic intrusives and a paragneiss) are
much younger (1600-2280 Ma), indicating considerable
continental crust formation (and at least one period of high-
grade metamorphism) in post-Archaean times.
Asimilar range ofrocktypesis presentnear the Soviet base
at Miny,350 km west of the Bunger Hills (Fig. 6) (Ravich
et al. 1968). Granulite-facies country rocks consist mainly
of interlayered tonalitic orthogneiss and mafic granulite,
withminorgarnet-bearing paragneiss. These areintruded,in
turn,bydyke-like dolerites andgabbronorites,''charnockites''
(orthopyroxene monzodiorite, quartz monzodiorite and
granodiorite; fayalite quartz monzonite and granite), and
several generations of aplite and pegmatite. The fayalite
granitoids have given a Rb-Sr whole-rock isochron age of
502S24Ma(McOueen etal. 1972),andigneous activitywas
accompanied by resetting of the K-Ar system in the country
rocks (Ravich et al. 1968). However, in the absence of
definitive geochronological data, the ages of high-grade
metamorphism and mafic intrusives at Mirny, as in the
Denman Glacier area, are unclear and regional correlations
(Table II) can only be tentative.
The nearestmajor outcrops to the east of the Bunger Hills
arethe Windmill Islands,about400 km distant(Fig. 6). The
Garnet compositions are commonly a basis for ther-
mobarometric calculations, and thus modifications to
prograde garnet zoning have been the subject of many
previous studies. Volume diffusion can modify garnet
compositions at temperatures greater than ca. 550-650
eC (Loomis et al., 1985; Florence and Spear, 1991). The
prograde composition of garnet around primary mineral
inclusions, trapped during garnet growth, can be modified
by exchange reactions (Tracy, 1982) and by net transfer
reactions (Whitney, 1991). Yardley (1977) suggested that
modifications to prograde garnet zoning could be region-
ally mapped and used to define an isograd comparable to
the breakdown of muscovite to form sillimanite and feld-
spar. Zones of abrupt change in prograde garnet zoning
patterns, such as those described in the present study, can
be formed by sequential episodes of garnet growth with-
out volume diffusion (Rumble and Finnerty, 1974), by
discontinuous reactions (Thompson et al., 1977), and by
resorption with volume diffusion (de Bethune et al., 1975;
Tracy, 1982; Karabinos, 1984).
This study focuses on modifications to prograde garnet
zoning patterns in two locations, representing two differ-
ent metamorphic episodes. These garnets abruptly change
composition in a zone 50-150 um wide at their rims,
around a variety of mineral inclusions, and along some
cracks in the garnet interiors. The zones are decorated by
small mineral and fluid inclusions 1-2 um in diameter.
These zones are interpreted to have formed by metaso-
matic dissolution and reprecipitation of preexisting garnet.
Rocks in this study were sampled from two locations
in the Acadian zone of western New England (Fig. 1).
Peak metamorphic minerals west of the Proterozoic mas-
sifs were produced mainly during Middle Ordovician
(Taconian) metamorphism and during Early to Middle
Devonian (Acadian) metamorphism east of the massifs
(Sutter et al., 1985). Both metamorphisms produced Bar-
rovian facies series mineral assemblages. The Acadian
metamorphic front (Fig. 1) represents a zone of transition
from rocks in which the mineral assemblages, textures,
and structures are dominantly the result of Taconian
metamorphism to those in which they are due to Acadian
metamorphism (Hames et al., 1991). Pre-Silurian rocks
east of the Acadian front, as at the locality described be-
low, locally contain polymetamorphic mineral assem-
blages that reflect both events.
Rocks within the Sharon Quadrangle and vicinity,
northwestern Connecticut (Fig. 1), were initially meta-
morphosed at middle amphibolite-facies conditions (Zen,
1981; Wang and Spear, 1991) during the Taconian orog-
eny and were again metamorphosed at varying grades
during the Acadian orogeny (Hames et al., 1991). Sample
WH141 is a metamorphosed shale, part of the Walloom-
sac Formation, which in New York, Massachusetts, and
Vermont contains Middle Ordovician faunas (Ratcliffe,
1974; Finney, 1986).
The structure and stratigraphy of the Strafford Quad-
rangle and vicinity in east-central Vermont (Fig. 1) were
described by Doll (1943), White and Jahns (1950), How-
ard (1969), and Ralph (1982), and the metamorphism of
the area was described by Menard (1991). Devonian rocks
exposed in the quadrangle were deformed and metamor-
phosed during the Acadian Orogeny. The major struc-
tural feature in the Strafford Quadrangle is the Strafford
Dome (Fig. 1), which exposes a sequence of rocks increas-
ing inward from chlorite grade to kyanite grade. The sam-
ple described here is from a region that Ferry (1990) pro-
posed to have experienced high synmetamorphic fluid
flow.
Mineral compositions for the sample from Connecticut
were determined at the Virginia Polytechnic Institute and
State University on a Cameca SX50 electron microprobe.
Areal maps of Fe, Mg, Mn, and Ca concentration were
based on X-ray intensity data accumulated from wave-
length-dispersive spectrometers. Similarly, mineral com-
positions for samples from Vermont were determined us-
ing the Jeol 733 Superprobe at Rensselaer Polytechnic
Institute. Analytical conditions for spot analyses of all
samples were 15 kV and 15-20 nA for a maximum of 40
s per element analyzed. Natural and synthetic silicates
and oxides were used as standards. FTIR and Raman
spectroscopic study was conducted at Virginia Polytech-
nic Institute and State University.
Sample WH141 (Fig. 1; UTM coordinate B33234370)
is a schist in which Taconian metamorphism produced
the assemblage kyanite + staurolite - garnet + biotite
+ quartz + muscovite -+ plagioclase + ilmenite with
minor apatite, graphite, and pyrite (Hames, 1990). Com-
positional layering in the exposure defines a crenulated,
composite surface that is overgrown by the garnet, kya-
nite, and staurolite (T, porphyroblasts of Sutter et al.,
1985; Hames et al., 1991). This composite surface and
the porphyroblasts are deformed by both northwest- and
northeast-trending Acadian folds (the D, and D, folds of
Ratcliffe and Harwood, 1975).
Porphyroblastic garnet, kyanite, and staurolite have ir-
regular, rounded shapes and are surrounded by coarse,
randomly oriented muscovite, suggesting resorption of
early-formed, less hydrous porphyroblasts and formation
of a more hydrous assemblage. Traces of acicular silli-
manite are present on the resorbed margins of garnet por-
phyroblasts in sample WH 141. Plagioclase porphyro-
blasts are texturally zoned, with fractured, inclusion-rich
cores and clear subhedral rims; however, they have a uni-
form composition of about An,,. The porphyroblast re-
sorption, coarse muscovite, and acicular sillimanite are
interpreted as having formed during Acadian metamor-
phism (Hames et al., 1991). The rim compositions of
porphyroblasts in WH 141 define P-T conditions of 510
s 25 %C and 4 > 0.5 kbar; from this sample locality,
Acadian metamorphic P- T' conditions increased mono-
tonically to greater than 650 C and 7 kbar southeastward
along the present erosion surface (Hames et al., 1991).
Textures of garnet representative of this locality are
shown in Figure 2. The garnet interior is characterized by
concentric changes in major cation concentrations that
are consistent with growth zoning. In contrast, there is an
abrupt decrease in J,,, and accompanying increases in
2.-. A4,, and 3-,, within -100 um of the garnet rim.
Identical compositional changes in garnet occur along
some cracks and around some primary apatite, musco-
vite, biotite, and ilmenite inclusions. These composition-
al changes are uniform and do not vary with respect to
which mineral is in contact with garnet along the rim or
inclusion interface.
The regions of sharp compositional change are marked
by abundant secondary fluid and mineral inclusions, up
to --2 um in diameter, that impart a clouded appearance
in thin section (Fig. 2A, 2C). The clouded regions along
cracks in the garnet cross the concentric growth zoning
pattern, and thus these small inclusions are secondary
with respect to prograde garnet growth. Mineral inclu-
sions in these regions are highly birefringent and appear
to be muscovite and sphene. The fluid inclusions are gen-
erally equant, and negative crystal shapes (reflecting the
host isometric structure) are common. Spacing between
these inclusions ranges from 2 to 10 um. Some inclusions
are arranged in parallel, linear arrays: these arrays are
generally perpendicular to the garnet surface. The largest
fluid inclusions (- 1-2 um in diameter) have distinct va-
por bubbles that adhere to the inclusion wall, suggesting
that the fluids are H,O-rich. FTIR and laser Raman spec-
troscopic data from garnets in sample WH141 (and oth-
ers in the vicinity) detect both CO, an H,O in the fluid
inclusions.
Sample 8516g is a calcic pelitic schist from the Devo-
nian Gile Mountain Formation collected from a roadcut
on the northbound entrance ramp of I-89 in Sharon, Ver-
mont (Fig. 1). The matrix assemblage is kyanite + stau-
rolite t garnet + biotite + plagioclase (An,;) f quartt
-muscovite + graphite, with minor rutile + ilmenite +
tourmaline + zircon, Ilmenite and the first generation of
muscovite and biotite define a crenulated S, schistosity.
Orientation of a second generation of biotite defines the
S, schistosity. The garnet core has straight inclusion trails,
composed of epidote and graphite, that define an includ-
ed S, surface at a high angle to the matrix S, foliation
(0Fig. 3.
Garnet in sample 8516g is intensely resorbed (Fig. 3)
and replaced along its rim and interior by coarse, unori-
ented muscovite that crosscuts the internal S1 fabric in
the garnet (Fig. 3). The garnet rim and surfaces of the
garnet adjacent to the late muscovite have a zone 30 um
wide with abundant micrometer-scale fluid and mineral
inclusions. Garnet compositions (Fig. 3C) are similar to
those in sample WH 141. A,,,, in the garnet core is high
and fairly uniform. However, in the outer 50 um of the
garnet rim, A,,,, decreases sharply as 2.., and A, in-
crease; similar sharp zoning patterns are evident adjacent
to some muscovite, ilmenite, and plagioclase inclusions.
Garnet in this sample grew during Acadian metamor-
phism by the reaction (Menard, 1991) Ch1 + Ep + Ms
+ Qtz + Gr = Grt + Bt + P1 + H,O + CO;, Garnet
was interpreted to have grown along a computed P. T'
path of heating during a 1-kbar pressure increase up to
conditions of 550 E 25 *C and 7.5 8 0.6 kbar (Menard,
1991). The subsequent development of sharp composi-
tional zoning at the rim of the garnet probably formed
without a large change in P and T: the distribution coef-
ficient of Ca between garnet and plagioclase inclusions
does not change substantially across the sharp composi-
tional zoning in the garnet. The large change of garnet
and plagioclase compositions under seemingly isother-
mal, isobaric conditions indicates open-system behavior
and metasomatic loss of Ca from the rock.
We propose that this single-crystal style of metaso-
matism formed the textural and compositional disconti-
nuities along garnet surfaces in these samples. The garnet
resorption textures, presence of fluid inclusions, and ac-
companying compositional changes suggest that the outer
surface of the garnet was dissolved in a metamorphic
fluid. The garnet being resorbed probably had relict, dis-
equilibrium compositions and the equilibrium garnet
compositions changed as a result of the resorption, This
mineral-surface scale metasomatism modified the origi-
nal prograde garnet zoning pattern along every pathway
accessed by the metamorphic fluid, even along cracks and
some inclusion boundaries inside the garnet. Reprecipi-
tation of garnet in equilibrium with the fluid, concomi-
tant with the dissolution, trapped both fluid and mineral
inclusions in pits near the garnet surface.
We cannot prove that the new garnet was in equilib-
rium with the fluid, yet fluid-controlled equilibrium is
strongly suggested because (1) garnet composition is sim-
ilar along every surface that contains the mineral + fluid
zone, irrespective of which mineral is in contact with gar-
net across the zone; (2) we obtain consistent pressure-
temperature estimates (i.e., consistent mineral K,,, and
thus P-T estimates) from the garnet rims and matrix min-
erals in these samples; and (3) the P-T estimates based
upon garnet rim and matrix mineral compositions are
compatible with the sample matrix assemblages and the
regional metamorphic setting.
These samples provide evidence of compositional
modification interpreted to result from fluid-assisted
metasomatism in metapelitic rocks at lower to middle
amphibolite facies. The extent to which processes more
rapid than volume diffusion can modify composition in
minerals places limits on the applicability of models based
on volume diffusion. Thus the validity of a volume dif-
fusion model to establish guidelines for which natural
minerals are safe to analyze in polymetamorphic rocks
(Jiang and Lasaga, 1990) relies upon the extent to which
volume diffusion is the fastest process for effecting a
change in a particular garnet sample. Tracy (1982) sug-
gested that hydration reactions can modify garnet rim
compositions more effectively than volume diffusion. This
study documents that these reactions occurred at am-
phibolite facies conditions and were metasomatic in the
sense of Lindgren (1928). This study also emphasizes and
expands a conclusion of Loomis (1983) and Whitney
(1991): postentrapment net-transfer reactions can occur
among garnet, primary mineral inclusions, and the ma-
tri if there are cracks or inclusion boundaries that pro-
vide a link with the matrix assemblage.
The data of this study are from two areas in the New
England Appalachians. In addition, previous studies in a
variety of metamorphic settings have documented zones
10-300 um wide enriched in fluid and mineral inclusions
that precede a reaction front. Zhou and Fyfe (1989) noted
that an inclusion zone precedes reaction fronts in altered
volcanic glass. Features analogous to the present study
have been reported in a separate part of Vermont by Ka-
rabinos (1984), in the Italian and Swiss Alps by de Be-
thune et al. (1975), and in the Dalradian of Scotland by
Yardley (1977), indicating that this style of garnet com-
positional modification is more extensive than previously
recognized.
Garnets from a prograde metamorphic zone in Ver-
mont and a polymetamorphic zone in Connecticut
changed composition by a similar, crystal-scale metaso-
matic process. This metasomatism dissolved garnet with
disequilibrium compositions and reprecipitated garnet
with a composition in equilibrium with the metamorphic
fluid and the evolving matrix assemblage. Fluid and min-
eral inclusions trapped within garnet and associated abrupt
compositional changes are the principal indications of this
process. The metasomatism also changed the composi-
tion of garnet along cracks that linked the garnet interior
with the matrix.
The research represented in this study was completed during the dis-
sertation work of the authors, and we gratefully acknowledge the advice,
guidance, and support of R.). Tracy and F.S. Spear in our respective work.
W,E,H. wishes to thank R.J. Bodnar for advice in this work and assistance
with the Raman and FTIR analyses. Comments and reviews from Donna
Whitney, Doug Smith, R.J. Tracy, Frank Florence, and K.V. Hodges pro-
moted considerable improvements in the final manuscript. Financial sup-
port was provided for W,E.H. through Connecticut and New York Geo-
logical Survey grants, and NSF grants EAR-88-16382 and EAR-86-96064
(Tracy). Financial support was provided for T.M. through NSF grant EAR-
89-16417 (Spear).
gramme (WCRP) with four components: data,
applications, mpacts and research. The World
Climate Research Programme was initially
established by the World Meteorological
Organization and the lnternational Council of
Scientific Unions (1CSU). n 1992, the ntergov
ernmental Oceanographic Commission (lOC) of
UNESCO became an additional sponsor. The
World Climate Research Prograrmme was given
specific objectives of determining:
Climate research then become a major
focus of international scientific activity. It was
recognized from the beginning that, to achieve
these objectives, it would be necessary:
The mandate of the WCRP is to study
climate variations and changes, both natural
and those induced by human activities, on time-
scales ranging from seasons to a century.
The scientific strategy of the WCRP has been
to build physicalmathematical models of the
coupled global climate system. There is a
need for process studies to improve under-
standing of key elements of the climate
system so that they can be adequately repre-
sented in climate models. There is a need for
global observations against which climate
simulations can be verified. The WCRP does
not attempt to organize all climate research
(let alone all atmospheric or oceanographic
research); instead the emphasis is placed on
those activities which require international co-
operation, either because of their global or
large regional scale or because of the human
and financial resource requirements. It is
recognized that much essential climate
research is being done-and is best done-in
small research groups at universities or
governmental institutes around the world.
The concept of time- and space-scales is
important for scientific analysis of the climate
system. There is a wide range of scales in the
climate system (Figure 2). We cannot hope to
represent the small scales explicitly in a
climate model, but it is necessary to include
their impacts through parameterizations. t is
useful to think of the climate system as being
divided into two components: the fast and the
slow climate systems. The fast climate system
is the atmosphere, the upper ocean (that part
subject to an annual cycle of vertical miing
due to wind) and the transient processes at
the land surface. The fast climate system is
active, driven by the atmospheric engine, and
comes to statistical equilibrium in a few years.
The slow climate system consists mainly of
the deeper ocean and the perennial land ice,
with a response time which may be decades
to centuries. The major interactions between
the fast and slow climate systems take place
in a limited number of areas where heat is
transferred by the up- or downwelling of ocean
water and at high latitudes, where cold, dense
water sinks to great depths. To a first approxi
mation, the magnitude of climate response to
a change in forcing, such as increasing green-
house gas concentrations or changes in solar
radiation, will be determined by the fast
climate system; the decadal rate of change
will be determined by the slow climate system.
The sponsors established the Joint
Scientific Committee (JSC) for the WCRP with
the mandate of providing scientific guidance,
determining the main research objectives,
reviewing and assessing the development of
the Programme, and giving overall co-ordina-
tion of efforts at the international level. The
members of the JSC are selected by mutual
agreement of the sponsors, on the basis of
their scientific knowledge, capability and
breadth of vision. The Director and the Joint
Planning Staff for the WCRP are responsible
for developing detailed plans for the WCRP,
within guidelines established by the JSC, and
following up with the implementation. Prior to
'Southern Africa experiences its worst drought
this century.'(BBC news broadcastt 22 April
1992)
Darkness falls on Colombia as lhydrolelectric-
ity plants run dry.'(The lndependent, London,
20 Apri1 1992)
Britain faces worst drought for centuries.
Prince Charles warns country is dying' from
water shortage.' (The Sunday Times, London,
24 May 1992)
These are but three climate anomalies noted
recently in the British news media. Earlier, there
were reports that Australia had experienced
severe drought in the latter part of 1991, while
the Gulf Coast of the USA was wetter than nor-
mal. Some people will recall something similar
hapening in 1986/1987 and those with a good
memory will remember the large-scale climate
disruption of 1982/1983.
The climate always seems to be anoma-
loUS SOmewhere, but there are times when it
seems to be more anomalous than others, and
the last several months is one such time, as
were the two episodes mentioned above. Part
of such behaviour is undoubtedly random and
not predictable. Flip a coin 100 times and you
might find a run of six heads. t is not instruc-
tive to ask why and-more to the point-
observing a run of six heads does not help you
to predict the next flip; it could just as likely be
a head as a tail. On the other hand, part of the
climate signal is not random, but results from
coherent interaction between the atmosphere
and the ocean. The largest such signal is
ENS0-El Nio/Southern Oscillation, where EN
refers to the oceanic component and SO to the
atmospheric component. TOGA was established
to observe the tropical regions, to investigate
why there are strong low frequency (three- to
fiveyear) climate signals there, and to try to
predict the coherent part of the signal.
Since space does not permit a thorough
review, the reader is referred to Glantz et al.
(1991) for a review of ENSO observations; to
released globally by deliberate or wild
biomass burning;
ln the aftermath of the fires, WMO re-estab-
lished the radiosonde and related meteorologi
cal observations facilities and telecom-
munications system in Kuwait and also estab-
lished three new GAW regional background air-
pollution monitoring stations. These are
situated in the downwind regions of the Gulf and
are located in lran, Pakistan (see photo below)
and on the Red Sea coast of Egypt. They moni-
tor atmospheric turbidity, precipitation chem-
istry, aerosol composition, black carbon,
surface ozone, UVE radiation and radiation bal
ance components, all of which, combined with
automatic station observations, are recorded
on computers for ready access.
Recognizing that the atmospheric mea-
surements collected in the Gulf region during
the Kuwait oil well fires constitute a valuable
resource for environmental and atmospheric
researchers, the Kuwait Data Archive (KuDA)
project to be located at NCAR (Boulder, Col
orado) has been established. t is planned that
this resource will be easily accessible and will
contain all pertinent data sets with a database-
management system providing software tools
for accessing the database.
The results of these two meetings and
concrete augmentation of both the GAW and
the WWW facilities in the region affected by the
fires demonstrate how an international scien-
tific effort, co-ordinated by WMO, can be highly
successful. Through this event, much has been
learned that can be applied in any future atmo-
spheric accidents.
After the Atmospheric Transport Model Evalua-
tion Study (ATMES) in which model calculations
were compared against the accidental release
of radioactive material, t was realized that fur-
ther testing of models was necessary and a
follow-on project was proposed: the European
tracer experiment (ETEX). The experiment will
consist of three distinct parts: the release of
an inert tracer sometime in early 1994, appli
cation of realtime models during the release,
and, when all the meteorological and tracer
data are available, a detailed model evaluation
will be performed along the lines of AIMES. It
is sponsored by the European Community, the
International Atomic Energy Agency and the
World Meteorological Organization.
At the consultation of experts held in the
WMO Secretariat (Geneva, 1-4 June 1992),
representatives of the Meteorological Services
which will participate in ETEX were brought
together for the first time. The ground pro
gramme of tracer release was discussed and
plans were initialized.
The tracer will be the same material that
was used in the long-range experiments con-
ducted in North America in 1983 and 1987. It
comes from a family of man-made perfluoro-
carbons that are inert in the troposphere and
have a very low background concentration.
This allows a release of the tracer at low con-
centrations and detection at thousands of kilo-
metres from the source. Over 200 synoptic
stations were chosen as sampling sites, rang-
ing from France in the west, to Poland and
Romania in the east. Collectors will be pro-
vided, consisting of a pump, a set of ion
exchange columns and a timer. The air is
At the invitation of the Government of Para-
guay, a WMO/North American Plant Protection
Organization symposium/workshop on meteo-
rology and plant protection was held in Asun-
ci6n from 1 to 10 April 1992.
The programme included sessions on:
There were about 40 participants frorm
Regions ll and V.
ln collaboration with the Technical Centre for
Agricultural and Rural Cooperation (CTA) and
FAO, WMO organized a workshop on the dis-
semination of agrometeorological information
by rural or national radio, television or the writ
ten press. At the invitation of the Government of
electricity production, medical treatment, envi-
ronmental research and other peaceful pur-
poses, Many countries were now applying
nuclear techniques to solve practical ecological
and social problems, and, through international
cooperation, progress had been made in the
areas of safety and waste management.
He said that countries would have to
assess carefully what mi of energy sources
was compatible with sustainable development
and what was the optimum use of energy. The
burning of fossil fuels, which made up about 90
per cent of the world's commercial energy was
also the cause of some of the world's most seri
oUs environmental problems. The use of nuclear
power would have to be part of the attempt to
meet growing energy needs in a safe and envi-
ronmentally sustainable way.
The AEA issued a booklet for UNCED entitled
Nuclear Power, Nuclear Techniques and Sus-
tainable Development. t contains sections on
energy use and the environment (energy sup-
ply/demand, radioactive waste and nuclear
plant safety) and on applications of nuclear tech-
niques to protect the atmosphere, manage
ecosystems, preserve the quality of water
resources, satisfy basic health needs and pro-
mote sustainable agriculture.
Single copies of this booklet are available upon
request from the AEADivision of Public nfor-
mation, P.0. Box 100, A-1400 Vienna, Austria.
Tel.: 431)23601270. Fa: 431)234564.
Telex: 1-12645.
Under an AEA project in Poland, electronbeam
(EB) technology is being applied to temove flue
gaS emissions, which are linked to acid rain. At
a pilot plant near Warsaw, it has been demon-
strated that approximately 80 per cent of the
sulphur dioxide and 90 per cent of the nitrogen
oxide can be removed from combustion flue
gases.
The technology consists in using electron
accelerators to irradiate the smokestack
exhausts of coalfired power plants, and to
transform sulphur dioxide and nitrogen oxide, in
the presence of ammonia, into ammonium sul-
phate and ammonium nitrate, which can then be
used as fertilizer.
More information may be obtained from the
Division of Physical and Chemical Sciences,
IAEA, P.0. Box 100, A-1400 Vienna, Austria.
Tel.: (431)23601270. Fa: 431) 234564.
Telex: 1-12645.
''Only when international research networks and
institutions have a welldefined funder whose
interest is international, not national, whose
agenda is dictated by international scientific
concerns, not domestic concerns, will the
global change enterprise flourish.'
So spoke Dr Peter E. de Jänosi, Director of
the lnternational lnstitute for Applied Sys-
tems, at the close of the conference held
from 12 to 13 May 1992 to mark its 20th
anniversary
The conference, which attracted more
than 300 experts from some 25 countries,
including policy-makers and scientists from
many different disciplines, centred on a num-
ber of workshops concerned with energy use
brighter trails. For example, for a 1'.0 increase in apparent
visual brightness, and a factor of -2.5 increase in pixel
responses, the tuning distance can be set at --86,700 km.
This distance corresponds to an angular scan rate with re-
spect to the celestial sphere of -110 uradls, (i,e., leading
Earth's rotation by -40 urad/s). However, the detection vol-
ume (see Figure 8) decreases severely with decreasing tuning
distance [hates, 1989], and the detection rate per frame is
less by a factor of --4.
The reduced mass estimate is important in assessing the
physical state of the water vapor responsible for the transient
absorption of ultraviolet dayglow observed as atmospheric
holes with Dynamics Explorer 1. Specifically, the mass of
the small comets is originally estimated by Frank et al.
[1986b] with the assumption that the water vapor in the
obscuring cloud is composed of unbound water molecules.
As discussed in section 2, the content of the water vapor
cloud may consist of dimers and molecular clusters, in which
case the ultraviolet absorption cross section is expected to
increase significantly with respect to the corresponding mul-
tiple of the single-molecule cross section. The mass of water
vapor required to produce an atmospheric hole is corre-
spondingly reduced to values in the range of the masses from
the direct telescopic sightings.
The brightness of a small
comet after fragmentation is a subject of considerable interest
because of the possibility of visual detection by a ground
observer positioned at predawn or postdusk local times while
the ice cloud is illuminated by sunlight at higher altitudes.
As a benchmark value, we note that the apparent visual
magnitude of the small comets just prior to breakup at an
altitude of --1000 km is M, 2 8 if we use the telescopic
Where does the water come from, where does it go, and
what happens along its path? These seemingly simple ques-
tions about the oceans' (semi-) permanent circulations are
at the heart of physical oceanography. Answers are sought
from hydrographic data and direct current observations and
tracer measurements, and ideas are dissected with analytical
fervor. The North Atlantic Ocean is the most completely
observed and extensively studied of all the world's oceans,
and yet it still resists thorough description and rationaliza-
tion, More observations and higher-resolution models using
cutting edge supercomputers have helped but have not yielded
satisfaction that an accurate picture is at hand. However, it
is now possible to present a replacement for the North Atlantic
circulation scheme hypothesized by Worthington [1976]
(hereinafter W76) and resolve many of the transport dilem-
mas he highlighted. Basically, what we do is describe a
general circulation that is compatible with community wis-
dom as opposed to one author's idea or the results of one
approach or method.
The difficulties experienced by W76 and us in producing
a circulation scheme arise to a large extent because there are
only a few circulation components that are quantitatively
tightly constrained. That is, we rarely are able to measure
transports very well. Uncertainties in transport estimates are
typically at least 5-25%, with the smaller error unique to
well-defined channels like the Straits of Florida. Careful
treatments of individual basins or smaller regions may not
be particularly compatible when merged, even though the
individual treatments may each appear ''right.'' Ror many
locations the intensity of the general circulation is obscured
by the presence of mesoscale eddies and recirculating gyres,
and choice of ''reference level'' is usually arbitrary. There
are, however, a few circulation elements that we feel are well
constrained by the observations. The northward transport of
the Florida Current through the Straits of Florida is close to
30 Sv, and approximately 45% is from the South Atlantic.
The net transformation of warm water to cold water in the
North Atlantic is 13 Sv. The rate of production of dense cold
overflow waters in the Nordic Seas is 6 Sv. Downstream from
the Straits of Florida, the 30 Sv transported as the Florida
Current is increased to roughly l00 Sv by recirculating gyres.
Our goal is to combine these four circulation elements with
what else is known of the distribution of currents and water
masses to produce a circulation scheme for the North
Atlantic.
Schmitz and Richardson [1991] (hereinafter SR91) re-
caf and Stalcup, 1967]. This is clearly true, with only about
1 Sv (comparatively fresh) in the 12'-24C range (the un-
dercurrent temperature (T) interval) moving into the Straits
of Florida from the South Atlantic. However, about 7 Sv of
water in the upper 50-100 m of the water column (T > 24C)
above the undercurrent depth and T/S range, and 5 Sv (7?
e: T e 12%C) below the undercurrent TS range, can move
into the Caribbean from the South Atlantic and out with the
Florida Current [SR91; Schmitz et al., 1993]. Worthington
[1976] closed the Florida Current (30 Sv) within his sub-
tropical gyre, which is confined essentially to the southern
recirculation of the Gulf Stream west of the Mid-Atlantic
Ridge.
Another low-latitude transport dilemma is associated with
the Sverdrup relation [Böning et al., 1991; Leetmaa et al.,
1977; Leetmaa and Bunker, 1978; Roemmich and Wunsch,
1985; Wunsch and Roemmich, 1985]. Sverdrup dynamics,
relating the Sverdrup transport distribution to the curl of the
wind stress, is a good first approximation for the currents in
the interior of oceans and is the cornerstone of widely ac-
cepted ideas about the wind-driven circulation. The reader
interested in pursuing this topic could start with Stommel
[1957, 1965], Wunsch and Roemmich [1985], and Schmit:
et al. [1992] including references. The question typically
posed is whether or not the well-established 30 Sv or so
flowing north through the Straits of Florida [Schmitz and
Richardson, 1968; Richardson et al., 1969] is balanced by
a southward Sverdrup transport in the interior North Atlantic,
and the answer is controversial. A partial resolution [Schmitz
et al., 1992] of this quandary appeals both to SR91 and to
the idea that the Sverdrup transport contribution to the Florida
Current is coming from east of roughly 55'W along 24N.
Schmitz et al. [1992] suggest that the more eddified ther-
mocline layer flow field west of 55%W is'associated with a
smaller-scale recirculation overlying a strong deep western
boundary current (DWBC) system.
The comparison of the geostrophic flow that would result
from the Sverdrup relation using known mean winds with
the geostrophic flow calculated from observed hydrographic
data and a reference level estimated by inverse techniques
by Roemmich and Wunsch [1985] indicated that at about
55%-60 W along 24N the theoretical and observed curves
begin to diverge. East of this longitude the calculated and
observed geostrophic transports agree to within a few sver-
drups for a total of 20. West of this line where the observed
geostrophic currents are varying on much smaller zonal
scales than the calculated transports and may be strongly
time dependent, the disagreement may reach 15 Sv east of
55 W the Sverdrup transport is 17 Sv, comprised of a north-
ward Ekman transport of 3 Sv and a southward interior
the layers. Cation substitution in phyllosilicates is a special
form of isomorphous substitution of a higher charged cation
by a lower charged one (e.g., Li' rcplacing Mg ', Mg'
replacing Al', or AP'* replacing Si'*). The negative layer
charge resulting from cation substitution is often times called
structural or permanent charge, the latter because the particle
charge is not dependent on the solution pH.
When the magnitude of the negative layer charge is
relatively low and the chemical potential of water high, the
phyllosilicate expands or swells as inter layer cations hydrate.
Swelling behavior and cation exchange capacity attract the
interest of mineralogists and chemists because they are the
source of many important chemical and physical properties.
Reference to octahedral and tetrahedral sheets (Figures 3
and 5), the basal-oxygen plane; i.e., the oxygens at the base
of the tetrahedral sheet (Figures 3 and 5), cation substitution,
and interlayer cations will appear time and again in the com-
ing sections. Because phyllosilicates are made up of sheets
of coordination polyhedra, clay mineralogists often refer to
all phyllosilicates comprised of one tetrahedral and one octa-
hedral sheet as ''1:1'' phyllosilicates (Figures 3 and 4). Min-
erals like pyrophyllite, the micas, talc, and the smectites
(Figures 5 and 6) are comprised of one octahedral sheet
between two tetrahedral sheets. These are referred to col-
lectively as ''2:l'' phyllosilicates. With the basics of nomen-
clature and structure defined, we can turn to the first topic:
quantum chemistry.
Quantum chemistry is a broad and often complicated sub-
ject. I will restrict this discussion to those concepts and
methods that are necessary to understand the quantum chem-
ical literature as it relates to phyllosilicates. The most popular
quantum chemical computing techniques are classified as
either ab initio or semiempirical. The former attempt, with
varying degrees of rigor, to explicitly compute the electronic
states of the material, while the latter simplify the consid-
erable computational task by a variety of semiempirical
expressions and the neglect of certain terms. Given the enor-
mous computational demands of ab initio methods and the
complexity of phyllosilicate structures, complete and rigor-
ous quantum chemical calculations of phyllosilicates are and
will remain impractical for some time to come. In the mean
time, most studies will use semiempirical methods.
The basic approach is to represent the electronic states of
a material, molecule or solid, using one-electron orbitals [,)
that are themselves a linear combination of atomic orbitals
LCAO):
The atomic orbitals (x.) in (1) are centered at atomic
positions. The molecular (crystal orbitals) [9,) extend over
the whole molecule (crystal). Ab initio methods construct
an antisymmetrized product of the [g,). Semiempirical
methods generally represent electronic states as they appear
in (1).
LCAO quantum chemical calculations solve an eigenvalue
problem.
The eigenvalues [e,) are the energies of the orthonormal-
ized one-electron states [g,}. The basis is the set of atom-
centered atomic orbitals [x,.).
Equations (2) of the LCAO quantum chemical eigenvalue
problem are used to generate a series of secular equations,
called Roothaan equations in honor of the scientist who sug-
gested the LCAO method. The power of this approach derives
from the matrix methods used to solve the Roothaan equa-
tions:
Evidently, organic loading into these systems is small in
comparison to inorganic loading. Prhaps this is not sur-
prising, inasmuch as suspended material delivery (with sus-
pended material contributing, on average, half of the organic
loading; Table 2) by northern European rivers to the ocean
is small [Milliman and Meade, 1983].
Nevertheless, we accept the data in Table 5 as our best
estimate of the relationship between primary production and
respiration in the coastal ocean. Clearly, more data are
needed to test this relationship further. With that caveat, (1)
can therefore be used along with the estimated primary pro-
duction rate of estuaries and the continental shelf in order to
calculate net system metabolism for these portions of the
coastal ocean.
Estuaries
Continental shelves
By inspection of Figure 1 we conclude that (P-R) at the
primary production rate estimated for estuaries (300 g C m
yr) is significantly less than zero. Estuaries apparently
respire an average of about 20% more organic carbon than
they produce. If we accept this conclusion that estuaries are
net heterotrophic, then there must be an external source of
organic matter. Within the resolution of this analysis, con-
tinental shelves respire about as much organic matter as they
produce; we assume (P-R) for shelves is indistinguishable
from zero. We cannot rigorously evaluate the error in these
estimates of (P-R), but we suspect that the numbers are
accurate to about s: 50%,
The data on delivery, burial, and metabolism of organic
carbon can now be synthesized into an estimate of organic
carbon flux in the ocean (Figure 2). First, let us consider
the coastal ocean.
Primary production of salt marshes, the open water area
of estuaries, and the continental shelf totals about 500 x
10' mol Clyr. About 80% of this primary production occurs
in the water column and 20% occurs on the bottom. Res-
piration exceeds primary production in the coastal ocean
(estuaries plus continental shelf) by about 7 x 10' mol C
yr. This number is reasonably close to the estimate, derived
fromIttekkot [1988] and Spitzyand Ittekkot [1991], that about
10 x I0 mollyr of terrigenous organic matter reaching the
ocean is labile on a time scale short relative to exchange
with the open ocean. When one uses the primary production
and net metabolism data, respiration in the coastal ocean is
thus estimated to be about 507 x 10 mol C/yr. About 30%
of the respiration occurs on the bottom, and the remainder
occurs in the water column. Respiration in the coastal zone
apparently exceeds primary production by about 1.4%.
Net heterotrophy of the coastal ocean is supported by input
of organic material from land. This input totals about 34 x
10% mol Clyr. From Berner's data, about 9 x 10% mol Cl
yr are buried in the coastal ocean. If the input, burial, and
net metabolism of the coastal ocean are all accurate, then
about 18 x 10'4 mol CIyr are not directly accounted for.
This material is assumed to be oxidized on a longer time
scale than the turnover of the labile organic carbon pool of
Ittekkot [1988] and Spitzy and Ittekkot [1991]. This estimate
is consistent with the conclusion that much of the POC de-
livered from land is converted to DOC and slowly oxidized.
The time required for this oxidation is apparently long in
comparison with the water exchange time between the coastal
ocean and the open ocean but short in comparison with the
calculated turnover time of organic carbon in the ocean
(4000-8000 years). Fx budgetary purposes we assign this
slow respiration to the open ocean.
Open ocean primary production is estimated to be ap-
proximately 3600 x 10'3 mol Clyr. Martin et al. [1987] used
sediment trap data to estimate that open ocean new produc-
Evidence from a variety of angiosperm species shows
that sperm cells maintain a close association with the
vegetative nucleus (VN) and each other as they move
down the pollen tube. These observations have led to
the proposal that the sperm and VN move as a Male
Germ Unit or MGU and that the association may be
important for successful transmission of the male ga-
mctes (Russell and Cass 1981; Dumas et al. 1985;
McConchie et al. 1985; for review, see Knox et al. 1988;
Russell et al. 1990; Mogensen 1992). Arguments against
the universality of such an association have also been
presented however (for discussion see Heslop-Harrison
et al. 1986).
The mechanisms by which these interactions are es-
tablished and maintained are unclear. In a number of
species with tricellular pollen, the association is clearly
present in the mature grain (e.g. Russell and Cass 1981;
Dumas et al. 1985; McConchie et al. 1987). On the other
hand, connections can also be established after pollina-
tion (Mogensen and Wagner 1987). Many studies have
focused on interactions between the VN and sperm cells
(see Russell et al. 1990), but involvement of the anteced-
ent generative cell (GC) has received attention as well
(Derksen et al. 1985; Mogensen 1986a, b; Kaul et al.
1987; Heslop-Harrison et al. 1988; Hu and Yu 1988:
Wagner and Mogensen 1988; Taylor et al. 1989). Avail-
able information suggests that an association with the
VN is present in the mature pollen grain but may become
more complex after division (Mogensen 1986a, b;
Wagner and Mogensen 1988; Kaul et al. 1987). Further-
more, the relative positions of the GC and VN are vari-
able between species and within a given pollen tube (e.g-.
Venema and Koopmans 1962; for review, see Heslop-
Harrison and Heslop-Harrison 1989a), lending addition-
al mystery to their relationship. A higher density of pores
The association is based at least in part on embay-
ments in the VN occupied by extensions at either or
both ends of the usually fusiform-shaped GCs and sperm
(for reviews, see Knox et al. 1988; Russell et al. 1990;
Mogensen 1992; Palevitz and Tiezzi 1992). Angiosperm
GCs and sperm also contain prominent Mt arrays in
the form of highly cross-bridged bundles (Palevitz and
Tiezzi 1992) that continue into the extensions.
concerned, from rudimentary ovaries with no style and stigma to
more developed ovaries bearing a style of different lengths up to
pistils with a long style and stigma, often evoking, in spite of their
sterility, female pistils. All of the flowers of one plant are, however,
strictly of the same type. Occasionally (from about 0.1%4 to 4%,
according to the cultivar), a few male plants bearing flowers of
the last type can also produce a certain number of berries (andro-
monoecious plants). On the contrary, to our knowledge, female
plants also bearing hermaphroditic flowers (gynomonoecious
plants) have never been reported.
A better knowledge of the genetic and/or environ-
mental factors controlling the development of the male
pistil in male flowers and of the behaviour of the differ-
ent types of male flowers in relation to pollination could
provide a glimpse to the origin of dioecy in Asparagus,
as well as further clarify whether, besides the primary
regulatory genes involved in sex determination, other
genes are involved in the inheritance of sex characters,
as first suggested by Franken (1970), with reference to
the heritability of andromonoecy. The investigation pre-
sented here was therefore mainly aimed at determining:
(1) whether and to what extent the character ''length
of the style'' in male plants is genetically determined;
(2) whether the general lack of fecundation in male ovar-
ies is due just to the absence of normal female gameto-
phyte inside the ovule, or if a mechanism inhibiting pol-
lination is already operating at the level of stigma or
style; (3) if there is always a correlation between the
length of the style, the presence of a stigma and the
extent of development of the megagametophyte in male
ovules.
Asparagus plants were grown at the Horticultural Research Insti-
tute of Montanaso Lombardo (Milan). Anthers for in vitro culture
were collected at the early uninucleate microspore stage from sever-
al male plants exhibiting a good aptitude to androgenesis (cvs'Ear-
ly of Argenteuil' and 'Lucullus') and cultured in vitro according
to previously described methods. (Falavigna et al. 1990; Qiao and
Falavigna 1990). After 25-40 days, embryos from the anthers were
transferred to T1 medium (Qiao and Falavigna 1990) where they
proliferated into morphogenic calli, The calli, divided into small
pieces and subcultured 2-3 times every 15 days, were induced to
root, and the rooted plantlets were transferred to the greenhouse.
Of these, 58% were diploid. Seven doubled haploid male plants
and seven females were subsequently selected as parents of the
F, all-male hybrids.
All YY and XX plants obtained from in vitro anther culture, F,
and backcross (XY males from F, x XX parental female) progenies
were grown in a greenhouse. During flowering the temperature
was maintained at 26* - 2 C during the daytirme under natural
light; at night the temperature was lowered to 188+- 2% C.
To visualize the growth of the pollen tubes within the styles, female
and male flowers (bearing pistils with a style 0-1.4 mm long) from
plants of a selected backcross, called backcross E (male parent:
doubled haploid from cv 'Lucullus'; recurrent female: doubled
haploid from cv 'Early of Argenteuil)', were hand pollinated and,
after 24 h, the pistils were collected. To test whether self-pollination
occurs spontaneously, pistils were also collected from opened flow-
ers of male plants that had been isolated for 72 h in insect-proof
cages, All of the pistils were fixed in FAA (formalin, 80%4 ethanol
and acetic acid at a 1:8:1 ratio) for 24 h, washed thoroughly under
running water, transferred to 8 N NaOH for 12 h, washed and
stained with 0.1% aniline blue in 0.1 N K,PO,. The pistils were
gently squashed under a coverslip and scored for pollen tubes with
a fluorescence microscope (excitation filter 350-460 nm, barrier
filter 515 nm).
Female pistils and male pistils of the different types (with a style
0, 0.2, 0.6, 1.4mm long) were fixed in 2.5 M, glutaraldehyde in
0.1 M phosphate buffer, washed, dehydrated and embedded in
epon araldite (Mollenhouer 1964). Serial sections (1 um thick) were
cut with a Reichert ultramicrotome and coloured with 2% Azur
In order to visualize the vascular elements, female pistils and
male pistils of different types were cleared and stained in fuchsin
as previously reported (Bracale et al. 1990).
Seven doubled-haploid Y Y male plants were crossed
with different homozygous females also obtained from
anther culture to yield seven different XY, all male, F, s.
Some of these plants were in turn crossed with the female
parent to give a backcross (BC , ) progeny. Two of these
seven crosses (specifically meant for use in a RFLP anal-
ysis in search for restriction markers localized on the
sex chromosomes) were particularly interesting with re-
spect to the present study: these were named cross E
and cross G. The male parent in cross G had pistils
with a long style (average length: 1.4 mm) and showed
the presence of stigma (incidentally, this shows that fac-
tors affecting style length and stigma development are
not localized on X chromosome). The male parent in
cross E exhibited, to the contrary, a reduced ovary, com-
pletely devoid of style and stigma. The male plants of
the F, (39 individuals of cross E and 43 individuals
of cross G) consistently had styles of an intermediate
length (0.5-+-0.07 mm for cross E, 0.65-4-0.05 mm for
cross G. Apparently in cross E factors for the presence
of the style had been introduced by the female parent.
Interestingly enough, when F, male plants were crossed
with the female parent, the character ''length of the
style'' segregated in the male plants of the backcross,
which exhibited a great variation in lengths of styles,
from 0 to 1.4 mm (Fig. 1). The length of the style was
always correlated with the overall size of the ovary. In-
side each male plant, however, style length and ovary
dimension were strikingly uniform. We focused our at-
tention on cross E, and only these results will be referred
in subsequent experiments and discussions, even though
data from cross G were quite similar. Backcross E gave,
ent tectum formation (Fig. 6). Detailed descriptions of
pollen wall synthesis during normal development are
presented elsewhere (Majewska-Sawka et al. 1992),
hence this report will center on the most important
events and their relation with aberrant wall development
around MS microspores.
Tapetum. During the tetrad period, the mitochondria
show a further reduction in size (Table 1), and the plas-
tids seem to present greater morphological variability.
In addition to the forms observed during meiosis of
MMCs (Fig. 3) another plastid type with more lightly
stained stroma and long, narrow, irregularly arranged
lamella appear. Figures of dividing plastids are seen fre-
quently. Occasionally, concentric arrangements of ER
are noted. In depressions formed between the plasma-
lemma and tapetal cell walls, large numbers of spherical
electron-grey pro-Ubisch bodies accumulate (Fig. 4).
tr/p'-S1-2 is a B. napus plant derived from selfing tr/p ', the fe-
male in the cross with pol-1 that gave rise to the first sexual cybrids
(Erickson and Kemble 1990). The tr designation indicates that the
plant carries the B. campestris cytoplasm with the mutant chloro-
plast gene conferring resistance to triazine herbicides (Reith and
Straus 1987). tr/p'-S1-1 was a B. napus plant closely related to
tr/p''-S1-2, Both were derived from a line which had lost the
11.3-kb mt plasmid characteristic of this cytoplasm during in vitro
culture. The plasmid had been restored to the cytoplasm of one
line (p'') following a sexual cross to a male carrying the plasmid,
but not to the other line (p ) (Erickson et al. 1989).
Westar-1, Westar-2 and Regent-2 are random plants from the Ca-
nadian B. napus cvs ' Westar ' and ' Regent '; both contain the
normal napus cytoplasm that does not confer triazine resistance.
Westar-1-S1-1 was a plant derived by selfing Westar-1. Similarly,
'Marnoo' and 'Andor' are normal B. napus cultivars of rapeseed.
'Bronowski' is a B. napus variety with B. campestris cytoplasm,
but is not triazine resistant; this particular accession of ' Bronows-
ki' does not carry the mt plasmid.
The data for this report are drawn from three separate pollen
transmission studies: PTS-1, -4, -8. Cross number one in Pollen
Transmission Study One would be designated PTS-1-1 and progeny
plant number 10 from that cross would be designated PTS-1-1-10.
As it is possible to maintain a B. napus plant for several years
because of its indeterminate growth habit, the same plant could
be used in subsequent experiments. Pol-2, for example, was used
as a male in PTS-1-1 (Table 1A) and several months later in PTS-1-
35 (Table iC).
The purpose of crosses in PTS-1-1, -1-2, -1-3, -1-4 (Table 1A) was
to compare the rate of pollen transmission between crosses. The
particular genotypes in PTS-1-1 (tr{p-S1-2x pol-2)had given rise
to cybrid progeny in previous experiments (Erickson and Kemble
1990), and we wished to determine if transmission could be
achieved with other genotypes by varying the male of the cross.
Cross PTS-1-1 also served as a positive control; the transfer of
paternal mitochondria to the female plant in this cross would elimi-
nate the possibility that a failure to observe pollen transfer in the
other crosses to this female was due to the female genotype and
not the male genotype. For statistical analysis the RFLP data from
the crosses in Table 1A was organized into a 2x 2 contingency
table. The two columns were the number of progeny with maternal
and mixed restriction patterns of mtDNA. The two rows consisted
of data from PTS-1-1 and the pooled data from the other three
crosses. A chi-square statistic was used to compare the differences
between the two groups of crosses with respect to cybrid progeny.
Crosses PTS-1-5 and -1-6 were reciprocals of PTS-1-1 designed
to test whether the direction of mitochondrial transfcr could bc
reversed using the same plants (Table 1 B). The males of these two
crosses are near-isogenic lines differing mainly with respect to the
presence of the mt plasmid, ie. tr/p'' versus tr/p'. In crosses PTS-
1-7. -1-8, -1-9 pol-2 was again used as a female, but with males
other than those carrying the tr cytoplasm (Table 1B). As above,
a contingency test was used to compare the frequency of cybrid
progeny in PTS-1-1 (Table 1A) with the pooled freuency of all
the crosses in Table 1B.
The purpose of crosses tested in Table C was to compare the rate
of pollen transmission between a cross where it was first observed
(PTS-1-10) in previous experiments (Erickson and Kemble 1990)
and other crosses by using the same male as in PTS-1-10, but
varying the female genotype. PTS-1-10 served as a positive control
since failure to observe pollen transmission in the other crosses
could not be attributed to the male if cybrid progeny were observed
in the control cross.
Detection of paternal mitochondria
Microspore culture
Role of the male genotype
The results from cross PTS-1-1 (Table 1A) were similar
to those presented in a previous report (Erickson and
staining with 4',6-diamino-2-phenylindole (DAPI) (Coleman and
Goff 1985). The FCR assay as described by Heslop-Harrison et al.
(1984) was used to assess the viability of the sperm cells. The pres-
ence of a cell wall was tested for by calcofluor white and analine
blue staining (Tanaka 1988).
The weight of one anther was 0.4 mg30.03 (SE), and the mean
number of pollen grains/anther was determined to be 5853 214
(SE), which was approximated to 6000. The number of sperm
cells present in the starting material was determined by multiplying
the number of milligrams of anthers by 3 > 10' (2.5 s 6000 x 2
sperm cells/pollen grain). After isolation 95-100% of the sperm
cells observed with phase contrast microscopy were FCR positive.
Therefore, the number of sperm cells counted in a hemocytometer
using phase contrast microscopy was considered to be indicative
of the number of viable sperm cells. The number of isolated sperm
cells from the pollen grains was estimated by dividing the number
of counted sperm cells by the calculated number of sperm cells
in the starting material times 100%.
pH and osmolality were varied to assess the optimal osmotic shock
conditions for sperm cell release from the pollen grain. The pH
of the isolation medium was varied in a range of 5.0 to 8.0 at
780 mosmol/kg H4O. The buffer at pH 5.0 and 5.5 was 5 mM
MES (2(N-Morpholino) ethane sulphonic acid). For higher pH
valucs thc buffcr was 5 mM MOPS. For sperm cell isolation at
different pH values, the concentration of fetal calf serum in the
storage medium was 0.83 (y/v) instead of the optimal concentra-
tion of 6.5%4 (v/v) as described in the isolation procedure. The
osmolality of the isolation medium was varied with sucrose 10-
20% (w/v) from 360 mosmol to 780 mosmollkg H4O at pH 6.0.
The effects of pH and osmolality on sperm cell isolation were
measured by estimating the number of disrupted pollen grains in
relation to the number of intact viable pollen grains x 100% after
osmotic shock, and by determining the yield of viable sperm cells.
exemplified in Fig. 2 is applicable in
this case' and others'': soil moisture
(water-filled pore space) controls both
(1) the specific ratios of NO and N4O
fluxes and (2) the contributions from
nitrifying and denitrifying bacteria.
The findings of Davidson et al.',
however, disagree with the results
from studies of other tropical soil
systemg'-' jn which the fluxes of
either NO, N4O or both were derived
exclusively from denitrification. While
these discrepancies have yet to be
clarified, one possible explanation
for these conflicting results may be
the difference in soil pH. The pH of
the soil studied by Davidson et a!,%1
is 6,4-7.8, while that of soils in
Amazon rainforest and other ecosys-
tems north of Manaus, Brazi'-1
and in the Guayana Shield, Vene-
zuela' are 2.6-4.0 and 4.6-5.2
respectively. The source of NO in
the acidic tropical soils may be more
clearly understood from results of
inhibition experiments with acety-
lene that are similar to those con-
ducted in Mexico?. On the basis of
the information from laboratory
studies', it was pointed out'? that
the production of N4O by chemo-
autotrophic nitrifiers may be insig-
nificant when the soil pH is below 6.
It would be worth examining closely
the lower limit of pH that supports
the activity of chemoautotrophic
nitrification in soil systems.
EcouOGICAL ASSESSMENT - predicting
or determining the effects of human
activities on natural populations - is
assuming increasing importance in
decision-making. Like other applied
sciences, it also has the potential to
contribute to the basic principles it
draws on, Some assessments can
be seen as quasi-experiments',
where ecological theories can be
tested and improved by comparing
predictions to results, with fewer
assumptions about extraneous vari-
ables than for observational studies,
though more than for experiments.
Indeed, assessments and deliberate
manipulations, on spatial scales too
large to allow the replication and
randomized assignment on which
many statistical analyses are based,
rmay provide information unobtain-
able otherwise'.
The value of the contributions, to
both decision-making and basic sci-
ence, depends on the reliability of
the determination of effects. Un-
fortunately, the most urgent and
dramatic assessment problems
often have the least reliable determi-
nations. They may be too wide-
spread to admit comparison or
'Control' areas (e.g. global warm-
ing), or too unexpected to allow
baseline ('Before') data to be gath-
ered (e.g. oil spills). Determining
what would have happened, had the
human activity not occurred, may
then require the modeling of pro-
cesses driven by large numbers of
variables (many of them unob-
served) con-nected by unknown
relationships, often nonlinear and
sometimes sensitive to small errors
in functional form or variable values.
Greater scientific benefits may
arise from assessments of small-
scale planned activities, which can
avoid these problems. The existence
of 'Before' and 'Control' data may
permit the use of simple models
whose errors are small compared to
the effects of concern. Such tasks
arise in the work of public agencies
granting permits or enforcing regu-
lations.
In practice, the scientific value of
these assessments is often low, in
part because the applicants hire the
investigators. But alternative struc-
tures are possible. The California
Coastal Commission (CCC), in allow-
ing Southern California Edison
(SCE) to expand its San Onofre Nu-
It is becoming well established
that there has been a progressive
decline in mean global oceanic tem-
perature over the last 100 million
years''; As continents assumed
their present day positions there
was a fundamental rearrangement
of oceanic circulation systems and
a dramatic steepening of polar-
equatorial temperature gradients.
Latitudinal climatic zones and their
attendant biotic provinces are as
distinct today as they have ever
been'. With the progressive reduc-
tion in the area occupied by tropi-
cal biotas, there was a concomitant
increase in their polar counter-
parts. Many marine and terrestrial
clades diversified through the
Cenozoic within these newly devel-
oped climatic zones and it is usual-
ly assumed that they did so in
comparative isolation. Interchange
of taxa between high- and low-
latitude regions would appear to
have been very much more difficult
through the Cenozoic era'.
Nevertheless, the overall tempera-
ture decline through the Cenozoic
was not a simple, smooth one. In a
discussion of climatic changes and
their likely biotic effects in the high
southern latitudes, Clarke and Crame'
identified a sequence of ten major
cooling and warming trendst* (Fig.
1). In addition to these longer-term
trends, which occurred on a time-
scale of 10%-10' years, we now know
that there were also smaller-scale
ones occurring on a timescale of
10'-10' years. Such individual
climatic cycles can be linked to
orbital (Milankovitch) cyclicity, and
in the last 2.4 million years alone
there may have been as many as 50
complete cycles'''. Evidence from
the deep-sea record suggests that
high-frequency Milankovitch-type
cycles are detectable in the latest
Miocene-earliest Pliocene, middle
Miocene and earliest Oligocene
epochs'. They are represented
within Mesozoic sequences too'',
and curves such as that depicted in
Fig. 1 would almost certainly have a
sawtooth aspect at a finer scale.
Evidence is accumulating that
there may have been times of sig-
nificant biotic interchange between
the high and low latitudes over
approximately the last 100 million
years. The taxonomic composition
of certain Cenozoic and living
rmarine invertebrate assemblages
may hold some important clues
here, with those of the highest lati-
tude regions being particularly rel-
evant. Possible longer-term effects
of repeated latitudinal range ex-
pansions and contractions should
also be assessed.
A widely held view of the living
Southern Ocean benthic fauna is
that it is the product of evolution in
isolation over long periods (often
held to be the greater part of the
Cenozoic eray'' This view is
within the deep-sea record of the
North Atlantic Ocean. Morozovellid
and globigerinid foraminifera
ddefine at least nine acme events
within the Paleogene sub-era, and
both these and other calcareous
nanoplankton taxa suggest that
there were at least eight further
events in the succeeding Miocene
epoch''. Substantial latitudinal
shifts of both coccolithophorids and
foraminifera have been tracked
over the last 225 000 years'
Although the true nature and scale
of biotic interchanges between the
high and low latitudes will only
become apparent when further tax-
onomic and phylogenetic studies
have been completed, it is import-
ant to emphasize how frequent
they may have been. As our knowl-
edge of high-latitude palaeocli-
mates has increased, it has become
possible to estimate meridional
temperature gradients through
time'% (Box 3). Unfortunately,
these estimates are necessarily both
incomplete and open to interpreta-
tion; this is especially so after the
establishment of a major East
Antarctic ice sheet (up to 36 million
years ago), when the volume of iso-
topically light water locked up in
the ice cap affects palaeotempera-
ture calculations. Nevertheless,
conservative estimates suggest
that, for perhaps as much as 96% of
Cenozoic time (65-2.6 million years
ago), polar-equatorial surface water
temperature gradients were less
than they are today; indeed, for
perhaps as much as 75% of the
Cenozoic (65-16 million years ago),
we can regard them as having been
substantially less. The sharply
defined climatic zones we see
today are atypical of the past.
Of course, we would not necess-
arily expect the faunal response to
marine climatic change to be on a
simple latitudinal basis. Because of
the predominant gyral circulation
patterns in the world's oceans, it is
likely that the interchange of many
shallow-water taxa would have
been concentrated along the west
coasts of major north-south trend-
ing continents. In particular, the
extension of eastern boundary cur-
rents and increased upwelling in
these regions during a global cool
phase would have facilitated
breaching of the tropics. Further
controls on gyral circulation by tec-
tonic events at key portals (Bering
Strait, Isthmus of Panama, etc.)
have been discussed elsewhere'*
The possible effects of repeated
range expansions and contractions
in the shallow marine realm can be
gauged by reference to hypotheti-
cal stenothermal taxa (Fig. 2). Both
stenothermal-warm (Fig. 2a) and
stenothermal-cool (Fig. 2b) taxa
could undergo significant range
shifts during the course of a full cli-
matic cycle. Such fluctuations may
not in themselves be of too great a
significance, until it is remembered
that there were at least ten major
cooling and warming trends
through the Cenozoic and super-
imposed on these was a whole
series of smaller-scale events. In
the last 2.4 million years ago there
may have been as many as 50 cli-
matic cycles. In response to these
climatic changes certain taxa may
have undergone concertina-like
range shifts over long periods, and
this in turn could have played a
major role in determining regional
patterns of taxonomic diversity.
Repeated latitudinal range shifts
are the basis of so-called taxonomic
diversity pumps''' If we consider
eauatorial regions first, it is likely
that two main types of processes
were in operation here. First,
repeated expansions and contrac-
tions served to stimulate speci-
ation by disruption of distributions
into allopatric fragments, followed
by renewed contact during the next
phase of the cycle'''. Secondly,
and on a somewhat longer time-
scale, certain taxa retracted into
broad-scale centres of endemism
from which they subsequently did
not re-expand. It is possible to
regard both the Indo-West Pacific
and Eastern Pacific high diversity
foci as major Cenozoic refugia''
Following similar lines of reason-
ing, it is possible to suggest that
repeated latitudinal range shifts
served to increase taxonomic
diversity in high-latitude regions
too. However, on this occasion the
result is intuitively less satisfying
as if Seems to run contrary to the
perceived nature of taxonomic
diversity gradients. Nevertheless,
our perspective on global biodiver-
sity patterns may now need to be
changed, particularly for the south-
ern hemisphere. In a recent review
of diversity patterns, Platnick'' has
pointed to two major biases, one
boreal and the other megafaunal,
that have persistently coloured our
perceptions. Cursory surveys of ter-
restrial groups such as arthropods
(and especially spiders) and
flowering plants suggest strongly
that there may be no simple
reduction in the number of taxa
between the southern high and low
latitudes, and the same is also true of
lLate Cretaceous (approximately 83-65 million
years ago): during this phase of global cool-
ing there were probably significant tempera-
ture contrasts between the sea surface tem-
peratures (SSTs) of high and low latitudes;
estimates of 25-27%C obtained for equatorial
Pacific and 12-14'C for Antarctica; palaeonto-
logical data certainly confirm warmer poles
but true nature of tropics less certain.
Earfiest Cenozoic (eart Paleocene) (65-60 million
years ago): continued phase of global cooling
but stable isotope data now suggests greatly
reduced meridional temperature gradients;
tropical SSTe may have been in 12-15%C
range and the difference between the equatorial
regions and poles as low as 3-5%C; however,
calculated values could have been affected by
anormalous high tropical surface water salinities.
Late Paleocene-late Eocene (approximately
59-43 million years ago): widespread marine
and terrestrial evidence for sustained period
of global warmth and ow polar-equatorial
temperature gradients; balance of evidence
5uggests Eocene tropical SSTs some 4-7%C
below present values.
Late Eocene-early Miocene (43-23 million
years ago): stable isotope evidence for lower
tropical SSTs and reduced meridional tem-
perature gradients is again backed by the
marine and terrestrial fossil records.
Early-middle Miocene (23-17 million years
0go); a conservative estimate for the early-
middle Miocene warming in both tropical and
subpolar regions is in the order of 3%C.
Middle-late Miocene (approximately 16-10
million years ago): a combination of tectonic
and oceanographic events (completion of
physical isolation of Antarctica, expansion of
the East Antarctic ice cap, constriction of the
Indonesian Seaway, etc.) lead to major cli-
matic changes; fundamental shift from pre-
dominantly equatorial to strongly meridional
oceanic circulation patterns; a drop of some
4-5%C in polar SSTs was not matched in the
tropics, and it has been estimated that during
the Miocene the meridional SST gradient in
the southern hemisphere doubled from
6-12%C; middle and late Miocene marine fau-
nas show a marked increase in provincialism;
nevertheless it should be emphasised that,
even at the end of the Miocene, latitudinal
temperature gradients were only in the region
of three-quarters of their present day values.
Late Miocene-early Pliocene (approximately
10-3.5 million years ago): gradients main-
tained their late Miocene values.
lLate Pliocene-Recent (from approximately
2.6 million years ago): available evidence sug-
gests gradients did not assume their present
form until the onset of bipolar glaciation at 2.6
million years ago. Data from Refs 2, 6. 7 and 29.
other adaptations. Hence, there is
no more reason to believe that the
brain is a tabula rasa than to
believe that the stomach is a gener-
al digester designed to track the
foods an organism may encounter.
In its pure form, DA focuses on
differences in LRS between individ-
uals encountering different en-
vironments, and uses the methods
of behavioural ecology to study
these differences. EP, in its purest
form, uses the methods of evol-
utionary biology and experimental
psychology to study the naturally
selected design of psychological
mechanisms. Consider how these
two types of researcher might
approach testing the Trivers-
Willard'' hypothesis about the allo-
cation of parental investment to
rmale and female progeny.
Trivers and Willard argued that if
(1) variance of male LRS exceeded
that of female LRS, (2) the relative
health and dominance of mothers is
passed on to their progeny, and (3)
healthy or dominant males obtain
more matings than males lacking
these attributes, then (4) females
will be selected to allocate invest-
ment in progeny as a function of
their health or dominance. Clutton-
Brock et alU''; in a comprehensive
study of red deer (Cervus elaphus).
found considerable support for the
hypothesis. Sons born to mothers
above median rank were more
reproductively successful than their
daughters, while daughters born to
subordinate mothers were more
reproductively successful than their
Sons. Moreover, the ratio of sons to
daughters produced by dominant
mothers was higher than for subor-
dinate mothers. Because the sex
ratio and reproductive success
were key dependent variables in
this study, it is similar to some
studies of sex allocation done by
DAs and described by Sieff'*
An evolutionary psychologist
attempting to test the Trivers-
Willard hypothesis would first con-
struct a selection model relating
Sexual dimorphism in variance in
reproductive success in males and
females and health or status of
mother to the benefits of differen-
tial investment in sons and daugh-
ters'*. Varying the parameters of
the model would provide a des-
cription of how sex allocation might
have been selected for in a particu-
lar species. The model would be
used in conjunction with infor-
mation about the natural history of
the species to explore the param-
eter space of the independent vari-
ables to determine whether a 'win-
dow' of opportunity could have
existed for the evolution of the
putative adaptation. If the results
of the modelling suggested that the
evolution of the adaptation is
plausible, a theory of the nature of
the adaptation, specified in terms
of decision rules assumed to be
instantiated in neural hardware,
would be formulated. The depen-
dent variables would be outputs
from the decision process affecting
nursing time, amount of protection
from predators, etc., given to sons
and daughters, rather than fitness
measures or behaviours assumed
to enhance fitness. Attitudes, val-
ues, intentions and motives would
be measured in human studies. A
decision rule might be something
like: 'If subordinate and physically
Weak, be more responsive to the
needs of daughters than of sons;
but if strong and dominant be
more attentive to the needs of
sons than of daughters'. It would be
necessary to formulate a theory of
the relation between ancestral and
CUrrent environments.
Such a theory requires a model
of how the crucial independent
variables, which are measures of
adaptation-relevant external and
internal environmental variables,
are represented to the ancestral
adaptation. Dominance, for exam-
ple, might have been represented
in terms of posture, frequency of
unreciprocateddd threat displays, or
resources held by different ances-
tral individuals. Once the decision
rules that describe the adaptation
increased with light intensity even as leaves lost turgor
and wilted. Similarly, 8. increased in response to HL
intensity, while C, decreased slightly. Drawdown of C,
with increased light intensity is regularly observed in me-
sophytes growing under benign conditions (e.g., Kiüppers,
1984; Chazdon and Pearcy, 1986; Kirschbaum and Pear-
cy, 1988). Drawdown in C, occurs with increased light
because photosynthetic demand for CO, is increased, but
leaf architecture and stomatal aperture nonetheless pre-
senta partial barrier to CO, diffusion (Farquhar and Shar-
key, 1982). Our hypothesis, which held that stomatal clo-
sure would markedly reduce C,, was not supported.
These findings contrast those found for other forest
herbs growing in HIL environments, Knapp, Smith, and
Y oung (1989) and Young and Smith (1979) observed
reductions in P,,,, and g. with reductions in , in the
subalpine understory herb Arnica latifolia exposed to pe-
riods of HL. Likewise, Schulz (1991) documents steep
reductions in both , and g. in Aster macrophyllus under
a similar light regime. These studies differ from the work
described herc bccause they concerned the exposure of
entire plants (or possibly of ramets) to high radiation
loads. In this study, the exterior canopy leaves intercepted
most of the radiation load and subsequently wilted.
Photosynthetic rates declined over several hours at HL.
However, the magnitude of this decline was small (ca.
1(0%) relative to the much greater limitation on photo-
synthesis imposed by light in the course of a typical day.
Throughout the day. . patterns changed in the same
direction as P,,,,, but were proportionally larger. Altera-
tions in g. under HL resulted in small changes in C,.
Morning to afternoon comparisons of gas exchange under
shade light intensities showed a comparable pattern. Al-
though P,,,, and g. values were smaller for the late after-
noon than for the morning, C, values differed little. Stable
C, rules out stomatal closure a mechanism for reductions
in P,,,,. Afternoon depressions in P,,, may reflect the in-
fluence of carbohydrate feedback inhibition on photo-
synthesis (e.g., Kiüppers et al., 1986; Rao and Terry, 1989;
Sharkey and Vanderveer, 1989; Goldschmidt and Huber,
1992), or overriding endogenous rhythms in photosyn-
thetic capacity (e.g., Pallas, Samish, and Wilmer, 1974;
between 2,300 and 3,100 m in parts of two watersheds
that cover about 20 km-.
Fargesia robsta - Fargesia robusta stands cover about
40% of the land surface below 2,600 m in the study area.
Culms of F. robusta are 2.5-3.0 m tall, unbranched, and
emerge from a densely packed (pachymorph) rhizome
system (Fig. 2). Shoots (culms -1 yr) are produced each
year between April and May and grow to full height by
mid-June (Fig. 3). Shoots have first order branches each
with a few leaves at the end of each branch at the end of
the first growing season. Second order branches and leaves
are produced in subsequent years and leaf biomass ap-
pears to remain constant after the third year (Taylor and
Oin, 1987). The oldest culm we aged from counts of
branch internodes (which reflects culm age, unpublished
data) was 12 yr old.
Plots were established throughout the elevational range
ofFargesia robusta to include observed variation in stand
density and culm size. Plot 1 (80 m) was placed in a
sparse stand with tall thick culms. Plot 2 (25 m) was
placed in a stand with four small densely packed clumps
with short thin culms. Finally, 35 2-mplots (plots 3-37)
were established systematically by elevation between 2,350
m and 2,600 m.
Bashania fangiana -- Bashania fangiana forms a nearly
continuous understory in the subalpine conifer forests in
Wolong at elevations between 2,700 and 3,400 m. Culm
density and size vary with forest canopy composition and
density, elevation, and slope aspect (Reid et al., 1991).
Average culm density is about 70 m '; and average culm
height is 1.5-2.0 m.
Bashania fangiana produces shoots annually between
June and August from clumps of culms along a spreading
(amphimorph) rhizome system (Fig. 4). Culms reach their
full height during this 3-mo period. A few leaves and
branches are present on the uppermost nodes of the shoot
by the end of the first growing season. Branches and leaves
are produced on most of the upper nodes in the second
and third growing season, and biomass appears to remain
Organs within each of the androecial whorls become equal-
ized; however, size distinction is apparent between the
two whorls of stamens. The floral whorls of Pisum sativum
(Tucker, 1989b) also showed this pattern of growth.
Nectary/stigma--The discoid-type nectary found in
soybeans is closely associated with the stamens (Waddle
and Lersten, 1974; Carlson and Lersten, 1987). From Fig.
53, it is evident that the nectary is fused to the inner wall
of the staminal column. The stigma possess two types of
papillae: short 'main body' papillae and several rows of
elongated 'lower whorl' papillae around the rim (Tilton
et al., 1984).
Carpel position-Carpels originate either as lateral or-
gans on the floral meristem, or in a few species a single
carpel is formed occupying an apparently terminal po-
sition on the meristem (Cutter, 1971). As with Pisum
sativum (Tucker, 1989b), Caesalpinia cassioides, C. pul-
cherrima, C. vesicaria(Tucker, Stein,and Derstine, 1985),
Ateleia herbert-smithii (Tucker, 1990), Neptunia pubes-
cens (Tucker, 1988a), and Acacia baileyana (Derstine and
Tucker, 1991), the carpel in normal soybean flowers is
initiated as a central mound and the cleft forms later;
thus the apical meristem is totally utilized and no apical
residuum persists in the region of the cleft.
Floral aberrancies- Loss of organs-Generally when
floral organs are lost, the missing organs will be either an
entire whorl (often the last initiated) or the last initiated
members within a whorl (Tucker, 1988b). This concept
held true for soybean flowers. When sepals, petals, or
stamens were missing, usually the missing organ was the
last organ to be initiated in that particular whorl. The
only whorl found to be completely lacking in some flowers
was the inner stamen whorl, i.e., the last whorl to be
initiated.
Extramerosity- At 18/14 C, extramerous soybean
flowers were much more common than flowers that lacked
organs, The extra organs appeared to originate from with-
in their respective whorls. The only cases of heterotopy
were found in the calyx with the convergence of bracteoles
into the sepal whorl. Extra organs were not necessarily
highly numerous pollen grains per flower (Fig. 2). There
were also strong negative correlations between mean pol-
len production per flower and mean flower diameter among
individuals in both P. scotica and P. stricta (Fig. 3). Genets
with large flowers produced significantly less pollen per
flower than those with small flowers in both species.
In P, stricta, there was a negative correlation between
mean ovule number per flower and mean flower diameter;
individuals with large flowers produced fewer ovules per
flower than individuals with small flowers (Fig. 4). Within
P. scotica, individuals that produced many ovules per
flower produced relatively small pollen grains (Fig. 5). Of
the 90 correlation coefficients estimated, the only signif-
icant positive correlation we detected among genets was
within P. stricta; individuals that produced many ovules
per flower also produced large numbers of pollen grains
(Fig. 6). The fact that five of the six significant phenotypic
correlations within species were negative suggests that we
were not detecting strong environmentally induced cor-
relations between characters (which would tend to be pos-
itive).
The two significant correlations detected among species
means (using the mean of the homostylous morphs for
P. farinosa) were also negative (Fig. 7). Among species
means, ovule number vs, flower diameter, and the modal
pollen grain volume vs. pollen production per flower were
inversely related. Since these correlations also appeared
within P. stricta (ovule number vs. flower diameter) and
P. farinosa (pollen volume vs, pollen number), they sug-
gest that there may be intrinsic genetic constraints on the
independent evolution of these characters.
Section Hymenasplenium is one of the best
defined groups within Asplenium, distinguished by
the following synapomorphies: creeping rhizomes,
dorsiventrally symmetrical steles, swollen petiole
bases, unique rachis-costae structure and chro-
mosome base numbers of Y = 38 or 39. All other
Asplenium species have erect or ascending rhi-
zomes, radially symmetrical steles, nonswollen pet-
iole bases, and n = 36 or multiples thereof (rare
exceptions differ in only one of these character-
istics).
Hymenasplenium was first described by Hayata
had cristate spores and would therefore belong to
this second group. Because most species of As-
plenium outside of section H ymenasplenium have
cristate spores (Tryon & Lugardon, 1991), the
spiny and papillate character states should be con-
sidered apomorphic.
The neotropical species of section Hymenas-
plenium are all endemic to the Neotropics. They
range from southern Mexico to Panama, the An-
tilles, and South America from Venezuela to south-
eastern Brazil, forming a wide arc around most of
Amazonian Brazil (Map 1).
The Andes from Venezuela to Bolivia harbor the
most species (8) and this is the only region with
endemics (i.e., Asplenium ortegae, AA. repandu-
lum, A. volubile). Costa Rica and Panama are also
species-rich, containing six species. The Antilles
and extreme western Amazonian Brazil both have
two species, and the Guianas, southeastern Brazil,
and Paraguay all have one species. The Serra do
Mar region of southeastern Brazil, which is a center
of species richness and endemism for ferns (lryon,
1972), has played a minor role in the diversification
of the section. Only one species (A. triquetrum)
occurs there and it is nearly endemic (Map 9).
Asplenium obtusifolium is notable for its nearly
circum-Caribbean distribution (Map 5) and its cor-
relation with geography of the 32- and 64-spored
Our recognition of 10 neotropical species of
Asplenium sect. Hymenasplenium is based pri-
marily upon qualitative characters as shown in the
key. In order to check the validity of our species
circumscriptions, we decided to do a Principal Com-
however, can be distinguished from A. triguetrum
by petiole length relative to the lamina, habitat,
and range (see key, couplet 4), and from A. vol-
ubile by the carinate rachis and rachidial wings
parallel to the plane of the lamina. These char-
acteristics were not included in the measurements
used for the PCA. Another result was that the two
spore-races of A. obtusifolium could not be sep-
arated by the morphological characteristics mea-
sured (Fig. 5).
We are not sure what group of species within
Asplenium is most closely related to section Hy-
menasplenium. Several species of Asplenium (A.
abscissum Willd., A. argentinum Hieron., A. host-
mannii Hieron., and A. otites Link) greatly resem-
ble certain species in section I ymenasplenitum in
leaf form. Consequently, they are often misiden-
tified as a species in section Hfymenasplenium,
especially A. laetum (which see for comparison).
These species, which can be immediately distin-
guished from section Hymenasplenium by their
erect rhizomes and rachis-costa architecture, may
be the closest group in zsplenium related to section
Hymenasplenium. This suggestion is based only
on similarities in leaf form and is therefore tenta-
tve.
Asplenium sect. Hymenaspleniumm (Hayata)
Iwatsuki, Acta Phytotax. Geobot, 27: 44.
[975. Basionym: Hymenasplenium Hayata,
Bot. Mag. (Tokyo) 41: 712. 1927. rYrE: As-
plenium unilaterale Lam.
Plants terrestrial, epipetric, or epiphytic; rhi-
zome creeping, green to blackish, with two rows
of alternately arranged petioles on the dorsal sur-
face, scaly near the apex; rhizome stele dorsiven-
tral, composed of two unequal meristeles connected
by lateral strands, the ventral meristele wider and
the dorsal one narrower, bearing roots from either
the ventral, dorsal, or connecting meristeles; petiole
terete, scaly at base, glabrous distally, greenish to
atropurpureous, the base swollen and often per-
sisting after the leaf has fallen and decayed; lamina
usually 1-pinnate or (in A. cardiophyllum) simple
and cordate; rachis not or very shallowly grooved,
with or without perpendicular or flat green wing8,
lacking buds; costae bordered by a flange of green
sification, and consequently requires a larger input
of information. In the case of Pleurothyrium, suf-
ficient information is available for taxonomic re-
vision, but not enough for a phylogenetic classifi-
cation, Obtaining sufficient information for a proper
phylogenetic analysis is a project unto itself and
making a phylogenetic analysis cannot be merely
tacked on to making a taxonomic revision.
Relationships between Pleurothyrium species as
expressed in the cladogram and as based on mor-
phological similarities show some congruence, For
instance, I consider P. racemosum, P. tomiwahlii,
and P. pilosum as closely related, a relationship
expressed in all cladograms inspected. Likewise,
the species with erect tepals, usually with inrolled
margin of the tepals, are closely related, as shown
Arbor, 20-25 m alta. Ramuli solidi, teretes vel paullo
angulati, rufo-tomentosi, Gemma terminalis ad I mm cras-
sa, rufo-tomentosa. Folia alterna, subcoriacea, 15-30 x
8-12 cm, oblonga vel oblongo-elliptica, basi rotundata,
apice paullo acuta, supra glabra, subtus rufo-tomentosa,
nervis lateralibus 14-20 utroque costae latere, prope
marginem sursum curvantibus, vena marginale in dimidio
distale praesente, venatione supra immersa, subtus costa
nervisque lateralibus elevatis, venatione tertia paullo ele-
vata, Petioli rufo-tomentosi, valde canaliculati, 10-15 mm
longi. Inflorescentiae ex axillis bractearum ortae, rufo-
tomentosae, 10-15 cm longae, paniculatae, ramulis vulgo
duplo cymae more ramosis; bracteis sub anthesi praesen-
tibus, rufo-tomentosis, eis ad ramulorum inferiorum basim
ovatis, 12-15 mm longis, eis ad cymarum terminalium
basim ca. 3 mm longis. Pedicelli florum apertorum ca. B
mm longi, Flores cremei, saltem 13 mm diametro. Tepala
O, subaequalia, late ovata, ca, 5 mm longa, intus tomen-
tella. Stamina 9, glabra, 4-locellata, locellis lateralibus,
glandulis permagnis, coalitis, stamina cingentibus. Ovar-
ium ellipsoideum, ca, 1.3 mm longum, basi glabrum, parte
media tomentellurm, parte superiore tomentellum D macu-
lis nudis praeditum; stylum glabrum, ca. 1 mm longum.
Tubus floralis intus tomentellus. Fructus ignotus.
Tree, 20-25 m tall. Twigs solid, terete or slightly
ridged, reddish-brown-tomentose, 5-6 mm diam.
5 cm below the tip. Terminal buds to 7 mm thick,
rufous-tomentose. lLeaves alternate, subcoriaceous,
15-30 x 8-12 cm, oblong to oblong-elliptic, the
base rounded, the tip slightly acute, glabrous above,
rufous-tomentose below, venation immersed on up-
per surface, midrib and lateral veins raised on lower
surface, the tertiary venation slightly raised; lateral
veins 14-20 on each side, curving upward near
the margion and united with the superior vein, form-
ing a marginal vein in the upper half of the lamina.
of stamens greatly enlarged, surrounding the sta-
mens, fused. Ovary and style densely brown-pa-
pillose, floral tube brown-papillose inside; ovary
globose, ca. 0.7 mm long, the style distinct, ca,
O.5 mm long. Cupule of young fruit cup-shaped,
ca, 2 cm wide and 1 cm tall; young fruit ellipsoid,
ca, 1.5 cm long. Fruits: July-October. Flowers:
June-July.
Collections studied. ECUADOR. NAPO: Aguarico, Re-
serva Faunistica Cuyabeno, Palacios 7667 (MO). PERU.
LORETO: uebrada Sucursari, tributary of Rio Napo, Gen-
try 54300 (MO); Pebas on the Amazon River, Williams
1766 (B); Maynas, lquitos, Asociaciin Agraria Paujil,
Fdsguez 10877 (MO); Maynas, Explornapo Camp, Rio
Sucursari, dsquea 8119 (MO), Fdsque 13078 (MO).
Pleurothyrium williamsii is only known from
the type collection, four collections, all from Peru
in the area north of the Rio Napo-Rio Amazonas,
and one collection from E.cuador. Unfortunately,
the holotype, which was requested from the Field
Museum, disappeared, together with other Pleu-
rothyrium types, while being sent to St. Louis, Of
the type collection, only a duplicate in B (with
buds) and some inflorescence fragments in G (with
a few flowers) exist. The recent collections are a
good match as far as floral characters and leaf
shape are concerned, but there are some differ-
ences, The B specimen has alternate leaves, while
the recent specimens and the photo of the holotype
(in NY and F) show clustered leaves, The photo-
types have the inflorescences alternate along a
leafless twig, the recent specimens have the inflo-
rescences near the tip of the stem, while the B
specimen has a detached inflorescence. The B spec-
imen is also more tomentulose. However, the sim-
ilarities in leaf shape and flowers outweigh these
differences, and I do not hesitate to assign the two
recent collections to P. williamsii,
A close relative of this species is Pleurothyrium
panurense (Meissner) Mez, a species with elliptic-
obovate leaves, an obtuse leaf base and similar
flowers. It differs, however, in its smaller leaves,
the whitish indument on the lower leaf surface, its
glabrous ovary, and longer petioles. Pleurothyrium
insigne differs from P. williamsii in its larger
leaves, the leaves not so gradually narrowed toward
the base, erect indument on lower leaf surface, and
its larger flowers, All three species occur in Am-
azonian Peru and/ or adjacent Brazil, Another close
relative is P. maximum, which see for further
discussion.
Among the collections were found a number of
specimens that do not belong to any of the treated
species and which very likely represent undescribed
species. Because these specimens are incomplete
(sterile or fruiting), they are not formally described,
but only listed below so as to call attention to their
existence, I hope that in the near future material
adequate for their description will become avail-
able.
G. Proctor Cooper 539, Panama, Bocas del Toro,
region of Almirante (F, NY, US).
A fruiting collection with large (to 35 cm), el-
liptic to elliptic-oblong, acuminate leaves, Leaves
are glabrous below and have a strongly developed
marginal vein; the twigs are solid, glabrous or near-
ly so. A distinct species, included in Burger & van
der Wer8 (1990) as Pleurothyrium sp. A,
Gentry 57004, Colombia, Valle, Bajo Calirma (MO).
A sterile specimen with gigantic leaves, accord-
ing to the label ca. 1 m long. The specimen has
leaves 60--70 cm long and 30-35 cm wide, densely
rusty-tomentose below.
Monsalve 1651, Colombia, Valle, Bajo Calima
(MO).
A species with clustered leaves, dark ferrugi-
nous-tomentose below, ca, [S x 6 cm. Young
inflorescences and infructescences are very short,
ca, 1 crm long, and carry only one bud or fruit.
There are five collections of this species, but none
with flowers.
L6pez & H. Triana 24, Colombia, Antioquia, Par-
que Nacional de las Orquideas (MO).
Characterized by its obovate to obovate-elliptic
leaves, glabrous below, with 15--20 pairs of lateral
veins and an obtuse to rounded leaf base, The
young cupules are covered with many small len-
ticels.
Fasquea 3220, Peru, Loreto, Requena (MO).
lLeaves glabrous, whorled, narrowly oblong with
abruptly rounded base. Related to P. williamsii,
but with narrower, oblong leaves and much smaller
inflorescences.
and subgenus Uichanthelium from most of the
other C, subgenera of Panicum. A detailed de-
scription of section Dichanthelium based on the
studied species is presented below.
Outline: expanded, either flat or very broadly
V-shaped; arms of lamina either straight or out-
wardly bowed; two halves of lamina symmetrical
about the median vascular bundle; leaf width vari-
able and leaf blade section includes between 17
and 117 vba; P. aciculare (Fig. 1) is an exception
with narrow (only l1 vbs), inrolled leaf blades. Ribbs
and furrows: variable, from flat adaxial surfaces
without ribs or furrows to medium furrows (about
a quarter of the leaf thickness); furrows wide and
open, occurring between all vbs; adaxial ribs, when
present, located over the vbs, with rounded apices,
and all are structurally uniform; abaxial ribs usually
absent but slight ribbing may be developed; in P.
aciculare abaxial ribs are clearly present (Fig. 1B).
Midrib: variable, from undifferentiated median vb
to definite keel; median vbs, structurally indistin-
er palea lanceolate, (1.5--)2.1 mm long, 0.6 mm
wide, membranous, the margins ciliate; lower flow-
er absent or present, male when present, with 3
anthers each 1 mm long. Upper anthecium ellip-
soid, 1.9-2.5 mm long, 0.9--1.2 mm wide, indu-
rate, smooth, the apex of the lemma apiculate,
scaberulous; stamens 3, the anthers 0.3-1 mm
long. Caryopsis ellipsoid, 1.7 mm long, l mm wide;
hilum punctiform, embryo W the length of the
caryopsis.
Distribution and ecology: Brazil, on mountain
slopes from Espirito Santo to Rio Grande do Sul,
900-2,650 m.
AAdditional specimens examined. BRAZIL. ESPIRITO
SANTO: Serra do Caparaö, rocky open campo, 2,650 m,
Aea 4014 (NY, P, US); Serra do Caparaö, 2,280-
2,400 m, Chase 10084 (IAN, RB, US). MINAS GErAS:
Barbacena, Serra Mantiqueira, Chase 8667 (F, US)( Ouro
Preto, Villa Rica, 1,100 m, Chase 9350 (F, GH, MO,
NY, IUS); cerrado on middle slopes of Pico de Itacolumi,
ca. 3 km S of Ouro Preto, 1,750 m, Irwin et al. 29483
(MO, UB); Mun. Itamonte, Parque Nacional de Itatiaia,
camino para las Agulhas Negras, 1,550-1,800 m, Zu-
loaga et al. 2374* (MO, RB, SI, US). FArANA: 4 km E
of Guarapuava along highway BR.277 to Curitiba, 1,050
m, Davidse et al. 11319(MO); Bocaiuva do Sul, Clayton
4285 (K). RI0 DE AnERO: Alto de Itatiaia, 2,200-2,400
m, Chase 8299 (NY, US, W)( Teresopolis, Posse, morro
das Antenas de Televisao, Sucre 2317 (SI); Tijuca, in
open spot at summit, Chase I2161 (US). RIO GRANDE DO
SUL: Cambarä do Sul-Itaimbezinho-Bela Vista, Valls et al.
1870, 2903 (CEN); Serra da Rocinha, prope Bom Jesus,
in dumetosis, Rambo 53841 (US). sANTA CATARINA: Mun,
Gagador, 9 km W of Cagador, Smith & Klei 10899
(NY, US); Mun. Bom Jardim da Serra, Alto, 20 km S of
Bom Jardim, Smith & Klein 15809 (HB, NY, P, US):
5 km S of Ponte Alta along highway BR-116 to Lajes,
Davidse et al. 11104 (MO, SP); Ponte Serrado, 94 km
W of Joagaba, 700-900 m, Smith & Klein 14008 (HB,
K, SI): Mun. Urubici, 19 km N of Perico, Smith & Klein
15889 (HB, US): Mun. Lajes, Serra do Ilheos, Smith &
Klein 15456 (K, US). s3o rAULo: Jardin Botänico e
Parque do Estado, Sendulsky 1063 (SI); Campos do
Jordio, Serra Mantiqueira, sandy campo, 1,600 m, Chase
9822 (NY, US).
Related to Panicum sabulorum and P. stig-
mosum, P. superatum can be distinguished by its
contracted panicles and appressed spikelets.
There are two indurate florets in Chase 9403.
Sendulsky 1063 has anthers only 0.3 mm long.
34. Panicum surrectum Chase ex Zuloaga &
Morrone, Novon 1: 111. 1991. rYPE: Brazil.
Minas Gerais: Barbacena, long and tangled in
moist brushy base of higher slope, Chase 8664
(holotype, US; isotypes, F, NY). Figures 24D,
30.
Short-rhizomatous perennial. Culms decumbent
to geniculate, then erect, branching at the upper
nodes, scandent, 45-120 cm tall; internodes com-
pressed or cylindric, 7-13 cm long, glabrous; nodes
compressed, glabrous, brownish. Sheaths 4--10.5
cm long, shorter than the internodes, glabrous to
papillose-pilose toward the base, shiny, one margin
ciliate, the other one ciliate toward the base, oth-
erwise glabrous. l.igules 0.2 mm long, membra-
nous-ciliate; collar pilose. Blades linear-lanceolate,
5-12 cm long, 0.6-- 1 cm wide, glabrous to short-
hispid, attenuate at the base and apex, the margins
scabrous, long-ciliate toward the base; midnerve
conspicuous. Inforescences terminal, exserted;
panicles lax, diffuse, 3.5-15 cm long, 3-10 cm
wide; main axis glandular or eglandular, flexuous,
glabrous, the pulvini glabrous; first-order branches
ascendent, whorled toward the base, then subop-
posite or alternate, the axis of the branches gla-
brous, flexuous, glandular or eglandular; pedicels
triquetrous, glabrous, glandular or eglandular.
Spikelets narrowly ellipsoid, 1.8-2.2 mm long, 0.8
mm wide, glabrous, greenish, nonstipitate, the up-
per glume and lower lemma subequal, the nerves
manifest. Lower glume ovate, 0.9--1.3 mm long.
5-V(-') the length of the spikelet, not embracing
the upper glume, (1-)3-nerved, the lateral nerves
inconspicuous. Upper glume 1.6-2 mm long, not
covering the apex of the upper anthecium, 9-nerved.
Lower lemma glumiform, 1.8-2,1 mm long.
9-nerved, Lower palea lanceolate, 1.5--1.8 mm
long, 0,4 mm wide, shortly pilose near the apex.
otherwise glabrous, hyaline; lower flower male or
sterile, stamens 3, the anthers 1 mm long. Upper
anthecium ellipsoid, 1.6- 1.8 mm long, 0.7 mm
wide, pale, indurate, papillose, the apex of the
lemma shortly crested and pilose; stamens 3, the
anthers 1.2 mm long, purplish. Caryopsis ovoid,
1.3 mm long, 0.7 mm wide; hilum punctiform,
embryo less than i the length of the caryopsis.
Distribution and ecology: Brazil, occasionally
present in Paraguay, found at forest edges, between
600 and 1,800 m.
morphism before or during the earliest stages of
the first foliation-producing deformation event.
Sheets of predominantly megacrystic granitoid
make up to 80% of the Proterozoic cordierite-
bearing, high-grade (low-pressure granulite fa-
cies), LPHT rocks of the SE Anmatjira Range,
Arunta Block, central Australia (Clarke et al.,
1990; Collins et al., 1991). The regional metamor-
phic zones form a broad aureole around these
sheets (Collins and Vernon, 1991), suggesting that
the main cause of metamorphism was intrusion of
granitoids. The fold and S-surface terminology
follows that of Collins et al. (1991), as explained
in the previous section.
Stromatic leucosomes outline the earliest folia-
continental heat flow data sets can yield signifi-
cant additional information beyond these first-
order generalizations. Do regional heat flow data
sets, along with ancillary geophysical and geo-
chemical data, provide sufficient constraints to
address variations on the first-order themes de-
scribed above?
We address that question by focusing on east
and southern Africa because there heat flow
measurements define a common first-order heat
flow pattern but also show clear regional variabil-
ity, The first-order heat flow pattern suggests that
the gross thermal structure of the lithosphere
beneath east and southern Africa is similar (Bal-
lard and Pollack, 1987; Nyblade et al., 1990)
however, the variations in heat flow superim-
posed on this common pattern suggest that there
may also be some differences in the thermal
structure of the lithosphere between these two
regions, Specifically, we wish to determine if vari-
ations in the heat flow pattern between east and
southern Africa can be easily interpreted to show
in what ways the thermal structure of the litho-
sphere may differ between these two regions. We
first briefly describe the first-order heat flow pat-
tern in east and southern Africa on which the
regional variability is superimposed, and review
our interpretation of this first-order pattern.
The southern African subcontinent is a com-
plex Precambrian terrain that comprises two simi-
lar tectonic regions, east and southern Africa.
zania and Kalahari Cratons or between the
Mozambique Belt and the southern African mo-
bile belts, they provide little information about
how crustal heat production at depth may differ
between east and southern Africa vis-a-vis the
linear heat flow-heat production relationship.
Crustal velocity models for the Namaqua
(Green and Durrheim, 1990) and Damara Belts
(Baier et al., 1983) in southern Africa and for the
Mozambique Belt (KRsF working party, 1991) in
east Africa can be used to constrain crustal heat
production in the east and southern African mo-
bile belts via an A-V, relationship. Using the
A-V, relationship from Cermak and Rybach
(1989) to convert velocities to heat production
yields a crustal column with a surface heat flow
of about 35 mW m* for the Namaqua Belt, 37
mW m% for the Damara Belt, and 34 mW m%
for the Mozambique Belt away from the Kenya
Rift Valley. The uncertainties in these estimates
are probably 8 40% or more; however, in spite of
these large uncertainties, the similarity in seismic
structure, and thus in the estimated total crustal
heat production between these mobile belts sug-
gests that the difference in heat flow between the
Mozambique Belt and the southern African mo-
bile belts may not arise from gross variations in
crustal heat production. There are no seismic
velocity data for the Tanzania Craton and so an
A-V, relationship cannot be used to estimate
differences in crustal heat production between
the Tanzania and Kalahari Cratons.
Comparing the crustal velocity models for the
Namaqua, Damara, and Mozambique Belts cited
above, also shows that the thickness of intra-
crustal layers in the three mobile belts is roughly
the same, further suggesting that crustal heat
production does not differ considerably between
the mobile belts in east and southern Africa. For
the cratons, crustal thickness estimates in the
Kalahari Craton are between 35 and 40 km (Gane
et al., 1956; Stuart and Zengeni, 1987) but, as
mentioned previously, there are no seismic obser-
vations of crustal structure for the Tanzania Cra-
ton. Given the uniformity of crustal thickness in
Archean cratons worldwide (Durrheim and
Mooney, 1991), it is not unreasonable to assume
that the Tanzania Craton crust does not depart
significantly from the thickness of the Kalahari
Craton crust. However, the lack of data from the
Tanzania Craton precludes any further discussion
of differences in intra-crustal structure between
the Tanzania and Kalahari Cratons.
In regard to constructing local models of crustal
heat production from representative rock sam-
ples, there are no heat production estimates for
xenoliths or sedimentary rocks from east Africa
that could be used to constrain the depth distri-
bution of crustal heat production in the Tanzania
Craton or Mozambique Belt, nor are there any
reported crustal cross-sections.
In summary, similar crustal velocity models for
the Mozambique Belt and two southern African
mobile belts suggest that there is no difference in
the total crustal heat production between the east
and southern African mobile belts. However, be-
cause corroborative evidence from surface heat
production and petrologic information is lacking,
it is not easy to make a robust case that differ-
ences in crustal heat production between the east
and southern African mobile belts do not give
rise to the differences in heat flow. There is
insufficient data of any kind to estimate the depth
distribution of crustal heat production in the
Tanzania Craton, and therefore no conclusion
can be reached about variations in crustal heat
production between the Kalahari and Tanzania
Cratons,
In this section we examine the possibility that
the lower surface heat flow in some areas of east
Africa relative to southern Africa may be due to
lower mantle heat flow into the base of the litho-
sphere. There are at least two possible explana-
recorded at Zuoshan. Hence, it is tentatively con-
cluded that five possible large earthquakes, in-
cluding the 1668 one, have occurred in recent
geologic time on the active Yishu fault from
Zuoshan to Hezhuang.
In addition to examining the stream channels
for multifaulting events, an attempt was made to
determine the profile evidence at many localities
along the fault. It turns out that an outcrop at
Mazhuang is very suitable. The fault outcrop
there is shown in Photo 1. On the eastern side is
Cretaceous sandstone and siltstone, whereas on
the western side are alluvial deposits. The fault
zone is composed of gouge and clastic sandstone.
The exposed gouge is 0.9 m wide at the top and
2.5 m wide at the bottom. This high-angle zone
dips toward the east, showing a small amount of
vertical movement due to transverse compression.
Within the gouge zone, there are many sandstone
lenticulars of various sizes. The alluvial deposits
on the west consist of five different layers. From
bottom to top they are designated as layers 1, 2,
3, 4 and 5 (Fig. 10). Layer 1 is Fe- and Mn-con-
cretion-bearing clay; layer 2 is comprised of sand-
stone fragments and sandy gravels; layer 3 con-
tains well rounded to subrounded coarse sand;
layer 4 is fine to silty sand; and layer 5 is residual
soil, with modern vegetation. Note that these
layers have different degrees of deformation. The
bottom layers folded more strongly than the top
ones, This is because the older layers have under-
gone more faulting events than the younger ones,
as interpreted in Figure l1. As mentioned above,
the active Yishu fault is dominated by strike-slip
with some amount of a thrusting component.
Therefore, on the cross-section it looks like a
high-angle thrust fault. Consider a layer l,, which
is deposited before faulting dFig. 11a). When a
faulting event occurs, it is deformed as in Figure
11b. A fault scarp on the east is formed after
faulting. The scarp is unstable, and easily eroded.
Hence, another layer, lg, is soon deposited over
layer 1 (Fig. 11c). Then, another faulting event
takes place, the crushed zone becomes thicker,
and l, is deformed further. However, l, just
begins to deform (Fig. 11d). During the quiescent
period of faulting, another layer, l4, is deposited
over l,, Finally, the fault outcrop looks like the
rectangle in Figure 1le. Thus, the number of
layers in Figure 11 may indicate the number of
faulting events. According to this process of de-
duction, the outcrop in Figure 10 may record five
possible events, Considering that layers 3 and 5
are very thin, they were probably associated with
the same events as layers 2 and 4, respectively. In
that case, at least three events are recorded, the
latest of which is the 1668 earthquake. Unfortu-
nately, the author did not find any charcoal for
carbon 14 dating in those layers. Thus the abso-
lute ages of the faulting events are not available
at present.
One of the typical geomorphic indicators of
faulted channels on the active Yishu fault is the
lous (Stevenson, 1990) because of the usu-
ally scanty nature of the secondary xylem
compared to the well developed parenchy-
matous pith and cortex.
There is also considerable diversity in leaf
and leaflet morphology. The general aspects
of leaf and leaflet diversity in Zamia has
been discussed elsewhere (Stevenson, 1991)
so that the following is limited to that found
in the Panamanian species of Zamia. Some
species (e.g., Z. dressleri) have only 4-8 pairs
of very wide leaflets in contrast to the up to
70 pairs of narrow leaflets found in Z. chi-
g4a. Zamia acuminata often has diminu-
tive prickles on the petiole in contrast to the
heavy, often branched, prickles found on
petioles of Z. chigua. Most Panamanian
species of Zamia have flat, smooth leaflets,
but three species (Z. dressleri, Z. neuro-
phyllidia D. Stevenson, and Z. skinneri
Warsz, ex A. Dietrich) have distinctive leaf-
lets that are deeply grooved adaxially be-
tween the veins so as to appear plicate. In
general Zamia leaflets are sessile on the ra-
chis but one Panamanian species, Z. mani-
cata Linden ex Regel, has distinct petio-
lules, In Z. manicata, there is also an abaxial,
semicircular, collar or gland-like structure,
of unknown function, at the junction of the
lamina and the petiolule (Stevenson, 1990).
Reproductive morphology, on the other
hand, is more constant, but here too the
variability is greater than for other neo-
tropical areas with strobili varying in color
from light yellow to deep red-brown. Three
species of 2Zamia found in Panama, Z. cu-
naria Dressler & D. Stevenson, Z. ipetiensis
D. Stevenson, and Z. obliqua, have very
distinctive microsporophylls in that micro-
sporangia are found on both the abaxial and
adaxial surfaces (Fig. 1) in contrast to all
other cycads which have microsporangia
only on the abaxial surface. The yellow seeds
of the endemic Z. pseudoparasitica are
unique in the genus.
Because the cycads are strictly dioecious
and reproductive structures are infrequently
encountered in the field, herbarium, or cul-
tivation, the emphasis in the following key
to species is on vegetative characters. Cer-
tainly, to use yellow seeds as the only key
character for Z. pseiudoparasitica would not
be useful for a microsporangiate plant. In
addition, the vegetative characters in Pan-
amanian Zamias are so distinct, as outlined
above, that their use in this case is war-
ranted.
As in previous works on cycads (e.g., Ste-
venson et al., 1986), specific localities of the
species of Zamia are not given in this paper
because of their endangered status and com-
mercial value which could lead to their
eradication either intentionally or uninten-
tionally by collectors. Some of the species
discussed and described in this work are
locally endemic and known from only one
or a few small populations and are thus par-
ticularly susceptible to over-exploitation.
All lectotypes and neotypes given in the
present work are from Stevenson and Sa-
bato (1986). Chromosome numbers are from
the works of Norstog (1980, 1981) and Mo-
retti(1990). Information on individual spe-
cies are given in alphabetical order.
pedicellate (to 1 mm), inserted parallel to
perpendicular, in diads distally; sepals con-
nate to middle, glabrous, membranous, two
ca. 2.5 x 1.5 mm, one larger ca. 3.5 x 1.5
mm, much smaller than the petals; petals
valvate, connate to middle, glabrous, mem-
branous, ca, 6 x 2 mm; stamens 15, ca, 6
mm long, the filaments not columnar, ca.
1.5 mm long, the anthers ca. 4 mm long,
sagittate at both ends; pistillode trifid. Pis-
tillate flowers inserted on proximal 5-12 cm
of rachis, fibrous; sepals free, triangular, 3-
5 x 3 mm, contorted to left, glabrous, ir-
regular to dentate on margin; petals free,
triangular, 3-5 x 3 mm, imbricate to right,
glabrous; staminodial ring adnate to petals;
pistil conical, 3-5 mm tall; stigma capitate,
shorter than 1 mm, with three short branch-
es, wrinkled, glabrous. Fruit ovate to tur-
binate, 1.5-2 cm long, ca, 1.3 cm diam., the
stigmatic remains truncate with stigmas ses-
sile and minute, the persistent perianth
shorter than half of fruit; endosperm ho-
mogeneous; seed 1.
Common names: ''Cachand6.''
Additional specimens: BRAZZIL. Bahia: Municipio
Salvador, Lagoa do Abiete, 10-20 m, 23 Sep 1976,
Davis 61032 (F); ca. 35 km NE from Salvador and 3
km NE from Itapoä, 30 Aug 1978, Morawetz d Mora-
wetz 21-30878 (BH), Salvador, dunas de Itapoä, Lagoa
de Urubu, 12'56'S, 38*21'W, 12 Dec 1985, Noblick d
Britto 4473 (F-n,v., LPB); Itapoä, Bondar 23 (F); Pi-
d6be, 1835, Blanchet s,n. (G); Esplanada, 1 Dec 1988
Noblick & Soeiro 4702(CEPEC-n.v.,F-n,v., HRB-
n.v., LPB); Esplanada, Lago apos entrada para Conde,
11*47'S, 37*55W. 15 Feb 1978, Orlandi 140 (RB);
central part of State, Oct 1942, Krukof'12631 (NY).
Distribution: Restricted to xeromorphic
vegetation of littoral sandy dunes and cer-
rado in northeastern region of Bahia, Brazil.
Populations occur to 40 km from coast from
O to 20 m elevation in restinga vegetation
(Entre Rios, Salvador) and from 100 to 150
m elevation in cerrado vegetation (Esplana-
da).
Allagoptera brevicalyx is named for its
perianth that is short in fruit, rather than
enlarged as in other species of Allagoptera.
It has been confused frequently with A. are-
naria, which grows in similar restinga veg-
etation but closer to the beach, and also with
A. campestris from the cerrado vegetation.
This new species differs from other spe-
Siphocampylus Pohl (Campanulaceae:
Lobelioideae) comprises over 200 species
of herbs, shrubs, and lianas. Its center of
diversity is the Andes of South America,
from Venezuela to Bolivia; species also oc-
cur in the Caribbean, Central America north
to Costa Rica, and extra-Andean South
America (Brazil, Paraguay, Uruguay, and
Argentina). The most recent monograph of
the genus is that prepared by Wimmer (1953,
1968) for Das Pflanzenreich. In this paper,
Idescribe a new species from northern Peru,
which was detected during routine identi-
fication of South American lobelioids de-
posited at F.
nov. (Figs. 1, 2)
TYrE: PERU. La Libertad. PRov. OTUzco:
ca, 20 km E of Agallpampa enroute to Qui-
ruvilca, ca. 3390 m, 6 Jan 1983, M. Dillon,
U. Molau d P. Matekaitis 2792 (HOLOTYPE:
F; soryPEs: MO, USM-n.v.).
A speciebus ceteris Siphocampyli subsect. Megas-
tomis pedicellis bibracteolatis, hypanthio 12-16 mm
diametro, calycis lobis 4-6 mm latis, corollae tubo sine
constrictione, diametro a basi (12-13 mm) ad orem
(14-16 mm) accrescenti, corollae lobis dorsalibus 32-
37 mm ventralibus 23-30 mm longis, et antheris 16-
17 mm longis differt.
Suffruticose, malodorous shrub; stems 2-
2.5 m tall, several from base, puberulent or
subglabrous; latex milky. Leaves alternate,
densely crowded toward the apex, marces-
cent; lamina 11-18 cm long, 1.5-3.8 cm
wide, narrowly elliptic or lanceolate, cori-
aceous; upper surface dull, dark green, gla-
brous or puberulent, the veins pale; lower
surface dull, green, puberulent on the veins,
the veins darker; margin denticulate, the
teeth ca. 1 mm apart; apex acuminate; base
narrowly cuneate; petiole 0.5-1 cm long,
narrowly winged, shortly decurrent, puber-
ulent or subglabrous. Flowers solitary in the
axils of upper foliage leaves, forming a 12-
17-flowered terminal, simple corymb; ped-
icels 10-19.5 cm long (the lowermost the
longest), bibracteolate 3-15 mm above their
base, glabrous or minutely puberulent to-
ward the apex; bracteoles 5-12 mm long,
O.5-1.2 mm wide, linear, ciliate. Hypanthi-
um 7-10 mm long, 12-16 mm diam., hemi-
spheric or broadly obconic, faintly 10-
nerved, glabrous or minutely puberulent.
Calyx lobes 23-35 mm long, 4-6 mm wide,
narrowly triangular, erect; margin entire,
ciliate; apex acuminate. Corolla 52-60 mm
long, cream or yellow with a tinge of purple,
glabrous or nearly so; tube erect, 20-25 mm
long, 12-13 mm diam. at base, gradually
The Guianas; Venezuela (Bolivar); wide-
spread in tropical and subtropical areas of
the New World; in savanna and old fields.
Guyana (Rupununi Savannas), in riparian
scrub.
Guyana, Surinam; Amazon Basin; in rain
forest.
Venezuela (Amazonas); NW Amazon Ba-
sin; in rain forest.
Venezuela (Amazonas, Bolivar); Brazil
(Amazonas); in scrub-savanna and dwarf
forest on tepui tops.
Venezuela (Amazonas); Colombia (Vau-
pes); in scrub-savanna on tepuis.
Venezuela (Bolivar: Carrao-tepui); in dwarf
and montane forest.
Venezuela (Amazonas: Cerro Sipapo); in
scrub and dwarf forest.
Venezuela (Bolivar: Cerro Venamo); in
scrub and dwarf forest.
influence themodeleven iftheyare valid afer the nominal analysishour;as the model
integration proceeds, fewer observations are left for assimilation and the system
gradually turns to forecast mode. For example, in the current operational system
observationsare assimilated up tofour hours after the nominal analysishour, providing
greater accuracy in the earlier periods of the forecast.
The primitive equations, on which numerical models are based, generally admit
high-frequency gravity-wave solutions as well as the slower moving Rossby-wave
modes. Both types of wave are found in the real atmosphere, but gravity waves,
being readily dissipated, are not of major meteorological importance and the
atmosphere is close to geostrophic balance. If he analyses produced by schemes
based on statistical interpolation are used directly as initial conditions for a forecast,
imbalances between the mass and wind fields will cause the forecast to be
contaminated by spurious high-frequency oscillations of much larger amplitude than
those observed in the atmosphere. Alhhough the damping terms, which are part of a
numerical model, will tend to dissipate these oscillations, they make the short-
period forecasts noisy and may be detrimental to the quality control and the
assimilation cycle. For this reason, an initialization step is performed after the
analysis with the object of eliminating these spurious oscillations. Analyses produced
by schemes based on repeated insertion do not require a separate initialization step
since balance is generally achieved during the assimilation process.
World Area Forecast Centres are required to distribute grid-point data of wind and
temperature at various levels, as well as information on the tropopause and the
maximum wind. Suitably dispayed in graphical or chart form, these data alone are of
great value to aviation forecasters. However, a wide range of ancillary fields and
derived data may be generated from an NWP system which provide a more complete
picture of the numerical forecast. H+24 forecasts from the UK operational models
serve to illustrate the range of products which can be generated from a numerical
system. Most are used regularly at Bracknell in its role as a Regional Area Forecast
Centre. Some are derived from fields issued routinely on the GTS, but many require
data at present only available at the centre. A mixture of charts from he UK global
and regional models is shown here, but all products could equally well have been
derived from a high-resolution global model. In all cases, the model products have
been projected onto a uniform grid (about 100 km grid spacing) for output purposes,
which is considerably finer than the resolution of data available on the GTS in
1990. The output resolution, both in the horizontal and vertical, of course greath
affects the detail that can be identified.
The products presented here relate to a case of explosive cyclogenesis in the
North Atlantic, and all are from the same data time of 1200 UTC 7 January 1990.
Fog predictions are also very difficult to assess because of the local nature of fog
occurrence. However, comparisons with routine synoptic observations have been
prepared on a monrhly basis for the UKMO model. For November 1989, tthese show
that forecasts from 0000 UTC data were far better than those from 12 U'TC data,
the latter producing too much fog by thc end of the night. For afternoon forecasts
not been corrected automatically, this can be done at forecast time. A
knowledge of the level of variance in the error will give a general idea of how
much confidence to put in the forecast. Higher error variances suggest a less
reliable product in day-to-day use. If there is any reason to assume that the
errors are serially correlated, a knowledge of recent performance of the
product will help in the interpretation. For example, if the fact that a
technique forecast the ceiling two categories too high six hours previously
means it is likely that it will do it again now, this information can be used to
adjust the current forecast in the right direction.
To interpret verification statistics, the most important characteristics to
know are: sample size, sample stratification and type of verification
measure used. For sample size, the larger the better; smaller samples may
give rise to misleading verificarion statistics. Sample stratification is
important to know to be sure that the results are representative of the
conditions which apply in the currenut situation. Seasonal stratification is
most common; verification statistics from winter forecasts will not
necessarily say anything useful about performance in summer. Of more
interest are sample stratifications based on the values of the weather
element. For instance, verification information could be computed for a
subsample consisting of all cases where ceilings less than 20O feet were
forecast. The verification statistics then could say, for example, that
forecasts of ceilings less than 200 feet tend to be one category too high
75% of the time and two categories too high 20% of the time. Such
verifications are of course valid only for forecasts that match the
subsample definition, but they convey specific and clear information
about the forecast. A word of caution, however: subdividing the sample
reduces the sample size - the benefits of stratified sample verification
may be lost if the subsample is too small to give stable results.
The characteristics of the different types of verification measures in general
use are briefly described in Appendix A, along with the measures that were
used to produce the results reported in section 3.5.
Current statistical interpretation products have some general practical limitations
with respect to aviation forecasting. First of all, tthey are tied to the production
schedules of the models that drive hem. NWP models normally run every 1l hours,
and the output is available three to four hours after initial data time. This means
forecast products cannot be expected to reach the forecaster's desk until at least four
hours after the data on which they are based. For many aviation applications this is
a long time, and model-dependent statistical guidance is not very responsive to short
range aviation needs, When products have been received, it is always possible for the
forecaster to add more recent information in a subjective manner. Objective
updating methods are under development (e.g. Glahn and Unger, 1986) but none is
in widespread use yet.
Second, NWP models tend to be subject to an adjustment period in the
shortest ranges, mainly due to the incompleteness of the initialization. The nature
and extent of the adjustment varies considerably, but it is common to see verification
statistics that sow, for example, that the moisture or precipitation forecasts are more
accurate at 24 hours and beyond than they are at 12 hours. To the extent that these
problems are systematic, they can be accounted for automatically in a MOS system
and the quality of statistical forecasts need not suffer, but it is a factor in determining
the responsiveness of statistical guidance in the shortest ranges.
Third, forecasts for some aviation elements such as ceiling and visibiliry are
required in considerably more detail than is required for other elements or for public
forecasts. The requirements generally exceed the capabilities of the resolution of
current models, and as a result, ceiling and visibility are widely regarded as the
hardest elements to show skill in forecasting. (Technique developers like to do them
last as a result - one cannot sell the value of statistical techniques on the basis of
ceiling and visibiliry forecasts). It is here that the greatest potential for improvement
exists resulting from improvement in NWP models.
A contingency table is a tabulation of the number of occurrences of all possible
combinations of observed and forecast categories in a verification sample of a cate-
gorical variable. Although they can be used for verification of probability forecasts,
contingency tables are more commonly used for evaluation of forecasts where a
specific category of a weather element has been forecast to occur. The entries of the
table are simply the counts of the number of times each particular forecast-observed
category combination occurred in the sample.
The sample table is for a three-category element. The following scores are
computed from the entries of a contingency table.
where: a, e,iare the number of correct forecasts in each category and T is the total
number of forecasts. The percent correct is the most commonly quoted score from
the contingency table. Events are all weighted equally, which means that the score
may be dominated by common categories. The percent correct can be quoted sepa-
rately by category to get around this problem.
The fals alarm ratio is the number incorrectly forecast divided by the total forecast
for each category
Th+ FAR is most often used for two category situations where one of the
categories is rare. One way of ensuring a high percent correct for a rare event is to
relax the criteria for forecasting it until all the occurrences are caught. The cost of
this isa high FAR, a tendency to forecast the event much more often than called for.
The FAR thus gives an indication of the tendency towards ''crying wolf'. It is desir-
able that the FAR be as low as possible.
This is the number correct divided by the number observed in each category. It is a
measure of the ability to correctly forecast a certain category, and is sometimes
referred to as the ''hit rate'', especially when applied to severe weather verification.
The POD has a range from 0 to 1, wih high values most desirable. It should
be used in connection with a measure of false alarms such as the FAR because it is
always possible to increase the hit rate for a particular category by forecasting it more
often.
The right oviduct is always longer than the left; this is associated with the
more forward position of the right ovary, Both oviducts are present in
henophidians. In Polemon notatus the left oviduct is missing.
In most henophidians a narrow, laterally compressed, dorsal process of the
premaxilla intrudes between the narrow anterior processes of the nasal bones, In
the study group the anterior margin of the compressed dorsal process may be
reduced so that it is hidden by the nasals in anterior view, there may be a broad
based process tapering to a wedge between the nasals or a broad process on
which the nasals rest. The low process is inferred to be the most ncarly primitive,
with the wedge and the broad process derived either separately or one from the
other. We have coded the low dorsal process as the all zero state. To allow the
derivation of one broad state from the other we add a third binary character for
which they both get a one score. Whichever of the three binary characters has
the lowest level of compatibility is deleted and the remaining two indicate the
preferred route of derivation.
In most henophidians the premaxillae have simple lateral processes, which do
not meet the maxillae. In lizards and in Anilius, Cylindrophis and Uropeltidae the
lateral processes do, however, meet the maxillae, On the basis that we root the
Caenophidia near to lLoxocemus, we regard a gap between premaxilla and maxilla
as primitive. In the study group we may find this primitive condition, the lateral
process may be turned backwards, a simple process may meet the maxilla, or the
lateral process may be bilobed. In Loxocemus and pythons paired ventral
processes of the premaxilla, separated by a deep cleft, meet the vomers, In the
study group there is a median process which may bear two lobes which are more
or less prominent, or may taper to a wedge between the vomers (characters 33,
34, 35).
there is no obvious basis for partition and we code only for prcscnce or absence of
teeth on the pterygoid.
In Loxocemus the palatine bone has a mesial process arching over the choanal
passage and approaching the vomer. This choanal process has a broad base
which extends back as a triangular process ovcrlapping the anterior end of the
pterygoid. There is a lateral, maxillary, process. In many henophidians this is
perforated by a foramen for the maxillary nerve with its posterior opening on the
dorsal side of the process, In the members of the study group the choanal process
may be long, short or absent. The maxillary process is always present but may be
imperforate. The palatine may overlap the anterior end of the pterygoid with a
broad process (Fig. 7) or a small finger-like process. The two bones may meet in
full width end-to-end contact. The ends of the two bones may taper, with either
point-to-point contact or separation by a gap (characters 58, 59, 60, 61; Fig. 6).
persuade us that there is a particular relationship between the American and
African genera. This may be revised if presence of a lateral rictal gland is
confirmed.
We therefore deleted the South American genera and pursued the analysis of
the 12 African taxa. Some binary characters were deleted as unvarying or
duplicating the scores of another binary component of the same variable. This
left 103 binary characters. We found a maximum clique of 24 charactcrs
marking 10 nodes of the dendrogram and giving complete resolution. Amongst
the 30 highest weight characters we found three further cliques marking nine
nodes in common with the first. We consider the alternatives.
T'he unique condition of the rictal gland (80) associates Aparallactus capensis,
Polemon and Chilorhinophis. This character is incompatible with character 48.1
which has some missing scores and is therefore discarded as of uncertain value. It
is also incompatible with character 58.1: complete versus reduced choanal
process. Reduction of this process is paralleled in many other snake lineages; we
therefore prefer character 80. These characters give us complete resolution
(Fig. 19). It is an anomaly that four stems of the dendrogram bear primitive
states of a total of six characters, which have to be interpreted as reversals to a
pseudoprimitive state. Insertion of the temporal ligament on a temporal scale
(73.1) is uniquely derived from the absent condition, Insertion of the ligament
on the postorbital (73.2) is homoplasiously derived from absence.
This study shows that, at least in M. jurtina, the male and female genitalia do
not form a precise lock-and-key. The portion of the genitalia used to classify
races of M4. jurtina in Europe (Thomson, 1973; Shreeve, 1989), namely the dorsal
process, has no obvious function during copulation. This suggests that variation
may be confined to non-functional parts of the valve, and is in accordance with
Lorkovic's view that male genital morphology is not tightly constrained: a
degree of variation may not be subject to direct selection. This is supported by
the absence of any apparent relationship between valve shape and either mating
success or strength of the male-female bond,
If variation in the shape of a large portion of the valve is not constrained by
selection, then differences between races or subspecies in the valve shape are
unlikely to act as barriers to cross-fertilization, Clearly there is some doubt as to
the accuracy of the lock-and-key analogy. This is of some concern to taxonomic
studies in which the genitalia are assumed to provide one of the most reliable
characters by which species may be distinguished. The use of genitalia in such
studies must be reconsidered: if intraspecific variation in valve shape is not
tightly constrained by selection then how reliable is its use in describing species?
The consistent use of valves in taxonomy over many years may be reason enough
to accept their use as valid, for if it regularly produced anomalies and
inconsistencies they would long ago have been discarded as an important tool.
Why then does the valve shape of the Lepidoptera remain a valuablc taxonomic
character? I would argue that evolution of male genital morphology is best
considered in two parts, those which contact the female during copulation, and
those which do not. The sexual selection of female choice model and the lock-
and-key hypothesis are both only of relevance to those portions of the genitalia in
contact with the female. The rest of the genitalia may be neutral to selection and
subject only to random processes, or to pleiotropic effects as suggested by Mayr
(1963).
This study may explain why genitalia are so valuable in taxonomic studies, for
neutrality to selection is one of the key features used by the traditional school of
evolutionary taxonomy to select characters for use in taxonomic studies, for
analogies (as opposed to homologies) are less likely to evolve when selection is
absent (Ridley, 1986).
I would like to thank Dr Denis Owen, Dr Andrew Lack and Dr Tim Shreeve
for discussion and advice while carrying out this work, and for constructive
criticism of my writing. I would also like to thank Oxford Polytechnic for
providing funding.
occupied by pallial organs. The kidney lies dorsally and is wide, enveloping
other organs from the posterior end of the pallial cavity to the anterior stomach.
The digestive gland is dark brown, occupying half the length of the body, It
consists of two separate parts. The posterior digestive gland is massive and
occupies most of the body situated just a short distance behind the stomach. The
anterior digestive gland is a thin, irregularly and deeply lobed mass, appressed to
the right side of the posterior stomach, although its upper end arches frontally
and reaches the anterior stomach. The gonad has a pale yellowish colour, in
sharp contrast with the underlying digestive gland.
The ctenidium (Fig. 5) has a slightly arched outline, blunt anteriorly but
slender posteriorly. It is almost as long as the pallial cavity, and over a third as
wide. It is placed to the left of the mid-dorsal line. The gill leaflets are thick, low,
and tightly packed into a battery of 28 in the male and 27 in the female.
The osphradium (Fig. 5) is annular, with an elongate, irregularly
subtriangular outline. It is short, spanning over less than one-fifth of the
ctenidial length. It is situated almost parallel to the mid-length of the ctenidium,
and closer to the neck.
The radula (Fig. 6) is typically hydrobioid. The central tooth of each row
bears a long, dagger-like middle cusp flanked by six smaller cusps on either side,
which decrease in size towards the outer edge and are slightly inclined to the
Computer vision is a subset of the
machine intelligence systems area. The
goal of a computer vision system is to
interpret the given ''visual'' data and
to use the interpretation to complete a
task. Typical tasks include (1) the navi-
gation of autonomous vehicles on the
land, in the air, or under the sea, (2)
the assembly or inspection of manufac-
tured parts, and (3) the analysis of
microscopic images and medical x-rays.
In a number of applications, the goal of
the vision system is to identify and locate
a specified object in the scene. In such
APPENDID A. DIFFERENTIAL GEOMETHY OF
SURFACES
APPENDID B. SURFACE PROPERTY
MEASUREMENTS
ACKNOWLEDGMENTS
REFERENCES
cases, a vision system must have full
knowledge of the shape of the desired
object. Such a priori knowledge of
the object is provided through a model
of the object, and in most cases it con-
tains information regarding the geome-
try of the objects; some models may
contain additional information such as
thermal and stress properties of the
objects. A vision system which makes use
of an object model is referred to as a
model-based vision system, and the gen-
eral problem of identifying the desired
object is referred to as object recognition.
While there is no single definition of the
object recognition problem, the objective
is to identify a desired object in the scene
and to determine its exact location and
orientation.
An ideal model-based vision system
should be able to locate objects in a scene,
assuming that any of the following are
true: (1) objects may have arbitrary and
complicated shapes or forms; (2) objects
may be viewed from any direction; and
(3) objects may be partially occluded by
other objects. Specifically, such a system
may be used in determining the location
of grasp sites for a robot arm to mani-
pulate an object, in the navigation of a
robot or an autonomous vehicle, and in
the assembly and inspection of parts
in a manufacturing environment. For
instance, robots with such vision capabil-
ities may carry out instructions regard-
ing the handling of objects with fewer
specifications than are currently required
and with more tolerance for minor
disturbances.
To design such systems, system
designers must resolve the following
issues: (1) the type of sensor for data
collection, (2) the methods of construct-
ing the necessary object model, (3) the
means of describing the collected data
and the model, and (4) the methods of
matching the object descriptions obtained
from the input data to that of the model.
The sensor determines the resolution (the
total number and frequency of sample
points) and precision (the accuracy of
each sampled point). More importantly,
it determines whether the data provides
2D or 3D information of the scene. Mod-
els provide the a priori knowledge of the
vision system. Representations are used
to describe the collected data and the
object model, a key issue in the field of
computer vision. The representations dic-
tate the matching strategy, its robust-
ness, and the system's efficiency. Also,
the descriptions are used in the calcula-
tions of various properties of objects in
the scene needed during the matching
stage. Matching strategies are performed
at run-time and must resolve many
ambiguities that exist between the data
and the model descriptions. Once the cor-
rect match has been determined, the ori-
entation and translation of the located
object, with respect to the model, can be
calculated, completing the task of object
recognition.
This paper surveys recently published
papers' addressing the problem of the
model-based recognition of objects in 3D
dense-range images.' Section 1 discusses
in detail the issues outlined above to
introduce the specific problems of model-
based vision. Next, Section 2 reviews var-
ious sensing modalities, beginning from
the data collection step, giving emphasis
to sensors providing 3D information.
Section 3 reviews various low-level pro-
cessing procedures necessary as a prestep
to the description and recognition of
objects. Section 4 reviews various repre-
sentations used to describe the objects.
Section 5 discusses modeling schemes,
giving emphasis to the computer-aided
design (CAID) systems used in the exist-
ing model-based vision systems. Section
6 presents a study of the matching
strategies. Finally, Section 7 presents the
summary and concluding remarks. A
brief overview of differential geometry of
surfaces is presented in Appendix A, and
in Appendix B various computational
methods to calculate surface properties
are reviewed.
The introduction outlined the issues
involved in the design of a model-based
vision system. This section analyzes fur-
ther these issues: data collection, repre-
sentation, model construction, and the
matching strategies (see Figure 1).
The first issue addressed in a com-
puter vision system is data collection,
which may be performed using one or
more of the many existing modalities.
The intensity camera is perhaps the most
commonly used sensing module, measur-
ing visible light. The output of the cam-
era from a scene is digitized to provide a
2D array of numbers; each number corre-
sponds to the average intensity sensed in
a sampled, typically square area. Exam-
ples of other sensory modules include
thermal cameras, which measure the
emitted thermal radiation rather than
the emitted visible light, and laser range
scanners and sonar sensors, which are
used to calculate the distance to the
objects in the scene. These sensors each
provide information on a different aspect
of their environment; the choice of the
sensor is largely application dependent.
For the task of 3D object recognition,
various object dimensions and surface
shape information are essential. Two
general approaches exist to collect the
necessary data. In the first approach,
sensing modalities, such as intensity
cameras, are used, and many depth cues
are analyzed to recover the necessary 3D
information. In the second approach,
external energy sources, such as lasers,
are projected onto the scene. While the
first approach is preferred (since no
external sources are required), the recov-
ered data lacks the necessary resolution
and precision for many common tasks;
therefore, the second approach is used
most frequently in 3D object recognition.
Section 2 studies the schemes for collect-
ing 3D information from the scene.
The second issue in a computer vision
system is to represent the collected data
and the modeled object. The 2D array of
numbers provided by the sensor is not
of much use in its ''raw'' form. A suitable
trepresentation scheme must, therefore, be
used to describe the data and the model.
A representation is desirable if it is (1)
unambiguous (no two objects have the
same representation), (2) unique (there is
a single description for each object using
the representation scheme), (3) not sensi-
tive (with respect to missing data points,
such as in the cases of partial occlusion),
and (4) convenient to use, in the match-
ing stage, and to store. Representation
is a key issue in computer vision. Vari-
ous schemes, such as surface-based and
volumetric-based representations, will
be discussed, with emphasis given to
recently published representation
schemes.
The construction of object models is
the third issue which must be addressed
in a model-based computer vision sys-
tem. There are two main approaches for
model construction. In the first approach,
the actual objects are used to generate a
model; i.e., data points obtained from
several viewpoints of the object are inte-
grated in a coherent fashion to provide
information from all the viewing angles.
In the second approach, a CAID system is
used, and a set of predefined primitives
allows the user to construct interactively
the model of an object. Much of the ear-
lier work in object recognition used the
first approach; however, most recent
research efforts use a CAD or similar
system. Both approaches are reviewed,
and the advantages and the disadvan-
tages of each are discussed.
Once the appropriate descriptions are
derived from the data and the models,
the vision system is able to match the
two descriptions. This is performed
in two steps. In the first step, a corre-
spondence is established between the two
sets of descriptions. Since in most cases
data is collected from a single view and
there may be partial occlusions present,
the matching strategy must establish
correspondence between the partial
description of the object and its full model
description. The correct match of the
collected data to the representation of
the given model ''establishes an interpre-
tation'' of the input data [Ballard and
Brown 1982]. The exact choice of the
matching strategy is dependent on
the representation scheme, the applica-
tion, and the system designer's expertise.
In the second step, using the established
correspondences, a geometrical transfor-
mation (usually a rotation matrix and a
translation vector) is derived such that
the model may be transformed to the
orientation of the object in the scene.
An important aspect of any computer
vision system is its data acquisition mod-
ule. This section reviews the schemes in
which 3D information is acquired from
the scene. The task is performed in one
of two approaches: passive or active. In
the passive approach, 3D information is
inferred from the scene using existing
energy in the environment, such as
reflected light. In the active approach,
the 3D information is derived by project-
ing external energy waves, such as sonar
waves and laser light. As mentioned in
the introduction, 3D data recovered from
current passive approaches lack the nec-
essary precision and resolution for 3D
object recognition; in this section, the
active methods are reviewed [Besl 1989;
Nitzan 1988; Freeman 1988; Fu et al.
1987; Kanade 1987; and Ballard and
Brown 1982].
Active-range sensing can be divided into
two main classes. In the first class, the
principle of triangulation is used. Each
point in a scene is highlighted, using a
sheet of light, and observed by the sen-
sor. Then, using the known geometry of
the imaging system, the distance of each
highlighted point to the sensor is calcu-
lated. In the second class of active-range
sensors, known as time-of-flight sensors,
a signal is emitted, and its return time is
measured and used in calculating the
distance.
This method uses a laser source which
projects a sheet of light onto the scene,
casting a line on the objects' (see Figure
2). A camera is positioned so that the
laser line is visible. Using the known
imaging geometry, i,e., the distance
between the laser source and the camera
(referred to as the baseline), their angles
a, and ag) with the z-axis (the axis
along which depth is measured) and by
applying the principles of triangulation,
the distance of the illuminated points,
along the cast laser line, to the baseline
is calculated (see Figure 2). Sweeping the
sheet of light across a scene results in a
range map. The sweeping of the sheet is
performed in one of two ways: a rotating
reflector can be used to project the sheet
of light, or the objects can be placed on a
stage which slides by or rotates in front
of the sheet of light. In all cases, the
laser may not illuminate some areas, or
the sensor may not be able to detect the
illumination (i.e., the object concavities
may occlude some areas of the object);
and there are no data points at those
locations of the scene. These areas of
missing points are referred to as shad-
ows,' An advantage of using the stage is
that more uniform spatial sampling
can be achieved, satisfying a common
assumption for surface property mea-
surement approaches (see Figure 3 and
Appendices B and A). The disadvantage
of using the stage is that it limits the size
and number of objects in the scene, and
it may not always be feasible to place the
objects on the stage. Also, the accuracy of
the motion control mechanism of the
stage or the reflector must also be taken
into consideration in determining the
approach used.
In most of today's available triangula-
tion systems, the time required to scan a
typical 256 x 256 scene with 8 bits of
accuracy is on the order of minutes.
However, some triangulation-based sys-
tems exist which are capable of scanning
scenes in seconds. For example, Rioux
[1984], and Rioux et al. [1989] developed
a system able to scan a 256 x 256 pixel
scene with a precision of 0.5 mm in less
than one second. Kanade et al. [1989]
and Gruss et al. [1990] introduced a small
prototype of a VLSI-based system, which
performs the triangulation on a 4 X 4
array of specialized sensors on a single
CMOS chip. Once fully developed, this
system could significantly reduce the
acquisition time in triangulation-based
range sensors.
To reduce the data acquisition time,
several sheets of light in parallel may be
projected onto the scene (first introduced
by Will and Pennington [1972]). The dis-
advantage of this approach is that,
depending on the scene's geometry, a
stripe position may be shifted more than
the existing spacing between the stripes
[Mundy and Porter 1987], causing ambi-
guities. Therefore, it is necessary to
determine which sheet of light is pro-
jected at each pixel in the scene. There
are several ways to resolve this ambigu-
ity. Mundy and Porter [1987] and
Inokuchi et al. [1984] use a set of gray-
coded stripes, assigning to each stripe a
unique code value. Boyer and Kak [1987]
and Vuylsteke and Oosterlinck [1990] use
only one set of simultaneous projections,
instead of a series over time, to obtain
the range information from the scene.
The advantages of using a single set of
patterns are that less time is spent pro-
jecting light patterns, and more impor-
tantly, nonstatic scenes may be scanned,
allowing a broader application area such
as robot navigation, motion analysis, and
moving-object recognition. Boyer and Kak
use a few colors to project single-colored
stripes. Vuylsteke and Oosterlinck use
a binary pattern to illuminate the scene.
Tajim a and Iwakawa (of NEC)
[1990] have reported on a triangulation-
based sensor using collimated white light
diffracted by a grating to form a rainbow
pattern on the scene capable of producing
30 3D frames per second.
One of the disadvantages of a triangula-
tion-based laser scanner is the shadow
effect, where a region of the scene is not
visible to either the laser or the sensor.
In time-of-flight range finders, a laser
beam is emitted and received along the
same path, eliminating the shadow prob-
lem. However, since the system depends
on the return laser light to measure the
distance, high-energy laser sources, pos-
sibly harmful to the human eye, are
required. Also, most time-of-flight range
finders require complex electronics, rais-
ing the cost of such sensors. Two classes
of laser sources are used in time-of-flight
scanners: pulsed and continuous-beam
lasers.
In this class of range finders the laser
light is reflected, and its return time is
measured [Lewis and Johnston 1977];
since the speed of the laser light is known,
the distance can easily be determined.
Such devices have ranges of from 1 to 4
meters with a precision of 0.25 inches;
however, such precision requires sensi-
tive electronic instrumentation capable
of resolving 30-50 psec time intervals
[Jarvis 1983a; 1983b].
The second class of time-of-flight range
finders uses a continuous-beam laser
rather than a pulsed one [Jarvis 1983b;
Fu et al. 1987]. In this method, the delay
6 in the returned signal (i,e., the 0 to 2m
phase shift with respect to the transmit-
ting signal) is used to measure the dis-
tance D to the object:
where A is the wavelength of the signal.
This measurement is performed one point
at a time, and the laser beam is scanned
horizontally and vertically across the
scene. An inherent problem in this
approach is that all distances corre-
sponding to 2m multiples of the phase
shifts will be measured as the same dis-
tance by the system (referred to as the
ambiguity interval); one may, however,
assume that 0 is less than 2m. Addition-
ally, since A is usually on the order of
nm, D has a very small range which
is not feasible for most object recogni-
tion tasks. The amplitude of the laser
light may be modulated, however, using
long-wavelength waveforms, effectively
increasing A.' However, the ''ambiguity
interval'' of the scanner still exists and
may be resolved by transmitting at sev-
eral frequencies and checking all fre-
quencies at the ambiguity intervals. To
decrease the effects of photon shot noise,
the distance at each point is measured
several times, and the average is used.
A problem common to all laser range
scanners is the specularity problem,
causing erroneous measurements in both
the triangular-based and the time-of-
flight methods. In laboratory setup it is
possible to decrease this problem by
painting the surfaces appropriately; how-
ever, in outdoor scenes this problem per-
sists. In addition, when laser lights are
used, other sources of error, such as
HE motivation for this paper is the observation that a
scene containing more than one object most of the time
cannot be segmented only by vision or in general by any
noncontact sensing method. Visual information may be suf-
ficient to accurately segment simple objects and nonoverlap-
ping scenes. However, in general, it is not sufficient for
random heaps of unknown objects.
If no a priori knowledge is available, the vision system
cannot reliably distinguish between overlaps caused by two
different objects in the scene and overlaps caused by a single
self-occluding object. A flat rigid object supported by and
totally occluding another smaller object may be recognized as
a large box-shaped object. Similarly, a flat nonrigid object
supported in the middle by a smaller object may be recog-
nized as convex, while if it is supported at the edges by more
than one object, it may be recognized as concave.
Therefore, machine vision alone (or any noncontact sens-
ing method) is not sufficient for segmentation and recogni-
tion. An exception to this may be the case when the objects
are physically separated so that the noncontact sensor can
measure this separation or one knows a priori a great deal
about the objects (their geometry, material, etc.).
The traditional approach is to segment the noncontact
sensory information (range, intensity, etc.) regardless of
scene complexity. Then, based on the outcome of segmenta-
tion, to interpret the scene and recognize the objects. The
problem with this approach is that reliability decreases when
scenes become more complex and when a prlori assumptions
are removed.
Our approach is different. Instead of trying to deal with an
ever increasing visual scene complexity, we use the manipu-
lator to make the scene simpler for the vision system. Our
paradigm is analogous to having the hand help the eye when
interpretation of visual information is ambiguous, or when
the scene is visually complex.
Our system is iterative because random arrangements of
objects form layers. Due to our noncontact sensor arrange-
ment, only the top layer of the heap is visible at any given
time and the objects are removed from the scene one at a
time. In general, the system must sense and manipulate more
than once for every random scene.
The system is interactive because the vision system may
request a manipulatory action to resolve an interpretation
ambiguity, to reduce visual complexity, or to grasp and
remove an object from the scene. The manipulatory action
must be monitored by the noncontact sensor (vision system)
as well as the contact sensors (force/torque) in a closed loop.
Our assumptions are:
The domain is the class of irregular parcels and pieces
(IPP) found in a post office environment. The class consists
of rigid and nonrigid flats, boxes, tubes, and rolls. The
objects have different weights, sizes, colors, visual surface
textures (address labels, stamps, and other markings), vary-
ing porocity, coefficients of friction, and rigidity. Because
many of these objects are not rigid, their true geometric
shape cannot be measured; it is rather a function of where the
object is in a random heap, how it is supported by its
neighboring objects and other objects that it supports. The
heaps are formed by emptying a sack of an unknown mixture
of IPP's on a conveyor.
Our immediate goal is to physically segment and sort a
random heap of IPP's into several output streams of single
similar-shape objects. In other words, our first goal is to
disassemble random heaps of separable objects (held together
by gravity and friction) into three output streams. The first
stream contains two-sided planar objects (flats). The second
stream contains three-sided nonplanar objects (tubes /rolls).
The third stream contains six-sided planar objects (boxes).
We view this physical segmentation of disassembly prob-
lem as a subclass of the more general disassembly problem,
which we will address in the future. We believe that the
solution to the general disassembly problem is active sensing
[3], as opposed to the traditional static analysis of passively
sampled data. The problem of active sensing can be stated as
a problem of intelligent control strategies applied to the data
acquisition process that will depend on the current state of the
data interpretation including recognition. This approach is
gaining more and more recognition in the literature, [2], [5],
[10], [21]. In this paper we shall describe our model of
segmentation via interaction between vision and manipula-
tion. We will generate several segmentation strategies. We
will describe the experimental system and the experiments.
The model of segmentation has the following components:
models of sensors, models of actions, a task/utility model, a
world model, and a control model. The segmentation process
is formulated in terms of graph-theoretic operations that are
mapped into corresponding manipulatory actions.
Sensor models include the characterization of the noncon-
tact sensor such as the spatial resolution, signal-to-noise
ratio, and their like; the physical parameters of the different
end effectors, such as a vacuum suction cup; the size of a
spatula for pushing objects; the span of a gripper; and the
maximum allowable forces and torques. Models of objects
are specified in terms of their geometry, size, and substance.
Our world consists of random arrangements of objects
called heaps. Object models are boxes, flats, and tubes/rolls.
A heap is represented by a directed graph. Objects in the
heap are represented by vertices, and the on-top-of relations
among objects are represented by directed edges. A scene is
a partial view of a heap as sensed by the noncontact sensor.
A scene is represented by a directed graph, where surface
segments are represented by vertices and the on-top-of rela-
tions among the surface segments are represented by directed
edges.
It is important to emphasize that, in general, the diagraph
representing the heap is different from the graph representing
a scene. This is because the scene diagraph represents spatial
relations of only the visible surface segments, i.e., as they
appear through the visual sensor, which may not always be
the same as the physical objects. The true physical arrange-
ments of objects in the heap (i.e., the heap diagraph) is not
known, unless given a priori. Only the scene diagraph is
measurable and constructable from noncontact sensory infor-
mation.
In this paper, our task is to measure and construct the
scene diagraph and to use the manipulator to decompose it. In
future work we will show how to use manipulatory and
exploratory actions to recover the true part- whole relation-
ships (i.e., to compose an object diagraph from its scene
diagraph).
Task models include the final goal of the process. An
example of a final goal may be the empty scene. Intermediate
goals may be those scenes that are simply measured by a
cost/benefit function. This cost/benefit function entails the
cost of performing the particular manipulation, and the bene-
fit is measured via the estimate of the outcome of the
manipulation with respect to the final goal, i.e., emptying the
Scene.
There are two types of actions: sensing actions (i.e., data
acquisition) and interpretation actions (such as: look, and /or
feel), and manipulatory actions, such as: pick, push, pull,
and shake. The purpose of the manipulatory actions is to
exert physical disturbances into the scene, being either global
(shake) or (push, pull). In view of our formulation of the
segmentation problem as a graph generation /decomposition
problem, we classify the manipulatory action in relation to
the operation that applies on the diagraph. There are two
such operations: the vertex removal, which means, in terms
of manipulation, removal of an object from the scene, and
edge removal, which in turn translates into object displace-
ment in the scene so that the on-top-of relationship does not
hold any more between the two objects. An isomorphism
exits between the manipulation actions and graph decomposi-
tion operations [23].
Our approach is to close the loop between sensing and
manipulation. The manipulator is used to simplify the scene
by decomposing the scene into visually simpler scenes. The
manipulator carries the contact sensors and the manipulation
tools to the region of interest and performs the necessary
manipulatory movements that will result in a visually simpler
scene. The control model deserves special attention and is
described next.
The control model is a nondeterministic finite-state Turing
machine (NDTM) and is shown in Fig. 1. The physical world
(scene) is the tape of the machine, the read actions are the
sensing actions, and the write actions are the manipulatory
actions. The model is a Turing machine because the manipu-
lation actions constantly change the physical environment
(tape) and hence its own input. The model is nondeterministic
because of the nonpredictable state of the scene after each
manipulatory step. From this of course follows also the
nondeterministic control of actions. In addition to the nonde-
terminism of the control strategies, the machine has finite
states, which are determined by the finite numbers of recog-
nizable scenes and the finite number of available actions.
This model is quite general, providing that one can quantize
the scene descriptions and the sensory outputs into unique
and mutually exclusive states, and of course one has only a
finite number of manipulatory actions.
As is well known, the nondeterministic finite-state automa-
ton (NDFSA) that controls the Turing machine is defined as a
quadruple (I, O, S,'T) where:
Fig. 1 describes the sensing and manipulation interaction
for segmentation. Relating this diagram to the NDFSA, we
shall describe in subsequent subsections the inputs, outputs,
states, and the transition function, i.e., the control, respec-
tively. There are several advantages to the formalism of the
nondeterministic finite-state Turing machine.
I) Inputs: As indicated above, the inputs come from
sensors. In our current implementation, the sensor is a laser
range imaging system (noncontact sensor). The scene is
segmented into spatially connected surface regions. For each
region, we compute the position of the center of gravity, the
orientation of the surface normal at the center of gravity, an
estimate of the size of the smallest parallelepiped bounding
the region, and an estimate of the maximum curvature. From
these measurements, the objects are initially classified into
one of three generic shapes such as: flat, box, and tube /roll.
These are four object models.
The on-top-of relation between all pairs of visible regions
in the scene is computed and the directed graph representing
this relation is constructed. Vertices represent visible, con-
nected, surface regions. Directed edges represent the spatial
relations between the vertices. See Figs. 2-5.
Top-most surface segments are important in physical scene
segmentation because they may belong to top-most objects in
the scene. Top-most objects are important because they usu-
ally have more surfaces exposed (more ways to be grasped).
The forces required to extract them from the scene are less,
and therefore the chances of loosing positional information
after the object is being grasped are minimized. Furthermore,
manipulating the top-most object keeps scene disturbances to
a minimum.
A partially dispersed scene corresponds to a disconnected
diagraph. An efficient algorithm based on ''fusion'' of adja-
cent vertices is given in [8]. A totally dispersed scene (as
well as a singulated scene) corresponds to a null graph (a
graph with vertices and no edges). Efficient graph theoretic
algorithms exist (testing the diagraph's adjacency matrix for
all zero entires) for singulation verification. Finding the
top-most objects in the scene corresponds to topological
sorting of the diagraph.
2) Outputs: There are two types of outputs. These are
sensing actions, (look, feel) and manipulatory actions (pick,
push, pull, shake, and stop). In this implementation, the look
and feel actions are only commands to take data. In our
future work, these actions will be more complex, i.e., the
system will choose its view point, sampling rate, resolution,
and other data-acquisition parameters. In addition, the cost of
the sensing actions will be included in the overall control
schema.
The manipulation actions are composed hierarchically from
simpler actions. Shake is the simplest action; it provides
global disturbance and displacement to the work place. On
the other hand, push and pick exert local disturbance and
cause local displacement of an object. In fact in our imple-
mentation, both the push and pick actions have two forms:
''push with spatula,'' ''push with suction tool,'' ''pick with
gripper,'' ''pick with suction tool.'' See Fig. 12 below for an
example of a ''pick with suction tool'' action. In addition,
each of these manipulatory actions is associated with an
''error recovery'' action.
The hierarchy of actions is in terms of composition of
complex actions from simpler actions and does not apply to
the execution of these actions. The hierarchy of action com-
position is given in [23]. An example of such hierarchy is
shown for the action: ''pick with gripper'' in Fig. 6. Each
node in the graph in Fig. 6 is a manipulatory action. Some of
these actions are modeled as deterministic finite state au-
tomata (FSA), while others are modeled as nondeterministic
finite-state automata (NDFSA). The lowest level in the hier-
archy of actions consists of very simple actions, such as:
''robot move to'' (RMT), ''robot move to while sensing''
(RMTS), ''gripper move to'' (GMT), ''gripper move to
while sensing'' (GMTS), and their like.
The advantages of hierarchical construction are modular-
ity, testability, and incremental growth. These actions (as
expected) use additional information from contact sensors.
Some of the contact sensors are as follows. Two force/torque
sensors (mounted on the gripper jaws) are used in closed loop
feedback during manipulation. Force feedback is used to
provide force servoing to the gripper, to sense collisions, to
measure the weight of objects, and to determine if an object
or tool is properly grasped. A finger position sensor is used
in a closed-loop feedback manner during manipulation. Posi-
tion feedback is used to provide basic position servoing to a
gripper and to refine size estimates of objects (computed
from vision). A vacuum sensor is used to verify proper grasp
and to differentiate small-size nonpenetrating cavities from
holes that penetrate an object. Note that all the contact
sensory feedback is carried out in a local reflexive mode
rather than in a planned mode with one exception, that is,
when a pathological state is detected.
3) States: This is a finite set of states describing the
environment of the Turing machine as perceived by the
sensors. If new sensors are added, the set of states is parti-
tioned to describe the scene as perceived by the additional
sensors. For example, if a sensor capable of determining the
''touch'' relations of objects in the scene is added, then the
set of five states, can be partitioned (a finer partition) to
describe both the ''touch'' and ''on-top-of'' relations. The
states of the machine are:
The goal of scene segmentation is the empty state. This
state must be not only reachable but also measurable with the
current sensors. In other words, for the machine to halt, the
system must have sensors to sense that the goal state has been
entered. In this work, the empty state is both reachable (see
section on strategies) and easily measurable (all range values
in the scene are zero, which means that no surface segments
and thereby no objects exist in the scene).
A specific place must be given to error states. They are
prioritized in order of severity (most severe first). For more
details, see [23]. The pathological states are: ''sensor dam-
aged,'' ''unable to get tool,'' ''lost tool,'' ''lost object and
tool,'' ''lost object in the scene, '' ''lost object away from
the scene,'' ''unable to reach object,'' ''unable to pick,'' and
unable to push.'' As more sensors and actions are added
into the system, more, yet finite, pathological states must be
defined. When the machine enters one of these states, error
recovery actions are evoked.
4) State Transition Function: The control problem is
transformed into the problem of topological sorting of object
arrangements. The manipulation actions of object acquisition
(pick) and local displacement (push) are defined as decompo-
sition operations on diagraphs representing the on-top-of
relation of objects in the arrangement. The pick action is used
to break the vertex connectivity of the diagraph by removing
vertices. Several tools may be used to implement this action.
An object may be picked and removed from the scene using
the gripper, or it may be picked by selecting a tool (i.e., a
suction tool). The push action is used to break the edge
connectivity of the diagraph representing the on-top-of rela-
tion. Several tools may be used to implement this action. An
object may be pushed using the gripper, or it may be pushed
by selecting a push tool (such as a spatula or the suction
tool). Complete planning of the push actions is very compli-
cated [14]-[17] and requires knowledge of the friction coef-
ficients of all objects in the scene as well as knowledge of the
spatial relations of all objects in the scene to decide where
and how far to push.
In Section II, we described a nondeterministic finite-state
Turing machine as the control model for sensing and manipu-
lation for scene segmentation. This very general model is
sufficient to describe every strategy for the following reasons:
Let us recall that the ''read from tape'' are the sensing
actions, ''write to tape'' are the manipulatory and error
recovery actions, and the states are scene descriptions. Even
with the restriction that one can categorize every scene into
distinct classes (discrete states) we had to add the following
rules:
With the above rules and the theory described in Sections
II and III, we can compose several different strategies to
examine the validity and generality of our theory for scene
segmentation.
Strategy 1 is a noninteractive loop: (look, pick, look,
.. .). The control structure is shown in Fig. 7. The strategy
does not use local displacement (push). The general idea is to
look, pick the top-most object, and look again. If the scene is
ambiguous or unstable, it shakes the heap. If shaking fails, it
continues with the pick action. This strategy is simple and
very effective in dealing with scenes where all objects are
graspable with the set of acquisition tools. The strategy
eliminates ambiguities via the shake and pick actions. If the
shake action fails to remove the ambiguity, then nontopmost
objects are picked up. This causes objects to be lost during
acquisition. For the strategy to succeed, the sensor thresholds
must be raised to enable the system to tolerate higher torques
caused by picking objects off the center of gravity. When the
threshold is raised, the probability of tool losses increases as
well as the probability of damaging the sensors. Therefore,
the probability of entering the fatal error state is increased. If
the weight of the objects is low, the probability of damaging
the sensors (even if the system picks objects supporting other
objects) is low, and the strategy converges; see [23].
Strategy 2 is a noninteractive loop: (look, push until
dispersed, pick, look, . . . ). The control structure is shown in
Fig. 8. This strategy allows no interaction between the pick
and push actions. The only interaction allowed is when the
push action cannot reduce the number of edges in the associ-
ated graph any further. The strategy enforces a rather strong
partition on the manipulation actions. This shows up as a
serial plan where a single action is triggered from one state
and the automaton iterates until the ''look'' action brings the
automaton to another state. This strategy is very effective in
dealing with heaps of few, small-sized objects relative to the
workspace. As object size and number increases, so does the
number of objects pushed out of the scene and never picked
up. For a proof of convergence, see [23].
Strategy 3 is an interactive loop: (look, pick/push,
look, . . .). The control structure is shown in Fig. 9. The
central idea is to allow immediate interaction between the two
manipulation actions. Since pick is more effective than push,
priority is given to pick. Only if an object cannot be picked
up after several unsuccessful attempts is the next immediate
action to push that object, and to immediately return to pick
Data structures used in machine vision are often
classified as either ''iconic'' or ''symbolic''. An iconic
data structure is one whose principal organization is
that of a two-dimensional array. The elements of the
array may be bits, integers, real or floating-point
numbers, or more complicated structures. Each cell
of an iconic data structure is implicitly associated
with a location in two-dimensional space, by virtue
of its being indexed by two numbers. On the other
hand, a symbolic data structure, although it may
represent pictorial information, takes the form of a
scalar, list, graph, string, or table. Unlike an iconic
structure, spatial information, if it is to be included
in a symbolic data structure, must be explicitly repre-
sented, for example, by including coordinate pairs
in the structure. It is usual for iconic data structures
to be used for image data (including ''intrinsic
images'') and symbolic ones to be used for more
abstract information such as scene descriptions in
terms of regions and relationships, highly codified
shape descriptions, and semantic models or inter-
pretations.'ol
Corresponding to the distinction between iconic
and symbolic data structures in machine vision, there
isa separation of ''retinotopic'' and ''nonretinotopic''
descriptions of neural areas in the neurophysiology
of the mammalian visual system. A retinotopic area
is one in which the neural activity is generally in
spatial correspondence (i.e. mapped in continuous
fashion) with the activity in the retina. A non-
retinotopic area is one in which the activity shows
no general spatial dependence upon the distribution
of activity in the retina.
Several factors motivate the study of special iconic/
symbolic architectures. Most importantly, parallel
image-processing systems such as the CLIP4' and
the MPP,P while very effective for point-neigh-
borhood image transformations, lose much of their
speed advantages when they must communicate with
single-processor hosts in computations of such non-
iconic transforms as chain encodings, polygonal rep-
resentations, Hough transforms, region-adjacency
graphs, syntactic descriptions of shape, or schema
instantiations.
A second factor is the awkwardness of computing
''hypothesis maps'' for topdown image analysis on
existing architectures; such operations seem to call
for special symbolic-to-iconic hardware that can per-
form certain kinds of ''plotting'' very rapidly.
A third motivating factor is that a study of the
architectural problems of iconic/symbolic trans-
formations may suggest new computational models
for related information-processing activity in natural
(human and animal) visual systems, and conse-
quently, to improve our understanding of natural
vision.
We note that, in general, multiprocessor systems
exist which can support both iconic and symbolic
processing, but these are expensive, and are non-
optimal for iconic-to-symbolic processing. Examples
of these include the following commercial and
research prototype systems: the BBN Butterfly,
Columbia University's Non-Von, the NYU Ultra-
computer, the CalTech Cosmic Cube, and others. A
survey of some of these parallel systems may be
found in reference (4).
In the following sections we present a discussion
of the relative strengths and weakneses of special
architectures that support rapid iconic-to-symbolic
and symbolic-to-iconic transformations.
One can classify the proposed architectures for
iconic/symbolic processing into two types: (1) hard-
wired transformation devices and (2) interfaces. The
hardwired devices provide specialized computing
capability for particular iconic-to-symbolic trans-
formations. The interfaces, on the other hand, make
it easy for iconic processors and symbolic processors
to communicate efficiently and leave the actual pro-
cessing to these more conventional components.
Four particular approaches are reviewed in this
article. Two of these are hardwired transformation
devices and two are interfaces. Each of the four
approaches is presented here in a somewhat abstract
form. That is, each method is described with a com-
putational model which is somewhat simpler than
what one might actually implement. Nonessential
features are omitted from the models to facilitate
comparison and analysis of the basic approaches.
Each of our four models is based upon a proposed
device or system that has appeared in the literature.
Our ''Image-Function Inverter'' is modeled after the
''ISMAP'' (Iconic/Symbolic MAPper) proposed by
Kent at the National Bureau of Standards and sub-
sequently developed by Aspex, Inc. of New York.')
The ''Chain-Run Encoder'' described here is based
upon a device designed by Pfeiffer at the University
of Washington.''! (Pfeiffer is now at New Mexico
State University.) The ''Bi-Modal Memory'' system
has been proposed by Tanimoto, and the ''Tile-
Based Interface'' is a model inspired by the
''CAAPP/SNAP'' (a joint effort between the Uni-
versity of Massachusetts''-*' and the University of
Southern California''' which was an important step
in the development of the DARPA Image Under-
standing Architecture). The first two of these pro-
posals fall into the hardwired-device category. The
other two are essentially interface schemes.
At the time of this writing, one of these devices
has been built and tested. The ISMAP is the least
complicated and is now available as an option for
the PIPE from Aspex, Incorporated of New York.
Two others have had chips designed and fabricated
(the BMM and the chain-run encoder), but the chips
have not yet been integrated into an overall system.
These four approaches give one perspective on the
range of problems and possible solutions that arise
in studying the issue of iconic/symbolic architecture.
Figure 1 is a chart which offers an analysis of the
appropriateness of each of these four architectures
to the iconic-to-symbolic computations listed. It is
assumed that each iconic/symbolic architecture
would be used in the context in which it is proposed.
Later sections of this article describe some of the
algorithms involved in this analysis.
The transformations between iconic and symbolic
representations of pictorial information constitute
what may be called ''intermediate-level'' computer
vision. This general problem of intermediate-level
vision has been explored in a collection of papers,''')
with a working definition given in the first of them.''
The three levels of processing for computer vision
are therefore the following:
Although this report is concerned primarily with
the intermediate level, the other levels must be men-
tioned at least to the extent to which they constrain
or help to solve the intermediate-level problems.
One can organize a treatment of intermediate-
level computer vision by focussing on several selected
problems. Let us consider six:
terms of its runs. Such an encoding may consist of
the starting coordinates for a contour, and a list of
runs, each described by a symbol (direction) and a
repetition count. Alternatively, each run may be
described with a pair of endpoints (since the run
represents a straight line segment). Several vari-
ations are possible, making use of absolute or relative
coordinates, explicit or implicit coordinates, runs in
order or out of order, etc. A chain-run encoding may
be a preliminary step towards obtaining the chain
code, or it may be regarded as an alternate form of
the chain code which, for some images, may be more
compact a representation than the chain code.
4. Hough transform. In order to detect the pres-
ence of lines or curves in an image, the Hough
transform or a generalized form of the Hough trans-
form may be used. For lines, the Hough transform
works by quantizing the space of possible values of
the parameters for the line equation. Each ''bin'' of
the quantized space is regarded as an accumulator
which accumulates evidence for the presence of a
line with a particular pair of parameter values. For
each pixel of the source image, a set of bins is
identified and a weight is added to each bin; the
weight is proportional to the extent to which the
source pixel seems to be part of a line (e.g. how
bright the pixel is).
5. Partial Hough transform. Rather than compute
the evidence for each of all the possible lines in the
image,it is often sufficient to determine the evidence
for the most prominent (one or a few) lines. When
this is sufficient, much of the computation can be
eliminated.
6. Region-adjacency graph. Some kinds of
machine vision require that a symbolic description
of a scene be computed before the objects are ident-
ified in the scene. One basis for a symbolic descrip-
tion is a graph structure called a ''region-adjacency
graph'', The problem of computing a region-
adjacency graph (RAG) is to take a segmentation
map (a two-dimensional array in which each element
contains the unique region-number for the region to
which it belongs; each region is a four-connected set
of pixels), and to produce a graph having a single
node for each region, and having an edge between
two nodes if and only if the corresponding regions
are adjacent (share a common boundary). A region-
adjacency graph is typically represented using adjac-
ency lists,
These six problems form a representative sample
of iconic-to-symbolic transformations. We use them
here to compare the four model architectures for
iconic/symbolic computing.
Other sets of intermediate-level operations have
been suggested as a basis for comparing architectures
such as the set consisting of area, perimeter, con-
nected components, convex hull, closest points, and
diameter.A This set emphasizes problems of com-
putational geometry, which are, no doubt, of wide
interest. The set presented in this paper, however,
focusses on pixel-value/coordinate relation inver-
sion, chain encoding, line detection and sum-
marization of region adjacency. Such a set covers a
variety of image processing styles that are more
typical of picture-processing applications, and this
set is particularly useful in bringing out major dif-
ferences in the architectures under discussion.
Several strategies can be used to come up with
machines that can efficiently handle iconic-to-sym-
bolic transformations. One approach is to begin with
an iconic image processor and then augment it gradu-
ally, adding capability for more and more symbolic
or abstract computations. This approach has received
the attention of theoretical computer scientists and
has supported some interesting mathematical
results,%)
Another approach is to attempt to tightly integrate
radically different kinds of processors, each of which
is optimized for a particular style of processing. This
approach can produce powerful systems that are
practical and easier to program than totally new
architectures. However, they may be harder to char-
acterize theoretically in an elegant way. The com-
bination of a pipelined image processor with the IFI
is a good example of such an architecture. The BMM
together with an iconic subsystem and a symbolic
processor network is another.
A third approach is to try to make general MIMD
systems cheaper and more parallel than they are
now. Ifthe cost of the general-purpose systems could
be brought down far enough, and the parallelism
increased enough, they would be more effective for
iconic-to-symbolic transformations than they are cur-
rently.
Next is a presentation of several particular archi-
tectures, and descriptions of how they handle iconic-
to-symbolic data transformations.
A device that we shall call the IFI (Image-Function
Inverter) is modeled after hardware proposed by
Kent in conjunction with Aspex, Inc of New York
to lend extra capability to the PIPE (''Pipelined
Image Processing Engine'') system. That device is
known as the ''ISMAP'' (Iconic/Symbolic MAPper).
The ISMAP is consistent with the PIPE convention
that computations are organized according to the
video field rate (i.e, one field is processed in each
one-sixtieth of a second). Consequently, the ISMAP
(and our abstraction, the IFI) scans an image at the
frame rate, in its normal operating mode.
Since the PIPE is a system which transforms
images into images, it is an iconic processing system,
and it does well at low-level computer vision tasks
such as filtering an image. On the other hand, the
IFI is designed to compute more global features of
an image such as the counts one obtains in a histo-
gram of pixel values. The IFI is illustrated schem-
atically in Fig. 2.
The normal operation of the IFI consists of three
steps. In the first step, the source image is scanned,
and a histogram of pixel values is produced by the
IFI in a special memory buffer. The second step
produces a cumulative histogram from the normal
histogram. The cumulative histogram contains, in
each bin, the number of pixels of the source image
that have a value less than or equal to the bin index.
In the third step, the image is rescanned and for each
pixel value, the IFI makes a list of the coordinate
pairs at which that pixel value occurs in the image.
The cumulative histogram gives the starting address
for each such list.
The IFI is an ideal device for inverting the pixel-
value/coordinates relation. In a sense, it transforms
the image representation from a coordinate-oriented
one into a feature-oriented one. The principal motiv-
ation for this operation is to permit the host to search
only the list of potentially relevant coordinates when
seeking global geometric relations among selected
types of features. The IFI could be extended to
permit its mapping to be inverted', however, we
shall not consider these possibilities here.
In spite of its simplicity, the IFI appears to be a
rather useful device. Two novel algorithms which
use it are described later.
A device has been designed''' which would accel-
erate the conversion of binary images into chain
codes, in conjunction with a ''systolic'' cellular array
computer. Pfeiffer's device, consisting of a VLSI
chip that receives inputs from the systolic array,
treats one column of a binary image at a time. It
rapidly scans the column, identifying the pixels in
which the value 1 is found. If, in the sequence of
columns processed, the value in a row changes from
one column to the next, then the chip outputs an
indication that a horizontal segment either began or
ended in that row (a status bit keeps the state of each
run). It is intended that two to four of these chips
be used for one iconic processor: one processing
columns (as described), another processing rows,
and optically two more of them processing diagonal
lines. The outputs of the chips would be collected by
one or several Von-Neumann-style processors and
sorted into run-length-compressed chain encodings.
For purposes of comparison with other
approaches, we model this device so as to omit the
details of how columns are scanned. Our model,
referred to as the CRE (for Chain-Run Encoder)
simply takes N binary inputs in each of N steps and
outputs a list of events where each event is of the
form (i,e)where itells which of the N inputs changed
and e tells whether the change was from a 0 to a l
or from a l to a 0.
Let us consider how two or four CRE units would
be used with an iconic processor and a symbolic
processor to determine chain encodings of edges in
an image. In order that each of the four encoding
devices receive only ones for the pixels that form
parts of edge segments in the appropriate direction,
the iconic processor must produce four separate
binary images, one for each direction. Figure 3 shows
how two chain-run encoders could be integrated
with a cellular-array processor and a collection of
microcomputers. It is possible to use a single chain-
run encoder for runs in all four directions (sequen-
tially) by providing additional switches and inter-
connections to permit either row data or column data
to be fed into the device.
In order to provide a high-bandwidth interface
between a parallel image processor (such an MPP or
CLIP4-like system, or a pyramid machine''') and a
collection of microprocessors, a ''bimodal'' memory
system has been proposed' ?l that would support
image-wide, parallel transfers with the parallel pro-
cessor, and ordinary byte-at-a-time transfers with
the microprocessors. By suitably partitioning the
memory, many microprocessors could access por-
tions of the memory simultaneously, and the parallel
processor could access a portion while most of the
microprocessors access portions. A clean implemen-
tation of the bimodal memory requires a memory
chip not commercially available at present. An MPP
or Pyramid, plus BMM, would provide a system
that is powerful, yet conceptually simpler than some
proposed extentions to pyramids and arrays.'''l At
the same time, it is less expensive than purely MIMD
systems such as the Ultracomputer,' ? The BMM is
shown between an iconic cellular array and a col-
lection of microcomputers in Fig. 4.
What the BMM permits is a flexible scheme for
sharing work between a parallel image processor and
a collection of microprocessors. One can imagine
that the images are stored in shared memory that is
accessible to the parallel image processor at the same
rate that the image processor's ordinary memory is
accessible. Similarly, the shared memory is accessible
to a microprocessor as fast as, and as simply as if it
were ordinary memory on the bus.
In order for such a system to compute a histogram
CELLULAR pyramid is an exponentially tapering stack of
arrays of processors (''cells''). Communication between cells
on successive levels of the stack allows global analysis of data
input to the base of the stack in log(base size) steps. Cellular
pyramids support fast parallel algorithms for multiresolution im-
age analysis. See the books edited by Rosenfeld [31], Cantoni and
Levialdi [8], and Uhr [37]; for solving computational geometry
problems [25], with the image input to level 0, the base of the
pyramid.
Usually the cells on each level are connected to form a square
lattice but triangular or hexagonal grids have also been used
[2], [6], [14]. A cell on level ! + 1 (the parent) is connected
to a K x K neighborhood of cells on level l (its children).
Neighborhoods associated with adjacent parents overlap by K-2
cells along both directions, yielding a fourfold reduction in
number of processors (twofold along each side of the square);
however, twofold reductions can also be achieved using modified
architectures [11], [18].
We restrict ourselves here to pyramids defined on a square grid
with fourfold reduction between successive levels. If an image
is input to the base of the pyramid, we can generate reduced-
resolution versions of the image at higher levels. Usually the
value of the parent is a weighted average of the values of its
children and the same set of weights is employed at every level.
Burt [7] defined rules for which the set of weights converges to
sampled Gaussians with increasing standard deviations. Optimal
weights have also been proposed [24].
Let the base of the pyramid be of size N' ee 2' s 2*. Then the
Ith level has size 2'*' y 2'', so that the total number of cells
is less than 3N'. The height of the pyramid, iie., the number of
levels, is n = log N. Many image analysis tasks which require
O(N%) operations on a single processor can be accomplished in
O(log N) on a cellular pyramid.
When a pyramid is used to reduce the resolution of an image,
features of the input image become smaller and move closer
together as one proceeds from the bottom level of the pyramid
to its apex. Thus at the appropriate level, local operations are
sufficient to detect and analyze global features (see [31] for
numerous examples). Reduced resolution representations are also
useful in image compression applications (e.g., [1]).
The case of K = 2, i.e., nonoverlapping 2 x 2 neighborhoods,
is related to the quadtree description of an image [34]. The
reduced resolution representations can be severely distorted when
the input is shifted [36]. This problem is known in the quadtree
literature as the shift-dependence of the description [20]. In the
worst case a one pixel shift of the input image can lead to a
significantly modified quadtree structure [35].
The dependence of the low resolution representations on
the position of the sampling grid and the input image is also
important in image pyramid applications. The shift-dependence
phenomenon is not restricted to the case of nonoverlapping
neighborhoods. Bister [5] shows many examples of such artifacts.
The rigidity of the pyramid structure may give rise to ar-
tifacts when pyramids are used for tasks such as analysis of
line-drawings [19], object-background discrimination [10], or
compact object extraction [13], [16]. To compensate for these
artifacts, in many of these algorithms the parent-child links (or
link weights) are iteratively changed after the initial resolution
reduction stage. Recently Baronti et al. [4] proposed a modifica-
tion of this concept by increasing the size of the neighborhoods
associated with parents once an initial segmentation of the image
is obtained.
Another approach to compensating for the artifacts of pyramid
structure is to adapt this structure to the content of the input
image. In custom-made pyramids [28] weights are defined based
on a local ''busyness'' measure during the construction of the
reduced resolution representations. Rom and Peleg [29] and
Chassery and Montanvert [9] employed the Voronoi tessellation
defined by a set of randomly chosen lattice points to build the
coarsest representation of the image, which was then adaptively
refined. Note that the method computes the representations top-
to-bottom.
In this paper we also use irregular tessellations to generate
an adaptive multiresolution representation of the input image.
In our approach, however, the hierarchy of representations is
built bottom-up and is adapted to the content of the input image;
thus most of the properties of ''classical'' image pyramids are
preserved. We employ a local stochastic process to build the
lower resolution representations.
In Section II we introduce the graph formulation of irregular
tessellations and the concept of a stochastic image pyramid.
In Sections III and IV we give two applications of stochastic
pyramids: connected component analysis of labeled images and
segmentation of gray-scale images. Further issues are discussed
in Section V.
In image pyramids based on regular sampling, e.g-, at points
on a square grid, artifacts caused by the rigidity of the sampling
structure are always present. On the other hand, an image
pyramid defined by an irregular sampling hierarchy can be
molded to the structure of the input image. Note, however, that in
such a pyramid the metrical relations among cells are no longer
carried implicitly by the sampling structure. A cell at level ! + 1
cannot know a priori where its neighbors on level I + 1 or its
children on level l are located relative to the original sampling
grid. To describe the structure of such an image pyramid it is
more appropriate to use the formalism of graphs.
The cells on level I of the pyramid are taken as the vertices of
an undirected graph G[l]. The edges of the graph describe the ad-
jacency relations between cells at level l. Thus G[l] = (V[l],E[l])
where V[l] is the set of vertices and E[l] is the set of edges. The
graph G[0] defined by the 8-connected square sampling grid on
level 0 is shown in Fig. 1(a). An example of a graph G[1] that
might represent level 1 is shown in Fig. 1(c).
We construct the pyramid by a sampling or decimation process.
Each level is constructed from the level below it by selecting a
subset of the vertices. Thus a vertex on any level can be regarded
as a vertex of G[0], the sampling grid of the original image. In
addition, when we decimate level l to construct level 1 + 1, we
associate each nonsurviving vertex with one of the surviving
vertices. Thus each vertex on level + 1 is associated with a
set of vertices on level l (itself and the nonsurviving vertices
associated with it). Each of these vertices is in turn associated
with a set of vertices on level I - 1, and so on; thus a vertex on
any level is associated with a set of vertices, called its ''region,''
in the original image. These regions define a tessellation of the
image.
If the pyramid is to be build recursively bottom-up we must
define a procedure for deriving G[l + 1] from G[l]. Since the
number of vertices in G[ + 1] must be less than in G[l] we are
dealing with a graph contraction problem. We must design rules
for:
In order to have any vertex (i.e., cell) in the hierarchy
correspond to a connected region of the image, the cell cfl +
1] V[! -4 1] must represent a connected subset of cells
ics[l],ci[l],-,c,[]} c V[l]. We shall use the convention
cg[l] s: c[l + 1], ie., the surviving vertex of the subset is first
on the list. In pyramid terminology, [cs[l], c;[l],- ,c,[l]} are
the children of c[l + 1]. Note that the location of the parent on
the sampling grid of the original image always coincides with
the location of one of its children.
In pyramid construction based on Voronoi tessellations [9],
[29] the parents are initially chosen by a random process. The
edges are given by the Delaunay diagram of the tessellation and
the children are grid sites inside the tiles associated with the
parents, The process can then be repeated for individual tiles
(by randomly choosing grid sites inside each tile) to obtain a
finer description. Note that such a pyramid is built top-down and
the definitions of parents and parent-children links are based on
nonlocal processes.
When we use graph contraction to construct a pyramid, two
constraints must be satisfied if we want to employ only parallel
local processes:
where c, d are survivors on level l. Constraint (1) assures that
any nonsurvivor on cell at level l has at least one survivor in its
neighborhood and thus can be allocated to a parent by a local
decision. In the example shown in Fig. 1(b) this constraint is
satisfied. Constraint (2) assures that two adjacent cells on level
I cannot both survive and thus the number of vertices must
decrease rapidly from level to level. In Fig. 1(b) this constraint
is not satisfied since the survivors d;, es and ca,9 are adjacent.
The construction of G[l + 1] can also be regarded as finding
a maximal collection of vertices of G[l] no two of which are
adjacent. This is the maximal independent set problem for graphs
(eg., [21]); we will return to it in Section V.
A possible alternative method of constructing G[I + 1] from
G[l] is to partition G[l] into connected subgraphs and then
select one cell in each subgraph as a survivor. However, if
we do so, the first constraint no longer assures locality of the
processing. In Fig. 1(b) cell b, has survivor cs adjacent to it,
but must be allocated to survivor b;; two sites away. Choosing
the survivor independently for each region may also violate the
second constraint since two adjacent regions can both have their
survivors at the border [Fig. 1(b)]. Thus the set of children should
be defined in G[l] only after the vertices of G[! + 1] (their
parents) have been chosen.
The last step in constructing G[! + 1] is to define the edges
E[1 + 1]. Let the connected subsets [cs[l],ci[],- ,c,[]) C
V[l] and d4[4],di[l],- ,d,,[]} c V[l] be the children of two
different parents. Our condition for an edge between vertices
c[! + 1] c,[] and d[! + 1] s d4[] in G[l + 1] is
In other words, two vertices are joined by an edge in G[I + 1]
if there exists a path between them in G[l] of length at most
three edges. (Note that by (2), the path cannot be of length 1.)
G[l + 1] is now completely defined. Fig. 1(c) shows, the graph
corresponding to the partition in Fig. 1(b).
The irregular sampling hierarchy is thus built recursively from
G[0] (the original sampling grid). The apex of the hierarchy G[m]
has only one vertex. Constraint (2) assures that the apex is always
reached.
In the next section we describe a probabilistic parallel algo-
rithm that constructs graph contractions satisfying (1) and (2).
The algorithm is analyzed in more detail in [22], [23].
We have seen that the derivation of G[ + 1] from G[l] must
start by defining the vertices of the new graph. Since V [! + 1] C
V[l] we are dealing with a decimation process, ie., only a subset
of the vertices V[l] are retained. We want the decimation to be
performed in parallel on G[l].
We will define a decimation process that is dependent on the
image data. We assume that every cell c; (a vertex of G[l]) carries
a value g, characterizing its region of the image-for example,
the average gray level of the region. Without loss of generality
we can assume that g; is a scalar value; the treatment of feature
vectors is identical. From now on the explicit indication of the
level I will be dropped to simplify the notation.
Let cell cg on level l have r neighbors on level l, ie., let its
degree as a vertex of G[l] be r. (Note that for the moment c, is not
necessarily a survivor and the set of its neighbors has no relation
with the connected subset allocated to a parent.) We examine
every neighbor c;, i = 1,-,r of cg and decide whether or not
it belongs to the same ''class'' as cg. This decision can depend in
any desired way on the values g,,4 = 0,---,r, We associate a
binary number A,, i = 0,---,r with each neighbor, where A, = 1
if c; belongs to the same class as ca, and A; = 0 otherwise; note
that A4 = 1.
The decimation algorithm employs three variables for every
cell: two binary state variables p and , and a random variable
uniformly distributed between [0, 1] with outcomes . The sur-
vivors are chosen by an iterative local process. Let k 0,1,--
be the iteration index. Initially all p,(0) = 0. A cell survives if at
the end of the algorithm its p;(k) state variable has the value 1.
Every iteration has two steps. First ;g(k) is updated based on
the states p,(k - 1) of neighboring cells in the same class:
In other words, q4(k) becomes 1 if and only if there is no survivor
among the cells belonging to the same class in the neighborhood
of cg, Note that the neighborhood includes the cell itself. The
initial conditions always yield 4(1) = 1. Then ps(k) is computed
on the updated values of ;;(k)
To become a survivor the outcome of the random variable
x drawn by the cell must be the local maximum among the
outcomes drawn by the neighbors in the same class. Note
that only those neighbors are taken into account which do not
already have a survivor adjacent to them (q;(k) = 1). This
condition extends the region of influence of a cell beyond its
immediate neighborhood and yields faster convergence of the
algorithm. The local maximum property assures that (2) is always
satisfied. The state of a survivor is not reversible. Once a cell
is labeled p(k - 1) = 1, at subsequent iterations the product
g4(k)g(k) (5) is always 0 by the definition of g,(k) (4). Thus
in (5) the second condition, preserving the current state is used.
It can be shown [22], [23] that after a finite number of iterations
(at most five, in the experiments reported there) the algorithm
reaches a final global configuration in which the survivors satisfy
(1) as well.
The algorithm is entirely local, every cell computing its states
based only on the states of its immediate neighbors. Except at the
highest levels of the hierarchy, where due to the small number
of vertices artifacts may occur, the decimation ratio between two
consecutive levels exceeds four. This lower bound results from
the fact that two adjacent cells never survive. On the random
graph structure of higher pyramid levels the average degree of
a vertex is around 6 [23]. To satisfy the nonadjacency condition
(2) the number of vertices must be reduced by about the same
order relative to the previous level.
By employing the algorithm an irregular sampling hierarchy
can be built in parallel in log(class size) steps. (The distinction
between class size and image size becomes clear in the next
section.) The stochastic decimation is performed independently
within classes. In the next two sections we describe two appli-
cations of the process, one to connected component analysis of
labeled images, the other to segmentation of gray level images.
In a labeled image the pixels are classified into a small
number of classes distinguished by different labels. A connected
component is a maximal set of connected pixels sharing the same
label. For simplicity we will restrict ourselves to the case where
there are only two labels, i.e., to the case of a binary image,
but images with multiple labels can be handled in essentially the
same fashion.
Sequential algorithms for analyzing the connected components
in a binary image usually employ a row-by-row scan [30]. An
alternative approach makes use of the quadtree representation of
the image [34]. In this section we apply the techniques described
in Section II to obtain in log(class size) steps a description of the
connected components in a binary image. The description takes
the form of a graph whose vertices represent the components
and whose edges represent the adjacency relations among the
components.
The fact that the pixels are labeled makes classification of the
neighbors of a cell immediate. Let the label of cell c; be g-
In the binary case the label can have only two values. Thus in
the neighborhood of cell c; we have for i = 0,--,r the class
membership variables
Note that (6) is symmetrical; cs gets the same value of A in the
neighborhood of c; as c; gets in c's neighborhood. Since the
definition of A, is symmetrical it can be regarded as the weight
of the edge (ca, c;). The case A = 0 is equivalent to removing
the edge from E[l]. Let E'[l] be the set of edges having ? = 1,
and let G'[l] = (V[l], E'[1}). The connected components in the
labeled image are represented by connected components in the
graph G'[l], for all 2 0.
The subgraphs of G' are processed independently, each sub-
graph being recursively contracted into one vertex, the root of the
connected component. The contraction process is based on the
technique described in the previous section: first the survivor
vertices are designated and then the nonsurvivor vertices are
locally allocated to survivors. If a nonsurvivor has more than
one survivor neighbor it chooses the one carrying the largest
outcome of the random variable x from the last iteration of
the decimation process. Because the neighbors are neighbors
in G', the survivors can only have children belonging to their
own class. Thus from each connected component of the input
image a pyramidal hierarchy of irregular tessellations is built in
O(log(component size)) steps.
The different hierarchies may have different heights, but in
log[max(component size)] steps the entire image is reduced to
roots, This situation is detected at the level m when E'[m]
becomes empty. Evidently component size can differ from im-
Ag 5ize. For example, a connected linear pattern passing through
every second row of the image has length N(N + 1)/2 pixels.
Since the hierarchy is built over the pattern the number of levels
depends on its intrinsic diameter.
At each level, the graph G[l] includes edges between cells
that arise from different labels; it preserves the spatial relations
among the connected components. At the root level, G[m] is
the adjacency graph of the original labeled image; it has one
vertex for each connected component and its edges represent the
adjacencies between these components.
Fig. 2.(a) shows an example of a graph G[l] superposed
on the binary image from which it was derived. The induced
graph G'[l] is shown in Fig. 2(b). Note that in G'[l] each
connected component corresponds to a connected subgraph. The
cells surviving level I and the allocation of the nonsurvivors are
shown in Fig. 2(c). The graph G[! + 1] of the next level is shown
in Fig. 2(d) and the corresponding graph G'[! + 1] in Fig. 2(e).
Level 1 + 2 is the root level and its graph G[m] is shown in
Fig. 2(f). It correctly represents the adjacency relations among
the three connected components of the image: the background
and the two blobs.
In Fig. 3 a checkerboard image and the adjacency graph of its
root level are shown. The checkerboard is a ''worst-case'' image,
the two connected components (both defined by the relation of
eight-connectedness) being distributed across the entire input.
Cibulskis and Dyer [10] employed a regular pyramid structure
to segment this image. In their results the ''white'' component
was allocated to one root at the apex, but the representation of
the black squares had to be spread over several levels. The size
of the image is 64 x 64 and the two roots were obtained at
the eight level of the hierarchy. Recall that the height of the
hierarchy depends on component size. Since random processes
are involved in the construction of the irregular tessellations the
location of the roots depends on the outcomes of local processes.
Nevertheless, the same root level adjacency graph is always
obtained at the top of the hierarchy.
The famous connectedness puzzle of Minsky and Pa-
pert [27, Fig. 5.1] can be solved by our technique in
O(log(component size)) steps. The pattern in Fig. 4a) contains
three black and two white bands, while in the pattern in Fig. 4(b)
the two white bands are connected, leaving only two black bands.
The adjacency graphs obtained at the root level clearly show the
different topologies.
The irregular tessellations that arise in the hierarchies defined
by the connected components do not convey meaningful rep-
resentations at intermediate levels. Let us define the receptive
field of a cell on level I as the set of all the pixels at level 0
(input) associated with it, This field is always a connected set
and the image is the disjoint union of the fields. Also, each field
is a subset of a connected component of the image. In Fig. 5 the
receptive fields of four levels of a hierarchy derived from a simple
image are shown. The different fields are randomly colored to
emphasize their shapes. At intermediate levels the shapes of the
fields are arbitrary, since they depend on the outcomes of random
variables. At the root level each field is a complete connected
component.
Our multiresolution representation consists of several inde-
pendent hierarchies, each built independently over a connected
component. The shape, size, position, and orientation of the
connected component have no influence on the final result.
The individual hierarchies can be used for the fast recovery of
geometrical properties such as area or perimeter [32]. It should
be mentioned that Miller and Stout [26] also proposed a data
structure in which a separate ''essential'' regular pyramid is built
over every object.
All the discussion in this section was restricted to binary
images. If more than two labels are used the discussion is
essentially identical. In particular, our method can be used to
label the connected components of constant gray level in an
arbitrary digital image. In the next section we study the less well-
defined problem of segmenting a gray level image into ''natural''
regions.
In gray level images the difference between the values of
two adjacent pixels is bounded below only by the size of the
quantization step. In our technique, to build the hierarchies
the pixels in a neighborhood must be assigned to classes. The
class membership induces the graph on which the stochastic
decimation takes place. For labeled images the classes correspond
to the labels and the hierarchy always converges to the same final
representation: the adjacency graph of the connected components
defined by the labels. For gray level images it is no longer
obvious how to define the classes (unless our goal is to segment
the image into connected components of constant gray level). In
the first part of this section we discuss this problem.
The simplest approach is to define class membership by
thresholding the gray level differences between the center cell
ca and its neighbors c,, i = 1,-,r, The class membership
variables A; are thus defined by
As in the labeled case, (7) based on an absolute threshold T is
symmetrical. This symmetry, however, can create artifacts when
we attempt to segment gray level images, as we show in the next
example.
Fig. 6 shows an object having four gray levels on a white
background. The graph G[l,] of an intermediate level is shown
superposed on the image in Fig. 6(a). Let the differences between
the gray levels be less than T and let the two lighter gray
levels be within T of the background. The resulting graph G'[l,]
is shown in Fig. 6(b). Note the edges connecting regions that
have different colors. The stochastic decimation algorithm selects
survivor cells and the nonsurvivors are allocated to their most
similar surviving neighbors. The survivors (parents) compute new
gray level values based on their children. After a few more levels
of the hierarchy we might arrive at the graph G'[lg], 7 4,
shown in Fig. 6(c). The difference between the gray levels of
the two cells located in colored regions now exceeds T and in
G'[l] these regions are no longer connected. If a different set
of outcomes of the random variables had been employed in the
stochastic decimation process, a different set of surviving vertices
might be obtained, and the new parents might have different gray
level values, yielding a new graph at level l4 [Fig. 6(d)]. We
conclude that using a symmetric class membership criterion for
gray level image segmentation strongly influences the structure of
the hierarchy and therefore the final representation of the image.
Our next example, a ramp image, shows the severity of
the resulting artifacts. In Fig. 7 (top-left) the image of a ramp
going from level 0 (black) to level 255 (white) is shown. The
difference between adjacent rows of pixels is either four or
five gray levels, depending on the quantization error. The pixel
values are the same along each row. The receptive fields of the
root level obtained for T 33 are shown in Fig. 7 (top-right).
The color of a region is the gray level value computed by its
Engineering design consists of transforming a set of
requirements into a set of physical descriptions that are
able to fulfil those specified requirements. According to
one school of thought,''' design can be systematized into
four general phases: clarification of task, conceptual
design, embodiment design, and detail design.
The above general design process is, more or less,
commonly applicable to any domain of knowledge, but
especially that in mechanical engineering design.
The use of computers in design is constrained mainly
by two factors, usability and usefulness.
The usability of computers in any domain of know-
ledge depends on how organized that domain is. In the
context of mechanical design, the available degree of
organization increases as we proceed down the design
phases. The conceptual design, therefore, is the least
organized, on through to the detail design which is highly
organized.
The usefulness is the advantage of using computers
over conventional methods. Computers already are estab-
lished as useful for high-speed calculation-intensive tasks
and are thus well-suited for the detail design phase.
FEM, CAD and, solid and surface modellers, along with
computer-aided manufacturing packages were developed
and are now being used increasingly for modelling and
manufacturing complicated shapes. Within the confines
of vertically-integrated programs, some optimization
programs were developed. Thus, computers are already
well integrated into the detail design and manufacturing
phases.
The remaining problem, however, is the extent to
which computers could be used and useful in the earlier,
looser, and more creative phases of design.
Observation indicates that human designers are good
at the creative aspects of design and at the intricate
details of each small part of a large design task. Problems
arise over the overall integration and consistency of a
large design project. An added complication is that teams
of specialist-designers are often employed to deal with
parts of a large design task. This brings, along with all its
advantages, considerable difficulties of communication
and integration. Normally, designers proceed by making
many assumptions during the early phases. These are
permeated into the design, and are often accepted by
sub-teams who are not necessarily in a position to judge
their accuracy or validity, or indeed they may not realize
that these are no more than best guesses and consequently
the basis of each decision may be lost. In complex
designs, the task of keeping track of all aspects, which are
though not explicit, structure of embodiment design. The
steps, stripped of their jargon, present us with an evol-
utionary design structure that descends through the
requirement identification, elaborating the details of
the components of the chosen structure in the light of the
above requirements, planning some design strategy based
on general guide-lines of form, layout, material, etc., and,
identifying auxiliary functions and function-carriers for
subsequent detailing. A question, therefore, arises as to
whether the decisions that form the above structure are
coherently related, or whether they are ad hoc decisions
based on empirical knowledge.
We believe that, if a structure exists, its decision-
making steps should be coherently related. The belief is
based on two intuitive observations. A wide range of
mechanical design is done by, firstly, using the same body
of fundamental knowledge, and secondly, following,
consciously or intuitively, roughly the same design steps.
The following sections are devoted to the search for
such a structure.
From the knowledge of purpose and the general under-
standing of the ability of known structures, a primitive
solution concept is evolved during the conceptual design
phase. Starting with the solution concept, embodiment
design consists of two essential stages. First, from the
knowledge of the primitive concept and the physical
situation in which it has to work, important physical and
functional constraints must be identified. Second, a
design strategy based on the constraints must be applied
to obtain the physical description.
The problem of delineating a primitive solution
concept was investigated mainly by Ulrich & Seering''
and Dyer et al.' Ulrich & Seering developed single-input
single-output mechanical systems by using bondgraph
elements as building blocks, while Dyer tried to satisfy
the design problem, defined in terms of a goal specifi-
cation, by sub-goal satisfaction procedures, that
employed qualitative reasoning on the knowledge of
naive physical relationships, planning and discovery
heuristics and abstract devices.
The applicaiton of a design strategy through physical
constraints has been investigated, for detail design, by
Popplestone'''' and Chan & Paulson.' In Popplestone's
work, a design is specified in terms of modules consisting
of variables and parameters, and their interdependence is
represented by interface modules consisting of con-
straints. Designer's statements are treated as assump-
tions to support the exploratory nature of the design.
Chan investigated the issues concerning the use of con-
straints as the basis of deriving design descriptions and
suggested simple schemes, involving partitioning of the
design modules, to effect design changes when constraint
violations occur.
The rest of this paper is devoted to the discussion of
the first embodiment stage, that of identifying constraints
from the knowledge of the primitive solution concept and
that of its physical situation; this problem, to our know-
ledge, has not been addressed before. As far as deriving
the concept goes, we will leave it to the human designer
number (I) = OR w number (O)
time (I) = OR w time (O)
The generally valid functions in this definition become
a set of functions having one common characteristic
rather than just one function. Of these, specific functions
represent the subsets. Look at Fig. 12, for instance, to
follow the path channel-transfer-apply-press, Channel is
represented by the following:
type or outward form (I) = type or outward form (O)
component (I) = component (O)
magnitude (I) = OR w magnitude (O)
place (I) w place (O)
number (I) = OR w number (O)
time (I) = time (O)
Transfer, a kind of channel, means:
type OR outward form (I)
= type OR outward form (O)
component (I) = component (O)
magnitude (I) = OR w magnitude (O)
tigated. A solution framework based on using behaviour
as the pivotal element between purpose and structure is
proposed and the suitability of a hierarchical object-
oriented database is discussed. The resulting model
would have a functional structure as well as a physical
one, and this would make it much more useful. This
research in progress is aimed at developing a symbolic
language sufficiently powerful to support the constraint
identification scheme on computers.
Amaresh Chakrabarti has been supported by the Nehru
Trust for Cambridge University, India. The work has
been carried out under the auspices of the Interactive
Decision Support Project, the Science and Engineering
Research Council, U.K.
Nuclear moisture/density meters are
complex electronic/mechanical devices
frequently operating in dusty and hot en-
vironments. During the introductory period
of the meters to the Main Roads Depart-
ment of Western Australia (MRD) the con-
sistency of their operation was assessed
by the standard counts obtained on the
manufacturer's referenceblocks. However,
in addition to instrument variabilty and
background radiation, the standard counts
are also influenced by source decay and
detector ageing. In the case of the last two
a drift of the standard counts may be ac-
companied by a proportional drift of the
operational counts thus maintaining the
validity of the calibration equations.
The continuing validity of the calibration
equations of nuclear meters can be as-
sessed with greater confidence by regular
measurements on stone blocks stored in a
protected environment. In 1980 the MRD
commenced to issue sets of stone blocks,
called secondary standard blocks, to rural
and field laboratories operating nuclear
meters.
A set of secondary standard blocks con-
sists of two blocks. The blocks were cut
from sandstone and granite, with nominal
dimensions of 500 mm x 600 mm x 250 mm
high. From direct measurement the aver-
age density of all the sandstone and gran-
ite blocks is 2.179 and 2.662 tU/m* respec-
tively. Sandstone and granite secondary
standard blocks are shown in Figs T and 2.
For consistency tests in the direct trans-
misssion modes two source access holes
are drilled on the longitudinal axis of the
blocks, approximately 100 mm from the
sides. To prevent the enlargement of the
source access holes in the sandstone
blocks, thin aluminium washers are glued
into a recess cut around each hole. Fig. 3
shows a consistency test in progress.
In a consistency test, an equal number of
counts is taken in each access hole to
determine a calculated density for that
block. For backscatter mode, the meter is
rotated 180 between the access holes.
Initially the consistency tests were carried
The authors thank the Commis-
sioner of Main Roads Department,
Western Australia, Ior permission
to publish this paper. Thanks are
also due to Otficers of Divisional
Laboratories who carried out the
testing. The opinions and conclu-
sions in this paper are no neces-
sarily those of the Main Roads
Department of Western Australia.
out in the backscatter, 100 mm and 150 mm
directtransmission mode of measurements
(MRD 1982). If there was no evidence of
malfunctioning, the tests were repeated at
approximately one month intervals. Addi-
tional consistency tests were carried out if
damage to or malfunctioning of the meter
was suspected.
The densities analysed for this paper are
the results of consistency tests by 14
nuclear meters. One of those is a Troxler
3411-B model and the others are Troxler
3401-B models. At the time of the analysis
the age of the meters was two to five years.
All meters operated satisfactorily during
the period covered by the analysis. The
consistency tests were carried out on ten
sandstone and ten granite secondary stan-
dard blocks. The number of laboratory
personnel involved in this series of tests
exceeded 20.
The continuing validity of the density sys-
tem was monitored by the examination of
The values confirmthe significantly greater
variability ofthe densities measured by the
backscatter mode. They also reflect the
greater density uniformity of the granite
secondary standard blocks.
This analysis demonstrates that the vari-
ability of densities determined by the
backscatter mode of measurement is
greater by a factor of approximately 2than
the variability of densities determined by
the 100 mm or 150 mm direct transmission
mode. The greater variability of densities
by the backscatter mode was also estab-
lished by an earlier investigation (Hamory
1981). This analysis further supports the
finding that the direct transmission modes
should be preferred for determination of
density.
lt is considered that the consistency of the
nuclear meters should be assessed in the
100 mm and 150 mm direct transmission
mode. The consistency test includes the
assessment of the mechanical and elec-
tronic components. The operation in other
depths of the direct transmission mode
involves only a different location of the
handle on the index rod.
The data forthis analysis were supplied by
routine testing (14 meters and more than
20 operators working in field conditions).
As the paper only discusses the differ-
ences between the calculated densities
and the established datum densities, the
variability of the differences includes instru-
ment and operator components. The re-
sults of consistency tests reflect not only
the consistency of the nuclear meter but
also the consistent application of the cor-
rect operational procedures. The impor-
tance of this second factor in the consis-
tency tests, as in all other tests, cannot be
over emphasised.
The high variability of the densities estab-
lished by the backscatter rmode suggests
that consistency tests using this mode are
of limited value. These tests should be
considered of value for projects where this
rmode of testing is specified for density
determination.
Routine consistency tests in the 100 mm
and 150 mm direct transmission mode are
adequate toprovide assurance of satisfac-
tory performance of the nuclear meters.
The datum densities for a series of consis-
tencytestsshould be establishedfromthree
sets of eight count ratios. Unless malfunc-
tioning is suspected, subsequent consis-
tency tests should be carried out at inter-
vals of one month. After minor repairs of
nuclear metersthe validity of the calibration
equations should be assessed by consis-
tencytests. New datum densties should be
established after each recalibration of the
nUClear meter.
Datum densities should be establishedfrom
the mean of three valid calculated densities
prior to operational use of the meter, The
calculated densities willbe valid if the range
of the three densities does not exceed the
following values.
The densitiesfrom subsequentconsistency
tests should be compared with the datum
densities. The differences should not ex-
ceed the following values.
The recommended procedure for the as-
sessment of the consistency tests is de-
tailed in Calibration Procedure WA 2040.2
ofthe Materials Engineering Branch (MRD
1989).
SightPlan refers to several knowledge-based systems
(KBSs) that lay out construction sites by arranging
rectangles in a 2-D orthogonal space (Tommelein,
1989; Tommelein et al., 1991). All systems were im-
plemented in a modular fashion, using the BB1
blackboard architecture (Hayes-Roth, 1985). They are
alterations of one another in that they share most
knowledge bases with control knowledge, domain
concepts, and heuristics, but differ in either the
problems they solve, the problem-solving methods
they apply, or the tasks they address. The SightPlan
implementations are discussed in terms of the types of
knowledge encoded in them, and their use and re-use
of knowledge.
The modular construction of the systems' knowl-
edge bases facilitated model construction and
encouraged experimentation. We dissect and compare
SightPlan systems that:
Early artificial intelligence (AI) programs, such as
Mycin (Shortliffe, 1976), were necessarily imple-
mented from scratch to suit a specific problem. Once
their researchers gained a deeper understanding of the
role played by the knowledge base vs. the inference
engine, a part of Mycin could be abstracted into a
domain-independent shell that lent itself to the rapid
implementation of application systems in other
domains. This first step, making the Emycin (van
Melle, 1980) inference engine re-usable for other
rule-based systems, has proven very successful and
resulted in a wide-spread use of this programming
technique.
Abstraction of common problem-solving methods
(PSMs) was the next step. A careful examination of
the inference processes used by several diagnostic
systems led Clancey (1985) to characterize heuristic
classification. In recent years, AI researchers have
become increasingly interested in understanding the
similarities and differences between their systems and
the tasks they address. Such understanding facilitates
the rapid prototyping of new systems and can guide
the knowledge acquisition process. It also enables the
community to establish a common vocabulary to
describe systems and report on their operation; the
lack there-of has slowed progress and hampered the
acceptance of AI work.
This use of a common language by control and
domain KSs greatly facilitates matching control and
domain actions as will be described later. Also, the
language templates are skill-dependent but domain-
independent, so programmers can write knowledge
sources that are applicable in multiple domains.
Finally, SightPlan calls a procedural constraint
satisfaction system, independent of BB1 and named
GS2D (Confrey and Daube, 1988), to satisfy domain
actions, i,e. to compute sets of positions for objects to
meet their constraints. GS2D maintains all acceptable
To serve as a reference for explaining and
comparing the knowledge in the various systems, all
systems' knowledge bases are tabulated in Table 2.
We rapidly prototyped the
SightPlan Prototype system by defining some
domain vocabulary and constraints for a fictitious
site (PROTOTYPE SITE KB) (Tommelein et al.,
1987a,bb).
Domain KSs were taken from the
Protean application. Protean is a KBS that keeps
track of all legal protein molecules constructed
from given constituent parts and experimentally
determined constraints between them (Brinkley et
al., 1986; Buchanan et al., 1986; Hayes-Roth et al.,
1986b). It applied the arrangement assembly skill
for performing its task and was also implemented
in BB1. In order to make Protean's domain KSs
applicable to SightPlan, we substituted domain
concepts from Protean by analogous concepts from
SightPlan's domain, resulting in the SIGHTPLAN
LAYOUT KSs.
objects and constraints, objects differed in some of
their attributes and in number, because of the
difference in project type, scope, climatic conditions,
site geography, as well as human organizational
structure. Due to a difference in scale of the two sites,
spatial constraints exemplifying the same constraint
types also reflected differences in magnitude of
distances. Moreover, constraints on objects such as
laydown areas can be imposed by the contractors who
use them. For example, different contractors may use
different delivery methods, and thus have different
access requirements to their laydown area. Many of
SightPlan's constraints are opportunistically intro-
duced decision variables of the managers involved on
the project and could not be determined ahead of
time by a generic program. This makes them not
reusable.
The application of the Expert Model to both the
IPP and the AM1 site showed that SightPlan could
successfully lay out different sites. Differences in
layout method (as captured in the skeletal plan) could
have been anticipated by us if we had inspected the
organizational structure and project parameters of
both projects. The dissimilarity of the two projects
prevents us from making stronger claims regarding the
generality of the PSM that is represented by the
systems' skeletal plans. Further validation using more
'similar' sites will demonstrate whether or not the
skeletal plan of the IPP-Expert model is generally
applicable.
What are the differences in knowledge needed by
systems that solve the same problem involving the same
skill in a single domain, but using different PSM
objectives'?
Objectives pertain to what is modeled by a
program, program performance, or program output.
They may state that a particular process is to be
modeled, a solution is to be reached quickly, storage
memory available during problem solving is limited,
one or several solutions are desired, and these ought
to be optimal or satisfactory based on some criterion,
and so on, Depending on the objectives set forth, one
PSM might be more appropriate than another one.
domain. By describing alternate SightPlan systems, we
have documented how systems with arrangement
assembly skills use a common language, yet,
incorporate domain vocabulary to suit their applica-
tion domains and to better meet their objectives.
Similar to SightPlan's use of Protean KSs, other KBSs
might re-use SightPlan's KSs. Clearly, the domain-
independent GS2D is readily re-usable, for example
by systems tracking positions in architectural or
electronic chip layout. By transferring skills to new
domains, we can prototype new application systems.
Subsequent knowledge acquisition is likely to be
needed for further system development and perfor-
mance improvement, and it is too early to tell whether
the cost of adaptation and refinement outweighs that
of implementing new systems from scratch.
SightPlan's modular KSs clarify what types of
knowledge may be needed and what role such
knowledge may play in layout design. A careful
examination of other systems in similar vein might
lead to a better characterization of the layout process.
Ongoing research focuses on the components of
expertise comprised in KBSs (Steels, 1990) and on
understanding the level of generality of design tasks
(Chandrasekaran, 1990). A substantial amount of
additional research needs to be done in analysing and
characterizing existing systems at the knowledge use
level. Furthermore, the development of modular
usable and re-usable programming constructs, used
for implementing new or existing systems, may enable
AI researchers to better understand the generality of
their work (Hayes-Roth et al., 1986a; Klinker et al.,
1990; Balkany et al,, 1991; Birmingham and
Tommelein, 1991).
Financial support from the Stanford Construction
Institute, the National Science Foundation (Grant
MSM-86-13126), and the Center for Integrated
Facilities Engineering at Stanford are gratefully
acknowledged. Although each of these organizations
has endorsed the goals of the SightPlan research by
their support, all of the opinions expressed in this
paper are those of the authors.
Besides acknowledging the benefits that we gained
from building our systems in modular fashion, which
greatly facilitated our experimentation, we recognize
that we are indebted to the many people that assisted
our work, We thank the many supportive and
cooperative people at Stanford that made work on
SightPlan possible, including Tony Confrey, M.
Vaughan Johnson, Micheal Hewett, Frangois Daube,
Gail Tubbs, Pat Steele, former Protean members Russ
Altman, Jim Brinkley, Craig Cornelius, and Alan
Garvey, and KSL support staff. Professors Bob
Tatum, Boyd Paulson, Hank Parker, John Fondahl,
and Clark Oglesby provided valuable input to the
early prototype systems and ongoing critique of the
work. An articulate and enthusiastic group of
engineers and managers from the Los Angeles
Department of Water and Power, Bechtel, Black &
Veatch, Stone & Webster, and Dillingham Construc-
tion contributed knowledge about real site layout
problems.
Finally, a word of thanks goes to all members of the
newly-founded Integrated Systems Design Group at
the University of Michigan, where discussions help us
to get a better grip on tasks performed by various
knowledge-based systems, and to Professor Bill
Birmingham and anonymous reviewers for their
valuable comments on earlier drafts of this paper.
Recently, there has been much interest both theoreti-
cally'' and experimentally'T? on long-wavelength optical
transitions with incident transitions with incident light po-
larized in a direction normal to the growth axis (i.e., x or
) polarization ). Because the intersubband transitions for
n-type quantum wells are forbidden' except for incident
light with a z polarization, whereas x, ), and z polariza-
tions are all allowed for p-type quantum wells.' This fea-
ture makes p-type quantum wells more favorable than n-
type quantum wells in most applications involving normal-
incidence intersubband photodetectors. Since it is much
easier to send a light beam along the growth direction with
an x(or y) polarization than along the in-plane direction
with a z polarization. All of these previous works, however,
had been done on p-type AlGaAs/GaAs systems, since the
growth and processing technology for them are more ma-
ture. Here, we propose an alternative structure using an-
other well-known material system, GaSb/InAs superlat-
tice. In GaSb/InAs superlattices, electronic properties, and
the interband optical transition have been studied by sev-
eral theoretical techniques,'!-!! Sut no detailed theoretical
calculations of hole intersubband optical transition has
been reported. In this letter, we present such a theoretical
calculation and obtain some interesting properties which
are much different from the p-type AlGaAs/GaAs inter-
subband optical transition.
The hole intersubband photoabsorption in GaSb/InAs
superlattices are examined theoretically with the bond-
orbital model,'' The model includes six bond orbitals per
bulk unit cell: two are s-like orbitals multiplied by the
electron spinor and four are p-like orbitals coupled with
the spin to form orbitals with; total angular momentum
J=3/2. Details of the application of this model to super-
lattices can be found in Ref. 12. The interaction parameters
between any two bond orbitals for InAs and GaSb are
related to the zero temperature band gap, electron effective
masses, and Luttinger parameters via relations given in
Table II of Ref. 12. The on-site orbital energies are related
to the band offsets and the band gaps.
The superlattice energy levels and eigenvectors at in-
plane wave vector k,;=0 are obtained by diagonalizing a
slab Hamiltonian in the bond-orbital basis. For nonzero
but small ( c 0.1(2m/a); a is the lattice constant) in-plane
wave vectors, we perform a unitary transformation of the
slab Hamiltonian by changing the local bond-orbital basis
to a new basis which consists of the eigenvectors of the
Hamiltonian at k;;=0. We then truncate the Hamiltonian
so that the diagonal elements are within an energy range of
nterest.
The absorption coefficient for intervalence-band tran-
sitions between subband n and subband n' is given by
where n, is the refractive index, m; is the free electron
mass, V is the volume of the crystal, o,,(k)sE,(k)
-E,(k), E,(k) is the energy of subband n at wave vector
k, i is the polarization vector of the incident light, P,, (k)
is the momentum matrix element, f,,(k) is the carrier dis-
tribution function associated with subband n, and T is a
broadening parameter which was taken to be 0.5 meV. To
calculate the optical matrix elements between superlattice
states, we need to know the optical matrix elements be-
tween any two bond orbitals. These are obtained by requir-
ing the optical matrix elements between bulk states ob-
tained by the bond-orbital model to be identical to those
obtained in the kp theory'' up to second order in k. De-
tails of such procedure were reported in Ref. 14.
Figure l shows the energy diagram of the GaSb/InAs
superlattice considered in the calculation. It consists of
50-period undoped GaSb/InAs superlattice sandwiched
between two n-type InAs layers (buffer or cap layer). If
the n-type InAs layers are thick enough, the Fermi level of
the entire structure would be determined by the doping
concentration of bulklike InAs layers and charge transfer
mechanism will occur. Electrons are drawn from GaSb
valence band to bulk InAs layers and a high density of
two-dimensional hole gas will occupy the first heavy-hole
subband of GaSb. The hole concentration of p-like GaSb
structure can be tuned by varying the doping concentra-
tions of bulklike InAs layers. Compared with p-type
AlGaAs/GaAs quantum wells, these GaSb/InAs struc-
tures can have much higher concentration of two-
intersubband optical transition between HH1 and LH1 ex-
hibits the behavior of an interband optical transition,
which has a much greater optical matrix element for light
polarized along the x direction. For q =m/d, the HH1-
HH2 transitions for k,, s 0.03(2m/a) are of principal inter-
est, because their transition energies are near the desired
value of 110 meV [see Fig. 2(b)]. The HH2 subband for
k; >0.03(2m/a) actually contains a large percentage of
light-hole or InAs s-like characters due to band mixing.
Thus, the HH1-HH2 optical transition for the x polariza-
tion again exceeds that for the z polarization [see Fig.
3(b)]. It is worth noting in Fig. 3 that the intersubband
optical matrix elements of the HH1-LH1 and HH1-HH2
intersubband transitions in the momentum regions of in-
terest are as large as 2 eV, which is about an order of
magnitude larger than that of the p-type AlGaAs/GaAs
system.' These characteristics are very attractive as far as
applications in long wavelength photodetectors are con-
cerned.
Because the superlattice considered here is thin, the ;
dependence of the subband energy and optical matrix ele-
ments cannot be ignored. Thus in our calculation of ab-
sorption coefficient, we have performed the summation
over g using a 20-point sampling. The absorption coeffi-
cient as a function of incident photon energy (fia)) with
light polarized along x and z direction, respectively, are
shown in Fig. 4. In these calculations, the concentration of
two-dimensional hole gas, N25. is chosen as 8.56x 10!
cm''f. A number of interesting results are worth noting.
First of all, the obtained absorption coefficient for optical
polarized along x direction is over 8.0 > 10' cm ' for pho-
ton energies near 0.1 eV, whereas the absorption coefficient
for optical polarization along z direction is about four
times weaker. Furthermore, it is found that the peak values
of absorption coefficient for optical polarization along z
and x directions occur at almost the same incident photon
energy. The absorption spectra are split into two peak
structures, which reflect the two major contributions from
HH1-LH1 transitions near k;,=0.025(2m/a) and HH1-
HH2 transitions k;; =0.045(2m/a) as marked by arrows in
Fig. 2
In conclusion, we have studied the optical properties of
GaSb/InAs superlattices with a bond-orbital model. A
new optical transition mechanism has been proposed
which results from mixing of InAs s-like conduction states
and GaSb p-like light-hole states. This leads to a strong
normal-incidence optical transition between the HH1 and
LH1 subbands with the wavelength near 10 um. We found
that the absorption coefficient of a (GaSb);;;(InAs);; su-
perlattices is over 8.0 5 10' cm'' as a result of high two-
dimensional hole concentration in GaSb layers which nat-
urally occurs due to electron transfer from GaSb to InAs.
When a bias voltage is applied to the structure shown in
Fig. 1, a weak dark current will be detected, since electrons
in the InAs electrode must tunnel through the HH1 sub-
band of GaSb which has an extremely heavy mass. When
the light is on, holes will be excited into the LH1 subband
which in turn can tunnel out, giving rise to substantially
increased tunneling current due to the small effective mass
of the LH1 subband. Thus, the InAs/GaSb superlattice
offers promise as a normal-incidence infrared detector.
This work was supported in part by National Science
Council under Contract No. 81-0404-E-006-106. Y. C. C.
is supported by the U. S. Office of Naval Research (ONR)
under Contract No. NO0014-89-J-1157.
The development work presented here is an effort to
combine the advantages of semiconductor devices in
laser Doppler anemometer (LDA) with those of fiber-
optic probes to achieve a mobile and robust measure-
ment system. Laser diodes, for example, have been
used previously in a large number of LDA systems'-
in which both a significant size reduction and a
reduction in power consumption have been achieved.
Several authors have also pointed out the advan-
tages of employing avalanche photodiodes (APD) for
light detection at the infrared wavelength of laser
diodes. While fiber-optic probes have been used
extensively for some time in LDA systems, there have
been only a few systems designed that combine fiber
optics with laser diodes,' and in only one case'? has
this been done with the express purpose of achieving
a portable measurement system. This work there-
fore has the objectives of achieving not only a portable
but also a mobile measurement system with low
power consumption, easy handling and high signal
quality. The resulting laser diode fiber-optic laser
Doppler anemometer (DFILDA) system is equal in
most respects to conventional LDA systems and has
the additional advantage of being suitable for mobile
and field measurements.
One of the distinguising features of all the above-
cited LDA systems is the manner in which directional
sensitivity (if any) is achieved. For almost all practi-
cal measurements, however, directional sensitivity is
necessary and this then requires a frequency shifting
or phase modulation of one or both light beams. Ad-
ditional components such as Bragg cells, rotating
gratings, or, in a novel manner, piezoelectric phase
shifters% must therefore be included in the optical
systems. These components are contrary to the
goals of size reduction, and the fiber-optic probe is one
solution in which the beam splitting and frequency
shifting is performed in a base optical unit, and the
fiber probe head, which can be made much smaller
and more rugged, can be placed near the measure-
ment location. This solution has also been chosen
for the present system.
The base optical unit includes all the optical compo-
nents up to and including fiber in coupling as well as
an APD for signal detection. While the main aim in
designing this system is not to minimize the size, it
should be kept small because of the intended mobile
applications e.g., inflight measurements). More im-
portant is the ruggedness of the system, especially the
fiber in coupling because high couping efficiency must
be maintained under a harsh mechanical environ-
ment e.g., train cars) and over a long duration in
order to minimize user adjustment. The optical
system developed for this purpose is illustrated sche-
matically in Fig. 1 and pictured in Fig. 2. Its physical
dimensions are 170 mm x 180 mm x 460 mm.
The laser source is a 100-mW single-stripe, mono-
mode laser diode operating at 838 nm and packaged
with a temperature regulator (x:0.1'C) and correction
optics for astigmatism and for achieving a circular
beam profile.' The output power of this package is
94 mW and the beam is nearly circular with a
diameter of 5.6 mm. This beam is collimated to a
diameter of 1.2 mm with the beam waist positioned at
the front face of the Selfoc lenses that were used for in
coupling. A passive optical isolator based on the
Faraday effect is used to suppress reflections back
into the laser diode and thus any external resonator
effects. Reflections are of particular concern when
fibers are used since they can be phase conjugate at
the in-coupling surface on which the phase fronts are
plane. Without suppression of these reflections, the
laser diode experiences abrupt phase and power level
variations'r that are reflected in a smearing of the
interference fringes in the measuring control volume,
as shown in the photographs of Fig. 3. The beam
power level after exiting the optical isolator is 88 mW.
The beam is split into two beams of equal intensity
by using zero- and first-order beams that exit from a
Bragg cell (Matsushita EFL D 250 R) operated at
50-55 MHz. The intensity split is within s:5% with
an overall efficiency in the two selected beams of
> 90%, To keep the system compact and to allow
the beams to separate in space, two prisms deflect the
beams 180' onto two in-coupling units (Polytec OFL-
800). These units provide four axes of movement
(two translation, two tilt) and exhibit high mechani-
cal stability and repeatability.
The in coupling to single-mode, polarization-
maintaining and absorption-reducing (PAN1DA) fi-
bers (Fujikura SM 85-P) is achieved by using 0.25-
pitch Selfoc lenses (Nippon Sheet Glass SLN 2.0 0.25
BC 0.83) butted directly against the polished fiber
ends and held together in a glued ferrule. The beam
diameter of 1.2 mm at the Selfoc front face results in
a spot size of 5 um, suitable for coupling into the
PANDA fibers with a core diameter of 5.5 um with
transmission efficiencies that exceed 60%.
This optical system results in a frequency shift of
- 50 MHz, necessitating either a downmixing before
processing or a processor of sufficient bandwidth.
A second optical unit that employs a beam splitter
and two Bragg cells, as shown schematically in Fig. 4,
permits more flexibility in the choice of shift fre-
quency and is also more appropriate when the optics
is being used as a transmission optics for a phase
Doppler anemometer (PDA). In the present system,
velocity, a higher frequency shift is required, and this
invariably lowers the effective accuracy of the proces-
sor in determining the Doppler frequency.
Assuming the limiting (and hypothetical) case of no
actual fringes in the MCV, the required frequency
shift f,, is approximately
where N,,, is the minimum number of periods in the
signal that is required by the processor and |U] is the
magnitude of the velocity vector. This illustrates
the fact that a larger beam expansion requires a
higher shift frequency. If this minimum shift fre-
quency is always used and a flow that is primarily
perpendicular to the MCV fringes is assumed, U] =
U;, a relative frequency error made by a counter
processor that is due to the zzl timing count error can
be derived as
where f; is the Doppler frequency that corresponds to
particle velocity; fa,,, is the oscillator frequency of the
counter time base; Na is the number of real fringes in
the MCV; U, is the flow velocity perpendicular to
MCV fringes. Using values that are typical for a
probe with strong beam expansion (see Table
I: Na = 8, U = 50 m/s, a. = 500 MH, N4 = 4,
4acv 20 u.m), we can expect an error of c 1%, This
indicates that the probe optimization through beam
expansion does not necessarily lead to serious errors
in the signal processing because of smaller measuring
volumes.
If a frequency shift is chosen independent of the
optical parameters and held fixed, as in the present
system, a more appropriate expression for the normal-
ized frequency is given by
where f, = f,, + f5 is the signal frequency. Examina-
tion of this expression shows that the error is, in
general, larger. Using the parameter values given
above, where f6 = 5 MHz and the f,, = 50 MHz, yields
a relative error of 15%, This error can be minimized
either by increasing the minimum number of periods
counted or by decreasing the shift frequency as much
as possible, thereby reaching the limit of the relative
error given by Eq. (4). The reduction of shift fre-
quency can be achieved either by electronic downmix-
ing or by using the double Bragg cell arrangement
described above.
It should be noted that the relative error considered
here that is due to a zzl timing count uncertainty in a
counter processor is a statistical error with a mean of
zero. This value expresses the smallest measurable
frequency difference, i.e., the resolution of the signal
processor. Other processors, for example, those
based on a Fourier transform, should behave simi-
larly since the accuracy of peak determination in a
spectrum is approximately constant over the operat-
ing bandwidthM as is the counter E:1 timing count
uncertainty.
Using the considerations mentioned above, we con-
structed two fiber-optic probes as shown in Fig. 6.
The first probe is 19 mm in diameter and employs
Selfoc lenses and a perforated negative lens to achieve
beam expansion in a small probe diameter. A simi-
lar approach was recently reported by Ikeda et al.'8
The second probe is 30 mm in diameter and uses
microlenses to collimate the diverging beam after
Using the 19-mm probe, equivalent measurements
should be possible when a 60-mm focal length is used.
The observed signal quality was, however, high, and
while quantitative data are not available, measure-
ments should also be possible at significantly longer
focal lengths.
A further application field of the DFLDA is in aerody-
namic studies, in this case, the investigation of bound-
ary layers on railroad cars. Tests were performed on
two occasions, once with the Deutsche Bundesbahn
and once with the Societe Nationale des Chemins de
Fer Frangais, France In the first instance, a smoke
generator that operates on the principle of oil vapor-
ization on a heated plate was used for seeding the
railroad car boundary layer. This proved to be inap-
propriate, yielding low data rates and having the
disadvantage of operating intermittently. In the
SNCF experiments, an atomization of diethyl-hexyl-
sebacat fluid that uses a commercial atomizer (TSI,
Inc., Model 9306) provided suitable particles. A
portable compressor supplying air at 4 bars was used
for the atomization. This arrangement yielded excel-
lent results.
The double Bragg cell arrangement was used in the
base optical system with a net frequency shift of 5
MHz. The 30-mm probe with focal lengths of 60,
120, and 250 mm was used. An IFA-550 (TSI, Inc.
signal processor was used for determination of the
Doppler signal frequency, and an acquisition inter-
face and computer similar to those used in the engine
measurements were used to acquire and process the
data.
Measurements were made at several locations in a
train car traveling at 100 or 140 km/h, including a
midsection location and a location at the end of the
car. The seeding probe could be placed up to one car
upstream of the measurement location, however
higher data rates were obtained when the seeding
probe was closer to the measurement location.
Although measurements could be made directly
through the car window, the coating on the windows
led to attenuation of the signals and therefore special
Plexiglas inserts were prepared at the desired mea-
surement positions. A heavy optical tripod provided
a traversing base for the probe inside the train car.
Some typical boundary layer profiles are illustrated
in Fig. 12 for various measurement positions and
train speeds. These profiles were obtained by using
the three different focal lengths, each lens used to
measure in a different distance range from the win-
dow up to a maximum distance of 23.5 cm. The
overlap in profiles when different focal lengths are
used is excellent despite the fact that the measure-
ments were often performed many hours apart. Us-
ing the 60-mm focal length, measurements could be
made up to 3 mm from the wall. At positions closer
to the wall, the shift frequency dominated the signal.
This is due primarily to the vibration of the probe
with respect to the window when the train is moving.
Thus the window surface periodically enters the
measurement control volume.
The primary reason for performing the measure-
ments in the railroad car was to test the handling of
the optics under adverse conditions. In this respect,
the optics performed excellently.
One major advantage of the DFLDA system is the
ease with which probes can be exchanged. For exam-
ple, a probe with 85-mm diameter has been designed
for applications that require longer focal lengths.
Table III indicates that, even for focal lengths of 500
mm, the optical figure of merit can still exceed values
typical of conventional systems.
Figure 13 illustrates a two-point probe based on the
use of two 19-mm probes as described in Section II.
Single-mode, polarization-preserving couplers are used
to split the coupled light equally into two sets of
fibers. The probes are then held in a positioning
device that permits adjustment of the position of the
two control volumes. The incident light power in
each individual probe is reduced, however Fig. 2
indicates that this power reduction factor of 2 will
decrease the achievable focal length by only 16%.
Such a probe is currently being constructed to investi-
gate the integral length scales in a compressing
cylinder flow.
Higher laser power is a second alternative to increas-
ing the achievable focal lengths of the probes.
Whereas 150 mW is expected to be the limit for
single-stripe, monomode laser diodes, diode-pumped
Nd:YAG lasers already provide 800 mW at 1064 nm in
commercially available packages.
The extension of the DDFLDA concept to two compo-
nents is more complicated than with conventional
Ar-ion LDA systems, since only one wavelength is
available. Present designs foresee two solutions.
The first is to use a second Bragg cell to introduce a
third beam at a different shift frequency. A three- or
four-beam system results, with bandpass filters used
to separate the signals from a common detector.
A second approach uses two base optical units with
one laser diode at 830 nm and one Nd:YAG laser at
1064 or 532 nm (frequency doubled). Color splitters
and interference filters are then used to separate the
signals from the two components.
In principle two laser diodes that operate at dif-
ferent wavelengths can be used for two-component
systems, as also suggested by Forder et al., however
for high-powered diodes (z 100 mW) this relies on a
selection process and thus the deliverability of the
diodes is not always assured.
The DFLDA system is suitable as a transmission
optics for a PDA system. In this application it is also
desirable to miniaturize the receiving optics and to
employ fiber optics to yield a compact and mobile PDA
probe. A diode-laser fiber-optic PDA is currently
being constructed for this purpose.
Several methods to program a robot have been pro-
posed. Such methods include: teach-by-showing, teleop-
eration [15, 12, 5], textual programming[3l, and automatic
programming [10, 8]. In teach-by-showing methods, an
engineer stores, using a teach pendant in teaching mode,
a path along which a robot should move repeatedly. This
methodrequires that an engineer is in the same environment
as the robot. Thus, we cannot use this method in hazardous
environments such as in nuclear plants, underwater, or in
outer space.
To remedy this problem, teleoperation methods have
been proposed. This method uses a master manipulator
for teaching and a slave manipulator for execution. An
engineer controls the master manipulator in a safe environ-
ment while monitoring the hazardous environment with a
remote TV camera and display. The slave manipulator in
the hazardous environment executes real operations based
on control signals from its master manipulator. By using
this method, we can only teach a robot trajectory informa-
tion. It is difficult to build a flexible robot system able to
use force control with error recovery capabilities. It is also
true that we have to reconstruct entire programs,even when
a very minor change in the program is desired.
Textual programming is often used in academic environ-
ments, A programmer stores a robot command sequence
in a computer as a textual program. It requires a long
development period and expert programmers.
In order to speed up the programming process, auto-
matic programming has been proposed. The method tries
to develop geometric reasoning systems which can gen-
erate textual programs to control a robot from geometric
information given by geometric models and task specifica-
tions. This direction is quite promising, however, there are
many issues to be addressed before we have a complete au-
tomatic programming system; It is quite difficult to build a
complete automatic programming system, though perhaps
not impossible.
We propose a novel method that combines automatic
programming and teleoperation. We propose to add a vi-
sion capability that will observe human operations to an
automatic programming system (a geometric reasoner). In
particular, we propose a system that observes a human
performing an assembly tasks while a geometric reasoner
analyzes and recognizes such tasks from observation, and
generates the same assembly sequence for a robot, We will
refer to this paradigm as Assembly Plan from Observation
(APO).
Due to the geometric reasoning capability, the APO sys-
tem understands the operations that the operator is perform-
ing, Thus,the system for example can discard unnecessary
motions which are often introduced by a human teleoper-
ator. The system can also insert error recovery routines
into the generated assembly plans. In this regard, APO is
superior to the teleoperation method.
Due to the vision capability, the system can solve sev-
eral otherwise extremely difficult problems, such as path
planning and determining the optimal assembly sequence,
by simply observing a human performing the operation. In
this regard, APO is superior to the automatic programming
method.
Several systems have been proposed to recognize hu-
man (robot) hand movements for various purposes. Hirai
and Sato proposes a system to recognize manipulator mo-
tions for maintaining the consistency between an internal
world model and the real world [5]. Herve et al developed
a system to recognize robot hand movements for visual
feedback [4]. These systems are, however,for monitoring
purposes not for program generation purposes.
Kuniyoshi et al [7] developed a system which tracks
movements of a human hand for program generations.
Their goal was similar to us. The system is, however,
restricted to pick-and-place operations. This is because the
system only analyzes apparent hand movements and does
not employ the knowledge given by analyzing geometric
models of objects.
In an APO system,a human operator performs assembly
tasks in front of a video camera. From the camera, the
system obtains a continuous sequence of images recording
the assembly tasks. In order for the system to recognize
assembly tasks from the sequence of images, the system
has to perform the following six operations (See Figure 1.):
In thispaper, we will concentrate on the task recognition
and task instantiation modules, because these two parts
form the main loop for the assembly plan from observation.
The outline of the modules are as follows:
Our object recognition module identifies each object
using the object models from a given image segment. The
module represents the recognition results in a world model,
as shown in Figure 1, by using the geometric modeler,
Vantage.
Our task recognition module recognizes object relations
in two image segments and extracts the transition between
twoobjectrelations from thetwo segments. The task recog-
nition system has abstract task models in a data base. Each
abstract task in the data base describes a transition between
two different object relations. From the task models in
the data base, the system identifies a task model that de-
scribe the transition needed to achieve the observed object
relations, as shown in Figure 1.
Our task instantiation module represents the recogni-
tion result as an instantiated task model. An instantiated
task model associates a transition with an action capable
of causing the transition. It also includes appropriate pa-
rameters to achieve the action based on the given scenes.
Such parameters include object locations and the grasping
locations for the action. The instantiated task model also
includes the global path along which to move an object.
The system, then, inserts the obtained grasp and stack lo-
cations into the command sequence. Finally, the command
sequence is sent to the robot.
In order to develop task models for an APO system, we
have to define representations to describe assembly tasks.
In this section, we will define assembly relations for such
representations. Then, we will consider how to define
assembly task models using the assembly relations.
In each assembly task, at least one object is manipu-
lated. We will refer to the object as the manipulated object.
The manipulated object is attached to other stationary ob-
jects, which we refer to as environmental objects, so that
the manipulated object achieves a particular relation with
environmental objects.
We will define assembly relations with respect to face
contacts between a manipulated objectand its stationaryen-
vironmental objects. The essential goal of an assembly task
is to establish a new face contact between a manipulated
object and environmental objects. For example, the goal of
a peg-insertion is to achieve face contacts at the side and
bottom faces of the peg against the side and bottom faces
of the hole. Thus, we will use face contact relations as the
central representation for defining assembly task models.
To make the overall problem manageable, we concen-
trate on a world of polyhedral objects in which only one
polyhedron may be moved by one assembly task. An as-
sembly relation will be defined between a manipulated
polyhedron and several stationary environmental polyhe-
dra, This restriction still leaves a diverse range of interest-
ing relationships, actions, and resulting assemblies.
Using such face contact relations as the basic repre-
sentations, we will describe an assembly task with a tran-
sition between pre-assembly relations and post-assembly
relations. Based on the description, we will build an APO
system in the following steps:
For geometric objects in a polyhedral world, our taxon-
omy identifies all possible assembly relations based on the
directions of contact surface normals. The taxonomy has
classes of uni-, bi-, tri-, tetra-,and hexadirectional contacts.
Nine different contact patterns are extracted.
We will represent the contact directions and possible
movement directions on the Gaussian sphere as shown in
Figure 2. The shaded areas indicate the prohibited move-
ment directions of the object with respect to the environ-
ment. The non-shaded areas indicate the possible move-
ment directions. Similar representations have been pro-
posed for automatic programming purposes [11, 2].
We will consider a sequence of manipulator operations
to achieve each assembly relation from assembly relation
3d-s, Suchasequence of manipulatoroperationsis grouped
into a motion macro, ie., a template of manipulator opera-
tions, which, when applied to an object, yields the desired
assembly relation. This is possible because each assembly
relation is defined so that we can apply the same manip-
ulator control strategy to achieve the relation by changing
only controller parameters, not the strategy.
In order to reduce the number of necessary templates,
we will analyze each assembly relation in an iterative man-
ner. We will analyze simpler relations earlier and more
complicated relations later. Also, instead of considering
a template to directly achieve a complicated relation from
3d-s, we will consider an intermediate relation, and then
try to achieve the complicated relation. First, we try to
achieve an intermediate relation from 3d-s by using the
templates already considered. Then we try to achieve the
final relation from the intermediate relation usitng a newly
considered template.
In order to find an appropriate intermediate relation, for
each assembly relation, we consider disassembly actions
from the assembly relation, and extract all possible im-
mediate intermediate assembly relations just prior to the
assembly relation. We do this because considering disas-
sembly actions is easier than considering assembly actions.
Several intermediate relations sometimes occur from the
same assembly relation due to 1) the variation in shapes of
contact faces, and 2) the variety of possible disassembly
operations.
In case that due to variations in the shapes of contact
faces, we have to analyze all intermediate relations and
assign appropriate motion templates to all transitions from
the intermediate relations to the desired relation.
In case that due to the variety of possible disassembly
operations, we can choose one appropriate intermediate re-
lation among the several intermediate relations. We choose
the one which is achieved by the simplest and most robust
operation under uncertainty in positional information.
By using these criteria, we will analyze each assembly
relation, extract all possible assembly relation transitions,
and prune unnecessary relation transitions.
We can represent relation transitions as a tree structure.
Each node in the tree represents one particular assembly
relation, and each arc represents corresponding assembly
relation transitions.
A procedure tree is created by placing a template of ma-
nipulator operations (motion macro) at each arc separating
the assembly relation nodes. The manipulator operations
chosen are those which can correctly achieves an assembly
relation on one node from the assembly relation on the other
node. Figure 3 represents a completed procedure tree. See
Ikeuchi and Suehiro [6] for more details.
A task model consists of an assembly relation transition,
a motion macro, and the necessary parameters required to
expand the motion macro into a sequence of manipulator
commands. For example, Figure 4 shows the task model
corresponding to the transition from 3d-s to 3d-a. The
starting and end relation slots contain 3d-s and 3d-a, respec-
tively, The action slot contains the move-to-contact motion
macro. In order to achieve the motion, it is necessary to
know the previous configuration and end configuration of
the manipulated object. The corresponding parameters are
prepared as task parameters. The values corresponding
to these parameters are obtained by the task instantiation
module at run time.
Thirteen task models corresponding to all arcs in the tree
are prepared. They are attached to the procedure tree.
How are task models used to recover human assembly
tasks in the APO system? The task recognition mechanism
will be explained in the following examples. The example
system consists of three classes of objects, (any of which
can appear in the scene): castle, block, and stick. See
Figure 5.
The system assumes that at the beginning of each as-
sembly task human intervention occurs in the scene and
at end of the assembly task the human disappears from the
scene. By using thisassumption, the APO system segments
a continuous image sequence given by a TV camera from
the scene into a finite number of meaningful chunks.
By using the level change in the brightness difference,
the system can detect human intervention. Before human
intervention,the scene consists of only stillobjects,thus the
difference between two consecutive images is at the quite
level. When human intervention occurs, the brightness
difference is large due to the motion of human and manip-
ulated object in the scene. This disturbance continues until
the end of the assembly operation. After the human hand
disappears, the scene consists of only still objects. Thus,
the brightness difference returns to the quite level.
Objects in the scene are recognized from range data.
In our current implementation, b/w images are used only
for detecting the completion of one assembly task. More
reliable range data are used for analyzing the scene. After
a certain period after the detection of the completion of one
assembly task,the APO system invokes therange finder and
measures range information in the scene. The APO system
then generates a difference image between the range image
from the previous step (before the assembly task) and the
range image from the current step (after the assembly task).
The system applies a segmentation program to the dif-
ference image and obtains any newly appearing regions.
These new regions correspond to the faces of the manipu-
lated object by the assembly task.
The system maintains the configurations of other envi-
ronmental objects. The system represents these manipu-
lated and environmental objects in the Vantage geometric
modeler [l] as shown in Figure 6.
By using the transformation from body coordinate sys-
tems to face coordinate systems, (available from the Van-
tage geometric modeler), the configurations of the faces of
the manipulated and environmental objects are obtained.
The system extracts contacting face pairs from the face
configurations. Here, a contacting face pair is a face from
the manipulated objects and a face from an environmental
object, which have the same face equations and whose
surface normals are opposite to each other.
The system determines the assembly relation based on
thecontacting face pairs by analyzing the contact directions
of pairs. Here,the contact direction is defined as the normal
direction from the environment faces to the manipulated
object faces as previously defined. The contact pairs are
grouped intoa set of contact directional groups so that each
group has face pairs with the same contact direction. By
examining the occurrence of directions, we can determine
which assembly relation occurs by the assembly task.
The system recognizes the contact faces and contact
directions as shown in Figure 6. From the contact faces in
Figure 6, the system determines that the current assembly
relation is 3d-a.
Before the assembly task, the castle does not exist in
the scene. Thus, before the assembly task, the assembly
relation between the stick and the castle was 3d-s. After
performance of the assembly task, the manipulated castle
established a 3d-aassembly relation with theenvironmental
object.
From this observation, the system recognizes that the
assembly relation transition, 3d-s to 3d-a, occurs due to the
assembly task. The corresponding task mode 3d-s to 3d-a
isextracted from the corresponding arc along the procedure
tree.
In this example, at the previous step, the castle was
stored on the warehouse table. Thus, the assembly relation
transitions during the entire assembly task are
Thus, the corresponding three task models are instantiated:
ad-t0-s, $-to-s,and s-to-a.
The following procedure is executed to instantiate a task
model:
The instantiation of task models occurs in the reverse
order, s-to-a,s-to-s, and a-to-s,
The s-to-a task model has a move-to-contact motion
macro in the action slot. The task model examines each
object model and determines grasp configurations, how to
grasp the object withrespect to the body coordinatesystem,
and the specified grasping method. In the current imple-
mentation, each object model has predetermined grasping
configurations. The task model chooses an appropriate
grasping configuration and recalculates it based on the cur-
rent body configurations. The task model determines the
grasping configuration of the castle based on the observed
castle configuration. The task model also determines the
stack configuration of the castle on the table in a similar
manner. The system then inserts these parameters to the
corresponding slots in the instantiated task model.
The global motion is also implemented as a task model,
s-to-s, This task model has a motion macro, move. The
disassembly task is also implemented as a task model.
The system finally performs the operations given by the
three task models sequentially: a-to-s, s-to-s, and s-to-a,
Figure 7 shows the final move-to-contact operation by a
manipulator.
Figure 8(a) shows a human operation for inserting a
stick in a hole of the block. The system recognizes the
contact faces (Figure 8(b)). From the normal direction of
contactfaces, the system generates tetradirectional contact.
By examining the directions of the contacts, the system
determines that the observed assembly relation is 3d-e. By
examining the shape of contact pairs, the system infers s-to-
e path occurs. The s-to-e task model has a motion macro,
insert-intoin the action slot. Using thepredetermined grasp
configuration and the observed stick position, the system
performs the insert operation as shown in Figure 8(c).
Figure 9 shows an example, having the 3d-d assembly
relation between the stick and the two castles, constructed
successfully throughs-to-btask model (insert-between)and
b-to-d task model (move-to-contact) by the system.
We have described a method that can recognize an as-
sembly task performed by a human and produce corre-
sponding operational plans for a robot. The current system
works among polyhedral objects. Future directions include
how to extend this method to handle general objects, how
to generate grasp plans and global motion plans from ob-
servation.
RajReddyand Takeo Kanade provided useful comments
and encouragements. Bradley Nelson proofread the draft
of this manuscript and provided useful comments.
Takashi Suehiro was on leave of absence from Elec-
trotechnical Laboratory, Tsukuba, Japan.
.,,the whole of analysis discovered is dependent in
great part upon modified algorithms of certain fixed
quantities..
Leonhard Euler, 1764!
his opinion was quoted in 1822 by Babbage, in a paper
on notation which was related to functional equations
(p. 1344).In this article I put forward a general thesis about
Babbage's work as a mathematician,engineer, and scientist,
of which several elements are exemplified by the quotation.
My claim is as follows:
The content of this thesis will become clearer in and after
the discussion (in the next section) of certain mathematical
trends in Babbage's time, but some preliminary expansion
on the word ''algorithm'' is necessary here. I intend it to
refer, in a very general range of contexts, to ideas, theories,
or procedures in which prominence is given to successive
repetitions of a process or maneuver, its reversal, its com-
pounding with other processes, and/or its substitution into
itself. In mathematical contexts the words ''iteration'' and
''combination'' will also be used (and indeed, quoted).
In addition, two related notions have to be included.
First, ''algebra'' refers both to the branch of mathematics
in which Babbage worked (specifically, functional equa-
tions) and to certain features of algebraic thinking and
proof which also arise elsewhere in his activities, Second,
''semiotics'' denotes theories of signs, symbols, and nota-
tions as such (in mathematics and elsewhere), in which is
stressed their importance in a theory and in its philosoph-
ical basis. The word was not used in Babbage's time,* but
it can be applied to several of his concerns and those of
some contemporaries.
The thesis, then, is that Babbage consciously followed an
algorithmic/algebraiclsemiotic approach in his choice and
solution of many of his problems and deployment of analo-
gies, and that his historians should give it proper attention.
For convenience I shall coin the word ''algorithmism'' to
refer in general to this characteristic.
This thesis is explored in approximate chronological
order of the development of Babbage's pertinent interests.
The section ''Mathematical orientations'' covers the part
that Babbage played in the reform of mathematics at Cam-
bridge and his researches in functional equations and some
other areas of mathematics. The section ''Calculations by
hand and by handle'' starts with mathematical tables and
moves on to the Difference and Analytical Engines.''Indus-
try and science'' notes a miscellany of other examples in
manufacturing,cryptography, and physics.'The final section
draws some conclusions about the importance of algorithm-
ism in Babbage (including a contrast with Boole) and spec-
ulates upon its origins. Reference is made in places to ''the
figure,'' which appears in the last section.
I give details of the principal pertinent events in or
related to Babbage's life in the box. For more details,I refer
the reader to A. Hyman's excellentbiography.'I rely almost
entirely upon Babbage's published writings and on certain
manuscripts that appeared posthumously, for enough mate-
rial is to be found there for my purpose. Many unpublished
documents reinforce the thesis, and a few have been cited.
The references will be found in the references list, but in the
text I cite by volume and page number from M. Campbell-
Kelly's fine new edition.' For example, in the reference at
the start of this article to an 1822 publication,''p. 1/344''cites
page 344of Volume l of the new edition, but the superscript
'-2'' cites the original publication. Dates associated in the
text with items are normally those of first publication. A
page range not preceded by a volume number and a slash
refers to the work indicated by the superscript number that
precedes it.
.,the dots of Newton,the d's of Leibnitz, or the dashes
of Lagrange.
Babbage, 1864 (p. !1/19y
One part of Babbage's life is well known; he played a
major part in the conversion of British mathematics by the
Analytical Society from Newton's fluxional calculus to the
Continental notation, However, this ''fact'' is not a fact; it is
also misleading in connotation. A revised account will be
briefly summarized here.*
First, before that Society set to work in 1812, reforms in
calculus teaching had been under way, at least among the
staff, in various British institutions: in Scotland, in the circle
around J. Playfair and also W. Spence; in Ireland, at Trinity
College, Dublin, in moves initiated in 1812 by H. Lloyd;and
in the Home Counties of England, at the Royal Military
College and the Royal Military Academy (with P. Barlow,
O. Gregory,C. Hutton,J. Ivory, W. Leybourn,and W, Wal-
lace). At Cambridge itself, R. Woodhouse had become
acquainted with, and even the currentoccupant of Newton's
chair of mathematics, I. Milnor (a quite insignificant math-
ematician), had been buying Continental mathematical
books.* The single most important stimulus for change had
been the publication of the first four volumes of P.S.
Laplace's Traite du mecanique celeste (1799-1805). While
the young men who founded the Analytical Society in 1812
made themselves remarkably familiar with Continental
mathematics, they may not have been aware of all of these
developments in Britain. Thus, while their movement cer-
tainly led to the most profound changes in British teaching
and research in mathematics, it was not the first such initia-
tive.
Second, the Society lasted as such only for a little over a
year, while its founders (principally Babbage, J.F.W. Her-
schel, and G. Peacock) were undergraduates at Cam-
bridge,''' However, their intentions were maintained after-
ward, and I use the word ''Society'' in this looser sense when
referring to the later activities of its former members.
Third, the change was not simply of notation but princi-
pally of theory. Fourth, there was no single Continental
theory into which change could be effected; on the contrary,
as Babbage himself indicated when referring to the pertain-
ing notations in the quotation above, three theories were in
competition:'' limits (although not usually formulated in
Newton's manner); the differential and integral version (dx,
]x, ]y dx as an area, and so on), proposed by G,W. Leibniz
but then used in the developed version largely created by
Euler; and an algebraic approach introduced by J.L. La-
grange.
One did not necessarily have to make a choice. In partic-
ular, S.F. Lacroix, the chief textbook writer of the day, was
a disciple of M.J. Condorcet and followed the eighteenth-
century ''encyclopediste'' philosophical tradition of present-
ing all available traditions in his writings. His principal text
was the great Traite du calcul differentiel et du calcul
integral,'*which had appeared in three volumes at the end
of the eighteenth century. Babbage had bought a copy of it
in 1811 (p. 11I19)7 and he and his colleagues would have
become very well acquainted with Continental traditions
from it alone,** At all events, he and Herschelshowed their
erudition when publishing in 1813 their preface'' to the first
(and only) volume of the Memoirs of the Society; it is quite
a comprehensive survey of Continental calculus over the
previous 30 years.
Out of the possibilities available to them the Analytical
Society chose the Lagrangian approach (the word ''Analyt-
ical''was then often used in mathematics to refer to algebraic
principles). The origin of this decision is not clear; it seems
most likely that a general consensus among the members
was made. The main principles of this approach are ex-
plained in the next subsection. The strength of their adhe-
sion to it was made evident in the preface to the Society's
translation of the second (1802) edition of Lacroix's shorter
treatise on the calculus, published in 1816. As a wandering
encyclopediste, Lacroix had allowed himself to shift his pen-
chant somewhat to limits; but the Young Turks from Cam-
bridge admonished their senior citoyen for this sad prefer-
ence ''in place of the more correct and natural method of
Lagrange.'''tIn an 1827 paper on notation in mathematics,
Babbage praised in a similar vein the (attempted) expres-
sion of mechanics in algebraic theories that Lagrange had
attempted to effect in his 1788 treatise Mechanique an-
alitique (p. 1397y''
The ''analytical''algebraization of mathematical theories
in France in the late eighteenth century related to a growing
interest there in semiotics. The abbe Condillac and his
semifollowers, the ''ideologues,'' had stressed the impor-
tance of signs, especially in or from his textbook, the (so-
called) Logigue (1780). Indeed, the word ''ideologie'' origi-
nally denoted ideas, their reference,and means of signifying
them. Further, for Condillac (common) algebra was the
(semi-) formal language par excellence; a posthumous book
on it called La langue des calculs was published in 1808,'*
While Condillac did not influence Lagrange personally to a
notable extent, the general connection with algebra was
then important - and not only in France, as we shall soon
lLagrange had developed an algebraic version of the
calculus, based on the assumption that every function
f(Y + h) of a real variable could be expanded in a Taylor
series for every value of x (apart from values of Y when f
misbehaved, such as taking an infinite value):
He also claimed that the derivatives could be defined, by
purely algebraic means, as the coefficients of the appropri-
ate powers of the incremental variable h. He introduced the
dash notation for derivatives, which was mentioned by
Babbage at the head of the previous subsection; it denoted
a functorial operator, going from the function f(x) to the
function f'().
This approach is based on a clear program - doubtless
one of the sources of attraction to the members of the
Analytical Society in 1812. Babbage retained his enthusiasm
for it even after belief in the Taylor series (Equation 1) was
refuted by A.L. Cauchy in 1820 with counterexamples such
as exp(-1Yf) when = 0. Indeed, Babbage did not realize
and perhaps did not know of Cauchy's work, for in 1827 he
spoke of the time that''has been required to fix permanently
the foundations on which the calculus of Newton and
Leibnitz shall rest,'' with a clear allusion to Lagrange's
approach (p. 11371).'' (The independent discovery of this
counterexample by W.R. Hamilton in the 1830s also es-
caped Babbage's attention.) However, according to J.M.
Dubbey' (p. 90), Babbage had sent three of his papers on
functional equations to Cauchy in 1820, and to that topic we
now turn.
In the course of pursuing his approach, especially from
the late 1790s, Lagrange gave considerable impetus to the
development of two new algebras: differential operators,
using D (:= dldx) as an algebraic object; and functional
cquations, in which the function was itself treated as the
object. These algebras flowered especially with a group
around the Alsatian L.F.A Arbogast, who developed the
operational aspects by ''separating the symbols'' (a phrase
of the time). They detached dldx from y in the derivative
dyldx, and ffromx in f(). Thereby they extended the realm
of algebra by considering objects which were not numbers
or geometrical sizes.
LLaws and rules of manipulation of these new algebras
had to be found. A notable contribution was made by F.).
Servois in 1814;'' Seeking the fundamental properties of
both algebras, especially functions, he characterized f as
''distributive'' and fand g as ''commutative with each other''
if, respectively,
This is the origin of these standard words in algebra.
Functional equations, in one and several variables, were
Babbage's main mathematical interest from 1813 until the
early 18204. He wrote nine papers in or around them, which
were published between 1813 and 1827. (They are repub-
lished in Babbage's works, Volume 1.') Herschel also
worked in this area, mostly on the special cases of difference
cquations, and with related summation of series; the two
men corresponded intensively. It is not my intention to
describe Babbage's methods in detail;* suffice it to indicate
some principal concerns, especially the algebraic and semi-
otic aspects,
A simple example of a functional equation is
(which is an equation in two variables, because of the form
of the left-hand side). The task is to find functions f which
satisfy the stated property for all values of x and y, or some
specified range of them. (The definitions in Equation 2
could be reinterpreted as functional equations.) Babbage
and Herschel were drawn to functional equations not only
by Lagrange's program but also by certain solution methods
developed by G. Monge and Laplace in the 17704.
In his first paper (published in 1815) and later, Babbage
gave special attention to
to solve for f in terms of a given g. (In the important
circumstance when g() = x in the second equation of Equa-
tions 4, f was said to be ''periodic'' of order n,) His most
general equation in one variable was
to solve for f given F and the lg,] (p. 1I120).'* Apart from
some examples from the geometry of curves, he did not
consider many applications, explaining in 1816 that ''my
object has been to direct the attention of the analystto a new
branch of the science'' and stressing that ''the doctrine of
functions is of so general a nature, that it is applicable to
every part of mathematical enquiry'' (p. 11193).%
At the time, the subject was often called ''the calculus of
functions,''referring to the determination of particular func-
tions (f''(Y), say) even if no equations were involved; for
them the phrase ''functional equations'' was used.
Babbage's methods of determination and solution, which
followed the French to some extent, were rather freewheel-
ing. He manipulated functions and series, used self-substi-
tutions of functions into equations, and deployed cunning
changes of variable. He tried to study the difficult question
of the complete solution of an equation, and made very
clever use of symmetric functions (where h(x, y) = h(y, )
for allx and y)to build up an iterative sequence of particular
solutions. These methods show very well his enthusiasm for
algorithmic and iterative procedures. Like his predecessors,
he did not normally consider conditions for existence and
uniqueness; Cauchy was soon to focus on such questions.
As the quotation at the head of this subsection shows,
Babbage was also very concerned with the inverse function.
In this connection he made much use of the form g''fg of
solution* -- perhaps the first example of this significant
''conjugate'' form (as it became known) in mathematics.
Although he was more chary than Herschel of treating
functions as objects, he exploited well the algorithmic power
of this algebra: to handle a function f, its iterations ff (= f'),
f', ..., the inverse function(s) f ' and their iterations f*, ..,
compounds with other functions fg, .., and so on,
Babbage also tackled related topics, which had been little
studied, such as ordinary and partial differential functional
equations, integral equations, and fractional differentia-
tion,* Inn 1817 he explicitly compared ''the calculus of func-
tions with other modes of calculation with which mathema-
ticians have been long acquainted'' (p. 1/216).'Summation
of (in)finite series was a special concern, in which he followed
Euler and others in seeking formal relationships between
functions and their series expansions - indeed, the kind of
procedure which Euler designated as an algorithm in the
quotation at the head of this article.t One of his methods is
described in a special algorithmic context in the last section.
These methods belonged to a tradition in British algebras
which started principally with Woodhouse and was to be-
come best known with Peacock. In an 1830 book on alge-
bra,''' and elsewhere, Peacock proposed ''the principle of
permanence of equivalent forms.'' It put forward conditions
under which a mathematical expression or equation could
be interpreted outside its ''respectable'' domain of interpre-
tation (such as the sum of a divergent series, or a relation in
which negative numbers were accepted as legitimate math-
ematical objects). In its emphasis on form, the principle
carried something of a semiotic ring. But the principle was
applied mostly to common algebra, and Babbage did not
discuss it in his papers on functional equations, although he
came close to it in some of his manuscripts. ft
In 1816 Babbage introduced some good notations in
developing his methods: underbars for homogeneous func-
tions, so that ''u2.1. ),,'' indicated a function of degrees
p inx and ytogether and qq in y and z together(in a somewhat
unclear passage (p. 1/144)'); and superscripts for iterative
substitutions, with overbars for symmetric cases, such as (p.
1I12Sy3
Later he even took the general case gr'''(Y, y) as a mathe-
matical problem of notations, for he found the numbers of
occurrences of y and of within it by forming and solving
simple difference equations (pp. 1/348-349).'
Such points are evidence only of a (well-developed)
normal desire of a mathematician to use good notations; but
Babbage went much further to show his semiotic side also,
considering families of symbols, and symbolism in general.
LUnlike its algebraic mathematics, French semiotics did not
come over strongly to Britain, either in the revival of math-
ematics or in that of logic (which dated from the mid-1820s,
with the publication of R. Whately's book The Elements of
Logic''). Nevertheless, Babbage was aware of it. In the early
1820s he wrote an encyclopedia article on ''Notation''''and,
more importantly, a paper on ''the influence of signs in
mathematical reasoning,''? which were published in 1830
and 1827 respectively. There is much material in common
between the two pieces.
Babbage noted the work of both the French philosophers
and mathematicians: He quoted from the book Des signes
et de lart de penser (1819) written by the ideologue J.M.
Degerando (pp. 1/374, 376),'' and also explained the over-
bar and overarc notations A B and AB used by L. Carnot to
represent respectively directed straight and curved lines (p.
11404).} He did not develop the ''ideological'' link, but
variants of Carnot's notations were to be used in computing,
as we shall see later.
Babbage also laid out various desiderata for notations,
including one of almost iconic character: ''all notation
should be so contrived as to have its parts capable of being
employed separately'' (p. 11418).'' He gave as an example
possible notations for the sine-squared function: He pre-
ferred ''(sin )' -- as found in the ''excellent work'' of
Arbogast, or even better ''sin 8' for avoiding brackets --
by contrast, ''sin.'8'' (including the period) was ''by far the
most objectionable of any, and is completely at variance
with strong analogies'' (p. 1422).
In holding this opinion, Babbage was close to his friend
Herschel, who had discussed in 1813'' (p. 25) the use of the
indices to denote powers of functions, suggesting the nov-
elty ''cos, 'e'' for the inverse trigonometric function. 'This
notation (without the period) has become standard, but the
positive powers of these functions are always normally rep-
resented by the form which Babbage criticized - rightly.
The same fate awaited most of Babbage's (and
Herschel's) work on functional equations and on notations.
The French took some note of it; in 1821 J.D. Gergonne
translated part of Babbage's paperin his own mathematics
journal,''while the Baron Ferrusac'sabstracting BBulletin for
science included routine short notices of his papers pub-
lished in the period 1824 to 1831 of its run, Lacroix noted
Babbage's 1816 paper*in 1819, in the second edition of his
large Traie''' (p. 595, and a note of Herschel on p. 732).
Surprisingly, Babbage seems never to have consulted this
new edition of a work that had helped him so much in his
youth, and so, for example, seems never to have discovered
Servois's 1814paper,''towhich Lacroix also gave publicity
Opp. 726-727)
Functional equations fell rather into the doldrums for
several decades, and the only substantial use of Babbage's
contributions was made in 1836 by De Morgan,' in the first
systematic study of the subject. However, although pub-
lished as an article in an important encyclopedia of the time,
unfortunately it did not make the impact that it deserved.t
Similarly, Babbage's concern with notations failed to raise
the interest it deserved, although again De Morgan was a
commentator.
Some evidence of algorithmic concerns can be found
elsewhere in Babbage's mathematics. His occasional writ-
ings on probability were concerned with combinatorial
cases, including an 1821 paper'on the ''martingal'' in con-
nection with successive betting, the estimation of mortality
for the calculation of annuities in an 1826 book (pp. 6/91-
96).'? and the interpretation of apparent miracles in terms
of some higher law unknown to man in his unofficial 1838
Bridgewater treatise (pp. 9T73-80).'' In the latter case his
algorithmic and analogical inclinations stood him in espe-
cially good stead. He gave as examples an iteration executed
by his engine which followed a mathematical law and then
''miraculously'' contravened it at some stage, which, how-
ever, had been prepared deliberately by the operator (pp.
9/52-55).''(See also Babbage's Passages from the Life of a
Philosopherf pp. 11292-293.)
A striking example of algorithmism comes from the
mathematics of chess: ''During the first part of my residence
atCambridge,Iplayed atchessveryfrequently,''he recalled
(p. 1125)' and in an 1817 paper''' Babbage followed his
hero Euler in studying the iteration of the knight's move so
as to cover every square of the board. A further feature of
this paper is displayed in the figure on pages 42-43.
In this section, I treat Babbage's concerns with compu-
tation and computing.
For about five years from 1826 Babbage concerned him-
self with the production of logarithmic tables.'' He drew on
existing tables for the basic numerical data and did not
introduce any major new idea about their calculation. But
concerning their physical appearance he showed his semi-
otic side. For ease of reading he spaced out the arrays of
digits into five-row bandst* and chose a font where all
numerals were of the same height, yielding clean rows of
digits. Forclarityandcompactness,he printed out digits only
after the second decimal place, indicating by small zeros in
the third place those cases when ''1'' had to be added to the
second;and at the final seventh place he indicated rounding-
up by setting a dot under the digit (explanations and sample
pages from 1827 are given in Babbage's Tables of Loga-
rihms of he Natural Numbers (pp. 2/72-107).'' Babbage
used a few of these principles also for the tables in his 1826
book on assurance (pp. 6/104-127).'* In 1831 he printed
some of the logarithmic tables with a variety of colored inks
on papers of different colors, to compare various possibili-
ties for clarity of reading (pp. 2/115-117y'* Part of a page is
contained in the figure on pages 42-43.
In the end, Babbage's findings were not of major signif-
icance. But they were unusual, and reflect clearly the semi-
otic cast of his mind.
Prior to these semiotic ruminations on tables, Babbage
had brought to their algorithmic side a remarkable insight
which was to influence his whole life: that logarithmic tables
''might be calculated by machinery.'' In his autobiography
he gave two occasions for this reflection -- in 1812-1813 and
1819(p. 11131)'--- and the latter is of particular significance.
During the 1790s a large set of logarithmic and trigono-
metric tables had been produced in Paris under the direction
of the French engineer scientist G. Riche de Prony. The
work was planned out according to Adam Smith's principles
of the division of labor, and a large number of unemployed
hairdressers were used to fill out the numbers on the sheets
by adding and subtracting. Although the tables were com-
pleted in 1801, their size made publication a costly task and
it was never done, despite the fact that printing was started
more than once and various efforts were made over the
years to find finance.''*
Now one of these occasions occurred during the late
1810s, when the detente between Britain and France after
the fall of Napoleon opened up the possibility that the
British government might share the expenses. Although the
plans did not come to fruit, they were proposed just at the
time when Babbage was thinking about mechanical compu-
tation for the second time and must have remained in his
mind, for he described de Prony's project in his ''letter'' of
1822 which secured the original governmental grant (pp.
2/10-12)'' It seems that that project, with its extension ad
absurdum of manual computation, helped Babbage to con-
ceive of the need for an automated alternative.* Further, de
Prony so conceived his tables that the hairdressers only had
to add and subtract over differences of various orders.
Babbage's variant was his projected Difference Engine no.
l (as he later named it), working the same way mechanically
up to S' and back again, ''either proceeding backwards or
forwards,''as Dionysius Lardner putitin 1834(p. 2/167)%
The analogies here extend not only to de Prony; f and f''
(see the earlier subsection on functional equations) also
readily come to mind. Another analogy is shown in the
figure (in the last section), concerning the layout of the
wheels. Babbage himself reported yetanother, in a paperon
mechanical computation. He posed there a question con-
cerning the determination of the digit in any place in the
array (rather like the numbers of x's and y's in Equation 7),
and in 1822 he came up with a difference equation ''which
had impeded my progress several years since, in attempting
the solution of a problem connected with the game of chess'
. 233)7
In 1848 Babbage thought out a simplified version of this
engine. He denoted it in his autobiography as ''Difference
Engine no. 2'' (pp. 11/75-85)'' It has recently been con-
structed, as a new-old machine.'' Little of the relevant
paperwork has been published so far; I would expect it to
provide further evidence supporting the claim.
In 1834 Babbage came to his next great idea; a machine
that would give commands as well as execute them (p.
11146)7 The quotation above suggests that with this ex-
tended algorithmism he had a glimmering of mechanical
recursion, as it was to be conceived (in electrical and elec-
tronic contexts) a century later, the whole of arithmetic,
numbers and operations with them. Thus was born the
Analytical Engine.''
Many analogies were brought into play when Babbage
developed this engine. For example, the Jacquard loom
cards which ran it were of three kinds, for numbers, vari-
ables, and algebraic operations - as one would expect from
a mathematician who distinguished f from Y.f They were
used to give compound instructions (like functions fg, .--),
and they were able ''to revolve back wards instead of for-
wards,'' in the words of Lady Lovelace in 1843 (p. 31135y%
so close to Lardner's quoted in the previous subsection.
Again, presumably drawing on the experience of printing
tables, in 1837 Babbage represented the four basic arithme-
tic operations on cards of four different colors (p. 3/52)'
In the mechanism Babbage distinguished between the
''store,'' which held operands and results between opera-
tions, and the ''mill,'' where they were sent to execute arith-
metic operations (see A.G. Bromley's article,'' p. 198).
Another mathematical analogy comes readily to mind:
Babbage did not miss this analogy. On the contrary, in
his autobiography he foresaw actual applicability of the
engine to functional equations (p. 11/325)'
Calculus of Functions.
This was my earliest step, and is still one to which
I would willingly recur if other demands on my time
permitted... It is very remarkable that the Analytical
Engine adapts itself with singular facility to the devel-
opment and numerical working out of this vast de-
partment of analysis.
With her usual acuteness, Lady Lovelace had also stressed
this possibility in 1843 (p. 3/116)%
In studying the action of the Analytical Engine,we
find that the peculiar and independent nature of the
considerations which in all mathematical analysis be-
long to operations, as distinguished from the objects
operated upon and from the results of the operations
performed upon those objects, is very strikingly de-
fined and separated.*
Menabrea had gone a little too far in his 1842 account of
the engine. In a fit of outdatedness he invoked Lagrange's
belief(Equation 1)in the Taylor seriesto stressthe (alleged)
generality of its range (p. 3176).'* Neither Lovelace nor
Babbage picked up this detail in her translation (p. 3/107y
and earlier we saw that Babbage may not have known of
Cauchy's refutation of the belief.
In an 1826 paper on ''expressing by signs the action of
machinery'' (akin to the paper on mathematical notation''
cited earlier, incidentally). Babbage varied his use of
Carnot's curved and straight overbars by deploying vertical
lines and curled left brackets to distinguish different types
of motion of parts of Difference Engine no. 1 (pp. 31215-
216)''' Later, in his account of the Analytical Engine, he
used Lagrange-like predashes as in Equation 1 to distin-
guish the axes; for example, in 1837, F, 'F, and ''F were used
(p.317).''Later,in the pamphletf'of 1851, he lettered parts
of the engines on his working drawings in a manner extend-
ing this practice to sub-, super-, and all-over-the place indi-
ces, which indicated the type of part involved and its rela-
tionship to other parts.
Most important of all for semiotics, Babbage developed
a ''mechanical Notation'' for all his engines, by means of
which ''the drawings, the times of action, and the trains for
the transmission of force, are expressed in a language at
once simple and concise'' (p. 11179).' It included rules on
using upright,italic,and small-font letters for differentkinds
of referents(pp. 11107-110)'Had he managed to construct
his engines as envisaged, he might well have developed
these ideas further in producing the envisaged printing
mechanisms. There are obvious cross-influences between
these concerns and his printing of mathematical tables (dis-
cussed in the earlier subsection on mathematical tables).
Babbage's style is evident in concerns other than math-
ematics and computing, as we shall now see.
It is not a bad definition of man to describe him as a
tool-making animal.
Babbage, 1851 (p. 10/104yy%
To the modern view it is an irony that Babbage, much
concerned as he was withvarious applications of probability
and statistics, failed to notice their place in production
engineering. Instead, the semioticist won: ''Nothing is more
remarkable, and yet less unexpected, than the perfect iden-
tity of things manufactured by the same tool,'' he wrote in
1835 (p. 8/47, italics inserted).Y' We may have a clue here
about his failure to complete any of his engines: A lack of
understanding of production processes led him to waste
time and money on the excessively precise manufacture of
some of their parts.
Babbage's statement was made in the most influential
book that he published, On the Economy of Machinery and
Manufactures (the 1835 edition is cited here). There was
much concern at thattime,especially among engineers, with
the mathematical analysis of economic questions,especially
concerning optimization''' and equilibrium'f (Chaps. 2-3).
However, in his usual lateral and algorithmic way, Babbage
focused instead upon the processes that take place: To the
extent that optimization is treated, it is usually in the form
oftime-saving or time-consuming. A wide variety of produc-
tion procedures and problems was given in the book, of
which one is worth noting here: an extensive account of
Adam Smith's principles of the division of labor and their
use by de Prony to manufacture his logarithmic and trigo-
nometric tables (pp. 8/124-126, 135-139y.m
Algorithmic thinking is evident in Babbage'sideas on the
postal services. His proposal ''for transmitting letters en-
closed in small cylinders, along wires suspended from posts,
and from towers or from church steeples'' (p. 111447)' has
the characteristics of compounding and reversal, and the
little model that he made around the mid-1820s in his own
house shows that he took it seriously. Again, in his book on
manufactures, he criticized the poor way in which letter
boxes were indicated and advocated a semiotically much
superior system: ''at each letter-box, to have a lightframe of
iron projecting from the house over the pavement, and
carrying the letters G.P., or T.P., or any other distinctive
sign'' pp. 832-33)
'*I was much struck with the announcement'' of F,
Arago's researches of 1824 ''on the magnetism manifested
by various substances during rotation,'' recalled Babbage in
his autobiography (p. 111339)7 and he and Herschel re-
ported their own researches in a joint paper''' of 1825 and
Babbage in one of his own' a year later. One may presume
that the repeated oscillations inherent in the phenomena
attracted the attention of this natural algorithmist.
In 1851 Babbage published his study of another case of
repetition, this time an optical one: He proposed occulting
systems for lighthouses, in which ''It would only be necessary
to apply a mechanism which should periodically pull down
an opaque shade over the glass cylinders of the argand [sic]
burners,'' Further, a lighthouse could identify itself by ex-
hibiting its identification number in a temporally semiotic
manner, in terms of the appropriate numbers of occultations
for each digit. For example, the lighthouse numbered 253
would signal
[time -]
00000000000 .0 . 0000.0. 0.0.000. 0.0.0000000000...
where the raised points indicate the (2, then 5, then 3)
interruptions of shining of the light (pp. 10/62-65)%
Another late interest of Babbage lay in cryptography; it
exhibits algorithmism and semiotics very clearly, especially
in the transposition and rearrangement of the letters of the
alphabet. In attending to the coding and decoding, he
thought once again of going forward and backward. He did
notpublish much on it(principally an 1854article' ), butO.I.
Franksen'''has shown recently from the manuscripts that he
gave it a great deal of attention. It also provides another
feature for the figure.
The next morning I breakfasted with Humboldt. On
the previous day I had mentioned that I was making
a collection of the signs employed in map-making.
Babbage, 1864 (p. 11I149y
Babbage's eclecticism is not as random as it might ap-
pear: The constant concern with algorithms, semiotics, and
algebraic thinking functioned together and gave hiswork far
more interconnections than are obvious at first. This is the
thesis proposed in the first section of this article, and it is
strengthened not only by the content of the cases exhibited
but also by their choice: That is, Babbage preferred to work
on problems and contexts in which they were prominent
rather than on the numerous other situations in which they
were not (so) evident.
A further common factor can be pointed out, as I fulfill
the promise of providing the figure. It shows four illustra-
tions of whati call an ''iterative array'': that is, some kind of
repeatable process or its product. Further, one passage from
a paper of 1819 on infinite series (see the subsection on
functional equations) proposed a mathematical analog:
namely, ''the method of expanding horizontally and sum-
maiIg vertically'' when a sequence of series were to be
summed together (pp. 1/248-249, 268). The wide range of
contexts highlights the strength of his liking for algorithmic
thought.
Babbage's position can be clarified by contrasting it with
that of his contemporary George Boole, another major
algebraist and a pioneer of logic. One might expect to find
that the two enjoyed fruitful contact, but in fact it never
developed, a point that has surprised some historians (see,
for example, D. MacHale's book,'' pp. 234235).
The two men did meet, in 1862, probably at the Septem-
ber meeting at Cambridge of the British Association for the
Advancement of Science.* Babbage explained the Differ-
ence Engine and recommended that Boole study
Menabrea's 1842 article'* and learn about the loom-card
system. Boole hoped to meet Babbage in London after the
Cambridge meeting, but he was called back urgently to his
institution (Oueen's College, Cork).* No significant con-
tact developed between them, but Boole enclosed an off-
print of a paper on probability theory, and this may have
been the stimulus on Babbage around that time to read
Boole's major paper'' of 1844 on differential operators. ''It
related to the separation of symbols of operation from those
of quantity,a question peculiarly interesting to me,since the
Analytical Engine contains the embodiment of that
method,'' Babbage noted (p. 11/105).''Nevertheless,despite
the quality of the paper,''There was no ready,sufficient,and
simple mode of distinguishing letters which represented
quantityfrom those which indicated operation''(p. 11/105),!
Boole's work on logic began to appear with a short
book'' of 1847. Babbage read it and annotated it with the
marginal remark ''This is the work of a real thinker''' (p.
244). This praise is itself noteworthy; for unlike Boole's
contributions to differential operators, his work on logic did
not arouse strong interest untilthe early 1860s, when Stanley
Jevons was the first to give it detailed scrutiny,t However,
Babbage did not make use of Boole's ideas developing his
cngines. To a modern view this is a pity, for his thinking,
especially on the Analytical Engine, was rather weak on
logical matters,''' However, there are intimate and explicit
links between Boole's work on differential operators and on
logic,'' and it seems likely that Babbage would have found
(or did find) similar and thus unsatisfactory conflations in
Boole's logic, with the same symbol (deliberately) serving
both for the operation of selecting of objects to form a class
and for the (objectual) class itself. Indeed, while his criti-
cisms of Boole's 1844 paper are rather overstated, they are
quite accurate (when meantas criticisms)on Boole's algebra
of logic.
Conversely, although Boole praised in 1860 Babbage's
contributions to functional equations'' (p. 208), he would
have found repellent the mechanical aspects of Babbage's
engines. He had stated in his 1847 book'' (p. 2)
Algorithmic theories and methods occupied Babbage
from his youth to his dying days; they constitiute a most
unusual grotup of concerns for a scientisL Their origins and
drives must have been powerful.
The case of algebra is particularly instructive, for it came
to him very young. He recalled that he was ''passionately
fond of algebra'' when still a schoolboy (p. 11/18)'' and soon
afterward he was advocating Lagrange's approach at Cam-
bridge. For a research area he chose functional equations, a
perfectly reasonable choice but by no means a frontline topic
at the time. Much more orthodox would have been, sav.
differential equations and applications to mechanics. Yet he
went to that algorithmic theory and stayed there for several
years of productive work. Indeed, he never lost or regretted
his interest in functional equations, as we saw in his testi-
mony in the subsection on the Analytical Engine and his
attention to Boole.
What kind of explanation can we offer for the strong
place of algorithmism in Babbage? Sociological elements
make some contribution, in that from Woodhouse
through Babbage and Boole right to the end of the cen-
tury with A. Cayley and J.J. Sylvester, English mathemat-
ics showed a marked concern with algebras of one kind
or another. But such factors are very limited, for an equal
number of nonalgebraic English (near-) contemporaries
and successors can be specified: the later work of Her-
schel. for example, W. Whewell, G.B. Airy, G.G. Stokes,
and so on. The most similar case is De Morgan, whose
fondness for algebras started out in the common versions
and then passed through functional equations (see the
subsection on semiotics in Babbage's mathematics) to the
mathematical analysis of syllogistic logic.tt Peacock is
another, less significant, example.
Thus a personal explanation seems to be required, cen-
tered primarily on Babbage himself --- 'psychological,'
maybe, though the scare quotes are there to scare off the
psychohistorians.* For some reason algebra came naturally
to Babbage, and the contexts of the time extended that
inclination into a lifelong interest in matters algorithmic and
semiotic.tt In the quotation set at the head of this subsec-
tion, he stressed the importance of the semiotic aspects of
his early orientations. The quotation ''What is there in a
name?'' is the first sentence of his autobiography.'' And the
quotation on ''tool-making animal'' -- serving as not a bad
definition of man -- is certainly a very good definition of
Babbage himself. Can we see him as a naturally algorith-
miclalgebraiclsemiotic mathematical thinker patched into a
practically oriented personality? Relative to the French
background,was he a fusion of the interests of Lagrange and
Material related to this article was presented at the Babb-
agelFaraday Bicentenary Conference, held at Cambridge,
England, in July 1991; and at the International Conference
on the History and Prehistory of Informatics, which took
place in Siena, Italy, in the September following. Ouestions
asked on these occasions led to valuable additions. Com-
ments on a draft were received from M. Panteki and O.I.
Franksen, and two anonymous referees. For permission to
quote from Lovelace's letter I am indebted to Taylor &
Francis Ltd., and the St. Bride Printing Library.
Traditionally, there have been two general approaches
to performance measurement of computer networks:
modeling [1] and monitoring [2-4]. The modeling ap-
proach is appropriate when the network is not yet
operational. Two modeling techniques, analytical and
simulation, apply a workload to a model of the network
in order to derive performance parameters. An analytical
model is a static, mathematical approximation, whereas a
simulation is a computer program that models a network
and provides data about its dynamic behavior. There are
inherent problems in any modeling technique, primarily
because the model is a simplification of the real system.
Simplifying assumptions may compromise the extent to
which the model represents the actual behavior of the
network under real operating conditions. Another
problem is verifying the accuracy of the model and the
results. For the emerging distributed systems, modeling
techniques may provide only limited results.
The alternative to modeling is the monitoring ap-
proach, which provides real-time surveillance and con-
trol of an operational network. A monitor observes the
network, collects data, and presents results in a usable
form. Monitors can provide traces or profiles of network
transactions. Although useful for debugging purposes,
traces, or complete records of network traffic, impose
cxcessive storage and processing requirements on a
monitor. Thus, a profile made up of certain statistics
about network traffic is often more practical.
There are three basic types of statistical network
monitoring [5]: performance analysis, performance
verification, and network management. Through perfor-
mance analysis, the actual network performance is
measured and can be compared with that predicted by
analytical or simulation studies. Performance verification
ensures that a network meets its design specifications.
Finally, network management involves determining
whether a network is operating properly and efficiently.
These functions of network monitoring are outside of
the scope of the modeling techniques. Hence, monitors
become a vital part of the network in order to effectively
characterize its performance. A network monitor serves
as the agent for making measurements of appropriate
network parameters and summarizing the results.
This article discusses a network monitoring measure-
ment technique and tool in the context of the CSMA/CD
protocol for local area networks. The second section
briefly reviews this protocol and its performance im-
plications. The next section describes existing monitor
measurement techniques. In the fourth section, a mas-
ter/slave monitor measurement technique and the func-
tions and measurements that the monitor supports are
presented. Finally, in the last section, an experimental
implementation of the monitoring technique is described.
Carrier Sense Multiple Access with Collision Detec-
tion (CSMA/CD) is a medium access contention method
that defines the means by which two or more nodes
share a common bus medium. The CSMA/CD protocol is
described fully by the IEEE Standard 802.5. A brief
overview of its operation is provided in this section.
A node wishing to transmit listens to the medium and
acts according to the following rules [5].
Selection and measurement of the network parameters
are often dictated by the protocol of the operating
network. Certain characteristics of the CSMA/CD protocol
are significant, in particular, the slot time and the
retransmission scheduling algorithm. The transceiver
cable interface defined by the Ethernet specification is
also important for the measurement technique proposed
in the fourth Section.
The CSMA/CD standard requires a certain minimum
packet length in order to ensure reliable collision detec-
tion by all the nodes. The minimum packet length is the
number of bytes that can be transmitted during the
round trip propagation delay between the two furthest
points in the network. This delay is called the slot time.
Transmitting a packet for a slot time ensures that a
collision signal has time to propagate back to all nodes
involved in the collision whhile the nodes are still transmit-
ting
The retransmission scheduling algorithm defined by
the IEEE Standard 802.3 is binary exponential backoff.
When a collision occurs during transmission, the node
backs off and schedules a retransmission attempt. If
there are successive collisions, the node continues to
back off and reschedules transmissions until either the
transmission is successful or a maximum number of
attempts have been made. Thus, CSMA/CD specifies a
non-deterministic delay for accessing the channel.
Finally, the transceiver cable interface is an important
interface in the network specification. When used in a
configuration, it ensures compatibility between physical
interfaces and the Ethernet. The transceiver cable, also
referred to as the Attachment Unit Interface (AUI),
interconnects the transceiver (which taps directly into
the coaxial cable) and the Ethernet controller of the
network node. The transceiver cable consists of five
twisted pair wires, each of which carries one of five
signals: transmit, receive, collision presence, control, and
power. In the measurement technique described in the
fifth section, the transceiver cable serves as the interface
between a monitor and a network node. Certain local
parameters at each node can be derived by monitoring
these signals.
Various monitor measurement techniques have been
proposed and implemented [2,5]. The following issues
should be considered when selecting an appropriate
technique for a given network.
Artifact-is the interference in a target system caused
by the introduction of a monitoring device. Besides
minimizing artifact, its magnitude should be known, so
that it can be removed from the measurements. This
ensures an unbiased indication of performance.
Artificiial tmffic genertion.-Traffic generators produce
traffic loads with known characteristics, thus facilitating
network testing and debugging. Artificial traffic genera-
tion can be used to emulate high load conditions,
produce repeatable and variable traffic patterns, and
extract timing information. For example, a traffic
generator can issue packets with periodic or Poisson
arrival rates, which facilitates comparison investigations.
Also, time-stamping of packets provides measures of
network traffic speed and node response time. If
generators communicate with a monitoring system, traf-
fic generation and data collection can be synchronized.
Real-time analysis--The data collected by the monitor
can be either analyzed immediately (in real time) or
stored for analysis later. When packets are transmitted at
a rapid rate, there is limited time to collect and analyze
data, which makes real-time analysis difficult.
Parameters of interest-In order to decide which
monitoring approach to adopt (centralized, distributed,
or hybrid, which are described below), the parameters
to be measured must be defined. For a given network,
the best approach may change depending on the
parameters of interest.
The monitor measurement techniques may be divided
into three categories, depending on the location of
measurement: centralized, distributed, and hybrid.
The centralized approach is natural for broadcast net-
works. Typically, a modified interface taps onto the
channel to support the functions of the central monitor.
Two major types of centralized measurement techniques
have been proposed: the probe monitor and the spy
monitor [3]. These are depicted in Fig. 1.
The probe monitor injects packets into the network at
specified intervals and can record network parameters
for each injected packet, such as the channel acquisition
delay, transmission delay, and number of collisions. It is
an active monitor and thus introduces monitoring ar-
tifact. The monitoring resolution is dependent on the
frequency at which the traffic is injected and observed. If
the resolution is increased, the artifact increases as a
result.
The spy monitor is a special node dedicated to moni-
toring the network passively. It receives and analyzes all
of the packets on the network, and introduces no artifact.
For complete measurement, the spy monitor must be
capable of processing packets as they arrive. Hence, the
monitor must be equipped with sufficient processing
power and data storagc space.
Although probe and spy monitors tend to be simpler
and less costly to implement than other types of monitors,
they cannot provide all of the desired information. Infor-
mation pertaining to a particular node interface cannot
be obtained. Also, central measurement biases some
timings, such as the arrival time of a packet onto the
network. An arrival recorded by a central monitor does
not represent when the packet entered the channel. It is
offset by the propagation delay of a signal over the
channel. The monitor needs to account for this bias to
provide accurate measurements.
In the distributed approach, complete information
about network traffic at all nodes is available. Each node
captures data and periodically transmits its information
to a central analyzer. The central analyzer only analyzes
data received from the distributed monitors, since it
does not monitor the network. Two broad approaches
are adopted for distributed measurement: hardware
monitoring and software monitoring.
Hardware monitoring requires that each node monitors
traffic. This approach is more suitable for future network
designs, since it is expensive to modify the existing node
interfaces. Software monitoring, on the other hand, is
more flexible, because the software can be modified.
However, since nodes are typically of different types,
different software may be needed for each node. Also,
monitoring software may increase the processing load
and memory requirements of the nodes, and some types
of nodes may not be capable of meeting these needs.
Distributed monitoring, in general, has certain draw-
backs. Communication overhead results when transmit-
ting large amounts of data to a central location. Unless a
dedicated channel is used, each node must send data
over the network to the central analyzer, which intro-
duces artifact. Besides introducing artifact, using the
target network as an integral part of the monitoring
system could cause loss of the monitoring functions if
the network fails. Finally, synchronization of real-time
clocks at each node interface to coordinate timing
measurements poses a classical problem. The distributed
approach is depicted in Fig. 2.
The hybrid approach combines the essential features
of the centralized and distributed approaches, as shown
in Fig. 5. It allows accurate and comprehensive measure-
ment, while reducing modifications to node interfaces
and the amount of data sent to the central monitor. The
central monitor directly observes the network to collect
its own data and also receives data from each of the
node interfaces. It analyzes all data to calculate the
parameters of interest. Although artifact is introduced
onto the network if a dedicated channel is not es-
tablished, the artifact should be less than in the dis-
tributed approach since the central monitor locally col-
lects some of the necessary data.
Tables I and II summarize the features, advantagcs, and
disadvantages of the monitor measurement techniques
described above. The remaining sections of this article
describe a variation of the hybrid monitor measurement
technique that has certain novel features.
The previous section pointed out that the hybrid
technique is the most advantageous measurement tech-
nique in the sense that most of the measurements of
interest can be obtained without introducing severe
traffic overhead. However, in spite of its advantages, the
hybrid technique is impractical to apply in an operating
network, primarily because substantial changes are re-
quired in both hardware and software for each node on
the network. To counter the drawbacks of existing hybrid
measurement techniques, this paper proposes a mas-
ter/slave measurement technique. Compared to existing
techniques, the master/slave approach differs primarily
in the following three areas: the location of the interface
between monitor and node, the use of an effective and
economical monitor bus, and the relationship betwcen
master and slave monitors.
A major difficulty of applying an existing monitoring
technique in an operating network is establishing an
interface between the node and the monitor. Figure 4
depicts possible locations for this interface. Current
techniques typically place the monitor at locations
denoted by a, b, or c. However, in the master/slave
measurement techniquc, a monitor resides at the loca-
tion denoted by a'. As a result, this approach does not
introduce changes in the network nodes or in the
software running on the nodes.
The master/slave measurement system consists of four
basic components, which are illustrated in Fig. 5: 1)
master monitor, 2) slave monitor, 5) tapping cable, and
4) monitor communication bus. In this system, slave
monitors are connected to the transceiver cable (Attach-
ment Unit Interface, or AUI) through the tapping cable.
The slave monitors function passively (that is, they do
not interfere with the normal operation of each node),
and they only receive data from the tranceiver cable. The
data include collision signal, received data, and transmit-
ted data. All received data are processed and reduced by
the slave monitor to minimize the monitor's memory
storage requirements and also the load on the monitor
bus. The processed statistics of each slave monitor are
collected by the master monitor via the monitor bus. No
statistics are transferred to the master monitor over the
medium. The monitor bus is a serial, asynchronous bus
which interconnects the master and slave monitors.
A master monitor collects statistical information from
the slave monitors via the dedicated communication bus.
A network user or manager can interact with the master
monitor to obtain network information. The information
available from the master monitor includes channel
utilization, total offered traffic, network delays, and in-
dividual node information. Analysis of the network delays
using artificial traffic generation and statistical informa-
tion is an essential function of the master monitor. The
functional block diagram of the master monitor is given
in Fg. 6.
A slave monitor passively monitors signals in the AUI
of an Ethernet node. The signals available for inspection
by a slave monitor are transmit data, receive data, and
collision detect. From these signals, performance infor-
mation is collected, processed, and transferred to the
master monitor by the slave monitor. The types of infor-
mation include the following:
A network user can then access this information via the
user interface in the master monitor. Figure 7 shows the
functional block diagram of a slave monitor.
Two different configurations can be established for
the master/slave measurement technique depending on
the network size and purpose of the measurement: 1)
fully distributed monitor system, and 2) locally dis-
tributed monitor system. Figures 8 and 9 depict the two
different architectures, respectively.
If the network size is small or the purpose of the
measurement is for system set-up or exhaustive testing,
then the fully distributed monitor system is the best
approach. In this configuration, every node is associated
with a slave monitor, and only one master monitor
exiSts.
When the network is large (for example, hundreds of
nodes), the fully distributed monitor system may not be
practical to implement. In this case, the locally dis-
tributed monitor system can be configured so that the
monitoring of the nodes is partitioned into smaller, local
areas as shown in Fig. 9. Only certain nodes are allocated
slave monitors. Since the slave monitors are designed as
plug-in modules, they can be easily moved from one area
to another. This configuration can provide a representa-
tive view of overall network performance if the nodes
with monitors are selected appropriately. Monitors should
be placed at nodes which are known to possess typical
or desired types of traffic.
The hierarchical of structure of this system permits it
to be used in a multiple network environment. Multiple
master monitors can be set up, one per networl, each
with its own set of slave monitors. Figure 10 shows a
configuration with three master monitors: MM1, MM2,
and MM3. MM3 is the main master monitor and receives
network information from MM 1 and MM2.
An implementation of the master/slave measurement
technique is currently under development for use with
an operating AT&T 3BNet network [6] at Iowa State
University. The 3BNet interconnects certain AT&T Unix-
based computers and network-compatible peripheral
devices to form a local area computer network. The ISU
3BNet currently consists of one 3B20, two 3B5, and
twelve 3B2 computers. All are located in the same
building and connected over a 50-ohm coaxial cable
operating at ten megabits per second. The topology of
the network is shown in Fig. 11. A 3BNet communica-
tion protocol is implemented on top of the Ethernet
protocol.
The 3BNet network provides an experimental environ-
ment in which to implement and evaluate the mas-
ter/slave measurement technique. However, the tech-
nique itself is independent of any particular Ethernet
configuration. The distinctive components are the slave
and master monitors.
The slave monitor is a custom-built hardware device
consisting of the functional blocks shown in Fig. 12. It
passively taps onto the network at a node interface. As
shown in the figure, a simple plug-in ''tee'' connector
attaches the slave monitor to the Medium Access Unit
(MAU) in the node interface. This tap is compatible with
the IEEE Standard 802.5 Attachment Unit Interface (AUI)
specifications. The slave monitor can be connected and
disconnected without interfering with the network opera-
tion.
The slave monitor has three primary functional com-
ponents: 1) an isolator unit, 2) a phase-locked loop
circuit, and 5) a processing unit. A standard, low-cost
processing unit which provides a small memory and
supports serial communications was selected, namely
the MCS-8051 processor [7]. Since the storage require-
ments of the monitor are not very extensive, no addi-
tional memory unit is needed, although memory can be
added with minimal cost. All monitor software can be
stored in the four Kbytes of program memory on the
MCS-8051. An RS-i22 driver interfaces the processing
unit with the serial inter-monitor bus. The serial port is
used to transfer data between the slave monitor and the
master monitor. RS-422 was selected for several reasons.
RS-422 differential line drivers provide good distance
and data rate characteristics. A multiple drop topology
for the inter-monitor bus is supported by the ''wired-
OR'' logical connection capability of the RS-i22 inter-
face. Finally, communication over twisted pair cables is
nexpensve.
The isolator unit performs two functions: 1) provides
isolation between the slave monitor and MAU, and 2)
improves the fanout of the monitored signals. The phase-
locked loop offers timing-related information to the
processing unit. It can detect the start and end of a
packet, which defines the packet length. The state of the
collision presence signal can be used to derive a count
of the number of collisions. The processing unit calcu-
lates and records the following local parameters: transmit-
ted load, packet size, maximum and minimum packet
lengths, inter-packet transmission time, number of pack-
ets transmitted, number of packets received, packet
reception rate, and number of collisions. Statistics are
transmitted to the master monitor when the slave
receives a request from the master.
Control information provided by the master monitor
assists the processing unit in its calculations. Control
information includes network data rate, the required
format of the data packets sent by the slave monitors to
the master monitor, and tuning parameters. The tuning
parameters correct for measurement discrepancies found
by the master monitor when the statistics provided by
the slave monitors do not match the statistics kept by
the master monitor.
The master monitor is an off-the-shelf hardware device
based on a single-board Ethernet controller. It serves
four essential functions: 1) monitoring data on the net-
work, 2) generating traffic on the network, 5) setting up
and polling the slave monitors over the serial inter-
monitor bus, and 4) presenting network parameters to
the user.
Figure 12 depicts the modules comprising the master
monitor. The main components are an Intel 82586 AN
Coprocessor [8], an Intel 82501 Ethernet Communica-
tions Interface [8], a standard Ethernet transceiver, a
processing unit, a memory unit, and two serial interfaces.
The shared memory stores any data transmitted or
received by the master over the network or the inter-
monitor bus. An RS-422 serial port serves as the interface
between the processing unit and the inter-monitor bus,
while an RS-232 serial port sets up communication with
the user. The master monitor can be attached to a
display terminal or a personal computer, depending on
the user's needs.
An 82501 Ethernet Communications chip provides the
interface between an 82586 LAN Coprocessor and the
network. The 82586 Coprocessor manages the medium
access mechanism for the master monitor. This scheme
relieves the processing unit from packet reception and
transmission overhead. However, the processing unit
handles the communication with the slave monitors over
the inter-monitor bus.
The processing unit is responsible for configuring the
slave monitors and resolving any traffic-mismatch measure-
ment problems. That is, when statistics kept by the
master are not consistent with statistics received from
the slaves, any slave monitor can be dynamically recon-
figured so that the necessary requirements are met. For
example, each slave monitor reports the number of
collisions at its station to the master monitor. The mas-
ter, however, observes the network and maintains its
own count of the total number of collisions. If the
master detects a discrepancy between its count and the
sum of the received slave counts, then it can invoke
diagnostics to check the operation of the slaves. Other
discrepancies might occur in the measurements of colli-
sion time or transmission time recorded by the master
and slaves, which can be corrected by fine-tuning the
slaves to improve the accuracy of local parameter
measurement.
The master monitor can calculate approximate net-
work delays by injecting its own packets onto the net-
work bus and asking receiving nodes to send back the
same packets. It time-stamps injected packets within a
resolution of ten microseconds. The delay is calculated
based on the time stamp of a packet (the time at which
the packet was sent) and the time at which the packet is
received back. Delay measured in this way is representa-
tive of actual delays only if a large user population is
assumed. If the number of users is small, the traffic
generated by the delay monitoring packets becomes
significant and introduces an error in the delay calcula-
tions. However, when there is a large number of active
users, delay packets have little effect on total network
traffic, and any error is small enough to be neglected.
To handle the computational load of the master
monitor, an Intel 8086-based single board computer can
be used with the Ethernet controller board. An Intel
Multibus (IEEE Standard 796 bus) interface, included on
the boards, supports multiprocessing and DMA transfers.
The extra processing unit can perform statistical com-
putations, data analysis, system control, and user in-
put/output for the master monitor.
In section three, several existing monitoring tech-
niques were examined and contrasted. This examination
led to the development of a master/slave monitoring
system. This monitoring system has several advantages
over the existing systems. By using a passive tap, the
slave monitors provide distributed measurement without
introducing changes in the network nodes or in the
software running on the nodes.
The slave monitors communicate with a master
monitor via an economical multidrop twisted pair net-
work. This network provides sufficient bandwidth without
the overhead and cost associated with a standard net-
work. The slave monitors consist of a small number of
off-the-shelf components. These monitors process the
data locally and send only the processed results to the
master monitor. The slave monitors can be reconfigured
by the master monitor to provide maximum flexibility.
The master monitor provides all the functions of a
central monitoring system. The monitor can generate
traffic with time stamps and can use the information
obtained from the slave monitors to provide detailed
node traffic information.
This system is under development at Iowa State Un-
iversity and will be used for both teaching and research.
The monitor system will assist in the development of a
distributed file system based on a large collection of
UNIDD machines connected via Ethernet.
Doug Jacobson is an Assistant Professor of Electrical En-
gineering and Computer Engineering at Iowa State University,
Ames. He received his B.S. in Computer Engineering in 1980,
his M.S. iin Electrical Engineering in 1982, and his Ph.D. in
Computer Engineering in 1985 from ISU. From 1981 to 1985,
he was a Senior Design Engineer at the Iowa State University
Computation Center. He is currently teaching in the area of
networking and data communication, including courses with
NTU. Areas of research include network performance and
protocol verification and specification.
Suunll S. Galtonde is a P.D. student in Electrical Engineer-
ing and Computer Engineering at Iowa State University, Mmes.
He received his B'Tech. in Electrical Eagineering from the
Indian IInstitute of Technology, Karagpur, India, in 1983, and
his M.S. in Computer Engineering from ISU in 1985. His re-
search interests are in the area of voice/data integration on
computer networks.
Joon-Nyun Klm is a Ph.D. candidate in Electrical Engineer-
ing and Computer Engineering at Iowa State University, Ames.
He received his B.S. in Electronic Engineering from Seoul Na-
tional University, Seoul, Korea, in 1978, and his M.S. in Com-
puter Engineering from ISU in 1986. He is currently invotved in
the performance analysis of local area networks and protocol
specification and verification.
Ji Yong Lee is a P.D. candidate in Electrical Engineering
and Computer Engineering at Iowa State University, Ames. He
received his B.S. in Electronics Engineering from Yon-Sei Uni-
versity, Seoul, Korea, in 1977, and his M.S. in Electrical En-
gineering from ISU in 1984. From 1977 to 1982, he was a
research engineer at the Agency for Defense Development of
Korea. He is now a temporary Assistant Professor of EE/CprE at
ISU. His current research interests include local area networks,
integrated voice/data communication systems, and protocol
design and analysis.
Dlane Thiede Rover is a Ph.D. student in Electrical Engi-
neering and Computer Engineering at lowa State University,
Ames. She received her B.S. in Computer Science from ISU in
1984, and her M.S. in Computer Engineering from ISU in 1986.
From 1984 to 1986, she was an Ames Lb Associate at the
Microelectronics Research Center, and since 1985, she has
been an IBM Graduate Fellow. Her research interests include
parallel computer architectures and performance of computer
Systems.
Mansoor Sarwar is a Ph.D. student in Electrical Engineering
and Computer Engineering at Iowa State University, Ames. He
received his B.Sc. in Electrical Engineering from the University
of Engineering and Technology, Labore, Pakistan, in 1981, and
his M.S. in Computer Engineering from ISU in 1985. His inter-
ests include functional programming languages, parallel pro-
cessing, computer architecture, and local area networks.
Muhammad Shafiqq is a P.D. student in Electrical Engineer-
Ing and Computer Engineering at Iowa State University, Ames.
He received his B.E. in Electrical Engineering from N.E.D.
University of Engineering and Technology, Karachi, Pakistan, in
1982, and his M.S. in Computer Engineering from ISU in 1986.
His interests include local area networks, protocol develop-
ment, and embedded computer systems.
HE motivation for this paper is the observation that a
scene containing more than one object most of the time
cannot be segmented only by vision or in general by any
noncontact sensing method. Visual information may be suf-
ficient to accurately segment simple objects and nonoverlap-
ping scenes. However, in general, it is not sufficient for
random heaps of unknown objects.
If no a priori knowledge is available, the vision system
cannot reliably distinguish between overlaps caused by two
different objects in the scene and overlaps caused by a single
self-occluding object. A flat rigid object supported by and
totally occluding another smaller object may be recognized as
a large box-shaped object. Similarly, a flat nonrigid object
supported in the middle by a smaller object may be recog-
nized as convex, while if it is supported at the edges by more
than one object, it may be recognized as concave.
Therefore, machine vision alone (or any noncontact sens-
ing method) is not sufficient for segmentation and recogni-
tion. An exception to this may be the case when the objects
are physically separated so that the noncontact sensor can
measure this separation or one knows a priori a great deal
about the objects (their geometry, material, etc.).
The traditional approach is to segment the noncontact
sensory information (range, intensity, etc.) regardless of
scene complexity. Then, based on the outcome of segmenta-
tion, to interpret the scene and recognize the objects. The
problem with this approach is that reliability decreases when
scenes become more complex and when a prlori assumptions
are removed.
Our approach is different. Instead of trying to deal with an
ever increasing visual scene complexity, we use the manipu-
lator to make the scene simpler for the vision system. Our
paradigm is analogous to having the hand help the eye when
interpretation of visual information is ambiguous, or when
the scene is visually complex.
Our system is iterative because random arrangements of
objects form layers. Due to our noncontact sensor arrange-
ment, only the top layer of the heap is visible at any given
time and the objects are removed from the scene one at a
time. In general, the system must sense and manipulate more
than once for every random scene.
The system is interactive because the vision system may
request a manipulatory action to resolve an interpretation
ambiguity, to reduce visual complexity, or to grasp and
remove an object from the scene. The manipulatory action
must be monitored by the noncontact sensor (vision system)
as well as the contact sensors (force/torque) in a closed loop.
Our assumptions are:
The domain is the class of irregular parcels and pieces
(IPP) found in a post office environment. The class consists
of rigid and nonrigid flats, boxes, tubes, and rolls. The
objects have different weights, sizes, colors, visual surface
textures (address labels, stamps, and other markings), vary-
ing porocity, coefficients of friction, and rigidity. Because
many of these objects are not rigid, their true geometric
shape cannot be measured; it is rather a function of where the
object is in a random heap, how it is supported by its
neighboring objects and other objects that it supports. The
heaps are formed by emptying a sack of an unknown mixture
of IPP's on a conveyor.
Our immediate goal is to physically segment and sort a
random heap of IPP's into several output streams of single
similar-shape objects. In other words, our first goal is to
disassemble random heaps of separable objects (held together
by gravity and friction) into three output streams. The first
stream contains two-sided planar objects (flats). The second
stream contains three-sided nonplanar objects (tubes /rolls).
The third stream contains six-sided planar objects (boxes).
We view this physical segmentation of disassembly prob-
lem as a subclass of the more general disassembly problem,
which we will address in the future. We believe that the
solution to the general disassembly problem is active sensing
[3], as opposed to the traditional static analysis of passively
sampled data. The problem of active sensing can be stated as
a problem of intelligent control strategies applied to the data
acquisition process that will depend on the current state of the
data interpretation including recognition. This approach is
gaining more and more recognition in the literature, [2], [5],
[10], [21]. In this paper we shall describe our model of
segmentation via interaction between vision and manipula-
tion. We will generate several segmentation strategies. We
will describe the experimental system and the experiments.
The model of segmentation has the following components:
models of sensors, models of actions, a task/utility model, a
world model, and a control model. The segmentation process
is formulated in terms of graph-theoretic operations that are
mapped into corresponding manipulatory actions.
Sensor models include the characterization of the noncon-
tact sensor such as the spatial resolution, signal-to-noise
ratio, and their like; the physical parameters of the different
end effectors, such as a vacuum suction cup; the size of a
spatula for pushing objects; the span of a gripper; and the
maximum allowable forces and torques. Models of objects
are specified in terms of their geometry, size, and substance.
Our world consists of random arrangements of objects
called heaps. Object models are boxes, flats, and tubes/rolls.
A heap is represented by a directed graph. Objects in the
heap are represented by vertices, and the on-top-of relations
among objects are represented by directed edges. A scene is
a partial view of a heap as sensed by the noncontact sensor.
A scene is represented by a directed graph, where surface
segments are represented by vertices and the on-top-of rela-
tions among the surface segments are represented by directed
edges.
It is important to emphasize that, in general, the diagraph
representing the heap is different from the graph representing
a scene. This is because the scene diagraph represents spatial
relations of only the visible surface segments, i.e., as they
appear through the visual sensor, which may not always be
the same as the physical objects. The true physical arrange-
ments of objects in the heap (i.e., the heap diagraph) is not
known, unless given a priori. Only the scene diagraph is
measurable and constructable from noncontact sensory infor-
mation.
In this paper, our task is to measure and construct the
scene diagraph and to use the manipulator to decompose it. In
future work we will show how to use manipulatory and
exploratory actions to recover the true part- whole relation-
ships (i.e., to compose an object diagraph from its scene
diagraph).
Task models include the final goal of the process. An
example of a final goal may be the empty scene. Intermediate
goals may be those scenes that are simply measured by a
cost/benefit function. This cost/benefit function entails the
cost of performing the particular manipulation, and the bene-
fit is measured via the estimate of the outcome of the
manipulation with respect to the final goal, i.e., emptying the
Scene.
There are two types of actions: sensing actions (i.e., data
acquisition) and interpretation actions (such as: look, and /or
feel), and manipulatory actions, such as: pick, push, pull,
and shake. The purpose of the manipulatory actions is to
exert physical disturbances into the scene, being either global
(shake) or (push, pull). In view of our formulation of the
segmentation problem as a graph generation /decomposition
problem, we classify the manipulatory action in relation to
the operation that applies on the diagraph. There are two
such operations: the vertex removal, which means, in terms
of manipulation, removal of an object from the scene, and
edge removal, which in turn translates into object displace-
ment in the scene so that the on-top-of relationship does not
hold any more between the two objects. An isomorphism
exits between the manipulation actions and graph decomposi-
tion operations [23].
Our approach is to close the loop between sensing and
manipulation. The manipulator is used to simplify the scene
by decomposing the scene into visually simpler scenes. The
manipulator carries the contact sensors and the manipulation
tools to the region of interest and performs the necessary
manipulatory movements that will result in a visually simpler
scene. The control model deserves special attention and is
described next.
The control model is a nondeterministic finite-state Turing
machine (NDTM) and is shown in Fig. 1. The physical world
(scene) is the tape of the machine, the read actions are the
sensing actions, and the write actions are the manipulatory
actions. The model is a Turing machine because the manipu-
lation actions constantly change the physical environment
(tape) and hence its own input. The model is nondeterministic
because of the nonpredictable state of the scene after each
manipulatory step. From this of course follows also the
nondeterministic control of actions. In addition to the nonde-
terminism of the control strategies, the machine has finite
states, which are determined by the finite numbers of recog-
nizable scenes and the finite number of available actions.
This model is quite general, providing that one can quantize
the scene descriptions and the sensory outputs into unique
and mutually exclusive states, and of course one has only a
finite number of manipulatory actions.
As is well known, the nondeterministic finite-state automa-
ton (NDFSA) that controls the Turing machine is defined as a
quadruple (I, O, S,'T) where:
Fig. 1 describes the sensing and manipulation interaction
for segmentation. Relating this diagram to the NDFSA, we
shall describe in subsequent subsections the inputs, outputs,
states, and the transition function, i.e., the control, respec-
tively. There are several advantages to the formalism of the
nondeterministic finite-state Turing machine.
I) Inputs: As indicated above, the inputs come from
sensors. In our current implementation, the sensor is a laser
range imaging system (noncontact sensor). The scene is
segmented into spatially connected surface regions. For each
region, we compute the position of the center of gravity, the
orientation of the surface normal at the center of gravity, an
estimate of the size of the smallest parallelepiped bounding
the region, and an estimate of the maximum curvature. From
these measurements, the objects are initially classified into
one of three generic shapes such as: flat, box, and tube /roll.
These are four object models.
The on-top-of relation between all pairs of visible regions
in the scene is computed and the directed graph representing
this relation is constructed. Vertices represent visible, con-
nected, surface regions. Directed edges represent the spatial
relations between the vertices. See Figs. 2-5.
Top-most surface segments are important in physical scene
segmentation because they may belong to top-most objects in
the scene. Top-most objects are important because they usu-
ally have more surfaces exposed (more ways to be grasped).
The forces required to extract them from the scene are less,
and therefore the chances of loosing positional information
after the object is being grasped are minimized. Furthermore,
manipulating the top-most object keeps scene disturbances to
a mmumum.
A partially dispersed scene corresponds to a disconnected
diagraph. An efficient algorithm based on ''fusion'' of adja-
cent vertices is given in [8]. A totally dispersed scene (as
well as a singulated scene) corresponds to a null graph (a
graph with vertices and no edges). Efficient graph theoretic
algorithms exist (testing the diagraph's adjacency matrix for
all zero entires) for singulation verification. Finding the
top-most objects in the scene corresponds to topological
sorting of the diagraph.
2) Outputs: There are two types of outputs. These are
sensing actions, (look, feel) and manipulatory actions (pick,
push, pull, shake, and stop). In this implementation, the look
and feel actions are only commands to take data. In our
future work, these actions will be more complex, i.e., the
system will choose its view point, sampling rate, resolution,
and other data-acquisition parameters. In addition, the cost of
the sensing actions will be included in the overall control
schema.
The manipulation actions are composed hierarchically from
simpler actions. Shake is the simplest action; it provides
global disturbance and displacement to the work place. On
the other hand, push and pick exert local disturbance and
cause local displacement of an object. In fact in our imple-
mentation, both the push and pick actions have two forms:
push with spatula,'' ''push with suction tool,'' ''pick with
gripper,'' ''pick with suction tool.'' See Fig. 12 below for an
example of a ''pick with suction tool'' action. In addition,
each of these manipulatory actions is associated with an
''error recovery'' action.
The hierarchy of actions is in terms of composition of
complex actions from simpler actions and does not apply to
the execution of these actions. The hierarchy of action com-
position is given in [23]. An example of such hierarchy is
shown for the action: ''pick with gripper'' in Fig. 6. Each
node in the graph in Fig. 6 is a manipulatory action. Some of
these actions are modeled as deterministic finite state au-
tomata (FSA), while others are modeled as nondeterministic
finite-state automata (NDFSA). The lowest level in the hier-
archy of actions consists of very simple actions, such as:
''robot move to'' (RMT), ''robot move to while sensing''
(RMTS), ''gripper move to'' (GMT), ''gripper move to
while sensing'' (GMTS), and their like.
The advantages of hierarchical construction are modular-
ity, testability, and incremental growth. These actions (as
expected) use additional information from contact sensors.
Some of the contact sensors are as follows. Two force/torque
sensors (mounted on the gripper jaws) are used in closed loop
feedback during manipulation. Force feedback is used to
provide force servoing to the gripper, to sense collisions, to
measure the weight of objects, and to determine if an object
or tool is properly grasped. A finger position sensor is used
in a closed-loop feedback manner during manipulation. Posi-
tion feedback is used to provide basic position servoing to a
gripper and to refine size estimates of objects (computed
from vision). A vacuum sensor is used to verify proper grasp
and to differentiate small-size nonpenetrating cavities from
holes that penetrate an object. Note that all the contact
sensory feedback is carried out in a local reflexive mode
rather than in a planned mode with one exception, that is
when a pathological state is detected.
3) States: This is a finite set of states describing the
environment of the Turing machine as perceived by the
sensors. If new sensors are added, the set of states is parti-
tioned to describe the scene as perceived by the additional
sensors. For example, if a sensor capable of determining the
touch'' relations of objects in the scene is added, then the
set of five states, can be partitioned (a finer partition) to
describe both the ''touch'' and ''on-top-of'' relations. The
states of the machine are:
The goal of scene segmentation is the empty state. This
state must be not only reachable but also measurable with the
current sensors. In other words, for the machine to halt, the
system must have sensors to sense that the goal state has been
entered. In this work, the empty state is both reachable (see
section on strategies) and easily measurable (all range values
in the scene are zero, which means that no surface segments
and thereby no objects exist in the scene).
A specific place must be given to error states. They are
prioritized in order of severity (most severe first). For more
details, see [23]. The pathological states are: ''sensor dam-
aged,'' ''unable to get tool,'' ''lost tool,'' ''lost object and
tool,'' ''lost object in the scene, '' ''lost object away from
the scene,'' ''unable to reach object,'' ''unable to pick,'' and
unable to push.'' As more sensors and actions are added
into the system, more, yet finite, pathological states must be
defined. When the machine enters one of these states, error
recovery actions are evoked.
4) State Transition Function: The control problem is
transformed into the problem of topological sorting of object
arrangements. The manipulation actions of object acquisition
(pick) and local displacement (push) are defined as decompo-
sition operations on diagraphs representing the on-top-of
relation of objects in the arrangement. The pick action is used
to break the vertex connectivity of the diagraph by removing
vertices. Several tools may be used to implement this action.
An object may be picked and removed from the scene using
the gripper, or it may be picked by selecting a tool (i.e., a
suction tool). The push action is used to break the edge
connectivity of the diagraph representing the on-top-of rela-
tion. Several tools may be used to implement this action. An
object may be pushed using the gripper, or it may be pushed
by selecting a push tool (such as a spatula or the suction
tool). Complete planning of the push actions is very compli-
cated [14]-[17] and requires knowledge of the friction coef-
ficients of all objects in the scene as well as knowledge of the
spatial relations of all objects in the scene to decide where
and how far to push.
In Section II, we described a nondeterministic finite-state
Turing machine as the control model for sensing and manipu-
lation for scene segmentation. This very general model is
sufficient to describe every strategy for the following reasons:
Let us recall that the ''read from tape'' are the sensing
actions, ''write to tape'' are the manipulatory and error
recovery actions, and the states are scene descriptions. Even
with the restriction that one can categorize every scene into
distinct classes (discrete states) we had to add the following
rules:
With the above rules and the theory described in Sections
II and III, we can compose several different strategies to
examine the validity and generality of our theory for scene
segmentation.
Strategy 1 is a noninteractive loop: (look, pick, look,
.. .). The control structure is shown in Fig. 7. The strategy
does not use local displacement (push). The general idea is to
look, pick the top-most object, and look again. If the scene is
ambiguous or unstable, it shakes the heap. If shaking fails, it
continues with the pick action. This strategy is simple and
very effective in dealing with scenes where all objects are
graspable with the set of acquisition tools. The strategy
eliminates ambiguities via the shake and pick actions. If the
shake action fails to remove the ambiguity, then nontopmost
objects are picked up. This causes objects to be lost during
acquisition. For the strategy to succeed, the sensor thresholds
must be raised to enable the system to tolerate higher torques
caused by picking objects off the center of gravity. When the
threshold is raised, the probability of tool losses increases as
well as the probability of damaging the sensors. Therefore,
the probability of entering the fatal error state is increased. If
the weight of the objects is low, the probability of damaging
the sensors (even if the system picks objects supporting other
objects) is low, and the strategy converges; see [23].
Strategy 2 is a noninteractive loop: (look, push until
dispersed, pick, look, . . . ). The control structure is shown in
Fig. 8. This strategy allows no interaction between the pick
and push actions. The only interaction allowed is when the
push action cannot reduce the number of edges in the associ-
ated graph any further. The strategy enforces a rather strong
partition on the manipulation actions. This shows up as a
serial plan where a single action is triggered from one state
and the automaton iterates until the ''look'' action brings the
automaton to another state. This strategy is very effective in
dealing with heaps of few, small-sized objects relative to the
workspace. As object size and number increases, so does the
number of objects pushed out of the scene and never picked
up. For a proof of convergence, see [23].
Strategy 3 is an interactive loop: (look, pick/push,
look, . . .). The control structure is shown in Fig. 9. The
central idea is to allow immediate interaction between the two
manipulation actions. Since pick is more effective than push,
priority is given to pick. Only if an object cannot be picked
up after several unsuccessful attempts is the next immediate
action to push that object, and to immediately return to pick
the next object (if one exists) or to the look action. This
strategy is most effective in dealing with heaps containing a
small number of top-most objects located far away from each
other. These types of heaps can be decomposed with the
minimum number of collisions.
Strategy 4 is an interactive loop: (look, push partially
visible, pick, push, look, . . . ). The control structure is shown
in Fig. 10. The central idea is to interleave the interaction
between the pick and push actions. In other words, the
strategy is to look, then execute a series of push actions and
displace partially visible objects out of the scene, then to pick
all topmost objects, then push all objects that the pick action
failed to remove after several attempts, and, finally, to look
again. This strategy segments the heap from both the top and
the sides. The partially visible objects are first pushed out of
the scene. This creates free space for future push actions and
minimizes the likelihood of collisions toward the borders of
the scene. By ordering the sequence of actions, we achieved
minimum interference between the manipulation actions and
we sequenced the execution of the look action to occur when
it is needed the most (after a series of local displacements). If
all the objects targeted for pick are graspable, they are
removed one by one, following the topological ordering of
the graph. This strategy is the most effective. It keeps action
interference to a minimum. It uses the most expensive action
(look) only when it is necessary (after a shake or a series of
push actions). This grouping and sequencing of actions has
performed very well for the majority of heaps and objects.
The strategy keeps the number of tool changes to a mini-
mum.
The system block diagram is shown in Fig. 11. It consists
of a range imaging system, a linear stage, a PUMA 560
robot, a LORD Corp. servoed instrumented gripper, a micro-
VAX-IDI computer, a support structure, several tools, tool
fixtures, and accessories such as a vacuum pump and a
solenoid valve.
The range imaging system communicates with the micro-
VAX via a video link. The magnitude of the video signal at
any point on the raster is proportional to the height of the
scene at that point. This signal is quantized and stored as an
array of 8-bit numbers (rangels).
The PUMA 560 robot communicates with the micro-VAX
via an RS-232 serial link and with the outside world via a
16-line input, 16-line output I/O module. This module is
used to control auxiliary devices such as the laser power, the
vacuum pump, and the solenoid valve. In addition, asyn-
chronous interrupts are supported by the robot language
VAL-II. The binary vacuum sensor is an example of such an
asynchronous interface.
The LORD Corp. gripper and sensors communicate with
the PUMA-560 via a 16-bit parallel I/O port, and with the
micro-VAX via an RS-232 serial link. The linear stage
communicates with the micro-VAX via an RS-232 serial link.
The range imaging system used in this research is de-
scribed in detail in [23]. None of the commercially available
range imaging systems were fast enough, easy to calibrate,
and inexpensive for our experiments. The range imaging
system is based on the invariance of the cross-ratio, a well
known ratio from projective geometry. In [23] we have
shown that by using this technique rather than the ''classic''
principle of triangulation it is possible to generate range
images in real-time without the need for further processing.
The range imaging system is comprised of a laser and
mirrors, a camera, and electronics for real-time generation of
range images.
1) The Laser Subsystem: The laser subsystem is com-
prised of a 5-mW helium neon (HeNe) laser light source
emitting at 632.8 hm. An oscillating mirror spreads the laser
beam into a straight line, thereby creating a plane of light. A
positioning mirror, mounted orthogonal to the first, deflects
the plane of light to a desired position in the scene. Both
mirrors are mounted on galvanometers. See Fig. 13. These
galvanometers are under computer control. The parameters
under control are: gain of the oscillating galvanometer that
controls the length of the plane of light and the offset of the
oscillating galvanometer, the deflection angle of the position-
ing mirror was well as the offset of the positioning mirror.
2) The Camera Subsystem: The camera subsystem is
comprised of a Sony XC-39, B/W, CCD video camera with
384 horizontal by 480 vertical pixels, a 16-mm lens, and a
632.8-nm laser interference filter mounted in front of the
lens. See Fig. 14. The purpose of the interference filter is to
filter out ambient light and to allow only the laser stripe to be
seen by the camera. The camera is mounted so that the image
of the laser stripe incident onto the horizontal plane is seen
by the camera as a vertical straight line.
3)The Electronics Subsystem: The authors have designed
and built an electronic device capable of solving the cross-
ratio equation at TV field rates, therefore generating a line of
a range image in real time (15 360 range elements per
second). The details of the device will not be discussed here
because they are well beyond the scope of this paper. For
more details, see [23].
4) Range Image Resolution: In our prototype, the verti-
cal field of view of the camera is 320 mm. We have 240 lines
in every field. Therefore, the vertical size (A) of each range
element (rangel) is 320/240 = 1.34 mm. The horizontal field
of view of the camera is 345 mm. We use only 176 horizon-
tal pixels. Therefore, the (Y) or horizontal rangel size is
345/176 = 1.96 mm. The maximum height is 384 mm,
quantized into 256 levels. This yields a height resolution of
384/256 1.5 mm.
5) Range Image Accuracy: In our prototype system the
measured range error, after quantization by the DT /2651
Data Translation image digitizer, was experimentally found
to be within (s 1/2 LSB), or (3 0.75 mm).
The linear stage is a subsystem that allows a scene to be
scanned under a stationary laser plane of light. This method
has two advantages over the method where the scene remains
stationary and a laser plane of light is scanning the scene.
The advantages are:
The linear stage, shown in Fig. 15, is comprised of the
following components: a linear slide, with a 25-in length of
travel, carrying a 20 x 20 in loading plate; a ball screw; a dc
servomotor; a dc tachometer; a servodrive; an incremental
position encoder; and a computer controller. The linear stage
can be programmed to move the loading plate at user-defined
speeds and accelerations. The positioning accuracy of the
linear stage is 0.004 in. The linear slide /loading plate were
mounted on a workbench.
The linear stage must be calibrated once and requires no
recalibration. The laser ranging system requires that the
linear stage transports the scene at constant velocity during
the range data generation cycle. Therefore, the loading plate
must reach constant velocity before its enters the scanning
volume. In addition, the velocity of the linear stage must be
set to match the range imager's speed to avoid shape distor-
tion. The correct velocity was found experimentally. This
was done iteratively by scanning a circular disk, computing
the moments of the resulting range image (an ellipse), and
modifying the velocity of the linear stage until the ellipse
became a circle.
The robot sensors used in this work are two force /torque
sensors mounted on the jaws of a LORD Corp. servoed
instrumented gripper (see Figs. 17 and 18), a gripper dis-
tance sensor mounted on the same gripper, and a vacuum
sensor mounted on the suction tool. Two tactile array sensors
are provided and constitute an integral part of the LORD
gripper. However, in the current implementation, no tactile
information was used.
1) The Force / Torqque Sensors: Each of the force/torque
sensors is a six-degree-of-freedom miniature sensor designed
to fit within each finger. The sensor transduction is based on
a Maltese cross arrangement of strain gauge instrumented
beams, riding on elastomeric bearings. The sensor provides
force data at a resolution at 0.01 Ib, and torque data at a
resolution of 0.01 in-lb, over a 0.0-20.0 lb range.
In this work, force /torque information is used to control
the clamping force of objects and tools, to monitor the weight
of an object, to avoid collisions with objects or tools, for
off-axis gripping of objects and tools, as well as for slip
detection by monitoring shear forces in the plane of the
gripping surface.
2) The Gripper Position Sensors: The LORD gripper is
equipped with two position sensors. Sony Magnascale posi-
tion encoders are used. These linear encoders have a set of
pickup coils that ride on a rod containing minute magnetic
domains along its length. This transduction mechanism is
noise immune, and it is impervious to dirt and oil, The
position sensors are attached to the base of each finger and
provide a resolution of 1 ym.
Position feedback is used to provide basic servoing infor-
mation, as well as to gauge something that the gripper is
holding. The minimum and maximum gripper openings are
16.0 and 50.0 mm, respectively. Therefore, objects larger
than the maximum gripper opening or smaller than the
minimum gripper opening must be handled by some other
tool.
3) The Vacuum Sensor: The first vacuum sensor used in
this research was a low-cost conductive-elastomer vacuum
sensor. A conductive membrane separated two air-tight
chambers; a pressure chamber and a vacuum chamber. The
conductive membrane was connected to an electrode. The
pressure chamber was left at atmospheric pressure, while the
vacuum chamber was connected to the vacuum hose. When
the vacuum exceeded a threshold of approximately 3 lbf/in,
the conductive membrane came into electrical contact with an
electrode inside the vacuum chamber and indicated the pre-
sense of vacuum. This sensor was found to be very sensitive
to environmental parameters such as temperature and humid-
ity. In addition, it suffered from hysteresis.
The second vacuum sensor used in our experiments is a
simple vacuum gauge, modified by the authors to operate as a
variable-threshold binary vacuum sensor. The threshold ad-
justability proved to be a useful feature in compensating for
small vacuum readings (Bernoulli effects) when the suction
tool was not attached to an object. In addition, it is possible
for a partial vacuum to be created when the suction tool
attempts to lift a porous object or an object partially within
the suction tool.
The main tool is a gripper (Fig. 17) attached to the PUMA
560 (Fig. 16). It is used to grasp objects that can fit within
the gripper such as tubes and small boxes. In addition, the
gripper is used to grasp additional tools such as a suction tool
and a pushing tool (spatula). The gripper is designed for use
on a PUMA 560, which has a payload capacity of 5 Ib. The
gripper's weight is 3 lb. Therefore, the maximum tool or
tool-plus-object weight must be kept under 2 Ib. The gripper
has two independently controllable fingers to allow for off-axis
gripping. The maximum finger velocity is 6.6 in/s. The
gripper can generate a 20-lb maximum clamping force.
1) Suction Tool: The suction tool is used for lifting
objects out of the scene having large enough, approximately
planar, surface regions for the tool to be attached. The
suction tool is a 2.5-in-diameter collapsible vacuum cup
mounted on a hollow metal block. See Fig. 19. Attached to
the metal block is a vacuum hose. The vacuum hose is
connected to a vacuum sensor and the common port of a
two-way solenoid valve. One port of the solenoid valve is
connected to the vacuum port of a vacuum pump, while the
other port of the solenoid valve is connected to the pressure
port of the vacuum pump.
The solenoid valve is controlled by the robot controller.
When activated, it applies vacuum to the vacuum cup. If a
large enough nonporous object is attached to the vacuum cup,
the pump creates vacuum high enough to lift the objects used
in our experiments. When the solenoid valve is deactivated, it
applies pressure to quickly neutralize the vacuum and release
the object attached to the vacuum cup.
The suction tool weighs approximately 1 lb. Although the
tool is capable of lifting more than 5 lb, payload considera-
tions on the PUMA 560 restrict the maximum weight of
objects to be lifted using the suction tool to under 1 lb.
2) Push Action Tool: The second tool used in the experi-
ments is a spatula. This tool is used for pushing objects in the
scene. See Fig. 20. Both the suction tool and the spatula were
placed on specially designed tool holders, which guarantee
that the tools, when released, will always be placed at a
known robot location. See Fig. 21. The spatula weighs
approximately 4 oz.
The laser, mirrors, and camera were mounted on an
overhead structure and are elevated approximately 48 in
above the workbench. See Fig. 22. The structure allows for
adjustments in overall height above the workbench. In addi-
tion, the camera-to-laser distance, the laser's angle of inci-
dence, and the camera's angle are all user adjustable. Fi-
nally, the height of the camera above the workbench is
adjustable. The structure was made of aluminum tubes and
was stiffened to minimized vibrations during range image
generation.
The domain was mostly objects found in the mail stream
such as parcels, flats, tubes and rolls. A number of additional
experiments were conducted with objects containing holes,
cavities, and some porous objects. The weight of every
object was under 1 lb. Typical scenes contained 10 to 30
objects of different shapes and sizes thrown at random. The
objects formed heaps two to six layers deep. The average
time to segment and extract an object from the scene was
4,12 s. We performed each experiment approximately 100
times. The results of our experiments and observations about
the performance of each strategy are described in the follow-
ing subsections.
The purpose of this group of experiments was to evaluate
strategy 1. The strategy performed well on unstable, over-
lapped, and dispersed heaps. Difficulties were observed with
ambiguous configurations. The shake action was not very
effective in removing ambiguities. One reason is that the
action was implemented using the linear stage in a vibration
mode at maximum speed and acceleration. These speeds and
accelerations were not enough to produce a significant change
in the scene. Using the pick action to remove ambiguities
resulted in an increased number of tool and object losses. The
shake action failed to eliminate the ambiguities caused by
configurations of flats. This is because flats form stable
configurations. However, because flats are rather lightweight
and flexible, it was possible to use the pick action to break up
cyclic object configurations without many tool or object
losses.
In 100 experimental trials, strategy 1 failed to converge a
total of 20 times. Six of these failures were caused by
insufficient depth resolution in the range imaging system.
These six failures occurred when very thin flat objects formed
cyclic configurations. Nine failures occurred when the heap
contained large porous objects. The system entered into a
endless loop because in this strategy we use only the pick
action. Three failures occurred when the suction tool slipped
off the gripper jaws. Finally, two failures occurred when the
force /torque sensors failed to respond.
In 100 experimental trials, the vision system identified an
average of 2.9 top-most surface segments per scene. The
theoretical average number of transitions through the state
diagram is (1 + 1/2.9) = 1.345. However, errors in com-
puting the true top surface centroid (caused by very dark
regions in the scene) resulted in frequent triggering of error
recovery actions. The average number of transitions per
object was found to be 1.67.
The purpose of this group of experiments was to evaluate
strategy 2. For the strategy to work efficiently, a very large
work space is required. This is because all overlaps must be
removed and the scene must become dispersed before any
pick action. In many unstable scenes containing a mix of
flats, boxes, and tubes, the shake action forced the tubes to
fall into the cavities of the heap. This stabilized the heap and
created more overlaps. The shake action was very effective in
dispersing heaps of cylindrical objects. In scenes containing
only tubes/rolls, the shake action removed all overlaps.
Strategy 2 performed well in heaps of few, small-sized
objects. As the object size and number increased, so did the
number of objects that were pushed out of the scene and
never picked up.
In 75 experimental trials, strategy 2 failed to converge 14
times. Six of the failures were caused by jams induced by the
push action and high friction coefficients between boxes and
the linear stage. Two failures occurred when two flat objects
were entangled and could not slide off the conveying mecha-
nism. Four failures occurred when the suction tool and the
push tool slipped of the gripper jaws. Finally, two failures
occurred when the force /torque sensors failed to respond.
Strategy 2 did not fail to converge when the heap contained
porous objects. These objects were eventually pushed out of
the work space.
If 75 experimental trials, the vision system identified an
average of 1.9 top-most surface segments per scene. The
theoretical average number of transitions through the state
diagram is (1 + 1 4 1/1.9) = 2.53. However, errors in
computing the true top surface centroid (caused by very dark
regions in the scene) resulted in frequent triggering of error
recovery actions. In 17 trials we observed that the push
actions generated more complex scenes. The average number
of transitions per object was experimentally found to be 4.24.
The purpose of this group of experiments was to evaluate
strategy 3. This strategy has the tendency to produce addi-
tional overlaps when an object being pushed falls on other
objects in the next layer of the heap. This was not catas-
trophic because this overlap was detected during the next
look action. We have observed that if the objects cannot be
picked up by any tool, then the strategy degrades into a
sequence of unsuccessful pick actions followed by push ac-
tions. The system picked all objects with cavities and pushed
all large objects containing holes, as well as all porous
objects. The major drawback of this strategy is that if push
actions are interleaved with pick actions, then the push action
may displace other objects targeted for the pick action.
Therefore, the next pick action will most likely fail. Al-
though this is not catastrophic, it makes it necessary to
trigger another look action. Better planning of the push
actions may help to eliminate some of these problems. How-
ever, one has to keep in mind that even better planning will
not solve the problem, unless assumptions about the heap and
knowledge of the surface properties of all objects composing
the heap is introduced.
In 100 experimental trials, strategy 3 failed to converge 12
times. Eight of the failures were caused by jams induced by
the push action and high friction coefficients between objects
and the conveyor. Two failures occurred when the suction
tool and the push tool slipped off the gripper jaws. Finally,
two failures occurred when the force /torque sensors failed to
respond.
In 100 experimental trials, the vision system identified an
average of 2.5 top-most surface segments per scene. The
theoretical average number of transitions through the state
diagram, assuming that we pushed each object to the top
surface centroid (caused by very dark regions in the scene),
resulted in more frequent triggering of pushing and error
recovery actions. In 27 trials we observed that the push
actions generated more complex scenes. The average number
of transitions per object was experimentally found to be 3.1.
This strategy is the most effective and capable of handling
a variety of objects and heaps. One reason for its success is
the way it manipulates the heap: from the top and sides.
Another reason is that the actions are ordered and vision is
applied when needed the most, after global displacement
(shake) or a series of local displacements (a series of push
actions).
In 115 experimental trials, strategy 3 failed to converge six
times. Two of he failures were caused by jams induced by
the push action and high friction coefficients between objects
and the conveyor. One failure occurred when the suction tool
slipped of the gripper jaws. Finally, three failures occurred
when the force /torque sensors failed to respond.
In 115 experimental trials, the vision system identified an
average of 2.4 top most surface segments per scene. The
theoretical average number of transitions through the state
diagram (assuming that we push each object 50% of the time)
is (1 + 0.5 + 1/2.4) = 1.917. However, errors in comput-
ing the true top surface centroid (caused by very dark regions
in the scene) resulted in more frequent triggering of error
recovery actions. Only in three trials did we observe that the
push actions generated more complex scenes. The average
number of transitions per object was experimentally found to
be 2.3.
We introduced the paradigm of iterative interactive scene
segmentation and simplification via vision, manipulation,
force/torque, and other sensory input. The scene simplifica-
tion is based on graph decomposition operation of vertex and
edge removal. These operations are in turn defined isomor-
phic to the pick and push manipulation actions. We have
shown that the sensors can be used as the partial graph
generators and the manipulator as the decomposing mecha-
nism of this partial graph. The actions and strategies are
modeled as nondeterministic finite-state Turing machines that
decompose these graphs under sensor supervision. The
strategies converge (for theoretical proof, see [23]). If patho-
logical states are detected, then error recovery actions are
invoked.
We have integrated a vision system, a manipulator, and
force/torque and other sensory input into an experimental
robot work cell and conducted experiments to test conver-
gence, error recovery, and graceful degradation of four
different strategies. We have found that many of these strate-
gies can recover from pathological states, tolerate errors in
the sensory data, recover from unsuccessful actions, and
converge. What we have learned during this work is:
CELLULAR pyramid is an exponentially tapering stack of
arrays of processors (''cells''). Communication between cells
on successive levels of the stack allows global analysis of data
input to the base of the stack in log(base size) steps. Cellular
pyramids support fast parallel algorithms for multiresolution im-
age analysis. See the books edited by Rosenfeld [31], Cantoni and
Levialdi [8], and Uhr [37]; for solving computational geometry
problems [25], with the image input to level 0, the base of the
pyramid.
Usually the cells on each level are connected to form a square
lattice but triangular or hexagonal grids have also been used
[2], [6], [14]. A cell on level ! + 1 (the parent) is connected
to a K x K neighborhood of cells on level l (its children).
Neighborhoods associated with adjacent parents overlap by K-2
cells along both directions, yielding a fourfold reduction in
number of processors (twofold along each side of the square);
however, twofold reductions can also be achieved using modified
architectures [11], [18].
We restrict ourselves here to pyramids defined on a square grid
with fourfold reduction between successive levels. If an image
is input to the base of the pyramid, we can generate reduced-
resolution versions of the image at higher levels. Usually the
value of the parent is a weighted average of the values of its
children and the same set of weights is employed at every level.
Burt [7] defined rules for which the set of weights converges to
sampled Gaussians with increasing standard deviations. Optimal
weights have also been proposed [24].
Let the base of the pyramid be of size N' ee 2' s 2*. Then the
Ith level has size 2'*' y 2'', so that the total number of cells
is less than 3N'. The height of the pyramid, iie., the number of
levels, is n = log N. Many image analysis tasks which require
O(N%) operations on a single processor can be accomplished in
O(log N) on a cellular pyramid.
When a pyramid is used to reduce the resolution of an image,
features of the input image become smaller and move closer
together as one proceeds from the bottom level of the pyramid
to its apex. Thus at the appropriate level, local operations are
sufficient to detect and analyze global features (see [31] for
numerous examples). Reduced resolution representations are also
useful in image compression applications (e.g., [1]).
The case of K = 2, i.e., nonoverlapping 2 x 2 neighborhoods,
is related to the quadtree description of an image [34]. The
reduced resolution representations can be severely distorted when
the input is shifted [36]. This problem is known in the quadtree
literature as the shift-dependence of the description [20]. In the
worst case a one pixel shift of the input image can lead to a
significantly modified quadtree structure [35].
The dependence of the low resolution representations on
the position of the sampling grid and the input image is also
important in image pyramid applications. The shift-dependence
phenomenon is not restricted to the case of nonoverlapping
neighborhoods. Bister [5] shows many examples of such artifacts.
The rigidity of the pyramid structure may give rise to ar-
tifacts when pyramids are used for tasks such as analysis of
line-drawings [19], object-background discrimination [10], or
compact object extraction [13], [16]. To compensate for these
artifacts, in many of these algorithms the parent-child links (or
link weights) are iteratively changed after the initial resolution
reduction stage. Recently Baronti et al. [4] proposed a modifica-
tion of this concept by increasing the size of the neighborhoods
associated with parents once an initial segmentation of the image
is obtained.
Another approach to compensating for the artifacts of pyramid
structure is to adapt this structure to the content of the input
image. In custom-made pyramids [28] weights are defined based
on a local ''busyness'' measure during the construction of the
reduced resolution representations. Rom and Peleg [29] and
Chassery and Montanvert [9] employed the Voronoi tessellation
defined by a set of randomly chosen lattice points to build the
coarsest representation of the image, which was then adaptively
refined. Note that the method computes the representations top-
to-bottom.
In this paper we also use irregular tessellations to generate
an adaptive multiresolution representation of the input image.
In our approach, however, the hierarchy of representations is
built bottom-up and is adapted to the content of the input image;
thus most of the properties of ''classical'' image pyramids are
preserved. We employ a local stochastic process to build the
lower resolution representations.
In Section II we introduce the graph formulation of irregular
tessellations and the concept of a stochastic image pyramid.
In Sections III and IV we give two applications of stochastic
pyramids: connected component analysis of labeled images and
segmentation of gray-scale images. Further issues are discussed
in Section V.
In image pyramids based on regular sampling, e.g-, at points
on a square grid, artifacts caused by the rigidity of the sampling
structure are always present. On the other hand, an image
pyramid defined by an irregular sampling hierarchy can be
molded to the structure of the input image. Note, however, that in
such a pyramid the metrical relations among cells are no longer
carried implicitly by the sampling structure. A cell at level ! + 1
cannot know a priori where its neighbors on level I + 1 or its
children on level l are located relative to the original sampling
grid. To describe the structure of such an image pyramid it is
more appropriate to use the formalism of graphs.
The cells on level I of the pyramid are taken as the vertices of
an undirected graph G[l]. The edges of the graph describe the ad-
jacency relations between cells at level l. Thus G[l] = (V[l],E[l])
where V[l] is the set of vertices and E[l] is the set of edges. The
graph G[0] defined by the 8-connected square sampling grid on
level 0 is shown in Fig. 1(a). An example of a graph G[1] that
might represent level 1 is shown in Fig. 1(c).
We construct the pyramid by a sampling or decimation process.
Each level is constructed from the level below it by selecting a
subset of the vertices. Thus a vertex on any level can be regarded
as a vertex of G[0], the sampling grid of the original image. In
addition, when we decimate level l to construct level 1 + 1, we
associate each nonsurviving vertex with one of the surviving
vertices. Thus each vertex on level + 1 is associated with a
set of vertices on level l (itself and the nonsurviving vertices
associated with it). Each of these vertices is in turn associated
with a set of vertices on level I - 1, and so on; thus a vertex on
any level is associated with a set of vertices, called its ''region,''
in the original image. These regions define a tessellation of the
image.
If the pyramid is to be build recursively bottom-up we must
define a procedure for deriving G[l + 1] from G[l]. Since the
number of vertices in G[ + 1] must be less than in G[l] we are
dealing with a graph contraction problem. We must design rules
for:
In order to have any vertex (i.e., cell) in the hierarchy
correspond to a connected region of the image, the cell cfl +
1] V[! -4 1] must represent a connected subset of cells
ics[l],ci[l],-,c,[]} c V[l]. We shall use the convention
cg[l] s: c[l + 1], ie., the surviving vertex of the subset is first
on the list. In pyramid terminology, [cs[l], c;[l],- ,c,[l]} are
the children of c[l + 1]. Note that the location of the parent on
the sampling grid of the original image always coincides with
the location of one of its children.
In pyramid construction based on Voronoi tessellations [9],
[29] the parents are initially chosen by a random process. The
edges are given by the Delaunay diagram of the tessellation and
the children are grid sites inside the tiles associated with the
parents, The process can then be repeated for individual tiles
(by randomly choosing grid sites inside each tile) to obtain a
finer description. Note that such a pyramid is built top-down and
the definitions of parents and parent-children links are based on
nonlocal processes.
When we use graph contraction to construct a pyramid, two
constraints must be satisfied if we want to employ only parallel
local processes:
where c, d are survivors on level l. Constraint (1) assures that
any nonsurvivor on cell at level l has at least one survivor in its
neighborhood and thus can be allocated to a parent by a local
decision. In the example shown in Fig. 1(b) this constraint is
satisfied. Constraint (2) assures that two adjacent cells on level
I cannot both survive and thus the number of vertices must
decrease rapidly from level to level. In Fig. 1(b) this constraint
is not satisfied since the survivors d;, es and ca,9 are adjacent.
The construction of G[l + 1] can also be regarded as finding
a maximal collection of vertices of G[l] no two of which are
adjacent. This is the maximal independent set problem for graphs
(eg., [21]); we will return to it in Section V.
A possible alternative method of constructing G[I + 1] from
G[l] is to partition G[l] into connected subgraphs and then
select one cell in each subgraph as a survivor. However, if
we do so, the first constraint no longer assures locality of the
processing. In Fig. 1(b) cell b, has survivor cs adjacent to it,
but must be allocated to survivor b;; two sites away. Choosing
the survivor independently for each region may also violate the
second constraint since two adjacent regions can both have their
survivors at the border [Fig. 1(b)]. Thus the set of children should
be defined in G[l] only after the vertices of G[! + 1] (their
parents) have been chosen.
The last step in constructing G[! + 1] is to define the edges
E[1 + 1]. Let the connected subsets [cs[l],ci[],- ,c,[]) C
V[l] and d4[4],di[l],- ,d,,[]} c V[l] be the children of two
different parents. Our condition for an edge between vertices
c[! + 1] c,[] and d[! + 1] s d4[] in G[l + 1] is
In other words, two vertices are joined by an edge in G[I + 1]
if there exists a path between them in G[l] of length at most
three edges. (Note that by (2), the path cannot be of length 1.)
G[l + 1] is now completely defined. Fig. 1(c) shows, the graph
corresponding to the partition in Fig. 1(b).
The irregular sampling hierarchy is thus built recursively from
G[0] (the original sampling grid). The apex of the hierarchy G[m]
has only one vertex. Constraint (2) assures that the apex is always
reached.
In the next section we describe a probabilistic parallel algo-
rithm that constructs graph contractions satisfying (1) and (2).
The algorithm is analyzed in more detail in [22], [23].
We have seen that the derivation of G[ + 1] from G[l] must
start by defining the vertices of the new graph. Since V [! + 1] C
V[l] we are dealing with a decimation process, ie., only a subset
of the vertices V[l] are retained. We want the decimation to be
performed in parallel on G[l].
We will define a decimation process that is dependent on the
image data. We assume that every cell c; (a vertex of G[l]) carries
a value g, characterizing its region of the image-for example,
the average gray level of the region. Without loss of generality
we can assume that g; is a scalar value; the treatment of feature
vectors is identical. From now on the explicit indication of the
level I will be dropped to simplify the notation.
Let cell cg on level l have r neighbors on level l, ie., let its
degree as a vertex of G[l] be r. (Note that for the moment c, is not
necessarily a survivor and the set of its neighbors has no relation
with the connected subset allocated to a parent.) We examine
every neighbor c;, i = 1,-,r of cg and decide whether or not
it belongs to the same ''class'' as cg. This decision can depend in
any desired way on the values g,,4 = 0,---,r, We associate a
binary number A,, i = 0,---,r with each neighbor, where A, = 1
if c; belongs to the same class as ca, and A; = 0 otherwise; note
that A4 = 1.
The decimation algorithm employs three variables for every
cell: two binary state variables p and , and a random variable
uniformly distributed between [0, 1] with outcomes . The sur-
vivors are chosen by an iterative local process. Let k 0,1,--
be the iteration index. Initially all p,(0) = 0. A cell survives if at
the end of the algorithm its p;(k) state variable has the value 1.
Every iteration has two steps. First ;g(k) is updated based on
the states p,(k - 1) of neighboring cells in the same class:
In other words, q4(k) becomes 1 if and only if there is no survivor
among the cells belonging to the same class in the neighborhood
of cg, Note that the neighborhood includes the cell itself. The
initial conditions always yield 4(1) = 1. Then ps(k) is computed
on the updated values of ;;(k)
To become a survivor the outcome of the random variable
x drawn by the cell must be the local maximum among the
outcomes drawn by the neighbors in the same class. Note
that only those neighbors are taken into account which do not
already have a survivor adjacent to them (q;(k) = 1). This
condition extends the region of influence of a cell beyond its
immediate neighborhood and yields faster convergence of the
algorithm. The local maximum property assures that (2) is always
satisfied. The state of a survivor is not reversible. Once a cell
is labeled p(k - 1) = 1, at subsequent iterations the product
g4(k)g(k) (5) is always 0 by the definition of g,(k) (4). Thus
in (5) the second condition, preserving the current state is used.
It can be shown [22], [23] that after a finite number of iterations
(at most five, in the experiments reported there) the algorithm
reaches a final global configuration in which the survivors satisfy
(1) as well.
The algorithm is entirely local, every cell computing its states
based only on the states of its immediate neighbors. Except at the
highest levels of the hierarchy, where due to the small number
of vertices artifacts may occur, the decimation ratio between two
consecutive levels exceeds four. This lower bound results from
the fact that two adjacent cells never survive. On the random
graph structure of higher pyramid levels the average degree of
a vertex is around 6 [23]. To satisfy the nonadjacency condition
(2) the number of vertices must be reduced by about the same
order relative to the previous level.
By employing the algorithm an irregular sampling hierarchy
can be built in parallel in log(class size) steps. (The distinction
between class size and image size becomes clear in the next
section.) The stochastic decimation is performed independently
within classes. In the next two sections we describe two appli-
cations of the process, one to connected component analysis of
labeled images, the other to segmentation of gray level images.
In a labeled image the pixels are classified into a small
number of classes distinguished by different labels. A connected
component is a maximal set of connected pixels sharing the same
label. For simplicity we will restrict ourselves to the case where
there are only two labels, i.e., to the case of a binary image,
but images with multiple labels can be handled in essentially the
same fashion.
Sequential algorithms for analyzing the connected components
in a binary image usually employ a row-by-row scan [30]. An
alternative approach makes use of the quadtree representation of
the image [34]. In this section we apply the techniques described
in Section II to obtain in log(class size) steps a description of the
connected components in a binary image. The description takes
the form of a graph whose vertices represent the components
and whose edges represent the adjacency relations among the
components.
The fact that the pixels are labeled makes classification of the
neighbors of a cell immediate. Let the label of cell c; be g-
In the binary case the label can have only two values. Thus in
the neighborhood of cell c; we have for i = 0,--,r the class
membership variables
Note that (6) is symmetrical; cs gets the same value of A in the
neighborhood of c; as c; gets in c's neighborhood. Since the
definition of A, is symmetrical it can be regarded as the weight
of the edge (ca, c;). The case A = 0 is equivalent to removing
the edge from E[l]. Let E'[l] be the set of edges having ? = 1,
and let G'[l] = (V[l], E'[1}). The connected components in the
labeled image are represented by connected components in the
graph G'[l], for all 2 0.
The subgraphs of G' are processed independently, each sub-
graph being recursively contracted into one vertex, the root of the
connected component. The contraction process is based on the
technique described in the previous section: first the survivor
vertices are designated and then the nonsurvivor vertices are
locally allocated to survivors. If a nonsurvivor has more than
one survivor neighbor it chooses the one carrying the largest
outcome of the random variable x from the last iteration of
the decimation process. Because the neighbors are neighbors
in G', the survivors can only have children belonging to their
own class. Thus from each connected component of the input
image a pyramidal hierarchy of irregular tessellations is built in
O(log(component size)) steps.
The different hierarchies may have different heights, but in
log[max(component size)] steps the entire image is reduced to
roots, This situation is detected at the level m when E'[m]
becomes empty. Evidently component size can differ from im-
Ag 5ize. For example, a connected linear pattern passing through
every second row of the image has length N(N + 1)/2 pixels.
Since the hierarchy is built over the pattern the number of levels
depends on its intrinsic diameter.
At each level, the graph G[l] includes edges between cells
that arise from different labels; it preserves the spatial relations
among the connected components. At the root level, G[m] is
the adjacency graph of the original labeled image; it has one
vertex for each connected component and its edges represent the
adjacencies between these components.
Fig. 2.(a) shows an example of a graph G[l] superposed
on the binary image from which it was derived. The induced
graph G'[l] is shown in Fig. 2(b). Note that in G'[l] each
connected component corresponds to a connected subgraph. The
cells surviving level I and the allocation of the nonsurvivors are
shown in Fig. 2(c). The graph G[! + 1] of the next level is shown
in Fig. 2(d) and the corresponding graph G'[! + 1] in Fig. 2(e).
Level 1 + 2 is the root level and its graph G[m] is shown in
Fig. 2(f). It correctly represents the adjacency relations among
the three connected components of the image: the background
and the two blobs.
In Fig. 3 a checkerboard image and the adjacency graph of its
root level are shown. The checkerboard is a ''worst-case'' image,
the two connected components (both defined by the relation of
eight-connectedness) being distributed across the entire input.
Cibulskis and Dyer [10] employed a regular pyramid structure
to segment this image. In their results the ''white'' component
was allocated to one root at the apex, but the representation of
the black squares had to be spread over several levels. The size
of the image is 64 x 64 and the two roots were obtained at
the eight level of the hierarchy. Recall that the height of the
hierarchy depends on component size. Since random processes
are involved in the construction of the irregular tessellations the
location of the roots depends on the outcomes of local processes.
Nevertheless, the same root level adjacency graph is always
obtained at the top of the hierarchy.
The famous connectedness puzzle of Minsky and Pa-
pert [27, Fig. 5.1] can be solved by our technique in
O(log(component size)) steps. The pattern in Fig. 4a) contains
three black and two white bands, while in the pattern in Fig. 4(b)
the two white bands are connected, leaving only two black bands.
The adjacency graphs obtained at the root level clearly show the
different topologies.
The irregular tessellations that arise in the hierarchies defined
by the connected components do not convey meaningful rep-
resentations at intermediate levels. Let us define the receptive
field of a cell on level I as the set of all the pixels at level 0
(input) associated with it, This field is always a connected set
and the image is the disjoint union of the fields. Also, each field
is a subset of a connected component of the image. In Fig. 5 the
receptive fields of four levels of a hierarchy derived from a simple
image are shown. The different fields are randomly colored to
emphasize their shapes. At intermediate levels the shapes of the
fields are arbitrary, since they depend on the outcomes of random
variables. At the root level each field is a complete connected
component.
Our multiresolution representation consists of several inde-
pendent hierarchies, each built independently over a connected
component. The shape, size, position, and orientation of the
connected component have no influence on the final result.
The individual hierarchies can be used for the fast recovery of
geometrical properties such as area or perimeter [32]. It should
be mentioned that Miller and Stout [26] also proposed a data
structure in which a separate ''essential'' regular pyramid is built
over every object.
All the discussion in this section was restricted to binary
images. If more than two labels are used the discussion is
essentially identical. In particular, our method can be used to
label the connected components of constant gray level in an
arbitrary digital image. In the next section we study the less well-
defined problem of segmenting a gray level image into ''natural''
regions.
In gray level images the difference between the values of
two adjacent pixels is bounded below only by the size of the
quantization step. In our technique, to build the hierarchies
the pixels in a neighborhood must be assigned to classes. The
class membership induces the graph on which the stochastic
decimation takes place. For labeled images the classes correspond
to the labels and the hierarchy always converges to the same final
representation: the adjacency graph of the connected components
defined by the labels. For gray level images it is no longer
obvious how to define the classes (unless our goal is to segment
the image into connected components of constant gray level). In
the first part of this section we discuss this problem.
The simplest approach is to define class membership by
thresholding the gray level differences between the center cell
ca and its neighbors c,, i = 1,-,r, The class membership
variables A; are thus defined by
As in the labeled case, (7) based on an absolute threshold T is
symmetrical. This symmetry, however, can create artifacts when
we attempt to segment gray level images, as we show in the next
example.
Fig. 6 shows an object having four gray levels on a white
background. The graph G[l,] of an intermediate level is shown
superposed on the image in Fig. 6(a). Let the differences between
the gray levels be less than T and let the two lighter gray
levels be within T of the background. The resulting graph G'[l,]
is shown in Fig. 6(b). Note the edges connecting regions that
have different colors. The stochastic decimation algorithm selects
survivor cells and the nonsurvivors are allocated to their most
similar surviving neighbors. The survivors (parents) compute new
gray level values based on their children. After a few more levels
of the hierarchy we might arrive at the graph G'[lg], 7 4,
shown in Fig. 6(c). The difference between the gray levels of
the two cells located in colored regions now exceeds T and in
G'[l] these regions are no longer connected. If a different set
of outcomes of the random variables had been employed in the
stochastic decimation process, a different set of surviving vertices
might be obtained, and the new parents might have different gray
level values, yielding a new graph at level l4 [Fig. 6(d)]. We
conclude that using a symmetric class membership criterion for
gray level image segmentation strongly influences the structure of
the hierarchy and therefore the final representation of the image.
Our next example, a ramp image, shows the severity of
the resulting artifacts. In Fig. 7 (top-left) the image of a ramp
going from level 0 (black) to level 255 (white) is shown. The
difference between adjacent rows of pixels is either four or
five gray levels, depending on the quantization error. The pixel
values are the same along each row. The receptive fields of the
root level obtained for T 33 are shown in Fig. 7 (top-right).
The color of a region is the gray level value computed by its
root, i.e., its average gray level in the original image. These
receptive fields define a possible segmentation of the image,
but the segmentation is not esthetically satisfactory; the different
local configurations generated by the stochastic decimation yield
rgged boundaries between the region. When at each level the
children can be reallocated to adjacent parents having similar
gray levels, the shapes of the segmented regions improve (Fig. 7,
bottom-left). The procedure is essentially the same as the linking
processes employed in traditional pyramid structures (e.g., [13],
[16]). The receptive fields, however, may now be fragmented,
and connectivity may not preserved, because of inconsistent
exchanges of children.
To achieve satisfactory results, a nonsymmetric class member-
ship criterion must be used. We now describe such a criterion.
Let cell c4 have r neighbors. In this neighborhood we will define
a local threshold S[c] such that 0 s S[c,] 5 T. Methods
for computing S[c,] will be discussed later in this section. The
definition of the class membership variables A, then becomes
for i = 0,1,--,r. If the number of neighbors having A, = 1
is s and the number of neighbors for which |g, - 9o] 5 T is t,
then 0 5 s 5 t5 r. The threshold S[ca] used in (8) is specific
to the neighborhood of cell c;, and therefore the criterion is not
symmetric: in general,
does not imply
since the two thresholds are computed based on neighborhoods
that only partially overlap. As a consequence of the nonsymmetry
the graphs G'[l] become directed. An arc from c; to c;, meaning
that in the neighborhood of c; the two cells belong to the same
class, may not be reciprocated by an arc from c; to c;.
The receptive fields of the ramp image's root level for a hier-
archy built with nonsymmetric class memberships (as described
below) are shown in Fig. 7 (bottom-right). The same absolute
threshold T 33 was used. The boundaries between regions
are now correctly delineated along the rows of the image. The
neighborhood-dependent local threshold S[cg] ensures that every
cell connects first to its neighbors that have the most similar gray
level. Thus the individual rows in the image are reduced to single
cells before two cells belonging to adjacent rows can become
neighbors in the graph G'[l]. The effect of the random processes
used in building the hierarchy is reflected by the different widths
of the regions. However, this artifact can also be eliminated as
we shall see at the end of the section.
We now discuss how the value of the local threshold S[c4] is
computed. Note that the extreme case S[c4] = 0 corresponds to
extracting connected components of constant gray level. Several
approaches are available to determine the S[c;] value that best
dichotomizes the neighborhood into two classes. A wide class
of thresholding methods (e.g., [33]) can be considered. Another
approach is to use the neighbor dichotomization techniques
employed in nonlinear image smoothing techniques (e.g., [15]).
Since the neighborhoods are defined on the graphs, the spatial
relations among the neighbors will not be used; we will employ
only gray level information in computing S[cg].
Let 8;;;, i = 0, 1,-,r be the ordered sequence of absolute
gray level differences b, = g, - gs], Thus
The simplest way of defining S[cs] is the most similar neighbor
method. In this case S[cg] = j; and s = max arg(, = &4;).
This is the method that we used for the ramp image (Fig. 7,
bottom-right). In gray level images of real scenes with less
regular structure this method could yield ''tall'' hierarchies,
because the neighborhood sizes in the G'[l] graphs are small,
and therefore the decimation ratio between consecutive levels is
too low. Generalizations of this method such as k most similar
neighbors [12] S[c,] = 2u;, or fued fraction of good neighbors,
S[c] o.@;;;, have the disadvantage that the constants k or o
must be defined arbitrarily.
We have obtained good results using the maximum averaged
contrast method in which S[c4] is set by detecting the most
significant step in the sequence of b;;;. For all the t neighbors
within the absolute threshold gray level difference we compute
Let u = min arg(max,(V, - U,)) and s = maxarg(&, = 4.).
The threshold S[c;] = 8,; is the first occurrence of the maximum
averaged contrast. For example, the sequence 8;;; = 84; = 2g; =
0, &i G; & = 1 yields S[c,] = 4 = 0. On the other hand,
the sequence &;; = 0,@; = 1. @4 = 2, i; = 3,&G; = 5, & = 5
yields S[c, = &i = 3.
The gray level value of a parent gs[l + 1] is computed as the
weighted average of its children's gray levels g,[l]:
where to bias the segmentation toward larger regions, the recep-
tive field area A,[l] (the number of pixels in the region) is used
as the weight; z is the number of children.
To see how this method works on a real image, in Fig. S (top-
left) a 64 x 64 aerial image magnified to 128 x 128 is shown.
The receptive fields for the root level and the locations of the
roots are shown at top-right. Since the gray levels of the roots
are used to color the regions, adjacent fields may appear fused. A
random coloring is shown in Fig. 8 (bottom-left). The root level's
adjacency graph, describing the adjacencies among the regions,
is shown at bottom-right. In this example the root level is at
m = 8. We have found that hierarchies built for this image with
different random variable outcomes can have m as high as 10,
significantly larger than n = 6, the number of levels in a regular
pyramid structure for a 64 x 64 image. The use of stochastic
decimation adapted to the local structure of the image slows
down the convergence of the process. As mentioned before, the
most similar neighbor method yields the slowest convergence;
in one hierarchy built using this method we obtained m = 15.
On the other hand, using an absolute threshold (T = 33) usually
generates hierarchies with m = n = 6 because the neighborhood
sizes used in the decimation are much larger.
Different outcomes of the random variables cause changes
in the hierarchy structure. For labeled pictures the final result
of the analysis, i.e., the connected component description at
the root level, is always the same; but this is not the case
for segmentation of gray level images. By changing the set
of survivor cells the values attributed to these cells may also
change slightly, yielding changes in the graphs on subsequent
levels. Four different segmentation results for the aerial image
are shown in Fig. 9. In each example different random variable
outcomes were employed, and the absolute threshold was always
T + 33. All the figures but the top-right have m = 10, while the
top-right has m = 8. The two-level height difference is mainly
caused by the relative inefficiency of the stochastic decimation
process when the similarity graph has only a small number of
vertices. The number of roots is 11 for the top-left image, 12
for the bottom-left, and 14 for the two images on the right.
As expected, regions with sharp boundaries in the input image
(Fig. 8, top-left) achieve very similar representations. The effect
of the stochastic processes is more significant in the segmentation
of smoothly changing regions.
Our method provides the adjacency graph of the segmented
image and a separate hierarchy (''pyramid'') for every region.
These tools can be used by model driven vision modules to
analyze properties of the regions and correct the segmentation
in a logarithmic number of processing steps.
The value of the absolute threshold T influences the result
of the segmentation by defining the ''good'' neighbors of a cell
that are taken into account in defining class membership (8).
Modifying T thus changes the structure of the hierarchies. In
Fig. 10 a medical image and its segmentation for three different
values of T are shown. We chose this image since many of the
features are less well delineated than those in the aerial image.
The height of the root level and the number of roots in each case
are given in the legend of the figure. The larger T, the smaller
the number of roots since the fusion of adjacent classes is more
probable. The correlation between T and the height of the root
level m is less immediate. At higher levels the decimation process
may slow down because of the smallness of the graphs. The effect
depends on the structure of the hierarchy, and therefore also on T.
Note that the medical image is noisy (Fig. 10, top-left) but only
a few noisy pixels remain at the root level. These single pixel
roots are easy to identify and remove from the segmented image.
Another example is shown in Fig. 11. The house image (left)
was analyzed with T 26. The height of the root level in this
example is m = 12 and 100 roots were extracted (right). Note
the accurate delineation of shadows and of the small feature
over the garage door, Segmentation of such complex images
is sensitive to the value of T. When we used T 33 the sky
and the roof fused in some of the hierarchies. However, once
the image representation is obtained, individual analysis of the
segmented objects can be performed to correct any undesired
fusion by lowering the threshold.
In gray level image analysis, the problem of root detection
also requires careful treatment. In labeled pictures a root (an
isolated vertex in the graph G'[I]) remains a root at higher levels,
while other (larger) connected components of the image are being
extracted. In gray level images this is not true. The value of an
already extracted root may become within threshold of some
neighbor's value at a higher level. The root cell then becomes
connected in the graph G'[l] of that level and must be taken into
account in the stochastic decimation process. Thus a root may
disappear at subsequent levels, its receptive field being fused
into a larger region. Nevertheless, we found this approach more
desirable than the following alternatives:
Additional control over the result of segmentation is gain:
changes in the local thresholds computed by a cell on succes
levels are taken into account. Let S;[c4] be the local thres:
of a cell surviving the decimation of level l. Assume tha
level ! 4 1 the cell computes the new threshold S),;[c6]. A r
detailed segmentation is obtained if we declare the cell to:
root when
where 2 is a constant. For example, by using a small .
the ramp image (Fig. 7, bottom-right) we can stop the fusic
adjacent rows. At the root level the adjacency graph becc
linear, with every vertex now representing an entire row o'
image. We can now apply the stochastic decimation algorithm
to this one-dimensional structure to allocate to every vertex its
address in the corresponding linked list. This operation is also
achieved in O(list size) steps [22]. Thus the adjacency graph can
be contracted into a graph in which the vertices correspond to
collections of rows; the new graph defines the segmentation of
the ramp into constant width bands.
In this paper we have presented an image analysis technique in
which a stochastic decimation algorithm constructs a tessellation
hierarchy that reflects the structure of the image. For labeled
images the final tessellation is into connected components, and
is unique. For gray level images the tessellation is not unique,
but it constitutes a reasonable segmentation of the image.
If our methods can be implemented on suitable parallel hard-
ware, every root can recover information about its region in a
logarithmic number of processing steps, and the adjacency graph
can become the foundation for a relational model of the scene.
On appropriate hardware our technique should be useful in real-
time analysis of the visual environment. We intend to implement
our methods on the Connection Machine where the architecture
allows communication among any two processors in a few steps.
This is essential since on a graph any two processors can become
neighbors.
Any definition of the classes can be used when building the
hierarchy. For example, the definition of the classes can take
into account the properties of the corrupting noise if they are
available, or positional information derived from the previous
level. In the latter case, however, the data driven component of
the method is weakened. The decimation process can be modified
to be biased toward cells with high informational value. Jolion
and Montanvert [17] have proposed an adaptive pyramid in which
cells belonging to the most homogeneous regions have priority to
become survivors. Such an approach, however, is not useful for
labeled images in which many cells carry identical descriptions.
The evolution of the local connections within the hierarchy,
driven by both the image data and the stochastic processes, is
itself of interest, and might serve as a neural model for early
visual perception. Appropriate class membership criteria might
transform the hierarchy in a connectionist model for extraction
of perceptual invariants [3].
The graph contraction used to build the hierarchy satisfies
two constraints: 1) a removed vertex always has a retained
neighbor; 2) two adjacent vertices cannot both survive. In graph
theory, finding a surviving subset of vertices is known as the
maximal independent set problem (e.g., [21]). The stochastic
decimation process that we employed provides such a subset
and therefore solves the problem. Our algorithm is different
from other solutions proposed in the literature. In these methods
vertices are first chosen at random and then some of them are
discarded in order to have the constraints satisfied. Such trial-
and-error approaches are considerably slower than our algorithm,
which has the constraints embedded within the selection process.
Simulations have shown a fourfold speed-up for the stochastic
decimation procedure relative to other algorithms.
We would like to thank W. Kropatsch for challenging us with
the checkerboard example, S. Banerjee for calling our attention
to the similarity between the decimation process and the maximal
independent set problem, and D. Y. Kim for suggesting the local
threshold computation.
URRENT radiofrequency (RF) radiation safety
standards (e.g., ANSI C95.1-1982) are based, to a
significant extent, on presumed rates of human whole-body
RF absorption. To date, whole-body absorption rates in
actual human subjects have only been measured by our
group. The experiments were performed using a TEM cell
as the exposure system [1]. Initially, the effect of frequency
and grounding on the E-polarization absorption rates was
studied [2]. In that study, only the ideal free-space and
grounded conditions were simulated. In the present work,
the effect of different spacings from the ground plane on
the E-polarization absorption rates is reported. The other
two possible body orientations with respect to the wave, K
and H, will be ignored since their absorption rates are
much smaller than for the E orientation [l].
All measurements were performed using the modified
version of the TEM cell [3] in which all the TE resonances
are suppressed. Tests showed that the modified cell could
only be used reliably at frequencies below 25 MHz or from
40 to 42 MHz. Within the latter range, the ISM frequency
of 40.68 MHz was selected as the measurement frequency.
Absorbed-power measurements were performed with the
RF system previously described and evaluated [1].
All volunteers were adult males in good health. Ex-
posures were limited to one hour per day at a power
density not exceeding 13 yW -cm''* and no subject ever
absorbed more than one W.
All experiments were performed with the body in an E
:rientation (electric field parallel to the body length) and
he subject's feet closest to, or touching, the ground plane.
3oth of the two possible orientations with respect to the
nagnetic field were employed: the EKH orientation, in
which the magnetic field was perpendicular to the chest;
Ind the EHK orientation, in which the magnetic field was
n the shoulder-to-shoulder direction. In general, the results
mffered little between the two possible E orientations.
Most of the RF absorption rates are displayed in a
;omparative way, i,e., as a ratio to a reference absorption
ate for the same subject. The reference value is either the
ee-space or grounded rate. Table I gives the reference
'gures for each combination of subject and frequency that
was used. Only three subjects were employed in the study
Necause other volunteers were not available for reasonable
eriods of time and the results for the three subjects were
Lways found to be quite similar.
The five frequencies used for the experiments are also
pven in Table I. Frequencies of 13.56 and 40.68 MHz were
mployed because they are both ISM frequencies, at which
tuman occupational exposures often occur. The highest
2sable frequency below all the interacting-resonance fre-
quencies of the cell is 23.25 MHz. The use of 10 MHz
nade a direct comparison possible between our measure-
nents and the two relevant published theories; the cylinder
deory of Iskander et al. [5] and the block model calcula-
bon of Hagmann and Gandhi [6], [7]. Finally, 7 MHz is the
owest frequency for which reasonably accurate results are
Nossible.
Since it was impractical to achieve a uniform-thickness
nr gap between the subject's feet and the ground plane,
hat ideal condition was simulated by using spacers of
ow-dielectric-constant materials. Two materials were used
both of dielectric constant 1.03): expanded polystyrene,
smd a hydrocarbon resin foam (ECCOFOAM PP-2 from
Smerson and Cuming, Canton, MA).
In the first RF absorption study, 40.68 MHz was found
o be at or near the grounded resonance. All frequencies
xlow 25 MHz were clearly in a different, below-resonance,
egion. This distinction is also supported by the present
esults.
Before the feet were separated from the ground plane,
the effect of separating the two feet on the ground plane
was tested. The results, in Table II, are clearly independent
of frequency and of which E orientation (EKH or EHK )
is employed. A small gap between the feet has only a slight
effect. A larger separation significantly reduces the absorp-
tion rate compared to the rate with the feet together. This
effect may be partly due to the reduction in effective height
of the subject as the feet are spread far apart. In the case of
subject L, used for Table II, his effective height is reduced
by 10 cm from its normal value of 173 cm when his feet are
90 cm apart.
The effect of an air gap on the normalized specific
absorption rate (NSAR) is shown in Fig. 1 for three
subjects exposed at the same frequency. Fig. 2 shows the
same plot for one subject in both E orientations and at
three different frequencies. It can be seen that the air-gap
effect depends only slightly on the subject and choice of E
orientation. Results for the two below-resonance frequen-
cies are very similar to each other, but strikingly different
from the results for the near-resonance frequency, 40.68
MHz. It can be seen in the two figures that, for below-reso-
nance frequencies, the absorption rate is reduced to half
the grounded value by an air gap of only three to six mm.
At the near-resonance frequency, on the other hand, an air
gap of 50 to 80 mm (based on Fig. 2 and other data) is
necessary to produce the same effect.
The two relevant existing theories are the approximate
cylinder calculation of Iskander et al. [5] and the block
model calculation of Hagmann and Gandhi [6], [7]. Since
the latter theory was only calculated for the EHK orienta-
tion at 10 MHz, the comparison between theory and
experiment was done for those conditions. The results are
presented in Fig. 3. The theories are normalized to NSAR(0)
for each theory, and not to the measured NSAR(0). This is
important to note because the two measured NSAR values
are both approximately four times the two calculated va
ues (see Table I). This discrepancy will be addressed ir .
later paper on improved models. Fig. 3 can be useo
however, to compare the relative decrease in absorption .
a function of separation distance from the ground plar:
For separations up to three cm, the measurements agr:
with the block model and disagree with the cylinder mode.
At a separation of five or six cm, the two theories bo:
agree reasonably well with the measurements.
At higher frequencies, quantitative predictions were on.
published for the cylinder theory. Since that model obs
ously does not distinguish between the EKH and EH)
orientations, the comparison was made to the average .
the measurements for the two E orientations. The resui;
are presented in Fig. 4. The measured curve for 23.25 MH:
is well below the cylinder theory, just as was found for th:
other below-resonance frequency (10 MHz). The measure-
ments at 40.68 MHz are compared with the cylinder theor-
for the same frequency, and also for 47 MHz because th:
latter frequency is calculated to be at the grounded pea
for the cylinder, The measurements are seen to be close t
the 47-MHz curve for all separations measured. This agree-
ment supports our contention that 40.68 MHz is actual.
very near the peak for grounded human subjects.
The block model only made a semiquantitative predi.
tion for the resonant frequency [6, p. 25], one that ;
obviously wrong: ''Several calculations made for th:
grounded resonant frequency of 47 MHz show a fall-off i
magnitude of grounding effects with increasing distanc:
from the ground plane, which is similar to the results at 1.
MHz.''
As far as the air-gap effect on the whole-body absorp-
tion rate is concerned, we conclude that the cylinder theor-
is more accurate for the near-resonant frequency, and the
block model calculation is more accurate for the below-res-
onance frequencies.
Finally, the block model calculation made one mor:
prediction that could be tested. That theory predicts [6.
fig. 5] that the SAR in the heel is much larger than in the
ball of the foot. This implies that most of the body's RF
current is carried to ground through the heel and tha:
contact with the ground plane is much more important fe:
the heel than for the ball of the foot. The data of Table Ill
prove that the opposite is the case. Most of the RF curren:
through the foot goes through the toes and ball of the foot
This may be due to the larger contact area of that part o'
the foot, or due to the complex bone structure of the ankle
and foot.
For small separations from the ground plane, the soles
of the subject's feet may be considered to form a parallel-
plate capacitor with the ground plane, The equivalen:
circuit representing the exposure situation then consists of
a resistive human body separated from its image in the
ground plane by a capacitive impedance. Iskander et al. [5
used this approach in calculating the RF absorption rate oi
a cylinder. To test the capacitance idea, the functiona.
dependence of the absorption rate on capacitance is not
needed. It is only necessary to utilize the fact that the
capacitance of a parallel-plate capacitor is proportional to
the ratio of the material dielectric constant to the separa-
tion between the plates. Thus, the capacitance idea is
confirmed if the absorption rate depends only on the ratio
of the two quantities.
This idea was tested using different numbers of plastic
sheets between the subject's feet and the ground plane. In
each test, the plastic sheets extended beyond the edge of
the feet by a distance at least as large as the total thickness
of the sheets. The two plastics that were used, and their
dielectric constants (measured in our lab), were: methyl
methacrylate (or lucite, K = 2.6) and cellulose acetate
(K = 40).
The results of these experiments are shown in Fig. 5. It
can be seen that most of the data points, after scaling for
the effect of the dielectric constant, lie on or close to the
curve for K = 1.0. The last point for cellulose acetate, at a
reduced separation of 1.6 cm, is likely off the curve because
the actual separation of 6.4 cm is comparable to the width
of the feet; this violates the assumptions underlying the
parallel-plate capacitor model. Overall, the data of Fig. 5
adequately confirm the capacitance concept. That idea will
now be applied, in a qualitative way, to the practical case
of the effect of footwear.
The effect of different footwear on the nearly grounded
absorption rates was first tested at 23.25 MHz. The data
are presented in Table IV.
The results for socks alone are interesting for two rea-
sons. First, even thin nylon socks, only 0.65 mm thick,
reduce the absorption rate by a measurable amount, to 87
percent of the grounded rate. Secondly, the results for both
the nylon socks and the wool socks (1.7 mm thick) are the
same as the results for an air gap of exactly the same
thickness. This is a rather indirect way of proving the
well-known fact that the bulk of the volume of a sock
consists of air pockets, not material.
The use of shoes as well as socks further reduces the
absorption rate (Table IV). It is not possible to compare
the data with shoes and socks to the capacitor model
because: the heel is usually further from the ground plane
than the ball of the foot; the socks and shoes form a
dual-layer capacitor; and the dielectric constant of the
leather soles (which could not be measured easily) depends
strongly on its moisture content. For the experiments using
both shoes and socks;, the absorption rate compared to no
footwear was found to vary from 543 1 percent (for nylon
socks and dress shoes with leather soles) to 323 1 percent
(for wool socks and rubber-soled athletic shoes).
The same two combinations of shoes and socks, which
represent the maximum range of the results for 23.25 MHz,
were used to study the effect of footwear as a function of
frequency. Those measurements are presented in Fig. 6.
Results are very similar at all the below-resonance frequen-
cies. At 40.68 MHz, however, footwear produces a much
smaller reduction in absorption; this is consistent with the
data of Fig. 2 for the effect of an air gap.
In occupational exposure situations, where the ground-
ing effect may occur, it is recommended that footwear
always be worn. This will prdvide some radiation protec-
tion at all frequencies: a rediuction in the RF absorption
rate of 15 to 35 percent near resonance; and of 45 to 75
percent at below-resonance frequencies. For the commonly
used ISM frequency of 27.12 MHz, the reduction is esti-
mated from Fig. 6 to be between 40 and 60 percent. A
second radiation protection alternative is the employment
of a thick rug, rubber mat, plastic sheet, or any other
insulating material over the ground plane.
The rate of change of the absorption rate with separation
from the ground plane is seen in Fig. 1 to diminish
considerably for separations greater than about 2 cm. It is
obvious that the graphical analysis of Fig. 1 is poorly
suited to extending the measurements to the free space
situation, which is simulated in our TEM cell by a separa-
tion of 90 cm from the ground plane. A much more
suitable plotting scheme, illustrated in Fig. 7, was found.
Absorption rates are plotted as a function of the inverse
separation distance d'', Two advantages of this method
are that the extrapolation curves are linear (at below
resonance frequencies) and that the absorption rate for the
ideal free-space condition (d = oo) is simply the intercept
of the regression line with the ordinate scale.
The data of Fig. 7, for subject L, overlap the data of
Fig. 1 for the same subject. The three points on the
right-hand side of Fig. 7(a) are the same as the three points
on the right-hand side of Fig. 1. It is seen in Fig. 7 that the
linear extrapolation to free space is valid for d''' g 0.5
cmT' or d a 2 cm. Graphs similar to Fig. 7 were also
plotted for the other two subjects at the same frequency.
All the lines fit reasonably well (R'> 0.9 for all six lines).
The regression-line slopes and intercepts for the three
subjects in both E orientations are compared in Table V.
Slopes range from 2.830.5 to 7.43 0.3 cm. Additionally.
the slope is not consistently larger for either of the two E
orientations. The reason for these variations is not known.
The farthest a subject can be from the ground plane in
the TEM cell is 90 cm, when the subject is located halfway
between septum and wall. The subject's length of about
180 cm half fills the distance from septum to wall. The
question of whether or not these spacings adequately
simulate the ideal free-space situation can be answered by
comparing the intercepts of the regression lines (d = oo)
with the last data points (d = 90 cm). In every case, the
difference was less than 10 percent and not statistically
significant. The average difference between the two values
was 43 2 percent. This result proves that, for below-reso-
nance frequencies, a septum-to-wall separation of twice a
body length provides an exposure situation which very
c:losely simulates the ideal free-space situation.
The linear extrapolation process which was found to
work so well for below-resonance frequencies did not work
for 40.68 MHz. The measurements for subject L, shown in
Fig. 8, are clearly not on a straight line. Neither are similar
plots for subject I. This difference in behavior at near-reso-
nance frequencies is not surprising since it is also observed
for small separations from the ground plane.
For the near-resonance frequency, a separation of 90 cm
from the ground plane may not be considered equivalent to
the ideal free-space condition. Based on the four available
curves, it is estimated that NSAR([90 cm]) is 10335
percent greater than the extrapolated intercept NSAR(0),
and that this difference is real. Thus, all our simulated
free-space absorption measurements at 40.68 MHz should
be reduced by 103 5 percent. This has the effect of re-
ducing slightly the frequency exponent n (NSAR c: f'') for
the free-space absorption curves from 18 to 41 MHz. The
corrected mean exponent is 2.7 0.2, in comparison to the
value of 2.90.2 originally reported by us [2, table 3].
A complete set of absorption measurements for one
subject in the EKH orientation is shown in Fig. 9. The data
for the grounded condition and the smallest separation (0.6
cm) are fit by a single (weighted) regression line on the
log-log scale. The data for the free-space condition are fit
with two regression lines, as was previously found neces-
sary [2]. Two lines were also found to give a better fit to the
data for a separation of 5 cm.
In terms of both the positions of the curves and the
number of required regression lines (one or two), it can be
seen that the data for a 0.6-cm separation are similar to the
grounded results, while the measurements for a separation
of 5 cm are more like the free-space results. This supports
our previous observation that a separation of about two cm
is the dividing line between the nearly grounded and nearly
free-space behaviors.
The results of this study fall conveniently into four
distinct categories, depending on whether the frequency is
near the grounded resonance ( f = 40 MHz) or below it
(f eg 25 MHz) and whether or not the subject's feet are
within two cm of the ground plane.
Near the ground plane, the decrease in NSAR with
increasing separation is very rapid. The absorption is re-
duced to half the grounded value by a separation of only 3
to 6 mm. The results agree very well with the predictions of
the block model for all separations out to 6 cm, while they
only agree with the cylinder model at a separation of 5 or 6
cm.
The idea that the soles of the feet and the ground plane
effectively form a parallel-plate capacitor was proved by
measuring RF absorption rates with different thickness of
three different materials between the two surfaces. Natu-
rally, the capacitor model only works for separations from
ground which are less than the width of the foot.
Ordinary footwear provides practical radiation protec-
tion by reducing the RF absorption rates, compared to
grounded, by 45 to 75 percent, depending on the choice of
footwear.
Finally, the absorption rates were found, when plotted
against inverse separation distance, to extrapolate in a
linear manner to the ideal free-space limit. The linear
relationship permitted the inference that a separation of 90
cm, the maximum possible in our TEM cell, is a very good
approximation to the free-space condition.
For near-resonance frequencies, the decrease in NSAR
with separation from the ground plane is an order of
magnitude slower than for the below-resonance frequen-
cies. The curve agrees fairly well with the predictions of the
cylinder model, but disagrees with the block model. Simi-
larly, footwear provide much less RF radiation protection;
the RF absorption rates compared to grounded are re-
duced by only 15 to 35 percent, depending on the choice of
footwear.
Finally, the absorption rates could not be extrapolated
to free space in a linear maner, and it appears that a
separation of more than 90 cm is needed to properly
simulate free space for frequencies near the grounded
resonance.
The author would like to thank J. A. Walsh for perform-
ing many of the measurements and S. J. Allen for review-
ing the manuscript. He is also grateful to the Division of
Biological Sciences, National Research Council, Ottawa.
for providing the facilities where the work was done.
Gvery computer system is accompanied by at least one
''program assembly language.'' This language is used
both as a hardware description language and as a pro-
gramming language. When it is used as a programming
language, it requires a ''program assembler'' to translate
the assembly programs into machine language. As a con-
sequence, at least one program assembler must be devel-
oped for every computer system.
This need led researchers to look for ways to automate
the construction of program assemblers. The result of
their research was a new software tool which was named
the ''program meta-assembler.'' The objective of pro-
gram meta-assemblers is thus quite clear-they are tools
that facilitate the construction of program assemblers.
Likewise, the development of microprogrammed com-
puter systems led to ''microprogram assembly lan-
guages,'' ''microprogram assemblers,'' and ''micropro-
gram meta-assemblers,''
Few program meta-assemblers were constructed with
large computers in mind. In practice, the few that were
developed were used for implementing program assem-
blers only for various models of the same machine. The
Meta program meta-assembler, for example, was used
for the CDC-3170, CDC-3300, and CDC-3500 models.
More program meta-assemblers were constructed (or ef-
forts were made for their construction) with minicom-
puters in mind. Still more were (and are being) con-
structed with monolithic (fixed instruction set)
microcomputers in mind.
Few microprogram meta-assemblers were constructed
before the advent of bit-sliced microcomputers (micro-
computers fabricated with LSI bit-slice microcircuits,
e.g., ALU slices, sequencer slices, and interrupt con-
troller slices). These microcomputers are micro;'
grammed' and many microprogram meta-assemt :
have been constructed for them.- It must be nc::,
however, that up to now only microprogram assem:
languages have been used to microprogram bit-sl..:.
microcomputers.
It is mainly because of microcomputers that me'.
assemblers have established their presence among s.
ware tools. Despite this, they have received little at::'
tion in the literature. Furthermore, what has b::
published is not particularly explicit.' Hence, in this s'
cle we will examine various meta-assemblers so tha' -
can judge the present status of their development s'.
A program assembler is a tool which translates':
assembly programs of a ccmputer into the mach:
language of that computer. Program assemblers tha: ;
this only for a specific program assembly languag<
called dedicated program assemblers. Program asse
blers that do it for many program assembly languages :'
called program meta-assemblers. Thus, program me'.
assemblers are program assemblers parameterized w
respect to the program assembly languages they tra:
late, while dedicated program assemblers are r
Dedicated program assemblers are expressly construc':.
to translate assembly programs for a particular type
computer; they cannot be used for any other type. T-.
first assemblers ever constructed were dedicated prog::
assemblers.
With the advent of microprogrammed computers, mi-
croprogram assembly tools analogous to program assem-
bly tools started appearing, i.e., ''dedicated micropro-
gram assemblers'' and ''microprogram meta-assem-
blers.'' At the beginning, these microprogram assembly
tools were physically and functionally distinct from the
program assembly tools. Later, meta-assemblers which
could act both as program meta-assemblers and as
microprogram meta-assemblers were constructed.
In this article, we will use the term meta-assembler to
mean both program meta-assembler and microptrogram
meta-assembler. The term assembler will stand for both
program assembler and microprogram assembler, and
the term assembly language will be used for both pro-
gram assembly lattguage and microprogram assembly
language. The term machine code will mean either
machine instructions or microinstructions. Furthermore,
the terms program assembler and software assembler will
be considered equivalent, as will microprogram assem-
bler and firmware assembler.
Figure 1 shows how a dedicated assembler works. A
meta-assembler functions differently. As explained ear-
lier, it is designed to be a tool that can be used to imple-
ment assemblers automatically and efficiently. A meta-
assembler is furnished with a ''meta-language.'' The im-
plementation of an assembler with the help of a meta-
assembler is a matter of describing the desired assembler
to the meta-assembler, using the meta-language. Here,
we will call this description the assembler definition.
With the help of the assembler definition, the meta-
assembler can either generate the desired assembler or
adapt itself to operate as the desired assembler. In the
first case, we will call the meta-assembler a generative
meta-assembler. The functioning of such a meta-
assembler is shown in Figure 2. In the second case, we
will call the meta-assembler an adaptive mela-assembler.
Its functioning is illustrated in Figure 3.
Fegtures of a meta-assembler. Ideally, a meta-assem-
bler should be
The first feature is self-explanatory. The utility of a
meta-assembler is highly constrained if it is not portable.
The second feature refers to how easily assemblers can
be implemented with the help of the meta-assembler. If
this task is not considerably easier than implementing the
same assembler from scratch using conventional means,
then the meta-assembler is useless.
The third feature refers to the power of the assemblers
which can be implemented with the meta-assembler, that
is, the power of the assembly languages which the assem-
blers are capable of assembling. Though no universally
acceptable criteria exist for measuring the power of an
assembly language, one can safely characterize an assem-
bly language as powerful if it
All but ''expressive power'' are self-explanatory. Ex-
pressive power has to do with the notation for coding the
assembly instructions, that is, with syntax and semantics.
More discussion on this follows below.
The fourth feature refers to the syntax and semantics
of the assembly languages that can be assembled by the
assemblers that can be implemented with the meta-
assembler. There should be no restrictions on syntax and
Semantics.
The fifth feature refers to the ability of the meta-
assembler to be used for implementing both program
assemblers and microprogram assemblers.
We will see that no existing meta-assembler possesses
all these features.
Assembly languages are machine-dependent. They
provide the programmer with two sets of symbolic state-
ments for coding his programs. The first set comprises
the so-called ''pseudostatements'' or ''directives,'' The
second set comprises the ''machine instructions'' if the
assembly language is a program assembly language, or
the ''microinstructions'' if the assembly language is a
microprogram assembly language. Assembly languages
are usually equipped with a macro facility which makes
them extensible. That is, the programmer can define and
then use new symbolic statements through macros. These
new statements are equivalent to strings of pseudostate-
ments or machine instructions (microinstructions). A
macro facility makes an assembly language richer and
more powerful.
Pseudostatements, This set of statements is not con-
nected with the instruction (or microinstruction) reper-
toire of a computer. These statements, which are directed
to the assembler and not to the hardware processor, pro-
vide the programmer with facilities for
Mgchine instructions. In machine language form, a
machine instruction is a string of fields, and each field is a
string of bits, For a particular instruction, the number of
fields is fixed, as is the bit-length of each field. In symbolic
form (program assembly language form), a machine in-
struction is a string of symbolic entities.
There is a unique relationship between these two
forms. This relationship should be portrayed in a simple,
understandable, and straightforward manner through
the chosen symbolic notation. The degree to which this is
achieved is a measure of how successful a program
assembly language is,
In program assembly languages, the following nota-
tion has prevailed:
label > aC!iOn verb > Operands-list> G COmments >
where operands-list > has the notation
c entity 1> c entity 2> .. . entity n>
s.action verb > designates the functional operation of
the machine instruction, while G operands-list>
designates the operands required by the operation of the
instruction. We will call this notation ''action verb nota-
tion,''
Each entity in operands-list > can be related to one
or more fields of the corresponding machine instruction.
If the entity is related to more than one field, then the
number of entities in a particular instruction will be less
than the number of fields for that instruction. This rela-
tion to more than one field is achieved through an en-
coding scheme. In entities designating addresses, for ex-
ample, special characters which stand for the various ad-
dressing modes (direct, indirect, indexed, etc.) are used.
The symbolic entity VECT1(B2) might designate in-
direct addressing, indexing with register B2, and the sym-
bolic address VECT1. Thus, this entity is implicitly
related to three fields. We will call this type of entity
notation ''encoded entity notation,''
If each entity is related to only one field, then during
assembly a value will be calculated directly from each enti-
ty and no decoding will be required. We will call this type
of entity notation ''non-encoded entity notation,'' With
this notation, the three-field example above would re-
quire three separate entities.
In both these notations the ordering of the entities
Aithin operands-list > is significant and cannot be
ciolated by the programmer. In practice, the number of
entities contained in most machine instructions is seldom
:oo large, so in both encoded and non-encoded entity no-
:ation remembering the fixed ordering of the entities is
nnOt a serious problem.
Microinstructions. Microinstructions are similar in
structure to machine instructions. However, they can
have more fields than machine instructions and may
designate more than one operation to be performed in
parallel, These two characteristics create two problems
concerning the symbolic form (microprogram assembly
language form) of microinstructions.
The first is that action verb notation is not very mean-
ingful for microinstructions, since this notation desig-
nates only one operation. For this reason the following
notation is used for microinstructions:
c abe! > C entity 1> entity 2> ... entity n > COmments >
Each entity is related, in microcode form, to one or more
fields of the corresponding microinstruction, We will call
this notation ''list notation,''
The second problem is to find a meaningful notation
for the entities of a symbolic microinstruction. This
problem is more difficult than the problem of finding a
notation for machine instructions, since microinstruc-
tions can have more fields. This problem has been tack-
led in various ways. The objective is to always keep the
number of entities as small as possible while at the same
time keeping the notation as meaningful as possible.
To keep the number of entities small, almost all micro-
program assembly languages use default values for those
fields to which no values have been assigned either direct-
ly or indirectly by the symbolic entities. This default
capability is used in three entity notations which we will
call ''positional notation,'' ''nonpositional notation,''
and ''free notation,''
Positional notatiorn. In positional notation, each entity
is implicitly related to one field. The ordering of the en-
tities is significant, since it is the position of an entity
within the list of entities which implicitly designates the
field to which the entity is related. The value of the entity
is assigned to the implied field.
Nonpositional notation. In nonpositional notation,
the ordering of entities within the list of entities does not
matter. They can be written in any order. This notation is
further divided into three different notations which we
will call ''function reference notation,'' ''value mne-
monic notation,'' and ''keyword notation,''
In function reference notation, an entity has
the format
fname P1., Pn)
where fname is the mnemonic name of an implied func-
tion and P1, . . . ,Pn is a list of parameters. This nota-
tion provides a useful mechanism for parametrically
assigning values to one or more fields; that is, it has a
dynamic multifield assignment capability. This notation
is appropriate for expressing the various operations
designated in horizontal microinstructions, since it codes
them as function references.' The mnemonic name of the
function in this case is the mnemonic name of an opera-
tion, and the parameters of the function are the operands
on which this operation acts.
In value mnemonic notation, an entity can only be a
mnemonic name which is implicitly associated with a
value and a specific field. This value is assigned to the
field. This notation permits the use of a mnemonic name
for assigning a predefined value to a predefined field.
In keyword notation, the entity has the format
fieldname = Value
where fieldname is a preassigned mnemonic name of a
specific field and value is a value to be assigned to the
field that is associated with the mnemonic name field-
name. This notation provides a straightforward way to
assign a value to a field.
Free notation, In free notation, there are no con-
straints, in principle at least, on the notation the pro-
grammer uses for assigning values to the fields of a
microinstruction, He designs a notation which suits him
and then uses it to code the microinstructions.
Assembly-language-independent statements. All pseu-
dostatements are computer-independent, except those
used for data generation. Data types supported by con-
temporary hardware are the integer numbers and the
floating-point numbers. There are only a few hardware
representations of these data types, as well as only a few
representations of the character set. One for each data
type is used in a particular computer, So, data generation
pseudostatements can be parameterized with respect to
the representation of the data they generate, Hence, all
pseudostatements are assembly-language-independent.
Machine instructions (or microinstructions) are
assembly-language-dependent. This observation, namely
that the set of pseudostatements is assembly-language-
independent while the set of machine instructions (or mi-
croinstructions)is assembly-language-dependent, greatly
influenced the development of meta-assemblers.
Every meta-assembler is furnished with a meta-
assembly language. The task of implementing an assem-
bler with a meta-assembler is restricted to preparing the
assembler definition (see again Figures 2 and 3) by means
of the meta-assembly language. Assembler definition by
means of a meta-assembly language is conceptually equi-
valent to assembly language definition by means of a
meta-assembly languagc.
As has already been noted, the set of statements pro-
vided by any assembly language can be divided into two
subsets. The first comprises those statements that are
assembly-language-independent and the second those
that are assembly-language-dependent. Thus, it is possi-
ble to think of a family of assembly languages, all mem-
bers of which have the same syntax, the same semantics,
and the same set of assembly-language-independent
statements. It is obvious that the definition of an
assembly language in this family will be restricted to the
definition of the assembly-language-dependent state-
ments only. This definition is done via the meta-assembly
language.
All meta-assemblers constructed so far are based on
the above principle. With them, one can implement
assemblers which are able to assemble assembly lan-
guages that belong to an assembly language family that is
meta-assembler-dependent. The task of implementing an
assembler with a meta-assembler will be confined to
defining the assembly-language-dependent statements of
the assembly language that the assembler under im-
plementation will assemble.
The definition of assembly-language-dependent state-
ments is done by means of the meta-assembly language
of the meta-assembler. To make this task easier, meta-
assembly languages should provide facilities to
What meta-assembly language is the most suitable is
still unknown. The ones utilized so far can be divided in-
to two categories. Those in the first category are macr-
languages. They provide some directives for specifyin;
the information required by (1) and (2) above, and the
provide macros for specifying the information require
by items (3) to (6) above. Meta-assembly languages i:
this category were the first ever used. The meta-assem
blers furnished with them are very powerful, though the;
have some drawbacks. Among them is their inability t
use encoded entity notation and free entity notation. A-
a consequence, these meta-assemblers cannot be used tc
directly implement assemblers as specified under the pre-
posed IEEE Microprocessor Assembly Language Draf:
Standard.'
Those in the second category do not use macros. Ir
principle, they allow encoded entity notation and free en-
tity notation. In practice, however, they greatly restric:
their use, since these notations are completely meta-
assembler-dependent.
In an earlier part of this article, we divided meta-
assemblers into two categories according to the way
they function. We called those in the first category
generative meta-assemblers (see again Figure 2) and
those in the second category adaptive meta-assemblers
(see again Figure 3).
The adaptive meta-assemblers can be divided into two
subcategories according to the nature of their meta-
assembly languages. We will place in the first sub-
category those which have a meta-assembly language
which is a macro language, We will call them ''Ferguson-
type meta-assemblers,'' We will place in the second sub-
category those which have a meta-assembly language
which is not a macro language. We will call these ''non-
Ferguson meta-assemblers.'' The first meta-assemblers
were of the Ferguson type, Indeed, the term ''meta-
assembler'' originally embraced only this category.'
Thus, assemblers can be classified according to the
scheme shown in Figure 4.
Meta-assemblers evolved as a generalization of con-
ventional assemblers. As a consequence, the same tech-
niques were used for the construction of the first meta-
assemblers as were used for the construction of conven-
tional assemblers. This was the case with the adaptive
Ferguson meta-assemblers.
For all other types of meta-assemblers, a mixture
of techniques, derived from ones used in the construc-
tion of conventional assemblers, compilers, and meta-
compilers, has been used.
Another method for constructing a meta-assembler is
the conversion of an existing, conventional assembler in-
to a meta-assembler. This can be done by employing
special preprocessors and postprocessors and by using
the macro language of the conventional assembler as a
meta-assembly language.'- The meta-assemblers that
have been implemented in this way have also been adap-
tive Ferguson meta-assemblers.
Meta-assemblers are a special case of meta-compilers.
Theoretically, a meta-compiler can be used as a meta-
assembler. However, it seems that there have been some
problems when existing meta-compilers have been used
this way. Besides these problems, meta-compilers are not
widely available. Nonetheless, attempts in this direction
have been reported,%.1? We should note that meta-
assemblers can be used to complement compilers and
meta-compilers. In this case, the high-level programming
language is compiled into assembly language,'' which is
then translated into machine code. The second step can
be performed by a meta-assembler.
For various reasons, the translation of assembly pro-
grams into machine code is done in three separate phases.
In the first phase (assembly phase), an ''assembler''
transforms the assembly program into a form of code
called ''assembled code.'' In the second phase (linking
phase), a ''linker'' transforms the assembled code into
another form of code called ''linked code.'' In the third
phase (relocating phase), a ''relocating loader'' trans-
forms the linked code into still another form of code
called ''loadable code.'' The loadable code contains the
machine code in absolute form; it can be loaded into the
memory of the target computer with the help of another
tool called the ''absolute loader.'' Note that the linker
and relocating loader can be combined into one tool, the
relocating linking loader.
Splitting the translation process into the subprocesses
described above is desirable because it allows program
modules from different language processors (e.g., as-
semblers, Fortran compilers) to be combined, and it
greatly facilitates the development of modular programs.
Another translation scheme has the assembler directly
produce loadable code, in which case no linkers and
relocating loaders are required. With this scheme,
however, we lose the advantages cited above.
If the assembler-linker-loader scheme is adopted, then
a meta-assembler should be accompanied by a ''meta-
linker,'' a ''relocating meta-loader,'' and an ''absolute
meta-loader'' (or by a ''combined meta-linker relocating
meta-loader'' and an ''absolute meta-loader''). These
tools facilitate the development of linkers, relocating
loaders, and absolute loaders in the same way meta-
assemblers facilitate the development of assemblers.
Special meta-languages are required for this purpose,
namely a ''meta-linking language'' for the meta-linker
and a ''meta-loading language'' for the relocating meta-
loader. The first is used for describing linkers to the
meta-linker and the second for describing loaders to the
relocating meta-loader, in the same way that a meta-
assembly language is used for describing assemblers to
the meta-assembler.
It has been demonstrated that the linking process is
target-computer-independent, '. 12-14 Hence, meta-
linkers can be constructed as simple linkers and no meta-
linking language is required.
A list of existing meta-assemblers is presented in Table
1. Although I do not claim that the list is complete, I
believe that it is a good approximation to a complete list.
The table presents general characteristics of meta-
assemblers, such as category, intended usage, and port-
ability. Also given are details about the construction of
the meta-assemblers, such as who constructed them, on
which host computer they were developed, and in which
language they are coded. More important, the table is set
up to provide answers to questions about the assemblers
that can be implemented by each meta-assembler: What
sort of assembly languages do they assemble? Can they
be readily used or does the user have to develop com-
plementary tools such as linkers and loaders?
The table is divided into five sections headed general,
Construction, assembly language, meta-assembly lan-
gtIage, and complementary tools,
The general section is divided into five columns. The
first gives the name of the meta-assembler. The second
gives the class of the meta-assembler according to the
classification scheme shown in Figure 4. The third col-
umn indicates the meta-assembler's usage as intended by
its constructor-that is, the kind of assemblers that it can
implement: software assemblers, firmware assemblers,
or both. The fourth column gives a ''yes/no'' answer to
the question of the portability of the meta-assembler.
The fifth column cites the pertinent literature, to the best
knowledge of the writer of this article.
The construction section of the table is divided into
three columns. The first gives the name of the construc-
tor of the meta-assembler, usually a computer manufac-
turer, software house, or research institute. In the first
and second cases, the names are given. In the third case,
the symbol ''R'' indicates that the constructor is a
'esearch institute, the name of which, and possibly the
ames of the particular scientists involved, can be found
n the cited literature. The second column gives the name
Nf the computer system used for the development of the
meta-assembler. The third column gives the name of the
programming language in which the meta-assembler is
coded.
The assembly language section of the table gives infor-
mation about the assembly languages that can be assem-
bled by the assemblers that can be implemented by each
meta-assembler. This section is divided into seven sub-
sections. The first subsection gives a ''yes/no'' answer to
the question of whether action verb notation (column
one) or list notation (column two) can be used. The sec-
ond subsection concerns entity notation and indicates
''yes,''''no,'' or ''not applicable'' for the various entity
notations. The positional and non-encoded notations
have been grouped, The same has been done for the free
and encoded notations, The third, fourth, and fifth
subsections give a ''yes/no'' answer to the question of
whether the corresponding assembly languages have,
respectively, an expression evaluation capability, a
macro facility, and conditional assembly statements. The
sixth subsection indicates whether the addresses are
calculated at assembly time relative to the start of the
program module, or are calculated as absolute addresses.
In the first case, the indication is ''yes'' and the object
code produced by the assembler needs to be processed by
a relocating loader. The seventh subsection gives a
''yes/no'' answer to the question of whether the assem-
bly language permits the splitting of assembly programs
into modules that can be separately assembled.
The general and assembly language sections of Table 1
indirectly give information about the meta-assembly lan-
guage of each meta-assembler. Some additional informa-
tion is given in the meta-assembly language section, The
first column gives the maximum length in bits of the ad-
dressing unit. The second column gives a ''yes/no''
answer to the question of whether the meta-assembly
language permits the programmer to declare the data
type representation-for example, for negative integers
one's-complement or two's-complement representation.
The third column gives a ''yes/no'' answer to the ques-
tion of whether the programmer can create his own error
diagnostics The fourth column gives a ''yes/no'' answer
to the question of whether the programmer can perform
checks on the values of the various fields during the code
generation process. This is useful in assembling horizon-
tal microinstructions, a process in which the values of
some fields are related. The ability to confirm that these
relations hold is a contribution to microprogram correct-
ness at the assembly level.
The complementary tools section gives an answer to
the question of whether each meta-assembler is comple-
mented with a meta-linker and a relocating meta-loader,
or with a combined meta-linker/relocating meta-loader.
The answer ''not applicable'' is given when such tools are
not needed. (This means that the implementable assem-
blers generate absolute object code.)
l)ndoubtedly demand exists for both software and
firmware assemblers. This is due mainly to the exploding
use of microcomputers, both monolithic and bit-sliced.
Meta-assemblers can be the answer to the problem of im-
plementing microcomputer assemblers easily and quick-
ly, However, very powerful meta-assemblers are required
because of the great number of incompatible microcom-
puter assembly languages now in use.
Constructing meta-assemblers is a difficult task, So is
using them. Existing meta-assemblers are of limited
usefulness because they are not powerful enough. And
constructing the relocating meta-loaders that are needed
to complement meta-assemblers is not a trivial task
either. Standardization seems to be the key to an easy
solution' to these problems.
The areas requiring such standardization are program
assembly language, microprogram assembly language,
meta-assembly language, relocatable object code lan-
guage, meta-loading language, and absolute object code
language.
A draft standard already exists for program assembly
languages. This standard could be expanded to include
language facilities not covered by the present draft, such
as macros, conditional assembly directives, and program
modularization directives. This standard, by suitable ex-
pansion, could also embrace microprogram assembly
language. For relocatable object code language, the one
developed and in use today at CERN could be used as
is, or suitably adapted. No standard exists for absolute
object code language, but devising one should be simple.
The question of finding standards for meta-assembly
language and meta-loading language is still open-and
difficult. However, the development of such standards
will be facilitated if it is based on already-adopted stan-
dards for assembly language and relocatable object code
language.
I would like to thank the referees for their constructive
comments,
Computer vision is a subset of the
machine intelligence systems area. The
goal of a computer vision system is to
interpret the given ''visual'' data and
to use the interpretation to complete a
task. Typical tasks include (1) the navi-
gation of autonomous vehicles on the
land, in the air, or under the sea, (2)
the assembly or inspection of manufac-
tured parts, and (3) the analysis of
microscopic images and medical x-rays.
In a number of applications, the goal of
the vision system is to identify and locate
a specified object in the scene. In such
APPENDID A. DIFFERENTIAL GEOMETHY OF
SURFACES
APPENDID B. SURFACE PROPERTY
MEASUREMENTS
ACKNOWLEDGMENTS
REFERENCES
cases, a vision system must have full
knowledge of the shape of the desired
object. Such a priori knowledge of
the object is provided through a model
of the object, and in most cases it con-
tains information regarding the geome-
try of the objects; some models may
contain additional information such as
thermal and stress properties of the
objects. A vision system which makes use
of an object model is referred to as a
model-based vision system, and the gen-
eral problem of identifying the desired
object is referred to as object recognition.
While there is no single definition of the
object recognition problem, the objective
is to identify a desired object in the scene
and to determine its exact location and
orientation.
An ideal model-based vision system
should be able to locate objects in a scene,
assuming that any of the following are
true: (1) objects may have arbitrary and
complicated shapes or forms; (2) objects
may be viewed from any direction; and
(3) objects may be partially occluded by
other objects. Specifically, such a system
may be used in determining the location
of grasp sites for a robot arm to mani-
pulate an object, in the navigation of a
robot or an autonomous vehicle, and in
the assembly and inspection of parts
in a manufacturing environment. For
instance, robots with such vision capabil-
ities may carry out instructions regard-
ing the handling of objects with fewer
specifications than are currently required
and with more tolerance for minor
disturbances.
To design such systems, system
designers must resolve the following
issues: (1) the type of sensor for data
collection, (2) the methods of construct-
ing the necessary object model, (3) the
means of describing the collected data
and the model, and (4) the methods of
matching the object descriptions obtained
from the input data to that of the model.
The sensor determines the resolution (the
total number and frequency of sample
points) and precision (the accuracy of
each sampled point). More importantly,
it determines whether the data provides
2D or 3D information of the scene. Mod-
els provide the a priori knowledge of the
vision system. Representations are used
to describe the collected data and the
object model, a key issue in the field of
computer vision. The representations dic-
tate the matching strategy, its robust-
ness, and the system's efficiency. Also,
the descriptions are used in the calcula-
tions of various properties of objects in
the scene needed during the matching
stage. Matching strategies are performed
at run-time and must resolve many
ambiguities that exist between the data
and the model descriptions. Once the cor-
rect match has been determined, the ori-
entation and translation of the located
object, with respect to the model, can be
calculated, completing the task of object
recognition.
This paper surveys recently published
papers' addressing the problem of the
model-based recognition of objects in 3D
dense-range images.' Section 1 discusses
in detail the issues outlined above to
introduce the specific problems of model-
based vision. Next, Section 2 reviews var-
ious sensing modalities, beginning from
the data collection step, giving emphasis
to sensors providing 3D information.
Section 3 reviews various low-level pro-
cessing procedures necessary as a prestep
to the description and recognition of
objects. Section 4 reviews various repre-
sentations used to describe the objects.
Section 5 discusses modeling schemes,
giving emphasis to the computer-aided
design (CAID) systems used in the exist-
ing model-based vision systems. Section
6 presents a study of the matching
strategies. Finally, Section 7 presents the
summary and concluding remarks. A
brief overview of differential geometry of
surfaces is presented in Appendix A, and
in Appendix B various computational
methods to calculate surface properties
are reviewed.
The introduction outlined the issues
involved in the design of a model-based
vision system. This section analyzes fur-
ther these issues: data collection, repre-
sentation, model construction, and the
matching strategies (see Figure 1).
The first issue addressed in a com-
puter vision system is data collection,
which may be performed using one or
more of the many existing modalities.
The intensity camera is perhaps the most
commonly used sensing module, measur-
ing visible light. The output of the cam-
era from a scene is digitized to provide a
2D array of numbers; each number corre-
sponds to the average intensity sensed in
a sampled, typically square area. Exam-
ples of other sensory modules include
thermal cameras, which measure the
emitted thermal radiation rather than
the emitted visible light, and laser range
scanners and sonar sensors, which are
used to calculate the distance to the
objects in the scene. These sensors each
provide information on a different aspect
of their environment; the choice of the
sensor is largely application dependent.
For the task of 3D object recognition,
various object dimensions and surface
shape information are essential. Two
general approaches exist to collect the
necessary data. In the first approach,
sensing modalities, such as intensity
cameras, are used, and many depth cues
are analyzed to recover the necessary 3D
information. In the second approach,
external energy sources, such as lasers,
are projected onto the scene. While the
first approach is preferred (since no
external sources are required), the recov-
ered data lacks the necessary resolution
and precision for many common tasks;
therefore, the second approach is used
most frequently in 3D object recognition.
Section 2 studies the schemes for collect-
ing 3D information from the scene.
The second issue in a computer vision
system is to represent the collected data
and the modeled object. The 2D array of
numbers provided by the sensor is not
of much use in its ''raw'' form. A suitable
trepresentation scheme must, therefore, be
used to describe the data and the model.
A representation is desirable if it is (1)
unambiguous (no two objects have the
same representation), (2) unique (there is
a single description for each object using
the representation scheme), (3) not sensi-
tive (with respect to missing data points,
such as in the cases of partial occlusion),
and (4) convenient to use, in the match-
ing stage, and to store. Representation
is a key issue in computer vision. Vari-
ous schemes, such as surface-based and
volumetric-based representations, will
be discussed, with emphasis given to
recently published representation
schemes.
The construction of object models is
the third issue which must be addressed
in a model-based computer vision sys-
tem. There are two main approaches for
model construction. In the first approach,
the actual objects are used to generate a
model; i.e., data points obtained from
several viewpoints of the object are inte-
grated in a coherent fashion to provide
information from all the viewing angles.
In the second approach, a CAID system is
used, and a set of predefined primitives
allows the user to construct interactively
the model of an object. Much of the ear-
lier work in object recognition used the
first approach; however, most recent
research efforts use a CAD or similar
system. Both approaches are reviewed,
and the advantages and the disadvan-
tages of each are discussed.
Once the appropriate descriptions are
derived from the data and the models,
the vision system is able to match the
two descriptions. This is performed
in two steps. In the first step, a corre-
spondence is established between the two
sets of descriptions. Since in most cases
data is collected from a single view and
there may be partial occlusions present,
the matching strategy must establish
correspondence between the partial
description of the object and its full model
description. The correct match of the
collected data to the representation of
the given model ''establishes an interpre-
tation'' of the input data [Ballard and
Brown 1982]. The exact choice of the
matching strategy is dependent on
the representation scheme, the applica-
tion, and the system designer's expertise.
In the second step, using the established
correspondences, a geometrical transfor-
mation (usually a rotation matrix and a
translation vector) is derived such that
the model may be transformed to the
orientation of the object in the scene.
An important aspect of any computer
vision system is its data acquisition mod-
ule. This section reviews the schemes in
which 3D information is acquired from
the scene. The task is performed in one
of two approaches: passive or active. In
the passive approach, 3D information is
inferred from the scene using existing
energy in the environment, such as
reflected light. In the active approach,
the 3D information is derived by project-
ing external energy waves, such as sonar
waves and laser light. As mentioned in
the introduction, 3D data recovered from
current passive approaches lack the nec-
essary precision and resolution for 3D
object recognition; in this section, the
active methods are reviewed [Besl 1989;
Nitzan 1988; Freeman 1988; Fu et al.
1987; Kanade 1987; and Ballard and
Brown 1982].
Active-range sensing can be divided into
two main classes. In the first class, the
principle of triangulation is used. Each
point in a scene is highlighted, using a
sheet of light, and observed by the sen-
sor. Then, using the known geometry of
the imaging system, the distance of each
highlighted point to the sensor is calcu-
lated. In the second class of active-range
sensors, known as time-of-flight sensors,
a signal is emitted, and its return time is
measured and used in calculating the
distance.
This method uses a laser source which
projects a sheet of light onto the scene,
casting a line on the objects' (see Figure
2). A camera is positioned so that the
laser line is visible. Using the known
imaging geometry, i,e., the distance
between the laser source and the camera
(referred to as the baseline), their angles
a, and ag) with the z-axis (the axis
along which depth is measured) and by
applying the principles of triangulation,
the distance of the illuminated points,
along the cast laser line, to the baseline
is calculated (see Figure 2). Sweeping the
sheet of light across a scene results in a
range map. The sweeping of the sheet is
performed in one of two ways: a rotating
reflector can be used to project the sheet
of light, or the objects can be placed on a
stage which slides by or rotates in front
of the sheet of light. In all cases, the
laser may not illuminate some areas, or
the sensor may not be able to detect the
illumination (i.e., the object concavities
may occlude some areas of the object);
and there are no data points at those
locations of the scene. These areas of
missing points are referred to as shad-
ows,' An advantage of using the stage is
that more uniform spatial sampling
can be achieved, satisfying a common
assumption for surface property mea-
surement approaches (see Figure 3 and
Appendices B and A). The disadvantage
of using the stage is that it limits the size
and number of objects in the scene, and
it may not always be feasible to place the
objects on the stage. Also, the accuracy of
the motion control mechanism of the
stage or the reflector must also be taken
into consideration in determining the
approach used.
In most of today's available triangula-
tion systems, the time required to scan a
typical 256 x 256 scene with 8 bits of
accuracy is on the order of minutes.
However, some triangulation-based sys-
tems exist which are capable of scanning
scenes in seconds. For example, Rioux
[1984], and Rioux et al. [1989] developed
a system able to scan a 256 x 256 pixel
scene with a precision of 0.5 mm in less
than one second. Kanade et al. [1989]
and Gruss et al. [1990] introduced a small
prototype of a VLSI-based system, which
performs the triangulation on a 4 X 4
array of specialized sensors on a single
CMOS chip. Once fully developed, this
system could significantly reduce the
acquisition time in triangulation-based
range sensors.
To reduce the data acquisition time,
several sheets of light in parallel may be
projected onto the scene (first introduced
by Will and Pennington [1972]). The dis-
advantage of this approach is that,
depending on the scene's geometry, a
stripe position may be shifted more than
the existing spacing between the stripes
[Mundy and Porter 1987], causing ambi-
guities. Therefore, it is necessary to
determine which sheet of light is pro-
jected at each pixel in the scene. There
are several ways to resolve this ambigu-
ity. Mundy and Porter [1987] and
Inokuchi et al. [1984] use a set of gray-
coded stripes, assigning to each stripe a
unique code value. Boyer and Kak [1987]
and Vuylsteke and Oosterlinck [1990] use
only one set of simultaneous projections,
instead of a series over time, to obtain
the range information from the scene.
The advantages of using a single set of
patterns are that less time is spent pro-
jecting light patterns, and more impor-
tantly, nonstatic scenes may be scanned,
allowing a broader application area such
as robot navigation, motion analysis, and
moving-object recognition. Boyer and Kak
use a few colors to project single-colored
stripes. Vuylsteke and Oosterlinck use
a binary pattern to illuminate the scene.
Tajim a and Iwakawa (of NEC)
[1990] have reported on a triangulation-
based sensor using collimated white light
diffracted by a grating to form a rainbow
pattern on the scene capable of producing
30 3D frames per second.
One of the disadvantages of a triangula-
tion-based laser scanner is the shadow
effect, where a region of the scene is not
visible to either the laser or the sensor.
In time-of-flight range finders, a laser
beam is emitted and received along the
same path, eliminating the shadow prob-
lem. However, since the system depends
on the return laser light to measure the
distance, high-energy laser sources, pos-
sibly harmful to the human eye, are
required. Also, most time-of-flight range
finders require complex electronics, rais-
ing the cost of such sensors. Two classes
of laser sources are used in time-of-flight
scanners: pulsed and continuous-beam
lasers.
In this class of range finders the laser
light is reflected, and its return time is
measured [Lewis and Johnston 1977];
since the speed of the laser light is known,
the distance can easily be determined.
Such devices have ranges of from 1 to 4
meters with a precision of 0.25 inches;
however, such precision requires sensi-
tive electronic instrumentation capable
of resolving 30-50 psec time intervals
[Jarvis 1983a; 1983b].
The second class of time-of-flight range
finders uses a continuous-beam laser
rather than a pulsed one [Jarvis 1983b;
Fu et al. 1987]. In this method, the delay
6 in the returned signal (i,e., the 0 to 2m
phase shift with respect to the transmit-
ting signal) is used to measure the dis-
tance D to the object:
where A is the wavelength of the signal.
This measurement is performed one point
at a time, and the laser beam is scanned
horizontally and vertically across the
scene. An inherent problem in this
approach is that all distances corre-
sponding to 2m multiples of the phase
shifts will be measured as the same dis-
tance by the system (referred to as the
ambiguity interval); one may, however,
assume that 0 is less than 2m. Addition-
ally, since A is usually on the order of
nm, D has a very small range which
is not feasible for most object recogni-
tion tasks. The amplitude of the laser
light may be modulated, however, using
long-wavelength waveforms, effectively
increasing A.' However, the ''ambiguity
interval'' of the scanner still exists and
may be resolved by transmitting at sev-
eral frequencies and checking all fre-
quencies at the ambiguity intervals. To
decrease the effects of photon shot noise,
the distance at each point is measured
several times, and the average is used.
A problem common to all laser range
scanners is the specularity problem,
causing erroneous measurements in both
the triangular-based and the time-of-
flight methods. In laboratory setup it is
possible to decrease this problem by
painting the surfaces appropriately; how-
ever, in outdoor scenes this problem per-
sists. In addition, when laser lights are
used, other sources of error, such as
''speckle,' exist [Technical Arts 1987].
The speckle phenomenon is a conse-
quence of the laser light coherency, caus-
ing patterns of darkness and brightness
on object surfaces. This phenomenon is
particularly unfavorable since the result-
ing errors may not be averaged out over
time during data acquisition; rather, the
error must be reduced optically.
Various other approaches exist in recov-
ering 3D information from the scene.
Additional methods include: tactile sen-
sors, force and torque sensors, proximity
sensors (which can be capacitive, induc-
tive, or ultrasonic), Holographic interfer-
ometry (which requires the surfaces to be
flat and smooth), and Moire techniques
(which require no depth discontinuities
present in the scene). These sensors have
very limited use in computer vision
although they are used in other appli-
cations. A brief overview of these sys-
tems may be found in Fu et al. [1987]
and in Besl [1989].
Once the necessary measurements from
the scene have been made, the data must
be represented using symbolic descrip-
tions to enable the system to carry out
the specified high-level processes. The
first step in obtaining the symbolic repre-
sentation is the partitioning of the input
data based on the desired description. In
general, this step depends on the nature
of the input data and the higher-level
processes, such as matching. Range scan-
ners sample points on the surface; there-
fore, the vision system must rely on the
information derived from the surfaces.
Further, as outlined earlier, the ideal
vision system must be able to recognize
arbitrarily shaped objects from any view-
ing direction. Also, the system must be
able to cope with possible partial occlu-
sion from other objects in the scene and
with cases of self-occlusion (for example,
the inside of a cup is self-occluded when
the cup is viewed from one side).
The key to performing such tasks is
the means by which shapes are described
by the system. In a large number of cases,
local-surface properties, such as surface
curvature and surface normal, are used
to describe the shapes. Briefly, surface
curvature is the rate at which the sur-
face deviates from its tangent plane. Cur-
vature is an important measure since
it is invariant to viewing directions
and does not change with occlusion.
More importantly, however, curvature of
smooth surfaces may be approximated by
using only a small neighborhood (i.e.,
a local neighborhood) on the surface.
Appendices A and B cover, in detail,
curvature, surface normal, and many
other surface properties used in shape
description.
Prior to the calculation of most surface
properties, such as curvature and sur-
face normal, the collected data must be
smoothed. The reason is twofold. First,
the measurements made by the sensors
are often inaccurate due to detector noise,
quantization, calibration errors, and as
mentioned earlier, other sources of error,
such as ''speckle.'' Second, the calculation
of curvature involves second-order par-
tial derivatives, magnifying the effects of
any noise present.
In the past, most noise processes
have been modeled as additive Gaussian
stochastic processes independent of the
scene [Pratt 1978]. While many studies
have been done on noise reduction for
gray-scale images [Rosenfeld and Kak
1982; Mastin 1985], very few such stud-
ies have been reported for range maps.
Rather, most researchers have adapted
noise reduction methods developed from
gray-scale images. Further, most studies
assume the error to exist only in the z
coordinate (the depth value) of the mea-
surements; i.e., the x and y coordinates
are assumed to be error free. With few
exceptions, making the assumption is not
true for cases where all coordinates have
been considered (see Jain and Dubes
[1988 and Jolliffe [1986]).
Brady et al. [1985] and Yang and Kak
[1986] have used Gaussian smoothing
[Marr and Hildreth 1980]. In Gaussian
smoothing, the data is convolved with
the rotationally invariant Gaussian filter
where (u, v) is the (intrinsic) coordinate
of each point in the data. Ideally, the
variance, o', is chosen such that the noise
in the data is suppressed while the ''sig-
nificant'' features of the data are pre-
served; however, larger o' s necessary
for effective noise reduction result in too
much blurring of the image and in the
loss of the image details. Alternatively,
the image may be convolved with a small
operator, such as
many times. Using the central-limit the-
orem, this operation approximates the
Gaussian window with a standard devia-
tion of yn , where n is the number of
iterations [Burt 1983]. In addition to
reducing the noise by Gaussian smooth-
ing, Besl and Jain [1988] approximate
the standard deviation of the noise and
use the measure to determine certain
thresholds used in the subsequent steps.
Ponce and Brady [1987] report on some
orientation-dependent errors observed
in smoothing with the Gaussian win-
dow. These errors are magnified dur-
ing the calculation of curvature values
partly because such calculations include
second derivatives). They suggest that
smoothing be performed in the intrinsic-
coordinate system (see Appendix A) to
minimize such effects. This may be done
by first approximating the surface nor-
mal at every point; then each point is
'moved along the direction of its normal,
a distance that depends upon the pro-
jected distances of the point's neighbors
from the tangent plane'' [Ponce and
Brady 1987, p. 227]. Simple iterative
averaging is equivalent to intrinsic-
coordinate smoothing when the surface
normal is along the viewing direction.
Citing expensive computational costs,
Ponce and Brady did not use this method
in their paper; however, an example was
given (see Figure 4).
Hoffman and Jain [1987] investi-
gated several smoothing techniques
such as the median filter, nearest-
neighborhood smoothing, and maximum-
likelihood smoothing [Hurt and Rosenfeld
1984]. In maximum-likelihood smooth-
ing, a 8quare neighborhood of each pixel
is considered, and each pixel is replaced
by the average of a subset of the neigh-
borhood pixels. For example, in a 3 x 3
window, four 2 X 2 subsets are consid-
ered, and the central pixel is then
replaced by the average value of the pix-
els in the subset which is the most repre-
sentative neighborhood. Using such an
approach, Hoffman and Jain report that
on synthetic images, the standard devia-
tion of the added noise was dropped by a
half using a 3 x 3 window.
Saint-Marc and Medioni [1988] presen-
ted an adaptive smoothing filter, which
is applicable to intensity or range images.
The image is smoothed iteratively with
a monotonically decreasing function
(monotonically decreasing as a func-
tion of increasing discontinuity proba-
bility) IPerona and Malik 1987]. To
preserve tangent orientation discontinu-
ities as well as step discontinuities in
depth, important measures in 3D vision,
the authors (Saint-Marc and Medioni)
suggest applying the filter to the deriva-
tives of the signal rather than to the
original signal. On the other hand, the
smoothed derivatives may only be used
to calculate the curvature and the sur-
face normal; lower-order properties, such
as the surface area of a patch, are not
computable directly. The integration of
the smoothed derivatives is therefore
necessary, a process which may introduce
further inaccuracies.
Once noise reduction processes are
completed, many of the surface proper-
ties such as curvature and surface normal
see Appendix A), may be approximated
using one of two general approaches. In
the first method, a mathematical func-
tion, such as a spline, is fitted to the
data, and then the necessary partial
derivatives are solved at each desired
Data structures used in machine vision are often
classified as either ''iconic'' or ''symbolic''. An iconic
data structure is one whose principal organization is
that of a two-dimensional array. The elements of the
array may be bits, integers, real or floating-point
numbers, or more complicated structures. Each cell
of an iconic data structure is implicitly associated
with a location in two-dimensional space, by virtue
of its being indexed by two numbers. On the other
hand, a symbolic data structure, although it may
represent pictorial information, takes the form of a
scalar, list, graph, string, or table. Unlike an iconic
structure, spatial information, if it is to be included
in a symbolic data structure, must be explicitly repre-
sented, for example, by including coordinate pairs
in the structure. It is usual for iconic data structures
to be used for image data (including ''intrinsic
images'') and symbolic ones to be used for more
abstract information such as scene descriptions in
terms of regions and relationships, highly codified
shape descriptions, and semantic models or inter-
pretations.'ol
Corresponding to the distinction between iconic
and symbolic data structures in machine vision, there
isa separation of ''retinotopic'' and ''nonretinotopic''
descriptions of neural areas in the neurophysiology
of the mammalian visual system. A retinotopic area
is one in which the neural activity is generally in
spatial correspondence (i.e. mapped in continuous
fashion) with the activity in the retina. A non-
retinotopic area is one in which the activity shows
no general spatial dependence upon the distribution
of activity in the retina.
Several factors motivate the study of special iconic/
symbolic architectures. Most importantly, parallel
image-processing systems such as the CLIP4' and
the MPP,P while very effective for point-neigh-
borhood image transformations, lose much of their
speed advantages when they must communicate with
single-processor hosts in computations of such non-
iconic transforms as chain encodings, polygonal rep-
resentations, Hough transforms, region-adjacency
graphs, syntactic descriptions of shape, or schema
instantiations.
A second factor is the awkwardness of computing
''hypothesis maps'' for topdown image analysis on
existing architectures; such operations seem to call
for special symbolic-to-iconic hardware that can per-
form certain kinds of ''plotting'' very rapidly.
A third motivating factor is that a study of the
architectural problems of iconic/symbolic trans-
formations may suggest new computational models
for related information-processing activity in natural
(human and animal) visual systems, and conse-
quently, to improve our understanding of natural
vision.
We note that, in general, multiprocessor systems
exist which can support both iconic and symbolic
processing, but these are expensive, and are non-
optimal for iconic-to-symbolic processing. Examples
of these include the following commercial and
research prototype systems: the BBN Butterfly,
Columbia University's Non-Von, the NYU Ultra-
computer, the CalTech Cosmic Cube, and others. A
survey of some of these parallel systems may be
found in reference (4).
In the following sections we present a discussion
of the relative strengths and weakneses of special
architectures that support rapid iconic-to-symbolic
and symbolic-to-iconic transformations.
One can classify the proposed architectures for
iconic/symbolic processing into two types: (1) hard-
wired transformation devices and (2) interfaces. The
hardwired devices provide specialized computing
capability for particular iconic-to-symbolic trans-
formations. The interfaces, on the other hand, make
it easy for iconic processors and symbolic processors
to communicate efficiently and leave the actual pro-
cessing to these more conventional components.
Four particular approaches are reviewed in this
article. Two of these are hardwired transformation
devices and two are interfaces. Each of the four
approaches is presented here in a somewhat abstract
form. That is, each method is described with a com-
putational model which is somewhat simpler than
what one might actually implement. Nonessential
features are omitted from the models to facilitate
comparison and analysis of the basic approaches.
Each of our four models is based upon a proposed
device or system that has appeared in the literature.
Our ''Image-Function Inverter'' is modeled after the
''ISMAP'' (Iconic/Symbolic MAPper) proposed by
Kent at the National Bureau of Standards and sub-
sequently developed by Aspex, Inc. of New York.')
The ''Chain-Run Encoder'' described here is based
upon a device designed by Pfeiffer at the University
of Washington.''! (Pfeiffer is now at New Mexico
State University.) The ''Bi-Modal Memory'' system
has been proposed by Tanimoto, and the ''Tile-
Based Interface'' is a model inspired by the
''CAAPP/SNAP'' (a joint effort between the Uni-
versity of Massachusetts''-*' and the University of
Southern California''' which was an important step
in the development of the DARPA Image Under-
standing Architecture). The first two of these pro-
posals fall into the hardwired-device category. The
other two are essentially interface schemes.
At the time of this writing, one of these devices
has been built and tested. The ISMAP is the least
complicated and is now available as an option for
the PIPE from Aspex, Incorporated of New York.
Two others have had chips designed and fabricated
(the BMM and the chain-run encoder), but the chips
have not yet been integrated into an overall system.
These four approaches give one perspective on the
range of problems and possible solutions that arise
in studying the issue of iconic/symbolic architecture.
Figure 1 is a chart which offers an analysis of the
appropriateness of each of these four architectures
to the iconic-to-symbolic computations listed. It is
assumed that each iconic/symbolic architecture
would be used in the context in which it is proposed.
Later sections of this article describe some of the
algorithms involved in this analysis.
The transformations between iconic and symbolic
representations of pictorial information constitute
what may be called ''intermediate-level'' computer
vision. This general problem of intermediate-level
vision has been explored in a collection of papers,''')
with a working definition given in the first of them.''
The three levels of processing for computer vision
are therefore the following:
Although this report is concerned primarily with
the intermediate level, the other levels must be men-
tioned at least to the extent to which they constrain
or help to solve the intermediate-level problems.
One can organize a treatment of intermediate-
level computer vision by focussing on several selected
problems. Let us consider six:
terms of its runs. Such an encoding may consist of
the starting coordinates for a contour, and a list of
runs, each described by a symbol (direction) and a
repetition count. Alternatively, each run may be
described with a pair of endpoints (since the run
represents a straight line segment). Several vari-
ations are possible, making use of absolute or relative
coordinates, explicit or implicit coordinates, runs in
order or out of order, etc. A chain-run encoding may
be a preliminary step towards obtaining the chain
code, or it may be regarded as an alternate form of
the chain code which, for some images, may be more
compact a representation than the chain code.
4. Hough transform. In order to detect the pres-
ence of lines or curves in an image, the Hough
transform or a generalized form of the Hough trans-
form may be used. For lines, the Hough transform
works by quantizing the space of possible values of
the parameters for the line equation. Each ''bin'' of
the quantized space is regarded as an accumulator
which accumulates evidence for the presence of a
line with a particular pair of parameter values. For
each pixel of the source image, a set of bins is
identified and a weight is added to each bin; the
weight is proportional to the extent to which the
source pixel seems to be part of a line (e.g. how
bright the pixel is).
5. Partial Hough transform. Rather than compute
the evidence for each of all the possible lines in the
image,it is often sufficient to determine the evidence
for the most prominent (one or a few) lines. When
this is sufficient, much of the computation can be
eliminated.
6. Region-adjacency graph. Some kinds of
machine vision require that a symbolic description
of a scene be computed before the objects are ident-
ified in the scene. One basis for a symbolic descrip-
tion is a graph structure called a ''region-adjacency
graph'', The problem of computing a region-
adjacency graph (RAG) is to take a segmentation
map (a two-dimensional array in which each element
contains the unique region-number for the region to
which it belongs; each region is a four-connected set
of pixels), and to produce a graph having a single
node for each region, and having an edge between
two nodes if and only if the corresponding regions
are adjacent (share a common boundary). A region-
adjacency graph is typically represented using adjac-
ency lists,
These six problems form a representative sample
of iconic-to-symbolic transformations. We use them
here to compare the four model architectures for
iconic/symbolic computing.
Other sets of intermediate-level operations have
been suggested as a basis for comparing architectures
such as the set consisting of area, perimeter, con-
nected components, convex hull, closest points, and
diameter.A This set emphasizes problems of com-
putational geometry, which are, no doubt, of wide
interest. The set presented in this paper, however,
focusses on pixel-value/coordinate relation inver-
sion, chain encoding, line detection and sum-
marization of region adjacency. Such a set covers a
variety of image processing styles that are more
typical of picture-processing applications, and this
set is particularly useful in bringing out major dif-
ferences in the architectures under discussion.
Several strategies can be used to come up with
machines that can efficiently handle iconic-to-sym-
bolic transformations. One approach is to begin with
an iconic image processor and then augment it gradu-
ally, adding capability for more and more symbolic
or abstract computations. This approach has received
the attention of theoretical computer scientists and
has supported some interesting mathematical
results,%)
Another approach is to attempt to tightly integrate
radically different kinds of processors, each of which
is optimized for a particular style of processing. This
approach can produce powerful systems that are
practical and easier to program than totally new
architectures. However, they may be harder to char-
acterize theoretically in an elegant way. The com-
bination of a pipelined image processor with the IFI
is a good example of such an architecture. The BMM
together with an iconic subsystem and a symbolic
processor network is another.
A third approach is to try to make general MIMD
systems cheaper and more parallel than they are
now. Ifthe cost of the general-purpose systems could
be brought down far enough, and the parallelism
increased enough, they would be more effective for
iconic-to-symbolic transformations than they are cur-
rently.
Next is a presentation of several particular archi-
tectures, and descriptions of how they handle iconic-
to-symbolic data transformations.
A device that we shall call the IFI (Image-Function
Inverter) is modeled after hardware proposed by
Kent in conjunction with Aspex, Inc of New York
to lend extra capability to the PIPE (''Pipelined
Image Processing Engine'') system. That device is
known as the ''ISMAP'' (Iconic/Symbolic MAPper).
The ISMAP is consistent with the PIPE convention
that computations are organized according to the
video field rate (i.e, one field is processed in each
one-sixtieth of a second). Consequently, the ISMAP
(and our abstraction, the IFI) scans an image at the
frame rate, in its normal operating mode.
Since the PIPE is a system which transforms
images into images, it is an iconic processing system,
and it does well at low-level computer vision tasks
such as filtering an image. On the other hand, the
IFI is designed to compute more global features of
an image such as the counts one obtains in a histo-
gram of pixel values. The IFI is illustrated schem-
atically in Fig. 2.
The normal operation of the IFI consists of three
steps. In the first step, the source image is scanned,
and a histogram of pixel values is produced by the
IFI in a special memory buffer. The second step
produces a cumulative histogram from the normal
histogram. The cumulative histogram contains, in
each bin, the number of pixels of the source image
that have a value less than or equal to the bin index.
In the third step, the image is rescanned and for each
pixel value, the IFI makes a list of the coordinate
pairs at which that pixel value occurs in the image.
The cumulative histogram gives the starting address
for each such list.
The IFI is an ideal device for inverting the pixel-
value/coordinates relation. In a sense, it transforms
the image representation from a coordinate-oriented
one into a feature-oriented one. The principal motiv-
ation for this operation is to permit the host to search
only the list of potentially relevant coordinates when
seeking global geometric relations among selected
types of features. The IFI could be extended to
permit its mapping to be inverted', however, we
shall not consider these possibilities here.
In spite of its simplicity, the IFI appears to be a
rather useful device. Two novel algorithms which
use it are described later.
A device has been designed''' which would accel-
erate the conversion of binary images into chain
codes, in conjunction with a ''systolic'' cellular array
computer. Pfeiffer's device, consisting of a VLSI
chip that receives inputs from the systolic array,
treats one column of a binary image at a time. It
rapidly scans the column, identifying the pixels in
which the value 1 is found. If, in the sequence of
columns processed, the value in a row changes from
one column to the next, then the chip outputs an
indication that a horizontal segment either began or
ended in that row (a status bit keeps the state of each
run). It is intended that two to four of these chips
be used for one iconic processor: one processing
columns (as described), another processing rows,
and optically two more of them processing diagonal
lines. The outputs of the chips would be collected by
one or several Von-Neumann-style processors and
sorted into run-length-compressed chain encodings.
For purposes of comparison with other
approaches, we model this device so as to omit the
details of how columns are scanned. Our model,
referred to as the CRE (for Chain-Run Encoder)
simply takes N binary inputs in each of N steps and
outputs a list of events where each event is of the
form (i,e)where itells which of the N inputs changed
and e tells whether the change was from a 0 to a l
or from a l to a 0.
Let us consider how two or four CRE units would
be used with an iconic processor and a symbolic
processor to determine chain encodings of edges in
an image. In order that each of the four encoding
devices receive only ones for the pixels that form
parts of edge segments in the appropriate direction,
the iconic processor must produce four separate
binary images, one for each direction. Figure 3 shows
how two chain-run encoders could be integrated
with a cellular-array processor and a collection of
microcomputers. It is possible to use a single chain-
run encoder for runs in all four directions (sequen-
tially) by providing additional switches and inter-
connections to permit either row data or column data
to be fed into the device.
In order to provide a high-bandwidth interface
between a parallel image processor (such an MPP or
CLIP4-like system, or a pyramid machine''') and a
collection of microprocessors, a ''bimodal'' memory
system has been proposed' ?l that would support
image-wide, parallel transfers with the parallel pro-
cessor, and ordinary byte-at-a-time transfers with
the microprocessors. By suitably partitioning the
memory, many microprocessors could access por-
tions of the memory simultaneously, and the parallel
processor could access a portion while most of the
microprocessors access portions. A clean implemen-
tation of the bimodal memory requires a memory
chip not commercially available at present. An MPP
or Pyramid, plus BMM, would provide a system
that is powerful, yet conceptually simpler than some
proposed extentions to pyramids and arrays.'''l At
the same time, it is less expensive than purely MIMD
systems such as the Ultracomputer,' ? The BMM is
shown between an iconic cellular array and a col-
lection of microcomputers in Fig. 4.
What the BMM permits is a flexible scheme for
sharing work between a parallel image processor and
a collection of microprocessors. One can imagine
that the images are stored in shared memory that is
accessible to the parallel image processor at the same
rate that the image processor's ordinary memory is
accessible. Similarly, the shared memory is accessible
to a microprocessor as fast as, and as simply as if it
were ordinary memory on the bus.
In order for such a system to compute a histogram
of an image several methods are possible. One
method uses the parallel array to make several copies
of the original image and store them in different
portions of the bimodal memory; then each micro-
processor scans a portion of its copy of the image,
forming partial histograms. Finally the micro-
processors combine their results through an
additional communication network (or bus), sum-
ming the component histograms to obtain a global
one. A variation of this method is to have each
microprocessor produce certain bins of the histogram
(which nonetheless requires each micro to scan the
entire image) and then for the sequences of bins
to be concatenated into a full histogram. Another
algorithm gives some of the responsibility for
accumulation to the parallel array. If the number of
bins is small, it may be better to have the parallel
array compute the bin values over neighborhoods of
size, say, 16 x 16. The partial histograms could be
stored in the same 16 X 16 subarrays, one bin per
cell. Then copies of this ''histogram image'' would
be placed into different portions of the BMM, where
each microprocessor would combine the sums for
one or several of the bins of the global histogram.
This requires each micro to look at only one cell (or
a small number of cells) in each 16 x 16 block of the
1mage.
Other iconic-to-symbolic transformations can be
performed by dividing up the image (or making
copies of it) and letting each micro transform a part
of the image.
The ''Tile-Based Interface'' that we describe here
was inspired by ''Interface Array Processor'' pro-
posed in a joint study by researchers at the University
of Massachusetts at Amherst and at the University
of Southern California.
The ''Content-Addressable Array Parallel Pro-
cessor'' is an MPP-like system that was developed at
the University of Massachusetts at Amherst. The
VISIONS group at UMass has proposed to interface
the CAAPP to a device that was designed at the
University of Southern California called the ''Sem-
antic Network Array Processor''(SNAP). The SNAP
consists of an array of modules each consisting of
a microprocessor and an associative memory. The
proposed IAP interface gives each SNAP module
access to a rectangular window of the 128 x 128 or
larger array handled by the CAAPP. The IAP itself
includes processing elements, memories and a pro-
grammable crossbar switch. The IAP is a more com-
plex scheme than the TBI one that we describe here.
The TBI is a model which substantially simplifies
the IAP. The TBI is more readily compared and
contrasted with the other three models in this paper.
The TBI omits the crossbar switch and the memory
and processing elements specific to the IAP. What
remains is a coupling of symbolic processors to zones
of the iconic array. These zones are a partition of
the image array into squares. The TBI scheme is
illustrated in Fig. 5.
The TBI team could extract a line drawing from
an image by having the CAAPP perform edge detec-
tion, spurious edge removal and gap filling. The
SNAP modules would then scan their windows for
edge segments, track them, and chain code them.
Higher-level processes would then take the local
chain-coded segments and merge them into a more
global description and approximate the chain-coded
segments with line or curve segments.
One can propose to build special hardware for any
of the infinite number of possible transforms that
map information from the space domain into a trans-
form domain, and it is expected that future work
will treat more of this kind of hardware. However,
because of its importance to computer vision, an idea
should be mentioned for a device to rapidly compute
the Hough transform of an image. Ballard and
Brown'% have suggested that a device with many
processing elements and an even larger number of
interconnections could allow the evidence-gathering
(or ''voting'') steps of the Hough transform com-
putation to be done in parallel, greatly reducing the
amount of time required to obtain the result. The
device would be difficult to build efficiently with
existing VLSI technology. However, the scheme is
given as a plausible model for line detection by
biological vision systems.
Each scheme for iconic/symbolic processing
described above is intended to be part of a larger
environment, generally including an iconic sub-
system and one or more Von-Neumann-style
machines. However, the proposed interconnection
schemes are all different.
1. The IFI takes a pixel stream from the PIP1)
and writes to a VAX-like host through a DMA port.
2. The chain-run encoder sits at the side of a
systolic-array image processor'' and outputs to one
or more Von-Neumann-style machines.
3. The BMM system is directly coupled by a
highly-parallel ''vertical'' bus to a cellular parallel
image processor, such as the proposed pyramid com-
puter, and also coupled by ''horizontal'' busses to
several conventional microprocessors.
4. The TBI architecture is a particular inter-
connection of the Content-Adressable Array Parallel
Processor and the Semantic Network Array Pro-
cessor (the proposed interconnection is organized
according to a windowing partition of the image
space).
In comparing the four proposals, an important
observation is that the IFI and the chain-run encoder
were designed as devices to speed up relatively
specific transformations, whereas the BMM and the
TBI architectures were designed as more general but
more costly interfaces that leave most of the actual
iconic/symbolic computation to conventional micro-
processors. However, the specific transforms of
feature/coordinate inversion and chain-run encoding
are operations that find a variety of uses and are not
as limiting as they might seem at first.
Other problems for iconic-to-symbolic processing
include connected-component determination, tex-
ture description and shape description. The con-
nected component problem is one of particular
interest because of its utility as a preliminary step to
shape description, blob counting, region-adjacency-
graph creation and other operations.
The sketches of algorithms in the previous section
give a general basis for comparing the architectures.
In this section, two algorithms are presented in
greater depth for three purposes:
1, to illustrate more clearly the nature of algor-
ithms for iconic/symbolic architectures by showing
the interplay of the iconic, iconic/symbolic, and sym-
bolic components of these algorithms;
2. to show how a problem can be re-expressed
to yield an efficient solution on an iconic/symbolic
architecture; and
3. because the algorithms are novel and may find
applications to, or lend insight into solving problems
that arise in the future.
We now give an algorithm for partial Hough trans-
formation which employs the PIPE and the IFI in
combination with a Von-Neumann host machine. It
would run on other pipelined systems to the extent
that they can support both the neighborhood opera-
tions (which are effectively handled by the PIPE)
and the global summarization operations (here
handled by the IFI). The algorithm makes the fol-
lowing assumption about the lines to be detected:
the local strengths and slopes (directions) of the lines
can be detected using local-neighborhood opera-
tions. The algorithm works as follows: an edge or
line operator is applied by the PIPE to the image to
obtain a line-strength image. This is thresholded and
used to select areas for slope determination. The
slopes are determined using local-neighborhood
techniques. The IFI is then used to make a histogram
of the slopes. The class of slopes with the most entries
is determined (by a host processor) and a new image
is produced in which pixels where this slope occurs
have value 1 and all others have 0. This image is
used as a ''region-of-interest'' selector, and masks
the next step (intercept computation) so that only
pixels in the region of interest are processed. The
intercept at each pixel of interest is computed using
two ''coordinate images.'' The x coordinate image is
an 8-bit deep image in which each pixel has its x
coordinate represented to 8 bits of accuracy. The y
coordinate image is similarly defined. Using the line
equation y ax + b, the intercept is given by b =
y - ar. The PIPE computes b by multiplying a by x
(using the 12-bits-in lookup table) and then sub-
tracting the product from y, using a 16-bit ALU.
Finally the IFI makes a histogram of the intercepts,
and the host determines the most frequently occur-
ring values. This gives the intercepts which go with
the slope already chosen, and together they give the
parameters for a prominent class of parallel lines in
the image. Successive prominent lines may be found
repeating the process for other slope classes. One
may find parallel lines by selecting the lesser peaks
in the histogram of intercepts, or one may go back
to the slope histogram and work with lines at a
different slope. This algorithm requires only a fixed
number of field times to determine a prominent line
or class of lines in the image. This scheme may
also be used with the polar line representation p =
x cos 8 + y sin 8 to avoid the problem with vertical
lines having infinite slope.
It is interesting to note that the host could easily
scan one of the lists of coordinates of pixels found
to lie on a particular prominent line, and determine
the extreme values, thus finding the two endpoints
of the shortest line segment that includes all of the
pixels in the class. With an additional step, consisting
of sorting the pixels byx coordinate (or alternatively,
by y coordinate), the component segments could all
be determined.
Let us now describe one of the possible ways in
which the Bimodal Memory may be used in com-
puting iconic/symbolic transformations.
In the following sketch of an algorithm, a classical
Hough transform is computed and its principal peaks
identified.
Step 1. In the iconic subsystem, a local edge detec-
tor is applied resulting in an edge image in which
each pixel has a magnitude corresponding with the
strength of the strongest edge passing through that
pixel in any direction.
Step 2. In the Bimodal Memory, m copies of the
edge image are produced. (Assuming that the mag-
nitudes are represented as 8-bit bytes, this requires
8 machine cycles, regardless of the value of m.)
Step 3. In the symbolic processing subsystem, each
processor reads its own (1/m)th of the edge image
and for each of these pixels, it plots and accumulates
votes in a parameter-space array which is another
layer of the Bimodal Memory accessible to this pro-
cessor. There are O(N) votes for each of the N /pm
pixels, and this step thus requires O(N /m) units
of time to complete. Since each processor works
independently and accesses its own layers of the
Bimodal Memory, there is no contention or inter-
action among the m processes.
Step 4. The Bimodal Memory does nothing now,
but it holds m different ''versions'' of the parameter-
space array. The iconic subsystem accesses each of
these versions in turn, adding and accumulating the
votes at each position in a single parameter-space
array, This requires m vertical-mode accesses from
the Bimodal Memory and m - 1 image additions.
Step 5. In the Bimodal Memory, the final par-
ameter-space array is replicated m times, with one
copy for each symbolic processor.
Step 6. In the symbolic subsystem, each processor
searches its own (1/m)th of the parameter-space
array for local minima. This requires O(N)/m time
assuming that the number of local minima desired is
a constant.
Step 7. Finally, the symbolic processors merge
their lists of peaks to get an overall list of peaks.
This requires O(log m) time, assuming again that the
number of peaks desired is a constant, and that
the symbolic processors are connected together in a
network of O(log m) diameter, such as a binary tree
or a hypercube.
The bottleneck in this algorithm is most likely to
be in Step 3, since, for reasons of cost, m is likely
to be small in comparison with N, A reasonable
assumption is that m would be on the order of N,
so that the computational complexity of Step 3 is
o(N).
However, there are many practical ways to further
reduce the computational effort required in Step 3.
It is usually the case that the edge pixels in an image
make up only a small fraction of the image. By
restricting the voting to only edge pixels whose mag-
nitudes exceed a threshold, approximately another
factor of N may be shaved off the expression, leaving
us with an O(N) effort. This effort may be further
reduced by adding in Step 1 the production of edge-
directional information-the approximate angle at
which the apparent edge passes through the pixel.
In Step 2, the angle image is replicated as well as the
edge-magnitude image. In Step 3, the edge angle
information for a pixel is used to limit the number
of votes cast by an edge pixel. Only lines at approxi-
mately the angle given may receive votes. If we limit
the number of votes per edge pixel to O(log N) votes,
then the amount of time required by Step 3 is reduced
to O(log N).
There is an interesting algorithm for computing
the region-adjacency graph that uses the PIPE, the
IFI and a host processor. The algorithm begins with
a segmentation map in one of the PIPE frame
buffers. It is assumed here that the number of regions
is small enough that unique region numbers can be
assigned to each region, and still fit in the 8-bit deep
PIPE frame (clearly no more than 256 regions can
be handled). Actually, the algorithm reserves region
number 0 for special use, and so a maximum of
255 regions are allowed. This algorithm builds an
adjacency list for each region in the image, and it
requires a fixed number of PIPE field times for each
adjacency list. In order to compute the adjacent list
for region k, the segmentation map is used to produce
an image of adjacency information for region k. This
image is the result of taking the segmentation map
and setting to 0 the value of any pixel which does
not have a neighbor (i.e. a pixel to the north, south,
east or west) in region k. The IFI then makes a
histogram for this adjacency image. Bin j has a
nonzero value if and only if region j is adjacent to
region k. From this histogram, the IFI then makes a
cumulative histogram. The ordinary histogram and
the cumulative histogram form a structure that the
host can scan in an amount of time proportional to
the length of the adjacency list for region k, rather
than the maximum number of regions one might
have. As the host examines this structure, it builds
the adjacency list for region k. It has been con-
jectured by Shneier and by Kent that PIPE and
IFI can process more than one adjacency image
simultaneously, further reducing the time required
to obtain the region-adjacency graph. If the con-
jecture is true, the maximum permitted degree of
the RAG may be reduced, and the maximum number
of regions permitted in the input image may also be
reduced; however, additional parallelism would be
gained.
The four iconic/symbolic architectures reviewed
here fall into two categories: hardwired devices (IFI
and the chain-run encoder) and interfaces (BMM
and the TBI). Notably missing are architectures that
are at once active computing components and pro-
grammable. Such a system might allow, for example,
the rapid computation of a variety of symbolic (or at
least non-iconic) transforms of images under pro-
gram control. Presumably, such an architecture
would contain several processing elements and an
interconnection scheme especially appropriate to the
class of transform it is to compute. Some possible
transform classes are these: generalized Hough trans-
forms, global image statistics, and matching geo-
metric models to image data.
As mentioned previously, there are several ways
toward the objective of high-speed iconic-to-sym-
bolic computation. One may begin with an iconic
structure and generalize it to make it more efficient
for symbolic work. One may take a collection of
symbolic processors and attempt to integrate them
in a way that aids iconic processing, or one may take
both kinds of processors and develop an efficient
interface between them.
The two most important parameters in the design
of this kind of system are performance and cost.
Performance may be evaluated in terms of speed and
generality, while cost may be evaluated by assessing
hardware complexity, hardware size, component
prices, and programming difficulty.
In order to increase speed, one can increase par-
allelism, elaborate the interconnection network,
and/or use higher-performance processing elements
and memory, generally all at increased cost. To
increase the generality of the system (i.e. widen the
class of transforms that can be efficiently computed),
the interconnection scheme may be enlarged, the
processing elements may be given more autonomy
(e.g, by changing from an SIMD system to an MIMD
one), and/or the speed of the system components
increased.
There need not be great complexity in an efficient
interface. The two interface schemes that have been
presented (the BMM and the TBI interface) may
be viewed as relatively conservative proposals for
iconic/symbolic systems, because they employ well-
understood processing components and fairly simple
interconnection schemes. They are substantially sim-
pler than, say, the MPP staging memory.'')
The size of an iconic-to-symbolic subsystem
impacts its cost strongly. The IFI has a constant size
for increasing image sizes, except that it may need
to address pixels or list items in a larger memory.
On the other hand, the size of a BMM increases as
the product of image size and number of layers
desired. The TBI interface size is directly pro-
portional to image size if the ratio of pixel processors
to semantic network processors is kept constant.
The chain-run encoder needs an amount of circuitry
proportional to the length of a row of the image, and
this isinexpensive for large arrays in comparison with
the interface schemes. To summarize, the greater the
parallelism across the iconic/symbolic gap, and the
greater the flexibility in programming those trans-
fers, the greater is the cost of the system.
Designing an algorithm for a system that includes
separate components for iconic, symbolic, and poss-
ibly iconic/symbolic processing presents a different
kind of challenge than does designing for a con-
ventional machine. Because an iconic processor
handles images very well but may be very slow for
symbolic operations, and because a symbolic pro-
cessor has just the opposite characteristics, it is very
important to factor a problem into its iconic and
symbolic components. These problem components
must be separated and distributed on the system, but
coupled in a way that allows adequate cooperation
between the iconic and the symbolic activities.
If it is not obvious where to draw the line between
iconic and symbolic parts of a problem, there are
several criteria that can be called upon to help.
Operations that work on local, rather than global,
neighborhoods are apt to be iconic. Operations that
admit high degrees of parallelism, rather than requir-
ing long sequences of steps, are more likely to run
efficiently on an iconic processor than a symbolic
one, particularly if the SIMD rather than the MIMD
variety of parallelism can be applied. Perhaps most
obviously, transformations that require images as
most of their working representations are more likely
to run well on iconic processors than are trans-
formations that are primarily list-based.
Some of the researchers who work with MIMD
systems may feel that since an MIMD system may
incorporate a large number of processors and a
reasonably general interconnection network, there
is little need to develop special systems for iconic/
symbolic processing. We feel that the specialized
architectures will probably not only cost less than
the MIMD systems that might be offered to fill
similar needs, but outperform the MIMID systems,
as well, This is because a specialized device can have
a precisely-tailored interconnection structure that
makes parts of an algorithm implicit whereas an
MIMD (having a ''general'' interconnection scheme)
would have to emulate the special interconnections
incurring a time penalty.
The problem of providing a way for a computer
vision system to rapidly transform an iconic rep-
resentation into a symbolic one is a challenging one
that is clearly of importance for the development of
machine-vision technology. An iconic-to-symbolic
transformation is one in which the input is in the
form of a two-dimensional array (where position
information is implicit) and in which the output is in
the form of a list, vector, graph, table, or array of
parameters in which the array indices of a parameter
are no longer an indication of location in the input
image. Typical transformations map images into lists
of line segments or region descriptions in terms of
sizes and adjacencies (or other features), or they
produce lists of locations at which particular local
features are found in the images.
Specialized computing hardware for iconic/sym-
bolic computations has thus far exhibited two forms:
hardwired devices for particular transformations and
high-bandwidth interfaces between separate iconic
and symbolic processor subsystems. Four proposals
for specialized hardware have been discussed and
compared with respect to a set of image analysis
problems.
The designers of algorithms and of hardware for
iconic/symbolic computing face many challenges.
However, the possibilities for interesting new
methods and devices appear to be wide open and
exciting.
Compared to structured environments, navigation in
an unstructured environment such as cross-country terrain
presents a significantly different set of problems for a
perceptual system. While recognition of man-made objects
performs adequately by utilizing rectilinear surface features,
color characteristics, or other well constrained surface
properties, there are no comparable structural expectations
available in natural terrain. It is reasonable for indo or
mobile robots to project a route using a mobility map that
includes obstacles identified as occupying vertical space
and assumes that the floor is flat. However, meaningful
objects in natural terrain are more diverse and difficult to
characterize. Specific features such as local slope, three
dimensional edges, or color of specific regions may or
may not represent obstacles. In general, the features are
difficult to extract and difficult to combine to form reliable
descriptions of the terrain. For example, trees can have
widely varying shapes, colors, and sizes making tree
recognition in itself a formidable problem. The
recognition problem is compounded by seasonal and
weathering effects, and the need to interpret the
environment in a timely manner.
The goal of the Knowledge Based Vision Techniques
(KBVT) research at the Hughes AI Center is to provide the
necessary perceptual descriptions of the environment
which allow an autonomous vehicle to successfully
navigate amongst obstacles. It is also our goal to transfer
technology and perform experiments on-board a vehicle.
The latter goal has constrained our research in the past
year to approaches and algorithms which could ''perceive
obstacles in a timely manner; that is, the perception
processing has to allow ample time for the vehicle to
safely respond to obstacles. An important feature of the
perception system is its evolution in conjunction with the
planning system also developed by Hughes AI Center.
Our approach defines a system architecture which
supports a decomposition of the problem directed by
immediacy and assimilation considerations [4,5]. A brief
overview of the architecture is included as an introduction
to the technology development and experimentation
results.
In this hierarchical control structure as shown in
Figure 2, the vertical structure generally reflects the level
of information fusion. Higher levels function on the basis
of highly assimilated data that are generally symbolic and
require longer interpretation time and larger spatial area;
the lower levels exploit more immediate data. A failure at
one of the lower levels is signalled to the next higher
level, which then re-assesses the situation and adjusts
accordingly.
For the ALV experiments a subset of the
hierarchical control system was used. A simple mission
was defined by start and goal locations with the path
constrained to maintain a direct line-of-sight with the
communications tower. At the route level of the hierarchy
a map-based planner was used to generate all experimental
routes. A route consisted of a set of subgoal points. Our
major emphasis in the past year was to develop and
experiment with the lowest level of the hierarchy. At this
level, virtual sensors and reflexive behaviors (or
behaviors) are used as the real-time operating primitives
for the rest of the system. Knowledge assimilation is
minimized in order to provide the fastest possible vehicle
response. Virtual sensors are sensing and processing units
that detect specific environmental features and relay
information about features to the reflexive behaviors. A
virtual sensor is ''contracted'' by a behavior to provide
information at a required processing rate and accuracy. The
reflexive behaviors are highly procedural units that operate
on virtual sensor data to provide real-time control. Virtual
sensors and reflexive behaviors are grouped into activities
so that multiple behaviors can operate concurrently to
produce control decisions. These activities are scheduled
by the local planner to achieve current goals.
No single reflexive behavior and virtual sensor
combination is ever expected to be able to handle vehicle
navigation problems in general; but rather, several are used
in conjunction, each designed to handle a specific sub-
problem within the overall range of navigation tasks, It
is the responsibility of the local perception and planning
modules to guarantee that the selected activities are
appropriate for the current environment. Our recent
experimentation on-board the ALV has shown the concept
of virtual sensors and reflexive behaviors provide a viable
approach to local vehicle control.
In the context of cross-country navigation, we have
defined an obstacle as any region a particular vehicle
cannot traverse. This definition allows obstacle detection
to depend upon the mobility characteristics of the vehicle.
Therefore, a natural approach to perception for cross-
country navigation models the vehicle's interaction with a
three dimensional representation of the sensed terrain to
determine traversability. The AILV is equipped with a laser
range scanner which measures the distance along the line
of sight to the nearest object. This sensor inherently
supplies information of surface geometries; however, the
interpretation of this information is difficult. Several
methods to interpret surface geometry in terms of an ALV
mobility model were developed that employ a down-
looking Cartesian Elevation Map (CEM). In addition,
methods were developed to weight obstacles such that
potential paths are penalized in areas ''rich'' with
obstacles. These methods are briefly described in the
following discussion. We also mention briefly parallel
considerations and implementations. A separate paper is
recommended for a more complete description of these
techniques [1].
The Cartesian Elevation Map (CEM) is a
representation for range information in which data from
the viewer centered coordinate system of the sensor is
transformed into a Cartesian z(x,y) coordinate system.
This results in a down-looking map view representation of
terrain which is useful for autonomous navigation. This
same representation may be obtained from other depth
sensors, such as stereo or sonar.
The development of the CEM required us to deal
with a variety of range processing issues. As one would
expect, the elevation data represented in the CEM are
highly oversampled in the immediate foreground and
undersampled at greater distance from the sensor. In
addition, there are some regions in the CEM that fall
outside the field of view of the scanner and other regions
that fall behind (in the shadows of) tall features in the
terrain. In the CEM, areas with sufficiently dense
sampling of points are fitted with a continuous surface and
undersampled areas are explicitly labeled ''unknown''.
Due to the limited vertical field of the laser range
scanner (30 degrees), terrain immediately in front of the
sensor is not visible. The closest scanned ground in our
experiments is approximated 13 feet in front of the
vehicle. We have investigated the fusion of data from
previous CEM's to fill in this unknown area. One method
uses the orientation sensors on board the ALV (heading,
pitch, roll, x, and y) together with an estimate of change
in the z-position to determine vehicle motion in all six
degrees of freedom. We are currently investigating another
method that recovers the motion directly from sequences of
range umages.
We have developed a relatively sophisticated three
dimensional model of the ALV. This model together with
the CEM yields a formal definition of obstacles, thus
avoiding ad hoc and incomplete definitions. The model is
currently represented by minimizing the energy of the
suspensions springs associated with each wheel as the
vehicle is applied at a position and orientation in the
CEM. Three types of obstacles are detected with the
vehicle model: suspension, slope, and clearance as shown
in Figure 3. This definition of obstacles has performed
fairly reliably in our experiments on the ALV. The model
will be extended to include constraints such as vehicle
weight distribution, weather conditions, risk factors, and
vehicle speed and dynamics.
The vehicle model allows us to produce a three
dimensional traversability map by applying the model at
each possible position and heading. In most cases, a
complete traversability map of the entire sensed area is not
needed. Because of constraints on perception processing
time, we have developed techniques for applying the model
only at those points necessary to fulfill requests issued
from the planner. This method, called the Vehicle Model
Trajectory (VMT) virtual sensor, simply calculates the
projected heading of the vehicle at each point along a
linear trajectory and applied the model at that heading and
location until it either reaches the end of the trajectory or
assumes an unstable configuration. The virtual sensor is
contracted with the behaviors to return the distance
travelled (or ''safe distance'') and the reason for
termination, The VMT virtual sensor can provide different
levels of accuracy and processing speed by varying three
parameters independently: CEM size and resolution,
vehicle model accuracy, and sampling frequency of the
range image.
In the recent ALV experiments, the VMT virtual
sensor was used along seven linear trajectories. The total
processing time from image acquisition to trajectory output
was approximately six seconds on a Sun 3 with a floating
poumnt coprocessor.
The current implementation of the VMTs uses only
linear trajectories although the Hughes planner controls
the vehicle through speed and turn rate commands which
result in curved trajectories. We plan to generalize the
VMT virtual sensor for any given trajectory; Figure 4
shows curved trajectories. Other enhancements are
discussed in [1].
The current VMT strategy simply tells the planner
how far the vehicle may go along a given trajectory. The
planner has no idea how close the trajectory passes by an
obstacle. IIn simulation and in real life, the vehicle tends
to pass too close to obstacles since no part of the current
system deals with side clearance.
Like hhe CEM, he Cartesian Weight Map (CWM) is
a down-looking map in the local coordinate system of the
vehicle. A pixel value in the CWM represents the weight
identified with the cost of traversing the corresponding
pixel in the CEM. Nontraversable obstacles found using a
Gradient of Gaussian (GoG) technique and verified by the
vehicle model are given an infinite weight in the CWM so
that no path will ever travel through an obstacle. To
solve the problem of the vehicle passing too close to
obstacles, all other CWM pixels are given weights which
exponentially decay with distance from the nearest
obstacle.
The CWM could be extended to penalize areas which
are ''bumpy'' and reward regions that are smooth. This
concept of multiple virtual sensors concurrently updating
the CWM is consistent with our hierarchical system
design. We have tested the CWM in simulation and found
that the technique safely guides the vehicle equidistant
between obstacles and avoids small cul-de-sacs.
A great advantage of the CEM and CWM methods is
that they are extremely parallel. In cooperation wih MIT
in March, 1987, we implemented (during a two week
programming spree) the CEM construction algorithms and
the GoG detection technique on the Connection Machine.
Creating a 128x128 CEM from a range image took less
than 0.5 seconds with unoptimized Lisp code and intensive
floating point calculations; the GoG required an additional
8 milliseconds of compute time. We expect these times
will be significantly reduced wiUh the CM-2.
In addition, we have explored implementing the
CEM on a WARP. Since the pixel in the range image that
contributes to a given location in the CEM is data-
dependent, the entire range image is distributed to each of
the 10 cells in the WARP array. The CEM is then divided
into 10 column swaths with several columns overlap
between each swath. Each WARP cell processes every
pixel in the scan, but saves only those points that fall
within its assigned column swath. Progress on the WARP
implementation has been delayed while we await the new
release of WPE 2.6.
An implementation design similar to the WARP
implementation has been designed for the Hughes
Hierarchical Bus Architecture (HBA) [6]. The HBA is
available for use in the lab simulation environment to
improve simulation throughput.
The planning system for obstacle avoidance is
designed to provide real-time vehicle control while
maintaining the flexibility needed for operation in realistic
environments. Because the primary objective of the cross-
country navigation experiments w as to test critical real-
time perception and planning interfaces, the map-based and
reflexive behavior modules were the primary focus for
development. More detail of the planning system are
included in these proceedings [2].
At the route level of the hierarchy resides the map-
based planner providing route information obtained from
digital terrain maps. The route planning data includes
maps of landcover, elevation, hydrology, roads, and
landforms, and also data, such as visibility from the
vehicle to the communications shell, that was derived from
these maps. The map-based planner generates all
experimental routes; a route consists of a set of subgoal
point locations.
The reflexive planning module is tasked with
achieving the subgoals received from the map-based
planner. The execution of the path requires the planner to
react to perceived information, but do so in a consistently
reasonable manner (i.e., intelligently). Reflexive
planning controls the vehicle within its immediate
environment with minimal data assimilation.
The behaviors used in the ALV experiments respond
to VMT virtual sensor. The interface between the
perception, planning, and vehicle control is critical. For
instance, planning to avoid an ''unknown'' area changes
dynamically as more information is perceived or as known
obstacles are detected. In addition, as obstacles pass
below the range scanner field of view a method is
necessary to determine when the vehicle has traveled
completely beyond the obstacle bounds. Other timing
related issues must also be addressed; such as, when the
vehicle reaches the end of a VMT it must slow down and
stop if necessary to wait for new VMT data.
For the first set of experiments, the behaviors were
grouped into two activities: the first activity was designed
to travel toward a goal when the vehicle was in a clear
area, and the second activity was designed to control the
vehicle when obstacles were present. For the recent
experiments, the behaviors were incorporated into a single
activity that used a technique which weighted the
importance of the goal according to the difficulty of the
terrain. This is desirable because as the vehicle's
movement becomes more restricted it becomes more
important to get clear of the rough area than to make
progress directly toward the goal. In the case when the
vehicle is in an area free of obstacles, the goal weight
becomes predominate and the vehicle tends to head straight
toward the goal.
Simulation plays a critical role in the development
and analysis of our perception and planning systems by
allowing us to discover and resolve many discrepancies
before attempting experiments with the ALV in the field.
In addition, it provides the essential link between vehicle
control commands and new perceptual inputs, The ultimate
objective of these simulations is to close the loop between
sensing and acting; that is, between the perception,
planning, and vehicle navigation systems. By simulating
the terrain, the sensor, and the vehicle, we are able to test
the efficiency, correctness, and usefulness of the virtual
sensors and behaviors as they are developed.
To provide an accurate model of the ALV terrain, we
extract a portion of the five meter resolution map
elevation data from ETL and interpolate a smooth surface.
Also we allow specific objects such as contours, ramps,
plateaus, walls, cliffs, and ravines to be inserted in terms
of elevations. The resulting surface defines the basic
structure of the underlying ground. We place cultural
features such as bushes, ditches, rocks, and grass over the
ground surface data. Finally, additive noise is applied to
''randomize'' the terrain. Interpolation schemes are used to
smooth areas or produce gently sloping terrain. The
resulting terrain provides both a source of data for the
synthetic scanner and the surface on which the simulated
vehicle moves.
To simulate the laser range scan, we apply a ray-
tracing algorithm in the synthetic terrain to produce a
synthetic range scan from any vehicle position and
orientation. The synthetic scanner reproduces many of the
image artifacts observed in actual scans, including those
due to vehicle motion during image acquisition. Using the
simulation, we are able to analyze the effect of the sensor
depression angle upon obstacle detection.
The vehicle simulation includes parameters for
vehicle dimension,the vehicle sensors for location and
orientation, and vehicle dynamics such as acceleration and
braking. We can simulate collision by applying the
vehicle model. In addition, we have simulated the actual
MMACJA1LV control algorithm. This was necessary to test
the conversion of the control algorithms based on speed
and turn-rate control used by Hughes into the vector
control algorithms used at MMAC. With this simulation,
we can evaluate the effects of time delays between
command and vehicle action.
The interface simulation also includes the ability to
mimic the software and hardware configuration at MMAC.
The full simulation requires the use of several Symbolics
Lisp machines and Sun workstations joined via pronet
interfaces. This environment identifies and resolves many
inconsistencies and shortcomings in the system interfaces
and load distribution without consuming valuable time at
the test site. Most importantly, such preparation allows
us to perform meaningful and successful cross-country
experiments within a relatively short experimentation
period at MMAC.
The experiments run on the ALV were designed to
determine the feasibility of cross-country terrain
navigation and to demonstrate the Hughes perception and
planning software. The experiments accomplished both
objectives [7].
The cross-country experiments required the
integration of multiple computers distributed on-board the
vehicle and in the ALV lab as shown in Figure 5. The
perception system resided on two Sun workstations on-
board the vehicle; one Sun was used for the virtual sensors
the other was used to archive data and experiment records,
The planning system resided on two lLisp machines in the
ALV lab: one Symbolics ran the reflexive behaviors, the
other was employed for map-based planning. In addition,
vehicle control was interfaced (by MMAC) through an Intel
computer.
For the cross-country experiments, we chose a site
rich with obstacles and potential paths for obstacle
avoidance. The area is a hillside consisting of steep
slopes (some over 15 degrees), rock outcrops, large scrub
oaks, very small junipers (15 inches high). as well as a
narrow sign post (post approximately 2 inches wide and
sign approximately 4 inches by 6 inches). In addition,
the area includes a complex set of gullies and ravines
caused by rain and snow runoff These gullies span a range
of widths from 6 to 25 inches and depths from 4 to 30
inches. A narrow area with a very constrained approach
angle is available to cross the area of gullies. The soil is
loose and sandy in the vicinity of the gullies resulting in
mud and vehicle slippage during the December
experiments. We also set up an obstacle course on a flat
field at the far end of our experimentation area. The
difficulties associated with this test area are sufficient to
require caution by a human driver.
In general, the perception system adequately
perceived the environment in that the obstacles were
detected. The VMT virtual sensor was used with seven
trajectories applied on every other pixel in a CEM with 1
foot per pixel resolution, We experienced the greatest
difficulty with gullies and rocks ''hidden'' in the grass. The
planning system utilized the VMT information to
successfully navigate around obstacles; the average speed
for the recent experiments was 3 km/hour. Specific
experiments and paths are described in greater detail in
another paper in these proceedings [2].
Autonomous cross-country navigation with obstacle
avoidance was successfully demonstrated. It is the first
such demonstration which integrated map-based and sensor-
based vehicle control. It also demonstrated the feasibility
of an experimental system operating with conventional
computing hardware located both on-board the ALV and
remotely in the ALV laboratory. The concepts of
behaviors and virtual sensors have been shown to provide
a viable approach to local vehicle control, responding
reliably to the dynamic conditions of the real world.
lLastly, it is significant that Hughes was able to
accomplish so much with relatively little vehicle time
(with only 2 weeks for he first experiment and 1 week for
the second). Much of this success must be attributed to the
preparation at Hughes through extensive simulation.
These experiments represent significant technological
progress for autonomous vehicles.
The interpretation of large high resolution images
with highly variable backgrounds can be facilitated by
examining features extracted from multiple resolutions of
the image. The objects of interest are modeled at each
resolution in terms of features that can be used to provide
evidence for the objects. By examining lower resolutions
during the initial stages of image interpretation, object
hypotheses can be made based on large, prominent
features. Given these initial hypotheses, higher
resolutions are examined only in those areas in which
objects of interest are expected.
An object is modeled according to its expected
characteristics in the image using two kinds of features:
salient features that create initial object hypotheses, and
supporting features that provide evidence for the
hypothesized object. At each resolution, hypotheses are
generated in two ways: first, a hypothesis may result from
a feature that is due to an instance of the object, and
second, a hypothesis may result from objects that already
exists at either the same or a lower resolution. In either
case, the object creates hypotheses in accordance with the
model that specifies the confirming evidence.
The image interpretation system does not follow an
algorithmic approach, but instead chooses procedures based
on the current state. The approach is both data-driven and
model-driven, utilizes hypothesis generation and
verification, and employs evidential reasoning to evaluate
the hypotheses. The system adheres to the principle of
least commitment in two ways: 1) object hypotheses
occur only if there exists supporting intrinsic feature
properties, and 2) final interpretations are not determined
until all hypotheses have been made.
The system has been exercised on two aerial
images: one consisting of a single submarine and the
other consisting of three airplanes. The submarine
scenario had sufficient resolution to analyze using three
resolutions. Features at each resolution that were used as
evidence for a submarine are shown in Figure 6. The
airplane image was analyzed using only two resolutions,
Features for detection of an airplane are shown in Figure 7.
In both of these examples, the original object
hypotheses are made at the higher levels, These
hypotheses are then projected to the lower levels. IIn the
submarine example, the lowest level model contains
additional detail, namely, the tail. In the airplane image,
only those areas near the original hypotheses are
considered at the lower level, thereby making the
interpretation process more efficient. The features used to
extract the airplanes are independent of the features used
for the submarines. By appropriately choosing features
the interpretation of a poor quality image was possible. A
more complete system description and discussion of results
is found elsewhere in these proceedings [3].
Ackno wledgements. The progress presented in
this overview represents the combined efforts of the
technical staff members of the Computer Vision and
Autonomous Planning Sections at the Hughes AI Centers.
In addition we acknowledge the many valuable discussions
with Dr. R. Nevatia. We also thank D.Y. Tseng for his
continued support of our technical pursuits,
Road tracking in remotely sensed imagery has often been
equated with linear feature extraction. The rational was that finding
linear features in imagery either by region extraction or line finding
was equavalent to finding roads. Further, t also worked for other
types of inear features such as drainage, bridges and railroads.
This view was appropriate considering the very low resolution
imagery, ANDSAT i,II that was available at the time. However, it is
no ionger appropriate given that the research community now has
access to large scale, high resolstion aerial mapping imagery
allowing for the structural analysis of the road surface. In fact, for
practical mapping applications such detailed analysis must be
performed in order for a road extraction system to be a useful
component in a digtal mapping environment.
A second development is the emergence of computer vision
systems that perform integration of image processing resuts,
notably edge-based and region-based techniques, to generate more
robust intermediate representations [5, 6, 2]. These systems can be
contrasted with traditional image processing systems that exhibited
a single line of sequential processing without much effornt in the
constrnuction of intermediate intepretation strn)ctures or high-level
analysis. However, even these innovative approaches exhibit single
thread control structures even though they integrate multiple ines of
sequential image processing.
ARF (A Road Follower), is the first system to utilize multiple
independent control structures resulting in multiple lines of analysis
and the generation of alternative intermediate representations. it is
also unique in that t explicitely uses a cooperative method to allow
for single point failures to be overcome by using an alternative
(successful) tracking method. In this paper we report on:
In Section 2 we present a brief discussion of previous work in the
automated extraction of roads from remotely sensed imagery.
Section 3 describes the two low-level tracking methods. Section 4
focuses on the nature of cooperation in ARF. Sections 6 and 7
discuss some tracking examples and a set of experiments
performed to establish performance of ARF rnun using individual
trackers, and both trackers in cooperative mode. Finally, section 8
briefly discusses some future work.
In this section we survey five research systems over the last ten
years that were developed specifically for road tracking, or were
developed as linear feature detectors and were demonstrated within
the context of road delineation. For each system we outline several
of the major assumptions based upon reading the published
literature and give some subjective indication of performance based
upon published examples. The types of 'road knowledge'' used by
each method is described when appropriate.
Bajosy and Tavakoli [1] used kow resolution imagery from
LANDSAT-1 where each pixel had a ground resolution of
approximately 57 meters by 79 meters. Due to this kow resolution,
only very major roads of three or more lanes could be found. This
system utilized the knowledge that roads are made of concrete or
asphalt to directly determine the approximate intensity range to
expect in MSS band 2 of the ANDSAT-1 images. The paper does
not say whether concrete and asphat surfaces are processed
separately or whether the resufts are merged at some point in the
processing; possibly both materials look the same in ANDSAT-1
MSS band 2. It first performs a threshold operation, setting all
points within some range of the expected road intensity to 1 and all
other points to 0. Then it finds likely road points by scanning the
entire image looking for points with the right intensity profile -- that
ls, points which match one of a number of templates indicating that
there is a line point there. The templates are constructed so as to
aocept points that kocally kook like roads of width 1, 2 or 3 points, a
total of 52 templates are used. Vertical, horizontal and diagonal
roads and points where the road width changes by one point are
aocepted. It then grows the road by linking connected road points,
using constraints on curvature and distance between road points. It
then thins the road points so that the road is only one point wide. It
then eliminates short segments as noise since short roads are not
expected. It also labels intersections athough the reason for doing
this is not clear.
The program seems to find some of the roads in the imagery
used although t also finds some spurious roads. The quality of the
roads found is difficut to judge since photographs are not inclhuded
in the paper. One obvious limitation of this work is that roads are
expected to exhibit very specfic spectral signatures. This
assumption may work if the 'rigght' muti spectral scanner (MSS)
bands from ANDSAT-1 images are used but t will not work for
images from many sources since different materials and lighting
conditions can cause roads to be of any grey vah)e. Also, t does
not use much road-specific knowledge. The road knowledge used
is knowledge about expected width for the given type of roads and
imagery and knowledge about curvature and length. It is claimed
that the program will find rivers, bkood vessels and bubble chamber
tracks as well as roads. This may be an indication of too little
specialization for the task of finding roads since t is clear that rivers,
bkood vessels and bubble chamber tracks are similar only at the
coarses level of description, ie., linear 2D shape. However, many of
the features that distinguish roads from other Iinear objects are not
visible at the resolution used so a general purpose line finder coukd
be sufficient for the data.
The sR low resolution road tracker [3, 4] used kow resolution
imagery (ground resolution is not reported). It kooks for line features
using some weak high level constraints on the shape of the road
path. First an area of the image to search is designated,
presurmably by a user or a higher level program. R computes scores
for several operators over the area designated, under the
assumption that a combination of operators can do better than a
single one. A distinction is made between object detection
operators and object analysis operators. Object detection operators
are binary operators which detect whether the object is definitely
present or not. These operators are designed to detect only points
which are very reliable, at the possible expense of a large nurmber of
omission errors. Object analysis operators are designed to gie
some quanttative measure of the quality of the object point once t
is found. The object detection operators are first used to find all
reliable road points. Then a cost array is generated for each object
analysis operator. The cost arrays contain 0 for reliable road points
and a function of the object analysis score for all other points. The
function used can be designed to favor paths of a particular shape.
A path optimization is then applied to each cost array and the path
with the kowest score is chosen as the final road path.
Road knowledge used by the program is very weak. Some
limitations in this approach are that both the road start and end
points must be known at least approximately before trackitng. In
addition the resuts of the object analysis operators must be used in
areas where roads have not been clearly detected in order to find a
path between the points found by the object detection operators.
However, the object analysis operators were designed to give
reliable results only when the object is already known to exist If it is
really the case that the object analysis operators are only valid at
points where the object is known to exist, A seems that they are
completely superfluous.
At sR, Ouam [12] used high resolution imagery, with ground
resolution of approximately i to 3 meters. Ouam's road tracker was
part of the sRI road expert, the HAWKEYE system. It uses an
agorithm based on correlation of the road surface pattem or
intensity profile in the forward direction along the road. It must be
given an initial starting point plus direction and width. This is
expected to be supplied either by a user or a road finding program.
It keeps a model of the road to use in finding the road points. The
road model contains a surface model and a path model. The path
model is a list of recent road points used for parabolic extrapolation
of the path, the surface model is an array of intensity values
sampled from the image in the direction perpendicular to the road.
The surface model is allowed to change gradually as the road is
followed.
The program first uses the path model to predict the posiition of
the next road point one step ahead. It then extracts a cross-section
from the image at the predicted point and performs cross-correlation
with the road surface model to compute an error in the predicted
position. The actual road position is computed from the correlation
offset. If the correlation peak is poor, the program uses the path
model to guess ahead another step and try to re-acquire the
surface. It will continue to guess ahead until it finds a good match
or until t has gone further than the length of the kongest expected
anomaly. Hf there are a large number of anomaly points, t then
hypothesizes a surface change and extracts a new surface rmodel.
If t can follow the road successfully with the new surface model t
continues, otherwise it quits.
The road knowledge used is the assumption that roads have a
consistent surface wear pattern but with possible anomaly points, an
assumption of constant width and the possibility of a change in
surface material. However, it does not use any high-level
knowledge about roads. For example, road width changes can
cause problems due to gradually changing surface patterns that
often occur at places where a new lane is added or one is deleted.
Gradually changing surface pattems can also occur at intersections.
This is a problem that does not occur with some of the techniques
using lower resolution imagery. This makes t clear that although
more information is available in higher resolution imagery, much
more processing is required to take full advantage of t.
Nevatia and Babu [11, 10] used medium resolution imagery
(ground resolution is not reported). Their program first runs an edge
operator over the entire image to compute an edge magnitude and
angle for each point. It then selects edge points based on three
criteria:
It then links the edge points together and fts piecewise linear
approximations to the connected edges. Finally, t groups the line
segments together into antiparallel pairs -- that is, pairs with
opposite directions -- under the assumption that the road is of
approximately uniform intensity and the background material is the
same on both sides of the road.
The road information used by this method is the assumption of
uniform road intensity and uniform background material, and
assumptions of minimum length and straightness of roads. One
problem is that t is difficult to group antiparallel lines properly,
especially in the case where there are more than two lines. Another
problem is that it is not ahvays the case that the background
material is the same on both sides of the road, for example, in the
case where the road is one lane of a mutilane highway and there is
a median strip of another material separating t from the other lane.
Finally, from a pragmatic standpoint, the edge test may be too strict
to find many roads.
Kestner [7] used medium resolution imagery where roads were
on the average 3 or 4 pixels wide. It uses two methods to track
roads -- a correlation follower and a region based follower. The
program is started by a human or an automatic road finder with the
road position and direction. The correlation follower kooks in the
direction of the expected road path for points with the expected
intensity profile of the road against the background, that is, light on
dark or dark on ight. Where the correlation follower fails to find an
acceptable path, a region based method is used to re-acquire the
road. The region based method extracts a two-dimensional area
from the image and searches the entire area for points with the
expected intensity profile, marking each point with a score which
indicates how well it matches the expected profile. It then
eliminates all but the best points and links them together usitng
constraints on road curvature to find the most likely road path.
According to Kestner, the two correlation and region based trackers
complement each other perfecty although t is not clear exactly how
they are used together. A third method, called the binarizing
rmethod, is also described. This method first binarizes the section of
the image which contains the road by setting all points within some
range of the expected road intensity to i and all other points to 0. It
then eliminates regions that are too wide to be part of the road and
links the remaining thin regions together. t is not clear whether the
binarizing method was implemented or how it was used in the
system.
The primary road knowledge utilized concerns road shape and
the assumption that roads will be of approximately constant
intensity. An implicit assumption of this method is that the
background is of approximately constant intensity. This assumption
of constant intensity is a disadvantage since t cannot accomodate
major surface changes or large changes in background intensity.
One can categorize road finderlfollowers into one of three major
types; correlation trackers, region based followers, and edge linkers.
All of these methods use a single local tracking strategy to find
roads. Therefor a major problem with previous work is that if the
method fails at some point t is very difficut to recover. Further, it is
often difficult to automatically recognize when the tracking method
has failed since the kocal nature of fhese methods assumes that the
kocal maximum, no matter how poor, is its best guess for the
position of the road. As a side note, it is difficult to compare the
performance of the different trackers against each other since they
all use different resolution imagery from different sources and there
is no consistent method for reporting resuts.
We believe that the main way to improve road tracking is to
develop multiple methods to extract more information about the road
and to use cooperative techniques to allow a failed method to be
detected and restarted using an alternative technique. In the
following section we describe two tracking methods in sufficient
detail that others should be able to implement similar road trackers.
ln section 4 we discuss the cooperative archtecture within which
these trackers are imbedded, and the use of feature specific
detectors for road events.
In this section we describe the two tracking methods currently
used by ARF, a surface correlation tracker and a road edge tracker.
Each tracker makes assumptions and uses road specific knowledge
in ts image analysis. The surface tracker makes the following
assumptions:
The edge tracker makes the following assumptions:
Figure 3-1 gives an pictoral description of each of the tracking
methods. These methods will be described in detail in the following
sections.
This section describes a correlation-based road tracker for
medium to high resolution images. It uses techniques first
described in [12] with several improvements. All correlation-based
techniques are based on the assumption that there is a discernable
intensity pattern or texture on the road surface. For example, there
is usually a llight colored line down the middle of multiple-lane roads
and there are usually darker wear patterns in the lanes themselves
We do not look for any such specific pattern but rather look for a
pattern that is known to be on the road at some starting point
Provisions for skowly changing road surfaces and for sudden
changes in road material (from asphalt to concrete, for example) are
made.
The correlation tracker takes as input parameters the starting
coordinates of the road, ts initial direction and ts width. In normal
operation these parameters will be supplied interactively by a
human or provided by a higher level program, perhaps by using
information in a database to determine where a road might exist or
by using some sort of road finding operator. A road is followed by
extracting a cross-section perpendicular to the road from the image
at points predicted by a road trajectory rmodel and using cross-
correlation with a cross-section model to determine the actual
position of the road. Hf the correspondence is poor, or f the offset
(the shift between cross-sections) is greater than expected, we
continue guessing the road position until we find a good
correspondence or we have gone so far that t is unlikely we will
re-acquire the road. In the latter case, we assume the road surface
has changed and attempt to find a new surface model
The cross-section model is an array of intensity values taken from
the image. The trajectory model is the coefficients of a parabola
ftted to the most recent points on the road.
We predict the position of the road one step ahead by ftting a
parabola to the most recent road points. We store a list of about
twenty road coordinates to use for curve fitting. In addition to the
points themselves we also store a correlation score, which is a
relative measure of the quality of the cross-correlation, for each
point, When we ft a curve to the points, we ignore those with the
worst scores to get a correspondence with higher confidence
In determining the number of points to use for correlation, several
things must be taken into consideration. In order to get reasonable
aCCuracy, we need to use enough points so that the fit is not too
dependent on any one point. However, we cannot use too many
RA be9ause the oad may not be well suited for modelng wiih a
parabola over kong distances. In addition, the maximum nuinber ot
points we can use for curve-fitting is dependent on the size of the
steps we take since the number of steps that can be modeled well
% 4 Praola w be smaier for aroer steps. Thus, ior yeate
4WeOY n taiectory prediction we need fo take smaller steps
However, with very short steps the acceptable correlation ghhh;
between successive cross-secfions (corresponding to a maximum
aoceptable change in road direction over a small distance) becomes
very small and neariy impossible to achieve. The selection ot the
best number of points to use is therefore a tradeoff between
8GGuacy of road prediction and confidence in the road positions
found.
To take a single step along the road, we use the following eight
steps, which are identical in form to those described in [12[. The
initial trajectory model is generated by extrapolating backwards
along the initial angle given and storing the necessary number of
points. In this way, we start out kooking straight ahead without
otherwise treating the first points as a special case. The initial
cross-section model is created by taking the average of several
cross-sections extrapolated forward from the initial point.
[ONE]: Extrapolate the postion of the road one step forwan
We represent the road path parametrically as two sepatate
functions x(t) and y(t) where t is the total length in steps that we
have traversed along the road's path. We use multiple regression
[13] with t and t as the independent variables to fit parabolas to
x(t) and y(t), getting approximate functions:
It can be shown, by eliminating t from these equations, that the
ftted curve is a parabola in the x-y plane as well as in the x-t and
y-t planes, Further, the parabola in the x-y plane can have
arbtrary orientation, having an equation of the form:
We do not actually use this equation since the parametric form is
more convenient for our purposes.
The position of the road is predicted by computing x'(t+1) and
y'(t+1). Since this does not guarantee equal step sizes, due to
nonlinearity of the fitted curves, we must adjust the predicted
postion so that the length of the step taken is equal to the step
size we intended. The problem is that we are using t to predict
postion and t has only an implich, and not necessarily constant,
relation to the length traversed along the road. The error for any
point is normally fairly small, the main concern is that these errors
do not accumulate. We first find the tangent to the fitted parabola
near the current point by taking the slope of the line connecting
the fitted points for t and ts1. This gives an average tangent
between the two points. We then project the previous road point
onto the tangent line by finding the intersection of the tangent line
and the line which is perpendicular to the tangent and goes
through the previous point. The predicted road point is then
found by taking one step forwward along the tangent ine from this
intersection.
TWO]: Extract road cross section.
We take a cross section perpendicular to the road at the point
predicted in step [ONE], using a linear interpolation to get muhhiple
sample points per pixel. To allow for cross-correlation, we take a
cross-section wider than the road tsef. We take a straight cross-
section as opposed to a circular one as suggested for kow
resolution images in [7] both for simplicity and because it is not
clear what radius of curvature to use or how to match circular
cross-sections when we are kooking for a pattern on tho road
itsef.
The linear interpolation used is a weighted average of the four
pixels around the floating point image coordinates desired. We
imagine the pixels as being squares with their integer coordinates
representing the point in the center of the square. To determine
the weight used for each neighboring piel, we draw a square of
area one pixel around the point requested, The weight used for
each pixel is the area of overlap between the square and the
pixelL The advantage of this interpolation method over non-linear
methods, such as that described in [14], is speed. We need a kot
of points to get a good correlation and we can perform several
linear interpolations in the time t takes to do a single non-linear
interpolation.
TTHREE]: Cross-correlation.
We do a simple valley finding correlation to determine the best
match between the cross-section model and the new cross-
section. We look for the offset that gives the smallest value for
the sum of the squares of the differences between corresponding
elements. We compare the cross-section from the model with a
window on the new cross-section -- shifting the window to get the
score for different offsets. The correlation is done only over the
width of the road. To reduce the number of sums we need to
compute, we stop as soon as we have a valley. In doing this, we
are assuming that the point we are kooking for will be close to the
center - using the first valley we find reduces the chances of
finding a good correspondence that is not what we are looking for
(for example, a different lane of the road that happens to yiekd a
better correspondence).
[FOUR]: Generate a mask indicating the positions of noise
elements.
Using the correlation computed in step [THREE], we create a
mask of the elements which differ from their corresponding points
in the surface model by a significant amount. These are noise
elements which will be ignored in a second, more accurate
correlation.
[FIVE]: Re-correlate over the unmasked elements.
To get a better match we re-correlate using only the unmasked
elements. The basic correlation used is the sarme as in step
ITHREE]. To get a sub-element match we ft a parabola to the
three points ckosest to the valley and take ts minimum. We
check several things to see if the correspondence is good
enough. We check the magnitude of the offset of the
correspondence, the sum of the squares of the differences
(correlation score) and the number of points marked as noise
points. By using three different measures, we need not be so
strict on any single one. H any one of these is too large, we will
not accept the correspondence. f the correspondence is not
good, we guess the position of the road on successive steps until
we find a good correlation and then backtrack to output the road
points.
[SD]: Update the cross-section model.
Hf the correspondence was good, we create a new surface
cross-section model from a weighted average of the okd surface
model and the matched cross-section. This essentially results in
an exponentially decaying model. A large weight is used for the
model and a small weight for the new cross-section to prevent the
model from changing too rapidy.
[SEVEN]: Adjust the postion of the road center.
Using the offset from the correlation, we generate the road
point, We store this in the output and also add t to the ist of
points used to predict the road posiion, and delete the okdest
point from the list.
[EIGHT]: Anomaly detection.
Anomalies are found in the sarme way the mask elements were
found in step [FOUR] except a greater difference is tolerated. We
are more restictive for detecting anomalies than we are for
ignoring noise and random fluctuations. In the current version,
the step size is greater than one pixel so we must do multiple
lines of anomaly detection for each step.
At each step akong the road, we check f the correspondence is
acceptable (in step [FIVE]). H t is, we update the surface and
trajectory models and try to advance another step. i
correspondence is poor we must figure out why and try to find
where the road really is. Poor correspondence can be caused by
two things:
We treat these two cases differently. In the first case, we skip over
the occluded section until we can re-acquire the road. We use the
backtracking as suggested in [12] to avoid outputiing false
anomalies. In the second case, we constrnuct a new cross section
model and continue following.
Believing that occkusions are more common than surface
changes, we check for occhisions first. We look ahead along the
extrapoleted road position and try to find a cross-section with a high
comeletion with the current model. When we are guessing the
postion of the road, we alkow a greater absolute deviation from the
expected postion f we have been guessing for a while (that is, the
acceptatte postion of the road spreads out as we guess for a
geater dstance). A inear function of the number of guessed steps
s added kD the normal acceptable offset to get the offset we will
actually tolerate. If we find a good corelation akong the guessed
trajectory, we test to make sure we have really found the road by
requiring that more than one consecutie cross-section give a
reasonable match. If we have been guessing the road for only a
few steps, we only require that one point give good correspondence.
If we find a good correspondence, we backtrack and do road track
ana anomay output tor the points we skpped over, starting from the
last successful point. We then continue following the road from the
newly found position.
When we are guessing, if we find a point with good
correspondence we add t to a temporary road model (lor both the
road surface and the trajectory). In subsequent extrapolations we
use the temporary models under the assumption that we are
actually folkowing the road. This alkows us to guess the road
position based on all the information we have and makes
backtracking easier f we decide we have found the road. Hf we
eventually decide the points added to the temporary road model are
really on the road, we backtrack, adding the intermediate points to
the permanent model.
H we decide the points are not good, we throw out the temporary
rmodel and make a new temporary model out of the okd permanernnt
model. When we create the new termporary model, we do not go all
the way back to where we started guessing, we just get rid of the
new points we added and continue from where we are.
If we have not found a reasonable correspondence with the
Current model by extrapolating forward we presume the road
surface has changed. We then try to start a new model by retrieving
a new cross-section at the last successful point to serve as the
model. f we find a good correspondence with the new model, we
backtrack over any points we skpped, then begin folkowing again. tf
we do not find a good correspondence, we use the cross-section
where the new cormespondence failed as the model and try again.
We continue doing this until we find a good correspondence or we
have gone so far that t is unlkely that the road will be re-acquired.
The edge tracker is based on the road finding method of Nevatia
and Babu. tracks the edges of the road by nking points with high
gradient and orientation in the direction expected for the road. The
gradient and orientation are computed by a 5 X 5 Sobel gradient
operator. We use the Sobel gradient rather than the Nevatia-Babu
operator because we found that the 12 value grain of the Nevatia-
Babu operator was too course for accurate angle comparisons.
During tracking, the position of the next oad point is predicted by
parabolic extrapolation from the recent path the same way as is
done in the surface tracker. The Sobel is then cormputed for the
points akong the line perpendicular to the road direction at the
predicted point. A score based on the weighted sum of several
cormponent scores is computed from the Sobel vales for each
point. Each of the components is a linear function between 0.0 and
1.0 in the range between some minimum and maximum, 0.0 outside
the range at one end and 1.0 outside the range at the other end.
The component scores are edge strength, orientation, difference in
magniitude from each of the neighboring points and difference in
angle from each of the neighboring points. The edge strength score
is high for edges of greater magnitude, the orientafion scofe is high
for angles of the expected orientation. The difierence in edge
magnitude score is high for larger differences, selecting for points
whose neighbor points have relatively small edge strengths. The
ditference in angle gNes high score to points whose neighbors have
similar angles, selecting for points in areas of consistenty linear
texture.
The final score for each point is taken as the sum of all of the
components. The point with the highest score is then chosen as the
edge point. If no points have a score higher than some minimum
threshokd, no point is selected as the edge point. f edges are found
on both sides, the center line point is marked as the point in the
middle. only one edge is found, t is assumed that the width has
remained constant and the center line point is marked as half of the
width from the one edge found. tf no edges are found, guessing
ahead is done the same way as for the surface tracker. As with the
surface tracker, f guessing ahead goes too far without finding a
good point, the tracker quits.
To decrease the possibility of finding an edge point that has a
high score kocally but is not part of the road edge, edge lnking is
done between points where possible. We use the same method as
Nevatia and Babu, looking ahead in the direction of the edge and
accepting the next point f t is a maximum in edge magnitude and
has the correct direction.
In this section we discuss the organization of ARF, an Automatic
Road Follower. ARF is organized into three processing and analysis
levels. The ow-level is composed of two independent kow-level
road trackers that generate an estimation of the center line of the
road and produce various tracker dependent features such as width,
surface material change, and anomaly detection. These trackers
have been described in detail in Sections 3.1 and 3.2. The
intermediate-level is cofmposed of several modules that detect and
report road features such as overpasses, road width changes and
vehicle detection. The high-level provides overall control and user
interface. detects siuations where one kow-level tracker must be
restarted from the other and allows users to interact to start or
restart the system. Figure 4-1 gives a description of the overall
organization of the cooperative road tracker system.
The basic control cycle in ARF is to invoke each of the road
trackers independently and allow them to track asynchonously until
each has generated a step forward. This step might require several
advance, fail, guess, and prediction steps within each of the road
trackers as described in Section 3. Once both trackers have
completed a step forward, feature detection is performed by
invoking each of the intermediate level detecters with access to the
internal path model of both trackers. These detector modules and
the type of state information that they require is described in Section
5. Finally the control queries whether a path divergence has
occured. Such a divergence is currently a difference of 7.5 meters.
If no divergence is detected the basic control cycle continues.
Upon detection of a divergence, the high-level invokes each
tracker to advance 5 additional units. The path history and
confidence values are used to evaluate the relative goodness of
each tracker, both in terms of ts history, and in terms of the 5
additional units. The high-level can then decide to terminate the
overall rnun, f the confidence in both trackers is sufficiently kow, or
can decide to restart one tracker from the path model of the other.
The process of restarting is made straightlorward since the
internal data structure representation of the path history for each
tracker is identical. Thus an edge tracker path history can be
oaded directly from the surface tracker, and edge values and
confidence can be computed without search. Similarily the
correlation tracker can be forced to assume a new path model, and
rebuild ts weighted decay cross section.
There are six types of road features which ARF attempts to detect
and delimit -- intersections, width changes, overpasses, surface
rmaterial changes, vehicles, and occlusions. Any of these conditions
can cause simple tracking schemes to fail. Multiple conditions, such
as occlusions on a sharp curve, width changes followed by a
surface rmaterial change require mutiple sources of road estimates
in order to be robust. Each of the intermediate level feature
detectors has access to each of the trackers internal road history
data structure. This data strnucture contains an entry for each path
point tracked on the road including center position, match
confidence, edge confidence, width estimate, leftlright edge track,
and model history. The results of each feature detector is combined
with the original tracker information to form a composite path model.
For each point in the composite path model we store
In the following sections we summarize how each feature
detector determines that a feature event has occured, how t is
verified, and what actions (if any) shoukd be taken by the high level
control. The major problem for most feature detectors is in the
generation of hypotheses about continuous features that occur over
several path points only using pointwise information as above.
There role is to survey and combine the noisy pointwise into the
occurance of a discrete event. A second problem is in the
determinization of the actual location of the feature. For example, in
the case of width changes, the actual feature occurs, usually slowly
over many road points. This kocalization is important since we rely
on reasonably accurate positions for overpasses and intersections
in order to skip over them.
To detect intersections, we detect cases where there are several
consecutive road points with bad edges and possibly with poor
correlation scores. We also expect no significant change in average
pixel value over the cross-section. On roads in residential and
urban areas, we might expect intersections to appear at fairty
regular intervals. The history then must contain a measure of the
quality of the edges and a measure of the quality of the correlation
scores. Verification can be accomplished by adding the intersection
kocation to an agenda of possible new road starting points and
invoking the tracker using the intersection width and direction'. To
skip over, kook ahead past the predicted width of the intersecting
road. It is generally not necessary to flush portions of the surface
rmodel.
The detection of road width changes requires the integration of
several clues. During a width change we expect to find good edges,
possibly poor surface correlation, and there shoukd be anomaly
points along one side of the road. We use the path history to
determine the number of anomaly points for each road point and
their ocation on the cross-section. In addition, there should be
some disagreement between edge center and correlation center f
both trackers giive good scores. Significant width changes can be
detected directly if a long enough history of the edge points is
maintained. Since width changes often occur on highways near
overpasses, iif an overpass has already been detected we increase
the possibility of width change. These types of interactions argue
for a tghty coupled path history accessible to all feature detectors.
In this case, verification is not necessary since the width can be
detected directly.
The most important implication of a road width change is the
eventual breakdown of the surface tracking rmodel because its
correlation cross section no konger rmodels the actual road cross
section. This is exactly what causes the generation of anomaly
points along one edge, usually the edge that is expanding or
contracting. As a resut of a road width change the model width is
updated and t is necessary to re-acquire a surface model using the
path model of the edge tracker.
Overpasses exhibit properties similar to intersections at the kow-
level path history. The major distinguishing feature is that we
should expect some changes in the average surface intensity, more
so than in an intersection. Generally we detect lack of consistent
road edges conincident with a series of bad surface correlation
scores. Overpasses are likely to have shadows that precede or
follow the previously mentioned features, and these shadows are
usually detected as anomalies. Verification can be perfromed by
finding leading and trailing edges of the overpass, usually also
detected with anomalies..
Surface material changes are relatively easy to detect, since the
pavement changes causing a large, abrupt change in average
surface intensty akong with poor surface correlation but possibly
good edges. Often surface material changes coincide with bridge
decks, and two ckesely spaced changes can be detected. Shont
changes due to patching are detected as anomalies. In this case
the path history is re-acquired using several consecutive matching
cross-sections.
Vehicles can be detected by looking for patches of anomaly
points of approximately uniform intensity. Since the path history
information stores the number of anomaly points, their average
intensty, and their ocation in the cross-section, the vehicles feature
detector performs simple grouping.. Since vehicles can appear at
any point on a road but there shoukd be some rules about where
they can be with respect to each other and rnules about which lanes
they are ikely to be in.
Occlusions are detected initially by looking for irregular patches of
anomalies akong the side of the road. Such irregular patches can be
caused by buikdings and trees. They are too kong and irregular to be
vehicles, tend to not cause bad correlations but may coincide with
intermttent edges on one side of the road.,
Photographs 6-1, 6-2, and 6-3 are three typical examples of the
ARF cooperative road tracker. Photographs 6-1 and 6-2 are
continuations of the same tracker run on a limited access highway
with a ground sample of about 1 meter per piel. Figure 6-3 shows
a similar road at more about 3.5 meters per pixel.
ARF generates a real-time display with four display windows. The
'edge tracker' window shows the independent progress of the edge
tracker including centeriine, edge points and predictions and
guesses. The same information is displayed in the 'surface tracker'
window, except that only the anomalies replace the edge points.
The 'cooperative output' window displays the resuft of the high-level
integration of centeriine paths and the superposition of road feature
events. The text window gives a rnunning interpretation of the
feature symbols superimposed on the cooperative output. The
three image windows scroll independently as edge and surface
tracking proceed. The cooperative output usually lags the output of
the trackers since it often must wait for delayed events from the
feature detectors.
Photographs 6-1 and 6-2 show several examples of road width
changes, surface material changes and overpass detection.
Anomaly detection is particularily visible in photograph 6-2 in the
area of the overpass. While initially this area is marked as an
anomaly, the cooperative output correctly identifies it as an
overpass (12).
Photograph 6-3 shows an example of nearly complete failure by
the edge tracker in a kow resolution road at the point where several
roads merge and the road direction changes. However, the surface
tracker correctly tracks the direction change and the cooperative
output reflects the use of each trackers confidence scores to
correctly weight the final output.
Figure 6-4 illustrates the symbolic descriptions that are produced
by ARF during a ttacking session. All imagery used by ARF is
maintained in the wAPS database [8, 9]. The wuAPs database
provides a diglal elevation model and a carmera model that allows
for image-to-map and map-to-image correspondence. ARF uses
these image-to-map correspondence equations to transform sub-
piel road centeriine posilions into geographic coordinates, and then
uses the geographic coordinate to index into a digital elevation
databae to cakculate an interpolated terrain elevation. This
capability allows for reasoning about image features in metric
distances as opposed to pixel units and is used to represent
distances in each of the intermediate-level feature detectors..
Our input data consists primarily of aerial photographs of the
Washington D.C. area from ho different sources. We selected 35
roads for tuning and initial debugging and 35 for testing, attempting
to select faity dfficult data so as fo allow ior future improvements 1o
be reflected in tests on the same input. We adjusted the
parameters of the program to obtain the best overall performanoe
(measured subjectively) when rnn on the tuning roads and then ran
it on the test roads also. We intend to run future versions of ARF on
this input and use the numbers given above as benchmarks.
Performance data for tuning and test roads are given in Figures 7-1,
7-3, and 7-5 for the training' data and Figures 7-2, 7-4, and 7-6 for
the blind 'test' data. For each table road Width and Length are in
pixels [meters]. Time given are system and user time, respectively,
ln seconds. Under Subiective Reason for Stopping, A indicates
automatic stopping, M indicates that the program was stopped
manually by the operator.
When ARF does not follow a road to the edge of the image, t
usually fails for one of the following reasons:
Additionally, in some cases the road path generated is not very
smooth or does not stay on the center of the road. This usually
happens in fuzzy images or on roads with little texture.
Speed is usually between 7 and 14 pixels of road length per
second of CPU time on a VAX 11-785. Timings depend mostly on
the road width, the number of anomalies and the amount of
guessing and backtracking necessary. About 35 percent of program
time is spent in piel access and interpolation.
Figure 7-7 gives the relative performance of the each tracker vs.
the other method and vs. the combined tracker. These results
indicate that the surface tracker is of significantly higher quality than
the edge tracker. The surface tracker produces a konger track for
nearly all of the rnins.
It is important to note that the combined tracker is better than
ether tracker alone in a significant number of cases. Athough the
individual trackers are better than the combined tracker in some
cases, improvements to the mechanism which measures the quality
of the path could increase the overall quality of the combined
tracker.
In this paper we have presented a complete system for road
tacking and feature detection in high resolution aerial imagery. The
sy5tem has been tested on a large number and variety of complex
aerial imagery. We believe thal ts pertormance would in many
caSes justfy ts use as an interactive assistant for road network
delineation. The quality and reliability derives from the use of two
independent methods for road tracking, organized into a cooperative
system architecture that tolerates failures and allows for automatic
restat.
There are many avenues for future work. First and foremost we
are investigating the use of an automatic road starting point finder
so that we can relax the requirement that ARF be given an initial
starting point, road width, and direction. Second, we believe that
other tracking methods could be integrated into the ARF system,
such as the use of multitemporal imagery, or stereo pairs. What
remains to be seen is whether additional information from other
sources actually improves overall system performance.
Finally, ARF should be viewed in a larger context of road network
extraction. That is, at many points where ARF fails a road finder
could be invoked to attempt to locate new plausible starting points
which would allow for continuation. ARF generates areas such as
overpasses or surface material changes which should also become
the focus of road finding. Road network extraction then becomes
the integration of various search strategies for invocation of road
finding and road tracking.
In recent years there has been a renewed interest in
tackling problems related to character and document
recognition. For handwritten numeral recognition, ex-
cellent systems were developed ([6] [7] [8] [18]). But
despite advances, human performance has not been
matched.
More sophisticated systems are needed, incorpo-
rating more knowledge in all stages of the recogni-
tion process-preprocessing, feature extraction, clas-
sification. However, in our view, the most important
problem to address is feature selection and ertraction.
In the literature, little is said about how and hou
well features are located and extracted by the var-
ious methods proposed. Furthermore, such aspects
are rarely addressed when a system's recognition per-
formance is discussed.
In this paper, we examine the problem of extract-
ing curvature features from the contours of objects.
While the focus of our work is handwritten numerals
we believe our remarks and experimental results have
broader applicability.
Our goal is to detect significant concavities and con-
vexities of contours. To be more precise, what is im-
portant here is the identification of regions of 'consec-
utive' pirels along the contours which correspond as
closely as possible to the global shape features identi-
fied by humans. For the '3' of Figure 1, these regions
can be roughly indicated by the sequences of circles,
delimited by square boxes. For now, we are not con-
cerned with the labelling of these regions as endpoints,
smooth or sharp concavities etc.
Furthermore, we are seeking methods which gen-
erate their feature regions automatically, without hu-
man interaction; the location and relative size of these
regions should be roughly invariant to rotation and
scaling; the bottom of concavities and the tip of con-
vexities, especially endpoints, should be located with
some accuracy (see filled circles of Figure 1).
In the remainder of this article, we shall use the
following definitions. Let p1, P2. .pN be the sequence
of N 8-connected contour points labelled in a counter-
clockwise fashion. Furthermore, let c; be the (Free-
man) direction code from p--1 t0 p-
The interest in corners (or dominant points) stems
from the knowledge that much information about the
shape of 2-D objects is concentrated at points of high
curvature along their boundaries.
In O'Gorman [13], methods which estimate the con-
tour curvature at a point based on the difference be-
tween the slopes of 2 line segments which fit the data
before and after that point are called DOS methods.
In these methods, the angle @;, used for curvature es-
timation at point p;, is the angle between vectors V,
and V7, where V' joins P-a/2-. % P-ai15 and V7
stretches from P4+/2 P-4e/24,. Figure 2 illustrates
the situation for s = 5 and m z 2.
In Rosenfeld & Johnston [16], m = 0 and the cur-
vature estimate is the cosine of the supplement of our
6. To improve this method, Rosenfeld & Weszka [17],
performed local averaging of the cosine values.
In Freeman & Davis [5], m = 2- s. The 'cornerity
measure' of each point is based on a local average of
the 6-values and on the lengths of the straight regions
before and after the potential corner. Beus & Tiu [2]
average these 'cornerity' measures at each point, for
several values of s, and impose an upper bound on the
lengths of the straight regions.
In O'Gorman [14], small positive values are used
for m. Regions of the @-plot which extend beyond
the 'zero-range', defined as the curvature due to noise,
correspond to corners and curves along the contour.
When the features of interest exhibit different levels
of detail, Teh & Chin [19] argue that the determination
of a proper region of support for dominant points is
more critical than the measure of curvature chosen.
There are several methods based on arc-chord dis-
tances, which are often used to find polygonal approx-
imations of planar curves. Another group of methods
is based on Gaussian smoothing. Here parametric rep-
resentations of contours are convolved with Gaussian
filters, for different scale values (o). Knowing the fil-
tered responses of basic curvature changes, it is pos-
sible to locate them by examining the movement of
peaks and zero crossings across several scales.
Due to limited space, we do not elaborate on these
types of methods any further.
Since corners (or dominant points) are not rigor-
ously defined, the value of different algorithms is best
judged by direct examination of their results against
the specific requirements of particular applications.
For example, methods which have no input param-
eters and detect all corners at the finest level of detail
[19] may not be appropriate for certain applications
because they can pick up too many noisy details;
For DOS methods, O'Gorman [13] has shown an-
alytically, using certain assumptions, that the signal-
to-noise ratio is better for small positive values of m
(DOS'*). For equal values of the signal-to-noise ratio,
the same study shows that this DOS' method yields
narrower peaks than the G aussian smoothing method,
thus having better signal detectability.
In recent years, other comparative studies have
been published ([19] [15] [11] [1]). Based on their re-
sults, it appears that Rosenfeld-Johnston, Rosenfeld-
Weszka, Beus-Tiu and Teh-Chin achieved some of the
best results. However, while their conclusions are in-
teresting, it should be kept in mind that the number
of images used in these studies is generally very small
(from 2 to 8 images).
D'Amato et al. [4], Commike & Hull [3] and Hull
et al. [] use 8 types of features based on the amount
of curvature present at any point. Their computation
of a curvature estimate at each point is analogous to
the DOS methods; the angle of curvature at point p;
is given by
For handprinted characters, the 'angle accumula-
tion algorithm' of Lee et al. [9] uses differential chain
code values as a measure of local change in curva-
ture. A concavity is defined as ''the longest sequence
of perimeter points whose angular changes between
consecutive elements are all non-negative and their ac-
cumulated sum is greater than 1'',
Legault & Suen [10] also use an angle accumula-
tion scheme to partition the boundaries of 2-D objects,
for piecewise approximation. This algorithm was used
successfully to extract feature regions for one numeral
recognition method described in Nadal et al. [12].
Based on our appreciation of the suitability of the
above-mentioned methods to achieve our stated goals,
we have chosen 5 corner and curvature detection algo-
rithms: Rosenfeld-Johnston, Rosenfeld-Weszka, Beus-
Tiu, Teh-Chin, O'Gorman; as well as 3 algorithms
from the recent OCR. literature: D'Amato et al (also
used by Hull et al and Commike & Hull,) Lee et al.,
Legault & Suen. The methods are implemented based
on the information given in their papers and additional
information obtained in private communications.
For the Rosenfeld-Johnston and Rosenfeld-Weszka
methods, the user-supplied parameter was set to
rma[N/10, 4}. The Teh-Chin algorithm was imple-
mented with 3 different ''measures of significance''.
The results being almost identical, we retained only
one version for comparison purposes. For the ap-
proach of Lee et al, we did not eliminate diagonal-
facing concavities, since these do convey relevant infor-
mation for handwritten numerals. To detect convexi-
ties, all differential chain code values were negated.
Several experiments were conducted to select the
Beus-Tiu input parameters. More importantly, some
criteria had to be established for the automatic selec-
tion of corner points, since their number is an input
parameter in the original algorithm.
The methods were tested on 100 binary images of
handwritten digits, selected from a subset of a 20 000-
sample CENPARMI database. Ten samples were cho-
sen for each numeral class, offering a variety of styles
and sizes. They are shown in Figure 3. Our observa-
tions concerning the specific strengths and deficiencies
of each method are summarized in the next section.
We have also tried to provide an overall quantifi-
cation for the performance of each method by com-
paring the feature regions that it detects to the fea-
ture regions perceived by humans. For each sample,
regions of the external contour corresponding to the
global shape features to be detected were determined
manually by the authors. The feature regions of Fig-
ure l illustrate the result: the selected regions are the
sequences of circles, delimited by square boxes; the
keypoint of every region is identified by a filled circle.
A 'measure of goodness' (M G) was computed for
each method. For sample i, let R; be the set of the
indices of all contour points belonging to the n; regions
selected by humans. Furthermore, let Sf be the set of
the indices of all contour points belonging to the mf
regions selected for sample i by method k. We define
QQf = R, O Sf and f = nj/mf or mf/n,, whichever is
less than (or equal to) 1.
The 'measure of goodness' of method k in deter-
mining the global feature regions of sample i is then
defined as
where u; is the weight associated with each point
whose index belongs to R;. For simplicity, we have
assigned a weight of 5 to points in 'endpoint regions'
and a weight of 2 to other points in R;. Results ob-
tained so far are presented in Table 1.
Among the methods tested, the Beus-Tiu (BT),
Legault-Suen (LS), and Rosenfeld-Weszka (RW) algo-
rithms give best results in terms of their MG-values.
The LS method was found to be the most reliable
in the sense that it detects some points within each
and every feature region. Only once is a feature re-
gion missed altogether. On the other hand, its major
weakness is in the extraction of long smooth curves,
which are not detected as single global regions; in-
stead, one or more smaller subsections of those curves
will be detected.
On average, within all the feature regions that they
do extract, RW and BT tend to capture a larger num-
ber of points than LS. However, this advantage is offset
by the fact that they altogether miss certain regions
completely. This generally occurs when we have pairs
of nearby features, but it is not limited to that situa-
tion. Another problem, common to these methods, is
their poor localization of certain endpoint regions.
RJ and LE (Lee et al) suffer from the same defects
that were just presented for RW and BT, only to a
greater extent. The definition of a concavity (convex-
ity) in LE includes the straight portions of contour
which precede and follow the actual curved portion.
In some cases, these extensions may even cover the
early part of following regions of opposite curvature.
The poor performance of TC is linked to its detec-
tion of a large number of tiny consecutive regions, thus
preventing the method from extracting features at a
higher, more global, level.
The problem of detecting significant concavities
and convexities of external contours, corresponding to
how human beings describe these contours, was ex-
amined. While none of the methods compared were
specifically designed for this purpose, the Legault-
Suen method probably represents the best compro-
mise, so far.
The problem of detecting feature regions related to
human perception is inherently difficult and escapes
straightforward definition. Nonetheless, we believe
that more careful feature extraction can play a key
role in improving recognition performance for hand-
written numerals.
Finally, we note the scarcity of information avail-
able when one tries to establish what methods are suit-
able for a specific purpose. Authors rarely discuss the
twveaknesses of their approaches. Also the comparative
studies involving different methods are often limited
to a very small number of images, making it difficult
to draw conclusions concerning their relative merits.
We would like to acknowledge the kind assistance of
A. Commike and D.-S. Lee. Their prompt and precise
replies to our inquiries were very useful. We particu-
larly thank Dr. L. O'Gorman for providing us with the
full code for his method and Dr. L. Lam for sharing
her code (and experience with) the Beus-Tiu method.
This work was supported by research grants
awarded by the Natural Sciences and Engineering Re-
search Council of Canada and an FCAR team research
grant awarded by the Ministry of Education of Que-
bec, as well as the National Networks of Centres of
Excellence research program of Canada.
In the literature several methods for the estimation
of curvature from digital image data are known. They
are based on the three different formulations of cur-
vature which are equivalent in the continuous space,
but not in the digital space. As a consequence of the
digitization we have that when a continuous object is
repeatedly placed on a discrete grid with random po-
sition and orientation the results will not be the same.
A method should be judged in terms of its accuracy
and precision under repeated measurement of such a
randomly placed object. Accuracy and precision of
a method are quantified respectively by the resulting
bias B and standard deviation S. The appropriate ref-
erence object for curvature measurement is a circular
disc.
The accuracy and precision are not only depending
on the curvature of the disc, but also on scale. To that
end, we use a scale parameter in all methods. Further,
due to the anisotropy of the grid, curvature estimation
is also depending on the local orientation of the disc
with respect to the discrete grid. Therefore, the esti-
mated curvature is studied as function of radius, scale,
orientation and resolution of the grid.
As methods for curvature estimation are based on
three different formulations of curvature, they fall in
three different catagories.
Orientation based: In [2], [8] and [10] curvature,
in effect, is estimated through low pass differential fil-
tering of estimated tangent (or gradient) direction. In
these methods (named I) only a limited number of dif-
ferent orientations are recognized (for this paper the
Freemancodes are used).
In [4] (method II) the effect of the anisotropy of
the grid is reduced by resampling the main and diago-
nal elements of the 8-connected contour in equidistant
samples along the digital contour, and then proceed as
before. We make one addition to the method, using
the fact that the average distance between two dis-
crete points (prior to resampling) is computed to be
1.107 [3]. On the basis of this result, we divide esti-
mated curvature by this factor. For the differentiation
step involved in all methods, we have chosen to use a
differentiating Gaussian kernel, as used in [2]. The fre-
quency response of this kernel is similar to the other
kernels used in the references. For all methods, scale
is now regulated by the standard deviation o of the
Gaussian kernel. In practice we have to truncate the
kernel to a finite size M. A common choice for M i,
to take M s 6o (see for example [5):
In [1] a line fit approach (method III) is used on
the digitized contour to estimate tangent direction.
The curvature estimation step is, in effect, done by
linear filtering of estimated orientation with the ker-
nel [-41,-1}. This differentiation step is followed by
division of the results by the distance between two
succeeding points. We introduced a Gaussian weigh-
ing function into the fitting procedure, to allow for
comparison with respect to scale.
Path based: In [5] and [7 a linear filtering approach
on the x- and y-coordinate sequences is applied using
differentiating Gaussian kernels (method IV). Rather
than using linear filtering, one can also fit a spline
through the coordinate sequences and compute deriva-
tives analytically [6]. However, this latter procedure
also boils down to linear filtering of the x- and y-
coordinate sequences. Frequency characteristics are
similar to a Gaussian kernel and therefore results of
this method are not shown explicitly.
Osculating circle based: Although not used for
that purpose in the reference, one can take the curva-
ture of the circular disc best fitting the contour points
[9] (method V). Similar to the case of line fitting, a
Gaussian weighing function is introduced.
Results were obtained using discs of radii 10 to
40 and for the following range of scales: o =
3.0, 4.0,5.0,6.0,8.0,12.0, 16.0. For the disc of radius
25 the bias is shown explicitly in figures 1-5.
Orientation based: The Freemanbased method I
shows an orientation depending bias (see figure 1),
where the difference found for 9 s 0 and 6 s >/4
decreases with increasing o. For o z 16.0 it reaches a
bias independent of orientation of approximately 10%.
For o 3.0 the difference in bias between 0 z 0 and
0 s< /4 is equal to 95%, This is far more than the
standard deviation which, at its maximum, is equal to
30%.
Resampling (method II) shows a significant reduc-
tion of this orientation depending error with approx-
imately equal standard deviation (see figure 2). In
method II, for o s 16.0 the bias is only -1%, where
the standard deviation is even lower than the absolute
bias. In fact the overall performance of method II is
superior to all the other methods considered.
Method III based on linefitting shows a lower de-
pendency on orientation as compared to the Freeman-
based method (see figure 3), however the standard de-
viation is high, even for o = 16.0 (7%-14%).
Path based: For all scales, the pathbased method
(IV) shows very poor performance (see figure 4). For
o e 3.0, bias ranges between 74% and 113%, where the
standard deviation reaches even higher values (151%
- 236%). This improves for higher scales, but for o =
16.0 bias is still 26% to 33%. Standard deviation for
this value of o is approximately 16%.
Osculating circle based: For small o the errors
found for the method V based on arcfitting are large
(see figure 5 ). For o = 3.0 the bias is ranging from
41% to 108% with a standard deviation between 10%
and 69%. For o = 16.0 the bias is a high 31%, where
standard deviation is less than 1%. An interesting
phenomenon occurring in the arcfit based method is
the fact that for small o large errors are found for an
approximate orientation of 9 e m/16.
Orientation based: The method I, based on the
Freemancodes, shows a large orientation depending
bias. This is an immediate consequence of the fact
that the average distance between samples is depend-
ing on orientation, leading to an orientation depending
frequency response. We computed the error for a con-
tinuous disc. For orientations in the direction of the
main grid lines, the error is 0, for orientations in the di-
agonal directions of the grid, the error varies, depend-
ing on the curvature of the disc between 44%(r s 40)
and 55%(r s 10). This computed error is independent
of the scale. The error of 10% found for large o is due
to the fact that the result is not normalized for the
average distance between two points.
Although similar to method I the resampling based
rmethod II shows far better performance. Equidistant
sampling of the contour turns out to be a crucial factor
when estimating curvature by linear filtering of esti-
mated orientation. The normalization step is also im-
portant. Application of method II without normaliza-
tion would, for large o, lead to a bias of approximately
10% similar to method I. The remaining orientation
depending bias for small o is a consequence of the
fact that the resampled points are equidistant samples
on the polygon through the original discrete points,
rather than an equidistant sampling of the original
continuous curve.
The results for the linefit based method III show an
orientation depending bias and poor precision. This
poor precision was to be expected, as linear filtering
is done using the filter [-41,-1} which has undesirable
frequency characteristics. Although orientation esti-
mation is very precise, the small errors are amplified
by this filter. We further note that using large win-
dows in orientation estimation is only valid for curves
which have local symmetry, which is not the case for
general curves.
Path based: The path based method (IV) shows
large bias as well as a high standard deviation. To an-
alyze this method, we took a close look at the parame-
terization of the curve, following from the digitization
method. It was found that the parameterization has
a discontinuous second derivative. In the continuous
case, this is ruled out by the fact that it is multiplied
by a factor which is zero, but this is not necessarily
the case for the discrete curves considered. Further,
and more important, is the fact that the claim that
taking a windowsize of 6o is sufficient [5] does not
hold. Results drastically improve when 8 or 10o is
used instead. Bias for o e 3.0 with window size 10o
is maximally 4% where for the window size of 6o it
was 113%.
In practice this large window size poses a lot of
problems when considering non-closed curves.
Another factor influencing the results is the fact
that the size of the disc decreases after Gaussian
smoothing. Therefore the curvature of the smoothed
disc is larger than the curvature of the original disc. In
[5] a method for correcting this error is given. It is not
applied in this comparison as calculations show that
these errors are only significant for very large scales.
Osculating circle based: The method V, based on
circular arc fitting, shows good performance, but only
when large o are considered. This good performance
was to be expected, because the object under consid-
eration, in principle, perfectly fits the model on which
the fitting is based. However, the model assumes a
complete circle. For small o, a circular arc of limited
extent would be the appropriate model. We found
that for small o it is difficult to distinguish the circu-
lar arc from a straight line. In [2], criteria are given to
make such a distinction. We could assign curvature 0
in the linear case, but this would yield a discontinuous
curvature function, which is undesirable. We further
emphasize that for objects different from a disc the
quality of the measurement degrades as the model of
constant curvature will not longer be valid. Taking a
fixed scale in that case is not the proper way of cur-
vature estimation.
A practical issue in the evaluation of the measure-
ment errors, is the question what resolution to use
to obtain a certain predefined accuracy or precision.
First we have to parameterize the notion resolution in
the context of curvature measurement.
Consider the curvature measurement of an arc of
size 2@ (given in radials). As the average distance
between points is 1.107 [3], the number of points N
on an arc of radius r is approximately equal to N z
r24/1.107.
This will serve as our parameter of resolution. As
we have chosen to take fo as number of points for the
Gaussian kernel, the scale parameter o of the kernel
is related to r and 28 by: o r26/6 + 1.107.
To find a relation between resolution on the one
hand and accuracy and precision on the other, the
bias and deviation for discs with radii between 10 and
140 were computed. The o to use for a particular
radius was calculated using the relation derived above
for 20 = /4 and 2@ = /2.
For the resampling method (II) errors as a func-
tion of resolution are shown in figure 6 (26 s /4
only). Note that in the figure the scale of the y-axis
has changed with respect to previous figures.
From the figure and further analysis of the exper-
iments we found that bias is a function of the local
orientation of the disc and not of resolution. In fact
for the smaller arc, depending on the local orientation,
bias will range between -7% and 4-5%. For the larger
arc the range is between -3% and -1%, A bias correc-
tion is recommended as a function of local orientation
only.
The deviation of the method decreases with increas-
ing number of samples. However, for the smaller arc
( 26 s x/4 ), far more samples are needed to reach
a similar precision as for the larger arc (26 = /2).
So, resolution should be based on the smallest arc of
interest given figure 6. From there one computes the
appropriate scale parameter o'.
Methods from literature to estimate curvature from
digital image data are based on the three different for-
mulations of curvature. These three formulations are
equivalent in the continuous case, but not in the digital
case. In fact, the accuracy and precision of a method
are clearly depending on the choice of the formulation.
Results show that for some methods the bias can
be very large (> 200%). For all methods, the bias is
depending on the local orientation of the object with
respect to the grid, as well as scale.
The method with the best overall performance is
clearly the method based on resampling (V). For this
method the relation between resolution and measure-
ment errors was studied.
This results in a practical guide for tackling the cur-
vature estimation problem. First one has to find the
approximate size of the circular arcs of interest. From
figure 6 one can find the required resolution to ob-
tain a certain predefined accuracy and precision. This
immediately gives the scale of the kernel to use in fil-
tering. From there, the resampling method (V) should
be applied, with the normalization by 1.107.
Echocardiography is a popular clinical method for the
identfication and assessment of an entire spectrum
of heart abnormalities [1]. In recent years visualiza-
tion and quantitative analysis of the heart from two
dimensional echocardiograms has received increased
attention. It is possible to detect, quantize and visu-
alize a large spectrum of heart abnormalities through
segmentation of various regions in a two dimensional
echocardiogram [2]. However, automatic segmenta-
tion systems developed for quantitative analysis of
echocardiograms have not been successful in a clini-
cal environment. Furthermore, owing to the complex-
ity and importance of the task, human supervision of
computer generated segmentation is often essential.
TTwo dimensional echocardiograms (2-D) are im-
ages of cross sections of the heart, obtained by me-
thodic registering of the echoes generated by sound
beam scans. The intensity of sound echoes are de-
pendent on the type of the tissue, and change in the
tissue type, at the point of backscatter. Echocar-
diograms record high intensity echoes at the points
that correspond to the heart walls, valves and other
tissue changes that lie within the scanning range.
Therefore, it is possible to separate the wall regions
from the blood volume by simple binary segmenta-
tion. Several researchers have used binary segmen-
tation to extract the wall regions [2]. However, be-
cause of the nature of imaging and complexity of the
data, automatic methods may misclassify some re-
gions. To alleviate this drawback, researchers have
proposed semiautomatic schemes that enable a user
to interactively correct the misclassification [3, 4]. In
this paper, we study this problem of interactive re-
finement from the point of view of both image pro-
cessing and human computer interaction, and develop
a collaborative method for accurate segmentation.
A schematic overview of the developed supervised im-
age segmentation scheme is shown in Figure 1. The
overall system consists of two major components, viz.
the Graphics user Interface (GUI) and the supervised
image segmentation algorithms. The user initiates
the binary segmentation process by simple mouse
based selection, and observes the computer generated
segmentation results along with the original images
on a graphics display. If the segmentation is satisfac-
tory, he uses it directly for further processing such as
quantitative analysis and visualization. On the other
hand, if he finds errors in segmentation, he indicates
corrections through simple mouse gestures, and the
refinement algorithms refine the segmentation.
A snapshot of the several stages of processing done
on two different frames of two-dimensional ultrasound
heart images is shown in Figure 2. Figure 2-(1) shows
a pair of two dimensional long-axis echocardiograms.
To accurately segment the echocardiogram a local
threshold is computed at the interactively suggested
wall section, and it is adaptively propagated to the
rest of the image, and the rest of the frames of the
sequence, if it is a dynamic scene [5]. This thresh-
old gives a good estimate of the walls for the most
part as shown in the pair of corresponding frames in
Figure 2(2).
The upper image in the Figure 2-(2) pair shows
a clear contiguous top-right chamber (left ventric-
ular chamber), while showing some fragmented re-
gions inside it. These fragments are quite often ex-
traneous reflections caused by the overlapping pap-
illary muscles, and the endocardium. The expert
points at those fragments using mouse-clicks as shown
in Figure 2-(3), and a region grouing algoritkm [6]
deletes those regions and displays the refined output
as shown in the upper frame of Figure 2-(4).
On the other hand, if the initial segmentation
yields poor results, as shown in the lower frame of
Figure 2-(2), the expert may want to delineate the
entire chamber. If so, he circles the chamber and
indicates that a contiguous internal contour be ob-
tained. A Constrained Contouring algorithm uses
that circled input as the approximation of the precise
contour, and generates a refined contour that closely
approximates the actual contour [7]. The lower frame
of Figure 2-(3) shows the approximate input contour
and Figure 2-(4), shows the corresponding final con-
tour,
These image analysis operations are performed
through a user friendly graphics interface that ac-
cepts interactions through natural gestures such as
pointing and circling [8, 9]. Furthermore we incor-
porate a user's visual attention model [10, 11], that
accounts for the spatial extents of the gestured cor-
rections and visual expectations. A snap-shot of the
GUI display is shown in Figure 3.
Posner et al. [12] and Hoffman et al. [13] experimen-
tally demonstrated a discrete bounded region around
the point of attention, called the spot-light, with an
uniform reaction time. The angle subtended by the
spot-light is about 1 at the viewer's eyes [14]. Later
Eriksen [15] and LaBerge [16] showed that the spot-
light has flexible boundaries and its size is task de-
pendent.
Hughes et al. [17] examined the spatial extent of
the visual attention by inducing subjects to expect
the target at a location and introducing occasional
probe ffashes at other locations throughout the visual
field. The main conclusion of their study was that the
directed visual attention was not a spatially limited
phenomenon, but it extended throughout the visual
hemifield. It was independent of both the expected
target locations and the validity of precue. They fur-
ther supported the spot-light theory by showing that
the field of attention had a uniform reaction time.
A psychophysical analysis of the visual attention
span is reported by Mangun et al. [10], who studied
changes in the event related brain potentials (ERPa)
with respect to visual stimuli as the locus of attention
was shifted across the visual fields. They observed a
decline in attention indicated by decline in the ampli-
tude of the sensory evoked components of the ER.P4
at increasing distances away from the attended loca-
tion. Their study supported both the spot-light and
the gradient models of the visual attention maps. Ad-
ditional evidence to support these models is provided
by Shulman et 4l. [18! and Bashinski [19]. Tesl [20]
showed that the decrease in attention was linear with
respect to the angle subtended at the viewer's eyes.
We have combined the spot-light and gradient
models of the visual attention maps proposed in psy-
chological and psychophysical studies in our super-
vised image segmentation algorithms. In Figure 4,
a schematic plot of the attention response curve
that combines the spot-light and gradient models, is
shown. The curve shows small reaction time at the
point of focus, and is constant for points that are
with a small but discrete distance. This constant fo-
cus area is called the spot-light. For all the points
away from the spot-light, the reaction time increases
linearly with respect to the angle subtended at the
vewmg point. A linear approximation of this re-
Sponse curve is used as a constraint in the supervised
segmentation refinement algorithms.
From the spot-light region of the visual attention map
we know that there is small region around the pointed
location that characterizes the precise region to be
reclassified. To trace the boundaries of the pointed
region, we collect all the connected points with the
same characteristics. Our region growing algorithm
uses the visual attention map as its distance mea-
sure used for gathering the points in the region. This
distance measures account for image features such as
intensity and edges. Figure 5 shows a segmented two
dimensional echocardiogram on the left side, and the
corresponding refined output on the right side.
If the initial segmentation is erroneous, or if there are
too many isolated misclassified regions, the user can
refine the region by circling. This gesture roughly
approximates the boundaries of the indicated region.
Our goal now is to modify this input so that it traces
the precise boundaries of the intended region. This
problem closely relates to the problem of active con-
touring proposed by Kass et al.[21].
We determine the precise contour indicated by a
circling gesture by a graph search algorithm. A Lo-
cally Optimal Contour Locations (LOCL) are com-
puted in all the radial directions according to the
constraints of context(shape), distance from the cir-
cled approximate contour, and the relevant image fea-
tures. The detected LOCLs are represented as the
nodes of a graph of all possible contours. The links of
the graph are weighted so that the shortest path be-
tween a pair of nodes is a smooth contour that passes
through the maximum number of intermediate LO-
CLs, and the shortest cycle in the graph gives the
optimal contour.
Figure 6 outlines the endocardium in a short-axis
two-dimensional echocardiogram. The left image
shows the circled input, and the right image shows
the refined boundary of the intended region.
The multimodal human computer interface module
translates commands and corrections from the super-
visor into operations that are to be applied to the
image. The interface also presents the results from
these operations to the user in a concise and useful
manner. Our current work is intended to allow the
user to verbally specify cardiac structures. The image
analysis system will combine the verbal and gesture
information identify and segment cardiac structures.
Effective human-machine communication, like
communication between two humans, is built upon a
shared domain model [22, 23]. In general, the domain
model defines a set of objects, attributes and relations
that serve to organize domain knowledge [24, 25].
For example, in the domain of kinematics, knowl-
edge is organized around the concepts of force and
mass [26]. These concepts are associated with math-
ematical equations that form the basis of procedures
for calculating the values of unknown quantities. For
our purposes, we will claim that the domain model is
meaningful to the extent that it associates procedures
and strategies with the included objects, attributes
and relations [27].
As illustrated in Figure 7 the interface mediates be-
tween the user and the system, and therefore should
reflect the cognitive processes of the user and the
capabilities of the image analysis system. In other
words, each party involved in the communication is
able to invoke a set of procedures based on exchanged
information phrased in terms of the shared domain
model.
To enhance ease of use by the human supervisor, the
model should allow the supervisor to maintain the
following features of his own domain model:
We have provided a brief description of the proper-
ties of human reasoning with visual stimuli, and sug-
gested examples of these in the domain of echocardio-
graphy. The advice that a human supervisor provides
most naturally to an image analysis system is likely
to reflect these properties. To promote an effective
exchange between the human and machine, we pro-
pose these properties as the basis of a shared domain
model. Thus, the model should define a preferred
level of analysis on echocardiograms, and identify the
cardiac structures required by clinical goals.
Figure 8 contains an exarmple of the model we en-
vision for mediating human-machine communication
in this domain. The information to the right of this
row defines categories of parts. Parts are connected
to category names through ISA links. The primary
purpose of the categorization scheme is to describe
expected changes in the visual appearance of parts
with heart movement. The expected changes with
movement are indicated by the changes-shape link as-
sociated with the category names. A heart wall, for
example changes ftom thin to thick proportions with
heart beats. These changes-shape relations are inher-
ited by the exemplars within this category, so that
the several heart walls change in similar ways with
heart beats.
Information to the left of the center vertical row
of objects defines their positions and shapes. This
part of the domain model links the qualitative and
symbolic information about the heart to quantitative
properties that can be identified on an image. To
identify the position of objects, we have made use of
polar coordinates, an arbitrary center point and max-
imal radii of 1 and -1 defined by the heart bound-
aries, We have illustrated the position information
for selected cardiac structures in the figure. The left
atrium has a reference point at z2,2, and occupies
a location from 0 to 360? in both directions at a ra-
dius from 0 to r;.. Shape information defines this
area more precisely, future work will precisely specify
these shape descriptions.
Figure 10 illustrates another set of information as-
sociated with the cardiac structures. It is represented
separately from the information in Figure 8 here only
for the purpose of having a readable figure. The infor-
mation in Figure 10 identifies prototypical patterns
that are generated from standard views. These view-
specific cardiac structures are linked to the general
three dimensional object from which they emerge.
These prototypes constitute the user's ordinary ex-
pectations for echocardiograms. Their appearance is
pre-stored, and they are easily recognized by the pres-
ence of one or two prominent features.
The psychological phenomenon of prototypes is re-
lated to the topological notion of a characteristic
view [31, 32, 33, 34, 35]. The concept of characteristic
views is often used when recognizing a 3-dimensional
object from a single image. While a 3-D object can
project into many different 2-D geometries there are
only a small number of topologies that it can assume.
Each of these topologies corresponds to one or several
convex regions of space from which the mathematics
of projection generates the topology. Each of these
regions of space is a characteristic viewpoint and the
topology of the object corresponds to a characteristic
view. As far as we know, this concept has not been
applied to density images. But we feel that in fact
the small set of ''views'' popularized in the cardiology
textbooks may correspond well to stable characteris-
tic views of the heart.
We have begun research on characteristic views by
studying characteristic views of polytopes. Figure 9
shows charts indicating characteristic views of sev-
eral simple polytopes. We will extend our results on
polytopes to complexes of polytopes and eventually
general three dimensional structures.
We have illustrated three of these prototypical
views: the five chamber view, the short axis view
near the apex, and a long axis view. Each view is
composed of its several 2-D cardiac structures.
In summary, we have defined a candidate domain
model intended to reflect some aspects of a cardi-
ologists interpretation of an echocardiograms, focus-
ing on cardiac structures and their types. We de-
scribed the positions of cardiac structures within a
3-D model. We linked prototypical views of echocar-
diograms with the concept of characteristic views in
topology, as means of singling out expected 2-D views
of the 3-D structure. This model will allow the inter-
face and image analysis system to map user feedback
and advice onto specific parts of the image, and prop-
agate this advice through its interpretation.
We suggest here some functions that can be provided
by image analysis systems using this model as the
basis of human system communication. Analogs of
these functions are already available in the prototype
implementation of our system.
Given an approximate contour of a cardiac struc-
ture in the model and an approximate position and
range of orientations for the contour the image analy-
sis system can find, using a minimization process, an
exact contour, whose shape and orientation is as close
as possible to the contour supplied by the interface.
This function allows the interface to determine exact
outlines for cardiac structures such as valves or walls.
The system also will track the structure through a se-
quence of frames in the echocardiogram, and report
apparent errors in such tracings.
If the user indicates the exact contour determined
by the systerm is incorrect then the system can ad-
just the contour. The user may correct small por-
tions of the contour or make general comments such
as, ''the contour is too thick,' The interface will
translate these comments into approximate geomet-
ric commands. The system will then try to adjust
the contour, taking into account the modifications
that are suggested by the user and translated by the
interface into geometric descriptions.
To fully take advantage of the psychological model
discussed in the previous section we intend to en-
hance the modalities available to the system for in-
put and output. The enhanced system will accept
input using these devices: speech input device, key-
board,graphics tablet, touch screen, and mouse. The
system will produce output via two output devices:
high-resolution color-graphics display and speech out-
put device.
Figure 11 provides an overview of the proposed
user interface design. The primary path that the in-
put data follows is indicated by the solid arrows in
the figure: (1) Input Coordinator, (2) Multi-Medis
Parser/Interpreter, (3) Executor/Communicator to
Target System, (4,5) Multi-Media Output Plan-
ner/Coordinator/Generator. The Input Coordina-
tor module will accept input from the several in-
put devices and fuse the input streams into a com-
pound stream, maintaining the temporal order of
data in the original streams. The Multi-Media
Parser/Interpreter will accept the compound stream
produced by the Input Coordinator and produce an
interpretation of this compound stream. Appropriate
action is then taken by the Executor module. The Ex-
ecutor module will be implemented by extensions of
the current system for interpreting echocardiograms.
This action may be a command/request to the image
analysis system or an action that requires participa-
tion of the interface system only. Visualization of
the interpretation and explanation of the visualiza-
tion will be generated by the Multi-Media Output
Planner/Coordinator/Generator.
Our computational methods and knowledge rep-
resentation paradigms will enable a computer pro-
gram to function as an intelligent multimodal user
interface[36, 37, 38, 39, 40, 41, 42, 43, 44] We expect
improvements on the state of the art in image under-
standing can be achieved by accepting a wider range
of inputs including speech, sophisticated gestures and
text and by generating a wider range of outputs in-
cluding speech, a variety of tabular and graphical out-
puts and a variety of imagery[45, 8, 46, 47, 48].
An example of a graphical output we will generate
is shown in figure 12. This output uses color and nu-
rmerical labels to indicate velocity information about
the heart wall. Using color to indicate velocity or
change in wall thickness can provide a cardiologist
with more information for diagnosis.
We will enable the user interface system to accept
and understand language that refers to either: (1)
parts of the image, (2) the object(s) portrayed by the
image (i.e., the semantics of the image), or (3) spatial
relations between objects. Previous interfaces that
we have developed understand input that referred to
presentation objects such as the icons or windows on
a computer screen as well as the objects represented
by the icons and windows (represented in the sys-
tem's knowledge base). However, such a capability
for an image analysis system is more complex since
the image is not a collection of predefined icons and
windows,
Little research has been conducted to determine
which media/modalities are preferred or superior to
others or the basis for the preferences and almost no
research has been performed to determine when to
use combined modalities for human-computer com-
munication; The task may indeed be the most im-
portant factor [49, 50, 51]. Other studies emphasize
the compatibility between the stimulus, central pro-
cessing, and response media [52, 53, 54, 55, 56, 57]
Coordination of media is critical for a multi-
media interface system to effectively support human-
computer communication. Our research on the
CUBRICON project solved the coordination prob-
lem for discrete speech and discrete pointing gestures
[39, 38, 40, 58, 41, 42]. The problem of coordinat-
ing continuous speech input and continuous gestu-
ral/drawing input remains to be addressed.
We will extend and refine the CUBRICON design
for accepting and fusing the input from different de-
vices into a compound stream, maintaining the infor-
mation as to when the different tokens (e.g., words,
graphic gestures) that it receives from the different
devices were actually input by the user. A possi-
ble solution to the problem of coordinating continu-
ous forms of input (e.g., speech, drawing gestures) is
to attach time tags, representing time of actual oc-
currence, to the signals comprising each of the input
streams from the different media devices.
This work is supported by Grant No. 91-050G to the
second author from the New York state affiliate of
the American Heart Association. We are grateful to
Steve Rosenthal, M.D. for serving as a domain expert
for the development of the expert model.
Matching is a ubiquitous problem in computer vi-
sion. Correspondence matching can be broken into
two general areas: model-to-image matching where
correspondences are determined between known 3D
model features and their 2D counterparts in an im-
age, and image-to-image matching where correspond-
ing features in two images of the same scene must be
identified. Fast and reliable matching techniques exist
when good initial guesses of pose or camera motion are
available [Beve90] or when the distance between views
is small [Anan87]. What is lacking are good meth-
ods for finding matches in monocular images, formed
by perspective projection, and taken from arbitrary
viewpoints.
This paper examines the problem of matching copla-
nar structures consisting of line segments. A simple
method is presented that, when applicable, allows fast
and accurate matching of coplanar structures across
multiple images, and of locating structures that cor-
respond to a model consisting of significant planar
patches. The main point to this paper is that the full
perspective matching problem for coplanar structures
can often be reduced to a simpler four parameter affine
matching problem when the horison line of the planar
structure can be determined. Given the horizon line,
it is possible to transform the image to show what it
would have looked like if the camera's line of sight had
been perpendicular to the object plane. This process
is called rectification in aerial photogrammetry.
Essentially all matching problems involve solving for
both a discrete correspondence between two sets of fea-
tures (model-image or image-image) and an associated
transformation that maps one set of features into reg-
istration with the other. These two solutions together
constitute matching: a match being a correspondence
plus a transformation. For planar structures under a
perspective camera model, the relevant set of transfor-
mations is the eight parameter projective transforma-
tion group [Faug88].
More restrictive transformations are worth special at-
tention. Often these transformations are more easily
computed, thus making matching easier. One such
special case occurs for frontal planes, planar structures
viewed 'head-on' with the viewing direction of the
camera held perpendicular to the object plane. When
the intrinsic camera parameters are known, perspec-
tive mapping of a frontal plane to its appearance in
the image can be described with just four affine pa-
rameters: an image rotation angle, a 2D translation
vector, and an image scale [Sawh92].
Under the standard pinhole camera model, the image
projection of a 3D world point (X,Y, 2) is the image
point (/2,Y/2). In this case, the appearance of any
3D object is governed only by the relative position and
orientation of the camera with respect to the object,
ie. , the camera pose. There are 6 degrees of freedom
for camera pose: three rotation angles of roll, pan and
tilt, and three translations T., T, and T,. If the cam-
era is constrained to point directly perpendicular to
an object plane, yielding a frontal vieu of the plane,
its two pan and tilt angles must stay fied. This leaves
one free camera rotation about the normal of the ob-
ject plane, and the three unconstrained translations,
four parameters in all. The effect of camera roll on the
image plane is an in-plane rotation about the origin.
Translation parallel to the planar surface shows up as
as a 2D translation of the image. Finally, translation
directly towards or away from the object plane mani-
fests itself as a uniform change in scale of the projected
image. These are precisely the effects of the four pa-
rameter affine similarity mapping. The pinhole camera
projection of a frontal plane is therefore described by
four affine parameters that are directly related to the
physical pose of the camera with respect to the plane.
A more realistic camera model must take into account
the intrinsic parameters of the camera lens. To a first
approximation, lens effects are often modeled by a set
of linear parameters including focal length, lens aspect
ratio, optical center, and optical axis skew, whose com-
bined effects can be described by a six dimensional
affine mapping of the pinhole image onto the actual
raster image [Born86]. A more realistic model of the
projection of a frontal plane is thus a four parameter
affine mapping of the plane onto an idealised pinhole
plane, followed by a six parameter affine mapping onto
the actual measured image.
In summary, the perspective projection of a frontal
plane is described in general by a six parameter affine
transformation. When a calibrated camera is used its
intrinsic lens effects are known, and can be inverted
to recover the ideal pinhole projection image. After
correction for lens effects, the frontal view of a plane
can be described by a four parameter affine similarity
mapping.
For planes viewed at arbitrary orientations, the full
six degrees of pose freedom may manifest themselves
in the image. The two camera rotation angles, pan and
tilt, not used for frontal images, are directly related to
the tilt of the object plane with respect to the camera's
line of sight. Under perspective projection, llines that
are parallel on a tilted object plane appear to converge
in the image plane, intersecting at a vanishing point.
The locus of vanishing points of coplanar parallel lines
of all orientations forms a line in the image called the
vanishing line or korison line of the plane.
For frontal planes, all parallel lines on the object plane
remain parallel in the image. By convention a set of
parallel lines in the image is said to intersect in a point
'at infinity.? When all vanishing points appear at in-
finity, the vanishing line passing through them is also
said to be at infinity. Since a transformation is affine
if and only if all parallel lines of arbitrary orientation
remain parallel, it follows that the defining feature of
a frontal view of a coplanar structure is that the van-
ishing line of that structure appears at infinity.
Conversely, by applying a projective mapping taking
the vanishing line of an image of a coplanar structure
to the lline at infinity, the vanishing points of all lines
in the plane will now be at infinity, hence all parallel
lines in the planar structure must now be parallel in
the image. This implies that the new image is a frontal
view of the original set of coplanar lines.
We have seen that the vanishing line of a frontal plane
appears at infinity in the image plane, and further-
more, that it is possible to recover a frontal view from
the image of a tilted object plane by applying a pro-
jective transformation that maps the object's vanish-
ing lline to infinity. There is, however, a whole si-
dimensonal space of projective transformations that
all map a given line in the image off to infinity. How
to choose a 'best' one is described in this section.
For a pinhole camera image, the position and orienta-
tion of the vanishing line of an object plane determines
the true 3D orientation of the plane with respect to the
camera's lline of sight. When the equation of the van-
ishing line is as + by k c = 0, the normal to the object
plane, in camera coordinates, is
For a frontal plane, the normal of the plane must be
parallel to the 2 camera axis, since the plane is per-
pendicular to the line of sight. If the camera could
move, the image of a frontal plane could be recovered
from the image of a tilted plane by merely rotating
the camera to point directly towards the plane. The
camera can no longer be moved physically, of course,
but the image can be transformed artifically to achieve
the desired 3D rotation.
Assume the unit orientation of the object plane has
been determined to be n, as in equation 1, oriented
into the image (c 3 0). To bring this vector into coin-
cidence with the positive 2Z axis requires a rotation of
angle CosT(n - (0,0,1)) abont the axis n x (0,0,1).
The effects of this camera rotation on the image can
be simulated by an invertible projective transforma-
tion in the image plane [Kana88]. In homogeneous
coordinates,
where
The image is transformed to appear as it would have
if the camera had been pointing directly towards the
object plane. The result therefore is a frontal view of
the object plane, as seen by a pinhole camera, ie. a
rectified four parameter affine view.
This mapping can be used to map a vanishing line to
infinity even when the intrinsic calibration parameters
are not known. However, when the original image is
not a pure pinhole image, the position of the vanish-
ing line in the image can no longer be interpreted ge-
ometrically in terms of 3D plane orientation, and the
resulting unwarped image wil be in general some sixz
parameter affine view of the object plane.
Plane rectification forms the essence of our approach
to matching perspective images of coplanar structures.
The idea is to find the vanishing line of an object plane
in the image by any means possible, then apply a pro-
jective transformation that maps it to the line at in-
finity, thereby producing an affine frontal view of the
original object plane. The effect is to reduce a per-
spective matching problem to a simpler affine match-
ing problem.
To search for the optimal affine map and correspon-
dence between two sets of line segments, an efficient
and robust local search algorithm is used [Beve90].
The local search matching algorithm searches the
discrete space of correspondence mappings between
model and image features for one that minimises a
match error function. The match error depends upon
the relative placement implied by the correspondence.
More particularly, to compute the match error the
model is placed in the scene so that the appearance
of model features is most similar to the appearance of
corresponding image features. The more similar the
appearance the lower the match error.
To find the optimal match, probabilistic local search
relies upon a combination of iterative improvement
and random sampling. Iterative improvement refers
to a repeated generate-and-test procedure by which
the algorithm moves from an initial match to one that
is locally optimal. This is done by repeatedly testing
a local neighborhood of matches defined with respect
to the current match. Each neighbor is a distinct cor-
respondence mapping between model and image fea-
tures. Tractable neighborhood sises, for instance n
neighbors in a space of 2? possible matches, tend to
yield tractable algorithms. However, there is an art
to designing small neighborhoods that do not induce
a profusion of local optima. New neighborhoods defi-
nitions have been developed that are particularly well
suited to shape-matching [Beve90].
Despite clever neighborhood definitions, local search
can become stuck on local optima. Random sampling
offers a probabilistic solution to this problem. The
probability of finding the globally optimal match start-
ing from a randomly chosen initial match is analogous
to the probability of getting heads when fiipping a
unfair coin. Even with an unfair coin it is a good bet
that heads will appear at least once in a large number
of throws. For instance, using a coin that only comes
up heads in 1 out of 10 throws, the odds of getting
heads 1 or more times in 50 throws are 99 out of 100.
Similarly for local search matching, even if the proba-
bility of seeing the optimal match on a single trial is
low, the probability of seeing the optimal match in a
large number of trials is high.
The combination of iterative refinement and random
sampling has proven to be very effective. This ba-
sic form of algorithm reliably finds excellent, and
usually globally optimal, matches under difficult cir-
cumstances. The algorithm performs well even when
scenes are highly cluttered and significant portions of
a model instance are fragmented or missing entirely.
Although other methods are available (see discussion
in Section 5), the results in this paper rely exclusively
on vanishing point analysis for finding vanishing lines
in the image. This simple approach works surpris
ingly well for many man-made scenes, both indoor,
outdoor, and aerial. Vanishing points are found using
a standard Bough ttansform approach [Barn83]. Each
line in the image is entered into a two dimensional
Hough array representing the surface of a unit hemi-
sphere. Each image line 'votes' in a great (semi)circle
of accumulators, and potential vanishing points are de-
tected as peaks in the array where several great circles
intersect. For most man-made scenes, either two or
three clusters will dominate the Hough array; clusters
corresponding to the vanishing points of the two or
three dominant line directions in the scene. Each pair
of vanishing points defines a vanishing line for planes
consistent with those line orientations.
At present, only a four parameter affine version of
the local search matching system is implemented. We
therefore needed to know the calibration parameters of
the camera for each experiment. It should be stressed
that only rough knowledge of the calibration param-
eters is generally needed to find acceptable matches.
The most important parameters to determine are fo-
cal length and aspect ratio. We assumed for all our
experiments that the image center was at the center
of the image, and the optical KI and Y axes were per-
pendicular (no skew) and aligned with the row and
column axes of the raster image. Aspect ratio was
determined from the camera manufacturer's specifica-
tions, when available, otherwise it was assumed to be
one-to-one (s4uare). The focal length for each experi-
ment was computed as a byproduct of vanishing point
analysis and apriori knowledge that the actual angle
made by the two dominant line directions is 90 degrees
[Capr90]. This focal length was chosen by finding the
distance of the focal point from the image that re-
sulted in perpendicularity of the two vectors from the
(variable) focal point towards the two (fixed) vanishing
points in the image.
Figures la) and b) show a set of straight line segments
extracted from an image of a wall poster using the
Burns algorithm [Burn86], and a set of model lines to
be matched to the image. The first stage in match-
ing is to detect two clusters of lines converging to the
two main vanishing points in the image, and from the
resulting vanishing line rectify the image to present a
frontal view of the poster (Figure 1c).
The four parameter affine match found by the lo-
cal search matching algorithm yielded a set of cor-
respondences between model lines and image lines.
These correspondences were used to estimate an eight
parameter planar projective transformation to bring
the model lines into registration with the image data
lines, using the least-squares estimation procedure of
[Faug88]. Figure 1d shows the transformed model
overlaid on the input image lines.
Because it does not rely on computing 3D object pose,
The current formalism extends easily to image to im-
age correspondence matching. In this case, both im-
ages are rectified using the techniques of the last sec-
tion, and one is treated as the model while the other
becomes the data to be matched. The goal is to dis-
cover a transformation that maps one set of rectified
image lines into another.
When both cameras are calibrated, both images can be
rectified into frontal views of the object plane. Since
the mapping from one image to another can be written
by inverting one transformation and composing it with
the other, and since the four parameter affine group is
closed under inversion and composition, the resulting
image to image transformation can be described by
a four parameter affine similarity map. As may be
expected, when either camera is uncalibrated, the re-
sulting transformation between unwarped views is a
general six parameter affine mapping.
Figure 2shows an example of image to image matching
in the context of aerial image registration. Figures 2a)
and b) show sets of extracted straight line segments
from two aerial photographs. In the first image, the
camera is nearly perpendicular to the ground plane,
a fact verified by vanishing point analysis, which finds
two orthogonal sets of nearly parallel lines. The second
image is clearly not a frontal view.' Figure 2c shows
the image after rectification based on vanishing point
analysis.
To apply the local search matching algorithm, image l
was assumed to be the model and the unwarped lines
from image 2 the data. Both line sets were filtered
to only include lines greater than 100 pixels long, re-
ducing the matching problem to 55 long lines in one
image and 68 lines in the other. The best match found
is displayed in Figure 2d.
The domains we anticipate are scenes depicting either
indoor or urban outdoor environments with much pla-
nar and parallel linear structure. Such scenes often
contain lines and planes in two or three dominant
directions. The approach to matching taken in this
paper requires each plane to be matched separately,
therefore there needs to be some way to partition lines
in the image into sets belonging to planes in the world.
TThis would be nearly impossible in monocular images,
were it not for the rich structure of man-made envi-
ronments, suggesting domain-specific heuristics based
on corners and perpendicularity. In particular, L-
junctions made of two lines from different vanishing
point clusters are good candidates for coplanar cor-
ners, We are currently exploring heuristic geometric
methods, as well as more formal approaches based on
projective invariance, for partitioning image lines into
coplanar groups.
We are also exploring other methods besides vanish-
ing point analysis for detecting the horison line of an
object plane's image projection. Some possibilities are
analysis of texture gradients [Gard91], and exploita-
tion of the perspective properties of convex planar
curves [Arns89].
When structures are present in the scene that devi-
ate significantly from coplanarity with respect to the
viewing distance, then their correspondences may not
be adequately found using the above techniques. Bow-
ever, to the extent that aome scene features are copla-
nar and are found, this initial set of planar corre-
spondences provide strong constraints on the remain-
ing features, particularly when the cameras are cali-
brated, in which case the relative rotation and direc-
tion of translation between the two camera positions
can be computed from the planar perspective trans-
formation [Faug88]. This reduces the problem to that
of induced stereo, where point correspondences must
lie along known epipolar lines. Even for uncalibrated
camera systems, knowledge of the perspective trans-
formation relating the image features of a single plane
in the scene constrains the positions of point features
in one image to lie along epipolar lines in the other
image.
In its current form, the kcal search affine matcher de-
scribed in this paper is used for image to image feature
matching simply by declaring the features in one image
to be a modelL This is not ideal, since the the current
treatment of model and image lines is not symmet-
ric. Future work on the affine matcher may include
developing a more symmetric error metric for image
to image matching, and extending the range of the
match transformation space to handle six parameter
affine matching so that images from uncalibrated cam-
era systems can be used.
According to sociologist Julien Freund, conflicts
among groups of people arise because they become
hostile over maintaining, reaffirming or restoring
their rights [3]. From the point of view of one of the
groups, it is necessary to break the resistance of the
rnembers of the other groups. The Latin word hostis
azd the Greek word polemios describe this type of
rival or enemy, rather than inimicus (Latin) or ejzrdz
G:eek), which characterize a private opponent [8].
Social conflicts may arise because different groups
told opposed economic, political, national or reli-
gfo2s views, For example, an economic conflict occurs
wEen the organized workers of a certain sector ask
fc: salary increases the organized managers consider
excessive. An example of a political/international
confict may arise because two Governments claim
screreignity over a certain geographical region for
tteir respective countries. In both cases, there may
be mediators (e.g., the Ministry of Labor in the for-
mer and the United Nations in the latter).
A conflict of this kind may be settled by force. Al-
teratively, there may be negotiation, that is, each
part agrees to talk in order to accept part of the
:tes' point of view if its own claims are partly ac-
zeted as well.
A typical negotiation setup is composed of two or
more parties (eventually one of them may be a medi-
ator), each represented by a group head or negotiator
and several assistants or advisers, The speaker of
each group is the negotiator and the assistants pro-
vide him with the necessary specialization in various
fields. Thus, for an international peace talk, for in-
stance, assistants may be experts in subjects such
as international law, geography, military affairs, na-
tional politics, etc. Typically, assistants are seated
behind their negotiator.
During the negotiation,an individual assistant may
approach his negotiator and deliver a short verbal
comment or quickly pass him a written note. In many
cases, that is all the information the negotiator gets
from his group during the talk. A negotiator may
request a break or a new meeting in order to have
enough time to discuss new proposals with his advis-
ers, Therefore, the negotiator-assistants interaction
during a meeting is poor, and the negotiator may get
worried of saying too much because he does not have
enough input from the experts of his side. As a conse-
quence, interruptions slow down the negotiation. On
the other hand, if a negotiator does indeed give away
what it is more than convenient to the other party
due to lack of counsel, later on he may be unwilling
to give any other concession and this may result in
stalling progress in other aspects of the negotiation.
An improvement over the previous situation is to
furnish a voice communication system linking nego-
tiators and their assistants. This system would com-
plement the traditional voice communication system
designed to listen the current speaker, in more than
one language if translators are provided. Bowever,
the interaction can be much richer if a multimedia
system is available: the negotiator can continue lis-
tening to the current speaker while asking counsel
with a few hand movements over a computer mouse,
or quickly see the first opinions of his experts on a
recent proposal presented by the opposing party.
An additional shortcoming of the traditional ap-
proach to negotiation is that the advisers do not en-
joy privacy or isolation to exchange views during the
meeting. As a result, the input they can provide to
the negotiator is only based on personal opinions.
These considerations led to the design of SHINE
(Shared Interactive Negotiation Environment), a
collaborative software system currently under con-
struction at the University of Chile.
Collaborative systems have been applied to brain-
storming [10], document writing [?, 7], drawing [l].
spreadsheet use [4], folder circulation [5] and software
development [6]. Among their advantages over stand-
alone systems, include freer participation, increased
productivity, and in the case of remote systems, no
burden of personal transportation to a meeting site.
Our proposal calls for a redesign of the physical ne-
gotiating environment (Fig. 1). A separate room is
provided for each group of assistants. This room has a
window to the negotiating room and a video monitor
to receive the audio and close-ups from the speaking
negotiators. Each assistant is provided with a work-
station and a video camera focusing on him.
The negotiators remain on the main floor, each
with a workstation in front. Workstation screens
are only visible by their corresponding negotiators.
There is a large electronic board, controlled from a
terminal by a technician.
A negotiator may speak as usual, but he may also
send short text messages to designated negotiators
by typing on his workstation and send text, pictures
or tables to the technician to be displayed on the
electronic board. Be may also have a contents-rich
communication with his assistants.
Up to seven advisers engage in a permanent face-
to-face meeting in their room and they communi-
cate through their workstations among themselves
and with their negotiator. They can have private
or shared views over text documents and send tables
or pictures to each other. Inspiration for the idea of
the assistants' reduced participation in the meeting
was taken from an early experience in group dialog
reported by Sheridan [9].
The advisers may send the negotiator written pro-
posals, short telegrams of high priority and evalua-
tions of various kinds previously asked by the nego-
tiator. They may also send a video message consisting
of a recording of one of the advisers speaking. The
latter type of message has the advantage of letting
the negotiator watch the facial emphasis the assis-
tant wants to provide. Bowever, it has the disadvan-
tage the negotiator has to switch audio channels and
perhaps miss another negotiator's speech.
The human-computer interface of SEINE was care-
fully designed considering that first, the users are
supposed to be computer novices, and second, the ne-
gotiator can not afford much time to interact with his
workstation. Thus, the user interface is icon-based.
The negotiator communicates with two basic
paradigms displayed on his screen: a push-button
panel and a bulletin board. The panel is very simple
to manipulate: the user chooses an icon, presses a
mouse button and the action occurs. For example,
if an icon signals that mail messages have arrived,
choosing it causes a window to be opened showing
the contents of the messages. The bulletin board is
of simple operation as well and it is explained in the
next section.
Display of results is also visual. For example, eval-
uation results are shown using faces, with obvious
meanings: smiling, annoyed, etc.
Figure 2 shows the negotiator's workstation screen.
The control panel is a private window shown on the
left hand side. In its upper part, there are icons
depicting the assistants; the first one represents the
whole group of advisers. These icons are useful to
selectively ask the services explained below. An arbi-
trary number of advisers can be chosen for a service
by pointing with the cursor over each icon in turn
and depressing the first mouse button (selection is
indicated by an icon shade change). A second button
depression deselects the corresponding icon. People
icons are also useful to show their current state and
keep the negotiator informed. In Fig. 2, Adriana is
ont of the room while Alejandra is in the assistants'
room but she is busy with another task,
The bulletin board is a non-strict WYSIWIS win-
dow [10] shared with the advisers. There are pinned
dowm items on this board which can be opened by
the negotiator or any adviser. If one person opens
an item by selecting its pin, another window pops
up only on his screen showing the contents of the
item; selecting that pin again closes the window and
a 'closed'' item icon appears on the board. Figure 2
shows three closed items: an evaluation, a reguest and
an agreement, and one open request. The closed re-
quest is ready while the advisers are still working on
the closed evaluation.
The open request of Fig. 2 illustrates the type of
services the negotiator can ask from the advisers.
When the negotiator asked for it, he pressed the re-
quest button on the control panel. A dialog box then
opened (shown on Figure 3) and he wrote the note
'ow about increases..,''; then he chose two advisers
(Edgardo and Eduardo) and selected the Send button
on the dialog box. Thereafter, the box disappeared.
Meanwhile, a shared window appeared at the chosen
advisers' workstations. Perhaps they talked about
the subject and each wrote a note; when they were
finished, one of them pressed a send button. Since
it was a shared service, the system asked for confir-
mation, and once received, the shared window was
gone and a ''ready'' signal appeared in the respective
closed item of all bulletin boards of the negotiating
group.
An evaluation is a service the negotiator may ask
the advisers to do. The assistants are asked to vote
on one or more texts. The vote format is very sim-
ple: only five choices are available and range from
ezcellent to unacceptable . An adviser simply selects
a ''face'' representing his choice and then selects the
urn at the side of the text being evaluated. The eval-
uation can be formatted by the negotiator himself
or he may ask one of the advisers to prepare every-
thing. The negotiator decides this after he chooses
the evaluation button from the control panel: the di-
alog box that pops up has buttons in its upper part
to send the prepared poll to the selected advisers or
send an unprepared poll to an adviser for its com-
pletion. The dialog box in this case has a vertical
line separating two columns: the left one to contain
text and the right one to place urns, Urns are placed
in front of the text to be evaluated, simply by de-
pressing the mouse button when the cursor is in the
right place: the system automatically inserts an urn;
in fact, in order to further help the user, the cursor
changes shape to a shaded urn when the cursor is in
the right column.
Important features of communication in the context
of negotiation are the specification of task urgency
by the sender and the perception of this specification,
by the recipient of the corresponding message. In re-
quests and evaluations, the negotiator specifies one
out of three urgency levels when completing the dia-
log box. The three levels of urgency have pre-assigned
suggested completion times, and when the task re-
Iated window appears at the advisers' workstations,
a sand clock in the upper part of the window depicts
the remaining time to complete the task. Therefore,
it is easy for the negotiator to choose an icon illus-
trating the very urgent concept to ask the assistants
maximum priority to get a task done quickly, as it is
the case of the open request in Fig. 2 which was a
very urgent task with a expected reply time of three
minutes.
Private communication is done with mail and tele-
gram messages. The difference between them is
again, the urgency the sender assigns to them. These
messages can be sent from the negotiator to the ad-
visers and viceversa and among negotiators. In the
case of negotiators, separate icon buttons are pro-
vided for mail and telegrarm messages from/to ne-
gotiators and advisers, in order to avoid confusions
(Figure 2]. These four buttons are reduced to two -
team mail and telegram - in the case of advisers, since
they are not permitted to communicate with other ne-
gotiating teams. The operation of mail and telegram
messages is similar; they will be referred as messages
in what follows. The receiver selection and message
sending has a similar procedure to the one needed for
sending a request.
If a negotiator chooses to send a message to another
negotiator, SHINE automatically opens another win-
dow with the pictures of the negotiators and selection
is enabled. This window is also automatically closed
when the operation is finished.
Reception of a message is signaled by a mark at
the corresponding icon in the control panel. Alterna-
tively, each workstation can be configured using the
Definitions button of the telegram or mail window
to display messages as soon as they arrive. Imme-
diate display of telegram messages is recommended
in the case of the advisers' screens in order not to
miss any urgent message from the negotiator. Defini-
tions buttons are provided in all other types of SBINE
windows to accommodate various preferences or sit-
uations.
To read or write messages, a user simply chooses
the mail or telegram icon button on the control panel.
A window is open displaying the first unread message.
Nert and Previous buttons on the window provide
the controls to read other messages. Another button
labelled Cancel lets the user close the window. Three
other buttons- WYte, Edit and Send - require further
explanation.
The Write option of the message window clears the
text part of the window and enables the user to type
a message. Edit provides several services to easily
prepare the message. Edit has a menu with options
to Paste a text from a buffer, which has previously
been Copyed from another text, Include the contents
of a file and Read from Clipboard. The Clipboard is a
private window, and the text transferred to the mes-
sage window can be all the contents of the clipboard
or only a part of it, as chosen with the Definitions
button of the Clipboard window.
Other options of the Edit menu enable the user
to Delete a part of the text been written. Undo, of
course, reverses the last action. Other Edit options
are useful also when reading messages: WYite to clip-
board, Save to file, Copy to buer. The Edit envi-
ronment is also provided in the Evaluation, Request,
Agreement and Share windows (the last two are de-
scribed below). Edit services are also provided with
the function keys on the left keypad of the keyboard
in the OpenWindows implementation for compatibil-
ity with other applications running on this environ-
ment.
Once a message is prepared, it can be sent using
the Send button. Again, the activity can be cus-
tomized with the Definitions button. For instance,
it can be specified that the system asks confirmation
before sending a message.
An adviser message may also be a video recording.
His window has an additional button called video;
when chosen, a familiar VCR control panel is drawn
on the window: record, play, pause, reuwind and ad-
vance foruard have the obvious meanings, The ne-
gotiator gets the control panel and a time duration
indication when reading the message (the record but-
ton is disabled); selecting the play button creates a
window where the video is shown.
SBINE also provides a tool to support face-to-face
discussion. The Share button in the control panel
opens a shared window for people in the same room:
negotiators or advisers. After choosing the Share but-
ton, the user chooses the people with whom. the shar-
ing is to occur (it may be all). The shared information
is saved when the person who initiated the window
selects the UUnshare button in the window. Previous
Share windows can be viewed again, saved in private
files, and deleted using the Read button of the win-
dow.
Finally, selecting the Electronic Board button of
the control panel opens a menu offering the Magni-
fier and Marher choices, The marker activates a tele-
pointer on the electronic board, with the negotiator's
name; its movement is controlled by the negotiator's
mouse and it is cancelled off by pressing the mouse
select button. The magnifier option opens a window
on the negotiator's screen showing an outline of the
electronic board; the cursor shape is now a magni-
fier. The mouse select button, in this case, opens an-
other window showing a part of the electronic board
pointed by the magnifier cursor. This latter window
has normal font size letters and vertical and horizon-
tal scrolling bars; it has edit and cancel buttons with
the usual meaning.
Figure 2 shows another type of item pinned down on
the bulletin board: an agreement. This is a presenta-
tion made by the assistants to the negotiator. It may
be a suggestion or a general proposal made by all
the assistants. To write it, one assistant chooses the
Agreement button from his control panel and writes
it similarly to a message.
The negotiator also has a mechanism to make a
general remark to the advisers: it is the Note which
gets also pinned down on the bulletin board. To write
it, the negotiator must select the typewriter icon on
the top of the bulletin board and gets a window re-
sembling a message window.
Comments can be placed by the advisers on any
item of the bulletin board. They simply select the
item on the bulletin board and then select the Com-
ment button. At this point, they are allowed to write
a comment which is afterwards concatenated with
other previous comments. These comments can be
viewed by choosing the Comment icon in the respec-
tive bulletin board item window.
Face votes can also be placed over requests, notes
and agreements. An adviser selects one of the faces
shown on the lower part of the control panel and se-
lects the item window. The system will place the
person's name under the face on the control zone of
the window.
Any comment or vote appearing on bulletin board
items are signaled in their closed (pinned down) for-
mat, For example, in Fig. 2 the closed Request has
votes and comments,
As it was mentioned, Share sessions can be reviewed
afterwards. Read messages are not automatically
deleted by the system and thus they can be reviewed
as well,
The log feature in the control panel displays - when
selected - a listing of all SHINE interactions of the
negotiating team including the time of occurrence.
A prototype of the system is under development. The
interface has been written in Guide 3.0 [11] using Sun
Sparc workstations running X Window X11R5 and
OpenWindows 3.0. Implementation is being done
using a client-server architecture: there is just one
server and all workstations are clients; therefore, par-
tition of the workstations into negotiating parties is
internally managed by the server.
The objective of this prototype is to show the capa-
bilities of the system and let some people involved in
real negotiations try it. This prototype has no built-
in tight security measures which a system applicable
to real negotiations should have. Also the approach
taken to issues such as performance, reliability and
data management is simple and not appropriate for
a final software product.
The design of a collaborative multitmedia system to
assist large-scale negotiation has been described. Its
implementation is under way and therefore has not
been tested yet.
Simplicity of use is one of the main design objec-
tives and perhaps many negotiators may get used to
the system. In the worst case, at the beginning, one
assistant may sit beside the negotiator and handle
the workstation, showing him all interesting input
the other assistants may provide him. In the long
run, even the most technology-opposed negotiators
may feel attracted to ask for services from his collab-
orators through the workstation.
It is not clear the usefulness of the video-voice mes-
sages, It may become valuable in some cases and not
in others, Other tools may be ignored by a negotiat-
ing team: they may not use agreements and notes at
all, and decide to use telegrams only for very impor-
tant messages, for example. In this same fiexibility
of tools vein, the Definitions feature makes possible
adaption to different preferences and cases.
Alejandra G&lvez helped in the production of draw-
ings used in this paper.
The focus of the UMass mobile robot navigation
project is robust landmark-based navigation, with
a focus on automated model acquisition and model
extension. Thus, for navigation in unmodelled or
sparsely modelled environments, our general
scenario would involve the initial acquisition of
prominent visual features that can serve as
landmarks. This initial phase of partial model
acquisition is necessary because there are few
situations where a model of a complex outdoor
scene will be available a priori, Once a sparse
model is available, then the vehicle position and
orientation (i.e. pose) can be recovered by
recognizing landmarks. The model extension
phase involves tracking new unmodelled features
(points and/or lines), and using the landmarks and
partial model to determine the camera pose for
triangulation of the new features and incorporation
into the 3D model.
Most of the algorithms have been described in
previous IUW proceedings and the general vision
literature [Beveridge 92, Kumar 92, Sawhney 92,
93]. These algorithms have been shown to be very
accurate in many indoor experiments using a
camera mounted on a mobile robot and on a
moving robot arm. One new experiment that
integrated several components involved the
detection of shallow structures - an aggregatation
of line features that can be approximated in an
image sequence as a frontal planar surface. The
3D position of these features served as the
acquired model, with a depth error of less than 4%.
As motion of the camera continues, the model is
extended with depth information on other tracked
points to accuracies of less than 2% error in depth.
The UMass Mobile Pcrception Laboratory (MPL)
is based on a significantly modified HMMWV.
The design of the overall system includes actuators
and encoders for the throttle, steering column and
brakes that closely match those being used by
CMU, controlled by 68020s in a 6u VME cage.
The low-level control software for controlling
speed and stecring angle will also be the same as
that of CMU. The modifications and component
installation is being performed by Red2Zone, Inc., a
Pittsburgh-based firm specializing in custom
robotics, and was completed at the beginning of
February 1993.
Electrical power is supplied by a 10kW diesel
generator, whose output is split into two 5YW
circuits. The first circuit is conditioned and
backed by a 5kW uninterruptible power supply
(UPS) system, and is used to supply power to all
sensitive electronic equipment. The sccond circuit
is not conditioned and is used to power the air
conditioners. Both circuits are attached to a shore-
power hook-up that provide an alternative power
source to the on-board generator.
The physical lay-out of equipment was designed to
The first programmer station is located in the
HMMWV's passenger seat, with a 17'' color x-
terminal fixed to the metal platform between the
passenger's and driver's seats. The second
programmer station is located bchind and slightly
above the driver, and includes a car seat, mounting
brackets for both an SGI color terminal and a
small SONY monitor for viewing raw TV signals.
The back of the vehicle is filled with equipment.
On the driver's side of the vehicle, behind the
second programmer station, is all equipment
associated with providing power. On the
passenger's side there are four enclosed, air
conditioned 19'' computer frames for the on-board
computer systems. The first frame will hold the
6u VME cage for throttle, brake and steering
controllers and a second 6u VME cage for holding
digitizers, image frame stores and a Datacube
MaxVideo20. The second computing frame will
contain a 9u cage for the Silicon Graphics four-
node multiprocessor, as well as the SGI's disk
drives, power supply and (removable) tape drive.
The third frame is reserved for the Image
Understanding Architecture (IUA). The fourth
frame is for future additions, including video
recorders for collecting data and recording
experiments. Together, the four frames take up
the length of the vehicle's bed, as do the
programmer station, UPS cage and generator on
the left side.
The vehicle's sensor package includes a Staget,
which is a rotating stabilized platform being
supplied to the UMass and CMU vehicles by
TACOM. The UMass Staget is mounted on a
level platform located at the center of the roof of
the cab. We are planning to put two CCD color
cameras on the Staget, one with a wide angle lens
and the other with a telephoto lens. The first will
be used to locate landmarks in the larger scene,
and the second will be used for landmark matching
and accurate pose refinement. The Staget will also
contain a FLIR sensor. The Staget's hardware is
mounted above the drivers head in the enclosure
originally occupied by the HMMWV's NBC
system. Forward of the Staget, at the edge of the
cab's roof, is a long (5'' by 12'' by 12'') rectangular
enclosure with a glass front and hinged roof for
forward-looking stereo cameras.
MPL is an experimental laboratory for testing and
integrating different approaches to problems in
autonomous navigation, including, but not limited
to, landmark-based navigation, obstacle detection
and avoidance, model acquisition, and road
following. It is therefore important that MPIL have
a software environment where multiple visual
modules, addressing different subtasks, can be
easily integrated, and where researchers can
quickly experiment with different combinations
and parameterizations of those modules. At the
same time, MPL's software environment must be
efficient enough to meet the demands of real-time
navigation research.
The need to balance between flexibility and
efficiency has led us to design a software
environment with two major components: the
ISR3 in-memory data store, and a graphical
programming interface adapted from Khoros.
ISR3 is the glue that binds independent visual
modules together [Draper 93a]. It is an in-memory
database that allows users to define structures for
storing visual data, such as images, lines and
surfaces. ISR3 then serves as a buffer, so that, for
example, lines produced by one module can be
used by another, even if the second module is run
later or on a different processor than the first.
ISR3 also provides modules with efficient spatial
access routines for visual data, and protects data
from being simultaneously modified by two or
more concurrent processes. The graphical
programming interface allows programmers to
easily sequence modules and modify their
parameters.
A preliminary version of a bchaviour-based
system for determining vehicle pose from known
landmarks has been designed. It is assumed that
pose estimates and associated covariance (error)
estimates are returned from several subsystems
(GPS, INS, Landmarks, and dead reckoning)
asynchronously. These estimates are continually
combined via a Kalman filter into a single pose
estimate (and associated covariance matrix
estimate) and stored in a vehicle state vector. 'The
vehicle pose error is continually monitored in a
simple loop which branches to a bchavior sclection
strategy when the vehicle pose error exceeds a
preset threshold.
The system also contains a video image frame
buffer and STAGET control subsystem. This
system maintains image and pose temporal
histories (time-stamped images and corresponding
pose estimates) in a fixed-length first-in last-out
queue. This information is available to the
remainder of the system. The STAGET control
interface permits the STAGET to be repositioned
relative to the vehicle and maintains information
about the various STAGET parameters and
conditions, including information about the current
lens aperture and focal length.
All the landmark matching and pose refinement
algorithms have been tested extensively, although
to a great extent only in indoor domains. A large
portion of the original LISP has been ponted to C.
The plan for the coming year of research is to
develop the following behaviors: road following,
obstacle avoidance, landmark detection, landmark
tracking, and model extension.
Initially, two types of landmark processing
behavior will be specified. 'The first behavior for
landmark tracking assumes that a landmark (or set
of landmarks) are currently being tracked via the
STAGET and all that is necessary is that the
vehicle pose be recomputed from the tracked
landmarks. However, there are computational
tradeoffs as a function of the speed of the vehicle,
and the distance and number of landmarks. Thus,
not all landmarks may be tracked frame by frame.
The second landmark navigation behavior assumes
that no landmarks are currently being trackcd and
therefore a new landmark must be acquired. This
will involve access to a stored 3D model of the
campus environment (which initially has been
constructed a priori) in order to control the Staget
and window on subimages via the Staget.
However, the availability and density of landmarks
will vary significantly in different areas of the test
environment, and therefore model extension will
be a necessary goal. Ultimately we seek to
demonstrate that an accurate 3D model of the
environment can be acquired via exploration in a
purely bottom-up manner, while carrying out
independent goal-oriented navigation tasks.
If the world changes or the robot fails to recognize
a landmark, the robot's perception of the world
will not correspond to its current map of the world.
However, there is ambiguity in whether the errors
are in its perception or its map, and if the latter, it
must update its map.
Pinette [Pinette 91] has been developing a
principled approach to automatic map construction
and maintenance. In place of the usual
construction of a geometric map, snapshots of he
world at selected target locations along the route
are stored as the robot's knowledge of that path.
By noting places where a set of memorized routes
intersect, a topological ''road map''of routes and
junctions are represented. To retrace a stored
route, a qualitative homing algorithm based on
purely local visual servoing is employed to home
between successive target locations along the
route. This homing algorithm uses no geometric
model or positional information; rather, it servos
directly on the stored image for a target location,
choosing headings that reduce the difference
between features of the current bearings and those
in the target snapshot. A ''consistency-filtering''
algorithm has been developed for handling
incorrectly matched landmark features [Pinette
92]. It is shown that this algorithm guarantees
reliable homing as long as more than two-thirds of
the landmarks are correctly identified.
A very robust implementation of a robot
navigation system has been developed using
image-based homing with a spherical mirror for
encoding a 360 degree view at each target
location. This navigation system has been
implemented as part of an indoor manufacturing
automation application domain. It is not yet clear
whether these techniques are directly applicable to
unconstrained outdoor domains and large-scale
space.
In robot navigation a model of the environment
needs to be reconstructed for various applications,
including path planning, obstacle avoidance and
determining where the robot is located.
Traditionally, the model was acquired using two
images (two-frame Structure from Motion) but the
acquired models were unreliable and inaccurate.
Generally, research has shifted to using several
frames (multi-frame Structure from Motion)
instead of just two frames. However, almost none
of the reported multi-frame algorithms have
produced accurate and stable reconstructions for
general robot motion. The main reason seems to
be that the primary source of error in the
reconstruction - the error in the underlying motion
- has been mostly ignored. Intuitively, if a
reconstruction of the scene is made up of points,
this motion error affccts each reconstructed point
in a systematic way. For example, if the
translation of the robot is erroneous in a certain
direction, all the reconstructed points would be
shifted along the same direction.
Recently, Thomas [Thomas 93a,b] has
mathematically isolated the effect of the motion
error (as correlations in the structure error) and has
shown theoretically that including these
correlations in the computation can dramatically
improve existing multi-frame Structure from
Motion techniques. In several experiments on our
indoor robot, the environmental depths of points
from 15 to 50 feet away from the camera (and for
which ground truth data was available) were
reconstructed with errors in the 1-3% range. In
one further experiment, the multi-frame full-
correlation algorithm was first used to create a
model (a set of points) of an indoor hallway from
several initial frames of image data. This model
was then used to compute the pose of the robot
over subsequent frames using Kumar's pose
recovery algorithm. The estimated robot pose and
actual robot position in the hallway differed by a
maximum of three to four inches over a 12.8 foot
path.
Deformations due to relative motion between an
observer and an object may be used to infer 3-D
structure. Up to first order these deformations can
be written in terms of an affine transform. The
recovery of an affine approximation to image
deformation has recently been the focus of a large
amount of research, and has found application in
such disparate areas of computer vision as image
stabilization, optical flow computation and
segmentation, structure from motion, stereo, and
texture, and obstacle avoidance.
Manmatha [Manmatha 93] has developed a
technique for measuring the affine transform
locally between two image patches using weighted
moments of brightness. Unlike previous methods,
this technique correctly handles the problem of
finding the correspondence between deformed
image patches, as is necessary for a correct
computation of the affine transform. It is capable
of determining affine transforms of arbitrary size,
whereas most previous approaches are limited to
small transforms. It is first shown that the
moments of image patches are related through
functions of affine transforms. Finding the
weighted moments is equivalent (for the purposes
of measuring the affine transform) to filtering the
images with gaussians and derivatives of
gaussians. In the special case where the affine
transform can be written as a scale change and an
in-plane rotation, the zeroth and first moment
equations are solved for the scale. In experiments
on synthetic and real images for this case, the scale
was recovered robustly and shown to give reliable
depth estimates. Work is continuing on extending
the basic techniques to the general case.
Grupen and Weiss [Grupen 93] have continued
their work on a multi-sensor approach to dextrous
manipulation. The goal of this project is the
integration of sensing and control for the task of
finding a stable grasp configuration for an
unknown object. A subgoal is the integration of
visual and haptic (proprioceptive) sensory data to
incrementally build a model of the object. This
approach uses knowlcdge of the task and the
accuracy and completeness of the model to control
the sensing actions.
The system consists of a camera mounted on one
robot and the Utah/MIT hand mounted on another.
The system calibration or identification problem
involves computing the transformation from the
coordinate system defined by the manipulator
robot to the coordinate system defined by the
camera robot. The pose determination algorithm
of Kumar and Hanson [Kumar 92] has been
adapted for this purpose. As the manipulator robot
moves, known feature points are tracked. Given
the kinematics of this robot, the pose of the camera
with respect to the coordinate frame of the
manipulator robot are computed and incrementally
refined using iterative, extended Kalman filtering.
Experiments were performed to demonstrate that
the accuracy of the filtering algorithm was
comparable to that of smoothing using a least
squares fit with all of the data, yet the computation
time was much less. An additional feature of the
method is that the kinematics of the camera robot
can be computed at the same time.
Grupen and Huber [Huber 92] have obtained 3D
surface points from the Utah/MIT hand without
the use of tactile sensors. 'The measurements used
are posture, velocities, and torques. This will be
integrated with the measurements obtained from
the camera sensor.
Recovering the shape of an object from two views
e.g, stereo) fails at occluding contours of smooth
objects because the extremal contours are view
dependent. For three or more views, shape
recovery is possible, and several algorithms have
recently been developed for this purpose. Szeliski
and Weiss [Szeliski 93] have developed a new
approach to the multiframe shape recovery
problem which does not depend on differential
measurements in the image, which may be noise
sensitive. Instead, a linear smoother is used to
optimally combine all of the measurements
available at the contours (and other edges) that are
tracked through the set of images. This allows the
extraction of a robust and dense estimate of
surface shape and the integration of shape
information from both surface markings and
occluding contours. The results provide an
extremely promising path for recovery of 3D
shape models in an industrial setting where the
motion is known.
Most knowledge-directed vision systems are
tailored to recognize a fixed set of objects within a
known context. Generally, the programmer or
knowledge engineer who constructs them begins
with an intuitive notion of how each object might
be recognized, a notion which is refined by trial-
and-error. Unfortunately, human engineering is
not cost-effective for many real-world
applications. Moreover, there is no way to ensure
the validity of hand-crafted systems. Worst of all,
when the domain is changed, the systems often
have to be rebuilt from scratch.
The Schema Learming System (SLS) [Draper 92,
93b] automates the construction of knowledge-
directed recognition strategies. Starting from a
knowledge base of visual procedures and object
models, SLS learns robust strategies for locating
landmarks in images and recovering their positions
and orientations, if necessary. Each strategy is
specialized to a landmark, taking advantage of its
most distinctive characteristics, whether in terms
of color, shape, or contextual relations, to quickly
focus its attention on the landmark and recover its
pose. Furthermore, because SLS learns from
experience by a strict generalization algorithm, it
is possible to predict both the expected costs and
the expected error rates (due to a lemma by
Valiant) of the strategies it develops.
Figural completion is the preattentive ability of the
human visual system to build complete and
topologically valid representations of
environmental surfaces from the fragmentary
evidence available in cluttered scenes. A
description of a grouping system developed by
Williams, employing a two-stage process of
completion hypothesis and combinatorial
optimization, appeared in a previous workshop
proceedings [Williams 90]. Preliminary
experimental results were also reported. Since that
time there has been significant progress in two
major areas. First, the mathematical basis for the
grouping constraints employed in the optimization
stage has been clearly elucidated. This has
allowed a proof of the necessity and sufficiency of
the grouping constraints for scenes composed of
flat embeddings of orientable surfaces with
boundary. Second, a more advanced grouping
system which uses cubic Bezier splines of least
energy to model the shape of perceptual
completions has been implemented. The new
system is demonstrated on a number of figures
from the visual psychology literature which are
beyond the capability of the old system.
During the past year, Dolan has continued his
work on curvilinear grouping [Dolan 92]. A
SIMD implementation of the curvilinear grouping
system has been developed, along with a
simplified, distributed representation of curves for
use in the CAAPP. The integration of multiple
grouping processes--in particular, curvilinear and
area grouping -- is currently being examined.
Many of these ideas are being incorporated in a
general grouping module for KBVision, which
will facilitate research and experimentation with
many diverse forms of grouping.
The use of projective invariants for object
recognition and scene reconstruction has been the
subject of intense interest in the image
understanding community over the past few years.
Although classic projective geometry was
developed with mathematically precise objects in
mind, practical applications must deal with
errorful measurements extracted from real image
sensors, A more robust form of projective
geometry is needed, one that allows for possible
imprecision in its geometric primitives. In his
Ph.D. thesis [Collins 93], Collins represents and
manipulates uncertain geometric objects using
probability distributions in projective space,
allowing valid geometric constructions to be
carried out via statistical inference. The result is a
methodology for scene reconstruction based on the
principles of projective geometry, yet also dealing
with uncertainty at a basic level. 'The effcctiveness
of this framework has been demonstrated on
several geometric problems, including the
derivation of 3D line and plane orientations from a
single image using vanishing point analysis, the
extraction of a planar patch scene model using
stereo line correspondences, and the reconstruction
of planar surface structure using multiple images
taken from unknown vicwpoints by uncalibrated
cameras.
More specifically, Collins shows that projective N-
space can be visualized as the surface of a unit
sphere in (N-+ 1)-dimensional Euclidean space.
Each point in projective space is represented as a
pair of opposing or antipodal points on the sphere.
By the identification of projective space with the
unit sphere, antipodally symmetric probability
distributions on the sphere may be interpreted as
probability distributions over the points of
projective space, and standard constructions of
projective geometry can then be augmented by
statistical inferences on the sphere. Probability
densities defined in this way can also be used for
representing uncertainty in unit vectors,
orientations, and the space of 3D rotations (via
unit quatemions).
Oliensis' previous work on shape from shading
[Oliensis 92] has been extended in a number of
ways, First, while our earlier work usually
assumed that the illumination was from the
direction of the camera, the shape reconstruction
algorithms and convergence proofs have been
extended more recently to the case of illumination
from any direction [Oliensis 93a]. As before,
these algorithms are provably and monotonically
convergent, and (in many cases) can be shown to
converge to the correct surface. Moreover, it has
been shown that a whole family of algorithms
could be developed, and that all would give
equivalent surface reconstructions. This is
convenient since some of the algorithms are better
for theoretical analysis while others are more
efficient in practice. The uniqueness proofs for the
surface given the shaded image, and the corollary
that regularization is not necessary for shape from
shading, have also been extended,
Experimentation with these algorithms on
synthetic and real images show that they are fast
and robust, taking less than 10 seconds on a
DECstation 5000 for a 200 x 200 real image.
These algorithms still require that a small amount
of information on the surface be provided, namely:
1) a list of those singular points (the brightest
image points) corresponding to local minima of
the surface height (as opposed to the other
possibilities of a local maximum or a saddle
point), and 2) the heights of these singular points.
However, in a second extension of previous work
[Oliensis 93b], Oliensis has developed a new
algorithm that is capable of determining this
information automatically, and thus can
reconstruct a general surface from shading with no
a priori information on the surface. In
experimental tests on complex synthetic images,
this algorithm has produced good surface
reconstructions over most of the image. For 128
x128 images, the reconstruction takes less than 30
seconds on a DECstation 5000. Moreover, the
algorithm appears noise resistant, giving good
reconstructions even in thc extreme case of an
added pixel noise of 10%. It appears that it will
also be possible to prove the convergence of this
algorithm to the correct surface in the limit of
perfect resolution.
All algorithms thus far have assumed that the
imaged surface was matte. Even with this
restriction, the algorithms are potentially useful in
controlled industrial or research applications. At
UMass these algorithms will be ported to the
robotics laboratory environment, and used in
combination with other means of shape sensing
and recovery to aid in research in grasping
partially or unmodeled objects. Further extensions
include adapting the current algorithms to the
realistic case of a partially specular surface. With
this extension, shape from shading could become
practical for a variety of applications.
Work on the IUA [Weems, 1993] has advanced in
three areas in the preceding year: compilers and
system software, hardware and architecture, and
applications and algorithms. The IUA is a tightly
coupled, heterogeneous parallel processor being
developed by UMass, Hughes Research Labs, and
Amerinex Artificial Intelligence (AAI) under
DARPA funding. It is intended to support real-
time knowledge-based vision applications and
research by providing three distinct parallel
processors in a single architecture: a fine-grained
SIMD/Multi-associative array for low-level vision,
a medium-grained SPMD array for intermediate-
level symbolic vision, and a coarse-grained
multiprocessor for high-level, knowledge-based
processing. A proof of concept prototype of the
IUA was constructed under a previous effort and
the current work is directed at developing a second
generation of the system with enhanced
performance and the ability to be fielded in the
DARPA Unmanned Ground Vehicles (UGV)
program.
AAI has completed development of the C++ class
library for the low level of the IUA. The class
library defines a set of image plane types upon
which parallcl operations may be performed.
Work at AAI includes the incorporation of
additional optimization code into the Gnu C++
compiler so that image planes are treated more like
first-class objects in C++. An automated test
system has also been developed for the machine's
microcode library, to facilitate regression testing
of new releases. For the intermediate-level
processor, basic operating system support,
multitasking, and messaging have been
implemented on a TMS320C30 Single Board
Computer (SBC), and recently these were
transported to another SBC with two TMS320C40
processors that are configured to simulate the
intermediate level of the IUA. A debugger has
also been implemented for the intermediate level.
Work is now under way to transport the
KBVisionM system to the IUA.
UMass has implemented a version of the Apply
language for the low-level processor of the second
generation IUA. The compiler generates code
compatible with the C++ class library. It permits
us to easily import image processing operations
written for the CMU Warp or Intel iWarp
machines.
The prototype IUA has been running at Hughes for
most of the last year. Under the prototype
development contract, only a very simple
controller was built to demonstrate the basic
functionality of the processor arrays. It was never
intended that the prototype controller be fully
programmable. However, Hughes and Amerinex
AI invested additional effort to develop software
that allows C++ code for the second generation to
execute on the prototype hardware. Because of the
nature of the controller, instructions can only be
issued at VME bus rates to the array, which is
significantly slower than the array can accept
them. However, it does permit demonstration of
the functionality of the array hardware on real
applications. Hughes has since implemented the
low-level portion of the DARPA IU Benchmark,
an SDI application, and an ATR application on the
prototype.
The custom chips used in the IUA have been
fabricated and are undergoing testing. Each chip
contains 256 bit-serial processors with on-chip
cache, and has roughly 600,000 transistors. 'The
system's array control unit, backplane, and chassis
have been built and tested. Processor and memory
boards are currently under construction. The /O
subsystem for the machine has also been designed,
and will support the equivalent of 20 simultaneous
sensor inputs at 512 X 512 X 8-bit resolution with
automatic mapping onto the processor
virtualization schcme used for the low level, with
almost no latency. The I/O subsystem will also
support the selection of multiple regions of interest
from an image. Hughes has indicated that the first
machine should be assembled by the end of
Fcbruary, 1993.
Work has already begun on the analysis and
design for the third generation IUA. UMass has
developed a system for capturing traces of
programs written in the C++ class library as they
execute on an abstract parallcl machine. The
traces are then fed to a simulation system that
models hardware architectures with different
features and parameters. 'The system allows us to
gather real performance data for different
architectural configurations, and to analyze the
data statistically. The performance data will then
be contrasted with cost estimates for the different
configurations to produce a specification for the
third generation IUA.
The low-level processor of the IUA is a square
mesh of processing elements, augmented with a
second (reconfigurable) mesh, called the Coterie
Network , This network allows the mesh to be
partitioned, for example, into areas corresponding
to regions in an image. One particularly useful
operation is the ability to enumerate elements
within a partition or to summarize (reduce) the
information in a partition to a single value. Thc
parallel prefix operation is the general form of this
type of operation. It is especially desirable to be
able to carry out parallcl prefix in all partitions at
once, i,e. to perform a multi-prefix operation
[Herbordt, 1992]. An algorithm has been
developed for multi-prefix that is significantly
faster than alternatives using general purpose
routing in the mesh.
As recommended by the DARPA IU Benchmark
Workshop participants, much of the benchmark
[Weems, 1988, 1990] has been recoded as a set of
library routines which are called by the core of the
benchmark. We have also begun developing the
second level benchmark, which will incorporate
tracking of moving objects over a sequence of
images. The goal of the new benchmark is to test
system performance over a longer period of time
so that, for example, caches and page tables will
be filled. The benchmark will also explore /O
and real-time capabilities of the systems under
test, and involve more high-level processing.
UMass has developed a parallel algorithm for the
IUA that computes a dense depth map for a scene
from a pair of images taken by a moving sensor
[Dutta 93]. The algorithm has an average error of
about 8 percent in depth, as computed from
randomly sampling points corresponding to
objccts in the scene with known distances from 21
to 76 feet from the camera. The experiments were
done with fairly large displacements (four feet of
forward motion between the images) so that a
large (41 X 41 pixel) search window was required
to establish correspondences, resulting in 1681
image-to-image correlations being performed. In
simulations of the second generation IUA, it was
determincd that the execution time will be about
0.54 seconds, of which 0.53 seconds is taken up
solely by the correlations. We are thus looking
into approaches in which an estimate of the motion
is available or in which a series of frames with
smaller displacements can be used (allowing the
search window to be constrained).
UMass has also developed a parallel algorithm for
extracting straight lines from an image. Using the
second generation IUA simulator, the algorithm
executes in as little as 31 milliseconds for images
that map to the array with a l:1 virtualization ratio.
We are currently evaluating the quality of the
results, but a preliminary examination indicates
that the algorithm gives very consistent lines over
sequences of images, which is an important
attribute in the support of algorithms that use line
tracking.
UMass is developing mechanisms for site model
acquisition, extension and refinement [Collins
93a] based on technology that has already proven
effective in the mobile robotics domain..
Automatically acquiring the initial 3D site models
from scratch is a challenging problem that will be
the focus of future research. Our current work
assumes that a partial model of the site is provided
apriori by the image analyst. Our model-based
refinement and extension algorithms are then
applied to automatically correct inaccuracies in the
initial site models, and extend them to include
previously unmodeled cultural features (buildings,
roads, etc.) based on information from new
1mages.
Rather than building a tum-key system, UMass
will be delivering a set of modules for performing
specific tasks of direct benefit to the image
analyst. The following is a list of the early
deliverable modules that are currently being
evaluated on the model board test imagery
supplied to the research community.
In addition to developing new techniques for
automatically acquiring initial site models, new
research will investigate statistical techniques for
applying projective invariants to the modeling
process to accurately derive structure without
explicit camera models or knowledge of
viewpoint. Initial experiments in this direction
have yielded promising results. OUher encouraging
results have been obtained regarding the difficult
problem of image to image registration. A
technique based on vanishing point analysis
[Collins93b] allows an oblique acrial view to be
rectified (unwarped) to present a simulated vertical
view, allowing full pcrspective acrial images to be
registered with a computationally tractable affine
matching approach.
lmage Understanding research at the MTT A1 Lab has
continued along a range of fronts, from low level process-
ing, such as stereo, motion, color and texture analysis,
through intermediate stages of integration of visual infor-
mation, to higher level tasks such as object recognition
and navigation. This report summarizes our main recent
accomplishments in these areas. As is usual in these re-
ports, we refer interested readers to other publications
for more details.
Because it has been one of our central focal points, we
bbegin with our recent work in object recognition. In ap-
proaching the problem of recognizing ob jects from noisy
immages of cluttered scenes, we have found it convenient
to separate out several different aspects of the problem:
We will describe our recent work in each of these areas.
We have argued for some time that robbnst and efficient
solutions to the selection (or grouping) problem are es-
sential to practical recognition systems. Earlier work,
using both formal analysis and experimental studies [22:
13; 27], has shown that the complexity of many ap-
proaches to recognition are dramatically reduced if de-
cent selection is provided, and that the false posi-
tive/false negative rates for such methods are also sig-
nificantly improved with good selection.
One advantage of focusing on the issue of selection for
recognition is that it provides constraints on the reqnire-
ments of early processing stages. For example. cues snch
as color or texture are often considered from the view-
point of extracting ob ject surface measurements, wtich
requires that one account for illumination and other
scene effects in inverting the image measure ments to ob-
tain object parammeters. If one simply wants to use these
cues to separate regions of an image likely to have cone
from a single object, much less stringent requirements
are placed on the task, leading to simpler and hopefully
more robust algorithms.
Towards this end, Tanveer Syeda-Mahmood has re-
cently completed a Ph.D. thesis [46] that explores the
role of cues such as color and texture in selecton for
recognition. She does this by developing and impleaent-
ing a computational model of visual attention, which
serves as a general purpose selection mechanism in a
recognttion system.
The approach supports two modes of attentional 1se-
havior, namely attracted attention and pay-attenlon
modes. The attracted attention mode of hsehavior is
spontaneous and is commonly exhibited by an unbiased
oobserver (i,e., with no a priori intentiotns) when son:
object or some aspect of the scene attracts his / her at-
tention, while the latter is a more deliberate behavior
exhibbited by an observer looking at a scene with a pru
goals (such as the task of recognizing an ob ject, say ) and
hence paying attention to only those obbjects/aspects .d
a scene that are relevant to the goal.
Briefly, the model suggests that the scene represented
by the image be processed by a set of interacting featurc
detectors that generate a hierarchy of maps, representing
features such as brightness, color, texture, depth, group-
ing of edges, and others such as shape, size, symmetry,
etc. The feature maps are then processed by filters in-
corporating strategies for selecting distinctive regions in
these maps. The choice of these strategies is guided by a
central control mechanism that combines top-down task
level and a priori information with the bottom-up infor-
mation derived from the features, to demonstrate either
mode of attentional behavior as desired. Finally, an ar-
biter module housing another set of strategies selects the
most significant features across the feature maps, which
can then be used in an object recognition system.
A system implementing the computational model de-
scribed here was built using three features: color, tex-
ture, and parallel-line-groups. The respective feature
maps were built, and the selection filters for finding dis-
tinctive regions in these maps have been developed, In
addition, a version of the arbiter module to combine the
saliency information from the various features has been
built.
Because we are interested mainly in separating regions
likely to have come from a single object, we do not need
to exactly recover ob ject pararmeters such as body color.
Rather, we can focus on methods that describe the color
image as consisting of perceptually different colored re-
gions. This can be done by focusing on the components
of a color signal that are most relevant to human cate-
gorization of colors (e.g. saturation and brightness, but
not hue). Syeda has developed such a method of per-
ceptual categorization of a color-space, which supports
fast color region segmentation. A color saliency map was
then built which used a color saliency measure that em-
phasized attributes that are also salient in human color
perception. The key point is that such a saliency mea-
sure serves to highlight regions of interest for a recogni-
tion svstem.
The texture feature map was generated by regarding
the image as being generated by a space-limited station-
ary stochastic process. Here, the segmentation of the
textured image was obtained by a comparison of the lin-
ear prediction spectra of adjacent windowed regions of
the image. Properties such as the relative distribution
of dark and bright blobs were then made use of to judge
the distinctiveness of a region. This was used to generate
the texture saliency map.
Lastly, the parallel-line-groups feature map high-
lighted groups of closely-spaced parallel lines in an edge
image. It has been found that some texture information
can be modeled this way. For example, printed letters on
a surface (such as a bottle) appear as a bunch of closely
spaced parallel lines when passed through an edge de-
tector. Similarly, some types of wooden tables show this
type of texture in an image.
These feature maps can then be combined to isolate
regions of an image for analysis by a recognition system
(in our implementation this was a combination of a tree
search algorithm and a linear combinations approach).
IThe feature maps and their associated saliency maps can
be driven in a pay-attention mode, in which the color
and texture information in the model (extracted using
the feature maps described earlier) is used to build a de-
scription of the object-model. This description was then
used to design strategies for the selection filters. This
involved developing new algorithms for finding instances
of regions in the image satisfying object-model color and
texture descriptions. Such regions are then passed to
the recognition system for analysis. Experimental results
show that the methods drastically reduce the complex-
ity of the recognition process by rejecting clutter from
consideration. The system can also be driven in attract
attention mode, in which the most salient portions of the
scene are analyzed first, again reducing the complexity of
the recognition stage. An example is shown in Figure l.
The key results here are a framework for combining
sensory information to support recognition, the use of
an attention mechanism to select targets for recognition.
and novel methods for handling texture and color infor-
mation.
Even if we can isolate key regions of an image, we still
need to know what objects may be present. In some
tasks, we are looking only for a specific target, or a small
number of targets, in which case model-driven selection
can be used to isolate regions of interest. In other cases.
we need to consider large libraries of ob jects, in which
case we need some means of using selected image featnres
to identify subsets of the library as candidate models.
CCCues such as color and texture, discussed above, can help
with this object indexing. In general, however, other
information is also needed.
David Jacobs, as part of his recently completed Ph.D.
thesis [2T], has shown that simple aspects of an object's
shape can often be used to efficiently index ob jects in
a library. Jacobs' method views the indexing problem
as stating: can one compactly represent the space of all
images that an object model can create, given the type
of projection model used'? If one can, then to handle
the indexing problem, we can precompute each model's
manifold of possible images in an image space. At recog-
nition time, we can compute the associated representa-
tion of the current image, use this to access image space,
and retrieve all models that could have caused it. The
key question is whether one can actually represent all
possible images of a model in an efficient way. Jacobs
has shown, perhaps surprisingly, that in several nontriv-
ial cases, one can.
IThe results are summarized as follows. We assume
that a 3DD object is transformed by an arbitrary affine
transformation, followed by a scaled orthographic pro-
jection. For the case of 3D points, this is equivalent to
applying an arbitrary 3 x 3 matrix to the points, then
translating them, then projecting them. To describe the
images that a model can produce under this class of
transformations, we first define image space, then deter-
mine the shapes of the model manifolds. Jacobs argues
for using affine coordinates to represent image space. In
particular, if one selects three ordered point features to
establish a basis, then all other points can be written in
terms of coordinates with respect to this basis: that is if
2).45,---4,; denote the image points, and if
are the affine basis, then all other points can be repre-
sented by coordinates (o, ;) by
These coordinates are invariant to any affine transfor-
mation, and hence an image is uniquely identified by
It turns out that the first three vectors do not provide
any information about whether a scene could produce
this image, so we use only the (o;, [7) parameters to rep-
resent an image. Thus, an image with n ordered points
maps to a point in a 2(n - 3) dimensional space, which
can be divided into two orthogonal n - 3 dimensional
spaces, one for the a coordinates and one for the co-
ordinafes. The advantage of doing this, as Jacobs has
shown, is that the set of all images that a model of n
ordered points could produce is simply a pair of parallel
lines, one in each space. In this case, indexing simply
says, given an image basis, compute the affine coordi-
nates of the points, then find all model lines that pass
through the associated point in o - p space. One must
allow for uncertainty in the measurements, but this can
be shown to be easily handled and simply expands the
image points to small regions in the o and d spaces [21]
An example of the indexing system correctly retrieving
candidate models is shown in Figure 2.
Extensions of this approach to deal with other types
of features are discussed in an article by Jacobs in these
proceedings. We note as an aside that considering this
model of linear transformations has proved fruitful in
other problems, such as the linear combinations ap-
proach to recognition [48] and in dealing with affine
structure from motion [28; 41].
The key result here is a very efficient method for han-
dling indexing for some classes of ob jects, as well as a
novel framework for investigating the interactions be-
tween ob ject structure and image pro jections.
A central part of recognition, once we have foundd sub-
sets of image features of interest and sets of models
of interest, is to determine whether the image features
are in fact consistent interpretations of the model fea-
tures. Over the past several years, we have developed
a variety of different approaches to this problem, in-
cluded Interpretation Tree Search [22], Alignment [24
25] and Linear Combinations [48]. Here we report on
Some new alternatives to these approaches, as well as
improvements on these approaches.
The alignment approach to recognition [24; 25; 6] pro-
ceeds by matching a small set (typically 3) of image fea-
tures to model features, using this match to determine
the associated transformation of the ob ject (modeled as
a weak perspective transformation), and pro jecting the
remaining model features into the image for verification.
In the original system, uncertainty in the image mmea-
surements was dealt with in a somewhat ad hoc manner.
Recently [21] we have shown how to analyze the effects
of that uncertainty on the set of possible transforma-
tions. Alter [l] has extended that work to supplement
alignment approaches with a verification stage that is
guaranteed to be correct. In particular, he shows that
using a bounded error model on the image features, one
can compute the range of image positions for all other
model features, both for planar and solid objects, and
for point and line features. This allows one to exactly
specify the range of image positions over which to search
for matching features, so that one will not miss any sup-
porting evidence, while at the same time keeping the
chances of false matches minimal. One can further ex-
tend this approach by adding a Bayesian analysis of the
actual matching regions, so that one can determine the
likelihood of each verified match actually being correct.
This allows one to determine the best regions in which
to search for features, by determining those most likely
to contribute to a correct interpretation. An exammple of
the image search regions is shown in Figure 3. Exten-
sions of the method to line features has also been done.
and results show, as expected, that lines are considerably
more powerful as verification features than points.
A related result concerns the models of sensor uncer-
tainty used both in the analysis of recognition methods
and in the derivation of verification and likelihood tech-
niques. Most of our earlier work has been based on a
bounded error of sensor uncertainty. haren Sarachik has
bbeen working on the problem of estimating the effects of
sensor noise on the problem of model based ob ject recog-
nition, for other classes of uncertainty. For the analysis
it is assumed that the location of a point feature in an
image is corrupted by noise, which is modeled as a two
dimensional Gaussian distribution, and the presence of
the model in the image is posed as a binary decision
problem. The positional uncertainties of the point fea-
tures are traced through the recognition algorithm, re-
sulting in analytic expressions for the confidence level of
the algorithm's decision. tUntil now the analysis has been
completed only for one algorithm, geometric hashing as
introduced by Wolfson, Lamdan, Schwartz and Hummel.
Using this technique it is possible to explicitly compare
the expected performance of different recognition algo-
rithms and noise models, a useful tool for the domain of
multi-sensor fusion.
In earlier proceedings, we have reported on our work
in developing recognition algorithms that work directly
in the space of possible poses of an object, rather than in
the space of feature correspondences. In the ideal case of
perfect sensor data, one can simply search over all pos-
sible pairings of model and image features, compute the
associated transformation and vote for that transforma-
tion in pose space, a la Hough transforms. When un-
certainty is allowed in the measurements, however, one
must be more careful about voting for the entire volume
of transformations consistent with the pairing of a noisy
sensor measurement and a model feature, and this in-
creases the demand on searching fine tesselations of the
pose space.
Todd Cass has recently complete a Ph.). thesis that
presents an elegant way around this problem, by ex-
ploiting the geometry of pose space directly. Cass [10]
has provided a formulation of the problem in which one
can develop a polynomial-time algorithm that guaran-
tees finding all feasible interpretations of the data, mod-
ulo uncertainty, in terms of the model. The approach is
based on representing the model and the sensory data
in terms of local geometric features such as vertices and
line segments. He assumes bounds on the uncertainty
in the position or orientation of the data features due
to sensor error. He then shows that there are only a
polynomial numbber of quantitatively different transfor-
mations that align the model and the data modulo error.
Object localization is accomplished using a polynomial-
time search through the set of all model transformations
to find those that align large subsets of model and data
features within the uncertainty bounds.
Intuitively, this approach can be considered as follows.
For each pairing of a data and model feature, there is
a set of transformations that will align the model fea-
tures within the uncertainty region about the data fea-
ture. This set of transformations carves out a volume
in pose space. If we consider all pairings of data and
model features, we get a set of snch volumes, and we
are interested in finding points in the pose space con-
tained within the intersection of a large number of such
volumes. One could find such points by simply sampling
points in pose space at some fine spacing, a method used
earlier by ass in implementing a very fast recognition
scheme on the Connection Machine. It turns out. how-
ever, that one can efficiently find such volumes by decou-
pling the search over the full pose space into a coupled
search over the translational components and a second
search over the rotational components. Mooreover, one
can use the structure of these geometric arrangements to
find very efficient, polynomial-time algorithms for find-
ing the boundaries of these pose-space volumes. Uass
has extended his earlier work to allow for unknown scale
factors, unknown uncertainty values, and has used re-
sults from computational geometry to provide efficient
algorithms for exploring pose space.
An alternative to Cass' approach to analyzing Pose
Spaces is the RAST algorithm (Recognition by Adaptive
Subdivision of Transformation Space; [8]). The RAS'T
algorithm solves bounded error recognition problems ef-
ficiently.
Bounded error recognition is one of the most com-
monly usedd formulations of the visual ob ject recogni-
tion problem and has proven its utility in a numbber of
practical systems (for further references and related re-
sults, see, for example, [4], [22]). The simplest form of
the bounded error recognition problem is the following:
given a set of model features (points in Rf oE Rr8) and a
set of image features (points in R%), find maximal sub-
sets of image and model features that can be brought
into correspondence under given error bounds using rigid
body transformations.
The recognition algorithm is based on the idea of car-
rying out matching with variable sized error bounds: if.
for a given set of transformations, a good match can-
not be found for large error bounds, then matches with
smaller error bounds need not be examined. The RAS'T
algorithm uses particularly convenient representations
for sets of transformations that make it simple to im-
plement, efficient, and flexible in the kinds of features
and similarity measures that can be used with it.
So far, we have applied the RAST algorithm to 2D
recognition problems that involve a very large number
(thousands) of very simple image features (short line
segments). In such applications, the RAST algorithm
is found to be faster than alternative methods (recog-
nition by alignment, Hough transform, search, or corre-
lation). Actual applications using the RAST algorithm
include the prototype of a view-based 3D object recogii-
tion system and a system for handwritten optical charac-
ter recognition (O(C R). Examples are shown in Figure 4.
For curved objects, both the visibility and the location
of object parts/features in the image varies in a compli-
cated way with the viewpoint. This has made the de-
velopment of efficient 3D object recognition algorithms
difficult.
In order to avoid these difficulties, many 3D object
recognition systems have heuristically used vie u-basod
representations, i,e,, representations that encode ob ject
properties and shape from a large number of different
v1ewponts.
However, using view-based representations only solves
the 3D object recognition problem approximately. In
order to understand the nature and significance of this
approximation, we have formalized the notion of view-
based representations and established error and com-
plexity bounds on the performance of recognition sys-
tems that are based on view-based representations[9].
The theoretical results suggest that, for the pur-
poses of object recognition from 2D images, view-based
representations are good approximations to true 3[D
shape representation. Furthermore, we have estab-
lished model-independent upper bounds on the number
of views needed in order to represent a model in a view-
based system. Finally, a complexity analysis suggests
that view-based recognition can be carried out more ef.
ficiently than 3IDD shape-based recognition.
These theoretical results are supported by a number
of simulations and experiments on real images.
An alternative approach to recognition is to treat it
as a problem of optimal estimation. Sandy Wells has
recently completed a Ph.D. thesis [53] that develops and
tests a framework for statistical object recognition.
To formalize this, let the image that is to be analyzed
be represented by a set of v-dimensional point features
The model to be matched is also described by a set of
point features, these are represented by real matrices:
To solve the recognition problem we need to find a
legal match between image features and model features.
Here, legal means that there is some physically meaning-
ful way of positioning the model in the scene so that it
would produce image features similar to those actually
observed, We can treat this as an optimization problem,
wherein we seek to estimate two sets of parameters: the
correspondences between image and model features, and
the pose of the model instance in the image. The corre-
spondences are described by an interpretation vector
Here T; z M; means that image feature i corresponds
to model feature j, and I; sll means that image feature
i is due to the background.
The pose of the model instance in the image, d, is
a real vector. An associated projection function P' is
defined:
F maps model features into the v-dimensional image ac-
cording to the model's pose.
Our goal is to obtain estimates of the correspondences
and pose by maximizing the posterior density with re-
spect to I and d, as follows
In other words, we want to find the assignment of
model features to image features, and the related pose
(position and orientation) of the ob ject that optimally
accounts for the observed data. We can treat this as an
estimation problem, and use Baye's rule to calculate the
a-posteriori probability density of f and d:
where C is a normalizing constant independent of f and
d, Then we seek estimates for I and d that optimize
this ob jective function.
Note that we couple the effects of the objects pose di-
rectly into the matching problem. There are, of course.
some sensor features that are not directly pose related,
such as the fractal dimension of a region. These features
can also be incorporated into the matching process, ei-
ther as priors on the correspondence, or as fi]ters that
remove some correspondences directly.
To solve this optimization problem, we need several
things. First, we need to model 6. This can be done
by a careful physical modelling of the sensor, by taking
into considerating the effects of noise on the transduc-
tion process, and providing careful models of the distri-
bution of uncertainty about the measured values. Such
models can be derived for widely varying sensors, other
than visual, and in we are in the process of applying this
approach to LADAR and SA R sensors.
As an example, one simple model is to assume that
the probability density function on features is uniform
for features arising from the background, and is normally
distributed about their predicted locations in the image
for matched features. (Of course, this is a simple model.
For some types of features, we have more explicit mod-
els of the distribution of the feature, which will simply
replace the variance of the normal distribution.
Second, we need to model the probability of an nter-
pretation (or matching of features) and the probability
of a pose. One simple method is the following. The
probability that a feature belongs to the background is
H; the remaining probability is uniformly distribhuted fr
correspondences to the m model features.
Our simple model also assumes that prior informa-
tion on the pose is a normal density. With this choice
for the form of the pose prior, the system is closed in the
sense that the resulting pose estimate will also be nor-
mal. This is convenient for coarse-to-fine techniques (or
multi-resolution methods), in which we use coarse data
to get an initial estimate, then refine this by focusing on
subportions of finer resolution data.
If little is known about the pose a-priori, the prior
tmay be made quite broad. This is expected to often
be the case. Note, however, that better models would
incorporate additional information about the scene. For
example, if we know the parameters of the ground plane.
and the target is known to be in a stable position on that
plane, we should be able to incorporate this knowledgc
into better priors on the pose parameters. For exammple
if range information is also known, then only two of thc
six parameters of pose are completely unknown. T]je
others can be constrained from such additional infornm-
tion, thereby reducing the complexity of the search for a
match.
If we assume independence of the correspondences and
the pose (before the image is seen), the composite prio r
is a straightforward product of the prior distribution on
pose and the probability distributions on matched and
unmatched features. Thus, in our simple example, we
can describe the probability of a pose and a correspon-
dlence in terms of measurable quantities in the data.
Giiven this, we need efficient methods for finding op-
timal estimates for the parameters of interest. We can
choose those estimates that maximize the a-posteriori
probability (MAP), by maximizing the posterior den-
sity with respect to the matched features and the pose.
But to find such estimates, we need efficient methods for
searching the objective function.
To handle this search process, Wells has considered
several approaches including beam search through a tree
of interpretations [5l], and posterior marginal pose es-
timation [52]. The latter is motivated by the obser-
vation that in tree searches of the ob jective function
of MA P model matching, hypotheses having ''poor''
matches scored poorly in the objective function. The
implication was that sumrming posterior probability over
all matches (at a specific pose) might provide a good
pose evaluator. This has proven to be the case in the
experiment described in [51]
The essence of posterior marginal pose estimation is to
choose the pose that maximizes the posterior probability
ddensity of the pose, given an image:
The posterior probability density of the pose is computed
from the joint posterior probability on pose and match,
by taking the marginal over the possible matches:
Using Bayes' rule, the posterior marginal may isolated
as a function of the priors described above, and this
leads to a convenient ob jective function for optimization.
OOne can utilize the EM algorithm to provide an efficient
method for optimizing this ob jective function, thereby
leading to solutions to the pose problem, A more com-
plete description is given the paper by Wells in these
proceedings.
Wells has applied the method to several cases, include
2DD point features, 2D oriented range features and linear
3d projection models. An example of recognition from
visible image features is shwon in Figure 5.
A second example shows the method applied to a very
different type of imagery. This work was done in con-
junction with Group 53 of Lincoln Labs, directed by
A. GGschwendtner. In this exarmple, the features were
oriented-range features, consisting of fragments of image
edge contours, augmented with a vector pointing in the
direction normal to a range discontinuity, with length re-
flecting the inverse range at the discontinuity. Two sets
of features were prepared, the ''model features'', and the
''image features''.
The object model features were derived from a syn-
thetic range image of an M35 truck that was created
using the ray tracing program associated with the BRL
C'AD Package [16]. The ray tracer was modified to pro-
duce range images instead of shaded images. The syn-
thetic range image appears in the upper left of Figure
In order to simulate a laser radar, the synthetic range
image described above was corrupted with simulated
Iaser radar sensor noise, using a sensor noise model
that is described by Shapiro, Reinhold, and Park [40].
In this noise model, measured ranges are either valid
or anomalous. Valid measurements are normallv dis-
tributed, and anomalous measurements are uniformly
distributed. The corrupted range image appears in Fig-
ure 6 on the right. 'To simulate post sensor process-
ing, the corrupted image was ''restored'' via a statisti-
cal restoration method of Menon and Wells [31]. The
restored range image appears in the lower position of
Figure 6.
(Oriented range features were extracted from the syn-
thetic range image, for use as model features - and from
the restored range image, these are called the noisy fea-
tures. The features were extracted from the range images
in the following manner. Range discontinuities were lo-
cated by thresholding neighboring pixels, yielding range
discontinuity curves. These curves were then segmented
into approximately 20-pixel-long segments via a process
of line segment approximation. The line segments (each
representing a fragrment of a range discontinuity curve)
were then converted into oriented range features in the
following manner. The A and Y coordinates of the
feature were obtained from the mean of the pixel co-
ordinates. The normal vector to the pixels was gotten
via least-squares line fitting. The range to the feature
was estimated by taking the mean of the pix.el raugr-
on the near side of the discontinuity. This information
was packaged into an oriented-range feature. The model
features are shown in the fourth image of Figure 6. Each
line segment represents one oriented-range feature, the
ticks on the segments indicate the near side of the range
discontinuity. There are 113 such. object feat ures.
The noisy features, derived from the restored rangr
image, appear in the fifth image of Figure 6. There are 62
noisy features. Some features have been lost due to tlc
corruption and restoration of the range image. The set d
image features was prepared from the noisy features by
randomly deleting half of the features, transforrmitng thc
survivors according to a test pose, and adding sufficient
randomly generated features so that 2 of the featnrcs
are due to the object. The 248 image features appear in
the sixth image of Figure 6.
Using this data, the EM algorithm was run in a mult i-
resolution manner, and Figure 7 shows the convergeno
of the algorithm to the correct pose.
In classic projective geometry of 3D space, pro jectivo
structure is typically defined by three cross-ratios usu;
five reference points (tetrahedron of reference and a unt
point) [39; 32] or, equivalently. by a tetrad of homogg-
nous coordinates. Wjth such projective structure onc
can reconstruct the scene up to an unknown pro ject iv
transformation in 3D projective space, or equivalently
the camera coordinate frame may undergo an affine tran-
formation in space followed by an arbitrary projective
transformation of the image plane (taking a ''picture''
of the image). A projective framework does not make a
dlistinction between orthographic and perspective views
and does not require camera calibration (internal cam-
era parameters are folded into the affine transformation
of the camera coordinate frame).
OOur approach has several characteristics: first, our
ddefinition of pro jective structure relies on four scene ref.
erence points and the camera's center (instead of five
scene reference points), and is defined using a single
cross-ratio (rather than three) - which means that it
is obtained by some invariant function of projective co-
ordinates. Second, the proposed projective invariant al-
lows us to reconstruct the scene up to an unknown 3D
projective transformation and, in addition, is particu-
larly useful for recognition, that is, reconstruction of
any third view becomes very simple in this framework
Thirdly, we make use of the epipoles to compute the
projective invariant using only linear image-based com-
putations ([18; 32] propose other similar techniques for
using the epipoles in a linear reconstruction of homoge-
neous or non-homogeneous coordinates). Finally, we do
not recover the camera transformation in the course of
reconstruction, but instead recover the pro jective trans-
formations of two faces on the tetrahedron of reference.
Taken together, projective structure can be computed
from two images, perspective or orthographic, using an
uncalibrated camera, The computation requires the cor-
respondences arising from four reference points and the
epipoles -- the latter can be recovered by a number of
methods using six to eight corresponding points [29: 19;
Key results here are that the structure invariant is
useful for 3D reconstruction up to an unknown 3DD pro-
jective transformation of the object, and for purposes
of recognition via alignment by creating an equivalence
class for different views of the same ob ject. In other
words, we can ''re-pro ject'' an ob ject onto any novel view
given two model views in full correspondence and a small
number of corresponding points between the novel view
and the model views.
The geometric relation between different views of a .3)
object can be used to recover the change in viewing
geometry across views, and to recover the ob ject's .4[)
structure. The work on pro jective structure demon-
strates that in a pro jective framework one can definc
and recover a structure invariant that is sufficient for
uniquely reconstructing the ob ject with respect to a
frame of reference consisting of four scene points and thc
camera's center. The location of the reference frate in
space is generally unknown, and therefore the objec1 -
structure can be reconstructed up to an unknow n .|1)
pro jective transformation in space. This allows us t
work in a framework that does not make a distinction
between orthographic and perspective views and does
not require internal camera calibration.
The geometric relation between ob jects and their
views can also be used for purposes of recognition. In
this case one is generally not interested in recovering ob-
ject structure from multiple views but instead in being
abble to predict the appearance of a novel view from a
small number of example views (''mode]'' views) given a
small number of corresponding points between the novel
view and the model views. The projective structure in-
variant can also be used for this purpose (see Figure ??)
but it is more desirable to achieve the same result di-
rectly without going through the computation of struc-
ture (metric or non-metric) and without the reconstruc-
tion of camera geometry (transformation parameters or
epipolar geometry).
In what is still an ongoing research we derive alge-
braic relations between image coordinates across three
views (two model views and a novel view). We show
here three results: first, the general result is that im-
8ge coordinates across three views (perspective or or-
thographic) are related by a small number of third-
order algebraic functions each having ll parameters
that can be recovered by linear methods. Second, if
the two model views are known to be orthographic,
then the algebraic functions reduce to second-order ones
with only 7 parameters. Thirdly, if all three views
are known to be orthographic, then the functions re-
duce further to first-order ones with only 4 parame-
ters. The latter is identical to the result derived by [48;
33] known as ''the linear combination of views'', and thus
the first two results can be viewed as an extension of the
linear combination of views to perspective.
In a projective framework, five reference points (a
tetrahedron and a unit point) are used for construct-
ing a coordinate system of 3D space ([49], for example).
A projection of a point P onto a plane with respect to
an arbitrarily positioned center of projection (COP) and
arbbitrarily positioned image plane can be achieved by
first mapj)ing the reference frame such that one of the
tetrahedron's vertices is aligned with the desired loca-
tion of the ('OP. and three other vertices are coplanar
with the desired image plane (in projective space five
points in general position can be uniquely mapped onto
any other configuration of five points in general posi-
tion). The point P is then projected onto the face of
the tetrahedron opposite to the ('OP (in homogeneous
coordinate representation of space, this is achieved by
an orthographic projection, see Figure 9). If we assume
that there exists at least one configuration of the ref-
erence frame in which three of the reference points do
not intersect any of the scene points, then it is not dif-
ficult to show (but not shown here) that the space of
iInages we can get out of this framework are no more
than perspective and orthographic images of the scene,
and images of images of the scene, produced by a pin-
hole camera in which the camera's coordinate frame is
allowed to undergo arbitrary affine transformations in
space (rather than similarity transforms used in metric
structure-from-motion models).
Let s,VE. <y be the affine coordinates of a point P
with respect to four of the reference points. If the fifth
reference point (taken to be the (COP) is not at infinity.
then the observed image coordinates (, y) can be de-
scribed by an affine change of coordinates followed by a
2D projective transformation:
for some fixed matrix A; and vector r and some scale
factor z (in a metric framework z would correspond to
''depth''). In the case of an orthographic projection
(COP at infinity and only 2D affine transformations of
the image are allowed), we have:
for some 2D affine transformation H; (third row is
(0, 0, 1)) and an ideal vector s (third coordinate of s
is zero) [28; 41]. We can use these two equations to
describe the transformation between image coordinates
in two views across four cases: two perspective views,
two orthographic views, a perspective to orthographic
case, and an orthographic to perspective case, This is
described below:
where p = z, p' s :', and A, v are general for the
perspective-to-perspective case; p = p' s e-, A is a 2D
affine transformation and v3 = 0 for the orthographic-
to-orthographic case; p = z, p' s 1, third row of A is
(0, 0, 0) and v3 = 1 for the perspective-to-orthographic
case; p 8, p' s 8-, and A, v are general for the
orthographic-to-perspective case. Similarly, the imagre
coordinates (r'', y'') of a third view satisfy the following
relation to the first view:
Note that p remains fixed regardless whether the third
view is perspective or orthographic. The algebraic func-
tions of image coordinates across three views can be de-
rived by first eliminating p', p'' and then isolating p.
where a; , 05, 03 are the row vectors of AA. and p
(.y, 1), p' = (r', y', 1). Similarly, from equation 2 we
obtain
where b; , by, b4 are the row vectors of 8, and p'' z
(r'', y'', 1). These two equations lead to nine alge-
braic functions of image coordinates across three views.
For example, the first two terms lead to the function
F(F'', r', z,y) = 0 of the form:
Note that the bracketed terms are first-order polynomi-
als of r and y with fixed coefficients (depending only on
parameters of camera transformations). In other words,
equation 3 is a third-order algebraic function of the form:
where the coefficients oy , ) = 1, ..., 12, are fixed con-
stants. Note that the constants can be recovered by
linear methods by observing ll corresponding points
across the three views (more than l1 points can be
used for a least-squares solution). Then, for any ad-
ditional point (,y) whose correspondence in the second
image is known (r', y'), we can recover the correspond-
ing x-component z'' in the third view by substitution
in equation 4. In a similar fashion we can recover the
y-component y'' by using one of the other functions, for
example:
The solution for r'', y'' is unique provided that v, va do
not vanish simultaneously, or tu;, t43 do not vanish simul-
taneously. These singular cases apply only to the two
functions above, and one can show that from the nine
functions we can always find two that are not singu-
lar under any viewing transformations that takes place
between the three views. The process of generating a
novel view can be easily accomplished without the need
to explicitly recover structure, camera transformation or
epipolar geometry - with the price of using more than
the minimal eight points that are required by less direct
methods.
The algebraic functions derived so far are general
in the sense that the scene is allowed to undergo gen-
eral 3D projective transformation in space. Reduced
lower order functions can be derived under more re-
stricted situations. For example, the third order com-
ponent of these functions vanishes when v = tu3 s 0
(see Equation 3). This situation arises, for example,
when the views are taken by a camera moving along
a base line perpendicular to the optical axis. One ob-
serves, as a result, that this situation is intrinsically
more stable (errors in correspondence multiply to a
second-order rather than to a third-order) than the gen-
eral case --- an observation experimentally made by [T;
Other results can be obtained by assuming that some
of the views are orthographic. This is especially impor-
tant in the context of achieving recognition via align-
ment: since the two model views are taken onlv once
(and offline), we may as well use a tele-lense for produc-
ing orthographic views. In this case we substitute vy 0
and aa p = 1 in Equation 3 and obtain a second-order
function with only 7 free parameters which has the form:
for some fixed constants o; , ) = 1, ...,5. As a result, we
can generate novel views (perspective and orthographic)
by observing only 7 corresponding points across the three
views. This result is both direct (avoiding structure and
motion) and requires less than eight points (the minimal
under general conditions using linear methods). For in-
stance, with other tools we do not have an easy way for
making use of the fact that the two model views are or-
thographic - because the determination of the epipoles
and epipolar geometry between a perspective and an or-
thographic view still requires eight points in general.
Finally, it is easy to see what happens when all three
views are orthographic. In this case we have also tu3 = 0
and by p 1, and thus Equation 3 reduces to a first-
order function, with only 4 free parameters, of the form:
for some fixed constants o;, J = 1, ...,5. This is iden-
tical to the ''linear combination of views'' result [48;
33], stating that under the orthographic assumption an
arbitrary view can be generated by applying certain lin-
ear combinations to the image coordinates of two model
VIews.
To summarize, we have shown that it is possilble to
represent views as a function of image coordinates of two
other views. In the general projective case, the image co-
ordinates of three views are connected via third-order al-
gebraic functions with ll free parameters. More restric-
tive cases (but applicable in the context of visual recog-
nition) reduce these functions to second-order with 7 free
parameters and to first-order with 4 free-parameters de-
pending on whether two or all the views are assummed
to be orthographic. The immediate application of these
results are in the context of visual recognition via align-
ment (especially the 7-point result), but other applica-
tions may also be possible. For example, the general re-
sult (Equation 4) may be useful in the context of model-
based image compression. In this case the number of
corresponding points required for reconstructing novel
views is not of critical importance whereas robuustness
and simplicity are more of a concern. The 22 parame-
ters required for reconstructing a novel view can be re-
covered by many points in a least-squares fashion, but
the receiver eventually requires only the parameters and
not the corresponding points.
According to the 1.5 views theorem [33; 6] recogni-
tion of a specific 3D object (defined in terms of pointwise
features) from a novel 2D view can be achieved from at
least two 2D model views (in the data basis, for each
object, for orthographic projection). Poggio and Vetter
studied how recognition can be achieved from a single
2D model view. The basic idea is to exploit transforma-
tions that are specific for the object class corresponding
to the object - and that may be known a priori or may
be learned from views of other ''prototypical'' objects of
the same class - to generate new mode] views fron the
only one available. Their work divides in two distinct
parts. In the first part, they discuss how to exploit prior
knowledge of an object's symmetry. They prove that for
any bilaterally symmetric 3D ob ject one non-accidental
2D model view is sufficient for recognition. They also
prove that for bilaterally symmetric ob jects the corre-
spondence of four points between two views determines
the correspondence of all other points. Symmetries of
higher order allow the recovery of structure from one
2D view. In the second part of their work, Poggio and
Vetter study a very simple type of object classes called
linear object classes. Linear transformations can be
learned exactly from a small set of examples in the case
of linear object classes and used to produce new views of
an object from a single view. More recently Vetter, Pog-
gio and Buelthoff have provided psychophysical support
for the hypothesis that the human visual system exploits
symmetry of 3D objects for better generalization from a
few views.
Over the last twenty years several different techniques
have been proposed for computer recognition of human
faces. Poggio in collaboration with R. Brunelli at IRST
compared two simple but general strategies on a common
database (frontal images of faces of 4T people, 26 males
and 21 females, four images per person). They have de-
veloped and implemented two new algorithms, the first
one based on the computation of a set of geometrical
features, such as nose width and length, mouth position
and chin shape, and the second one based on almost-
grey-level template matching. The results obtained on
the testing sets, about 9(0% correct recognition using geo-
metrical features and perfect recognition using template
matching, favour their implementation of the template
matching approach. Present work aims to extend the
system to deal with arbitrary poses and expressions of
the face.
CComplementary to our work on object recognition, we
have also investigated issues and methods in navigation.
One such method has built directly on our earlier work
in recognition by Linear Combinations, and is reported
in a separate article by Basri in these proceedings.
A second approach to navigation has been part of a
broader research project, executed by lan Horswill. The
problem under consideration is the development of sim-
ple real-time vision algorithms suitable for low-cost com-
puter systems such as personal computers. The specific
goals are to develop (a) simple vision algorithms useful
for problems such as robot navigation and interacting
with people, (b) a theoretical framework for analyzing
such systems, and (c) a concrete implementation demon-
strating the algorithms in a robot which gives primitive
'tours'' of the seventh floor of the laboratory.
This follows from the motivation of making vision
cheap as a necessary part of making it useful. For vi-
sion technology to be used routinely in construction and
manufacturing equipment, consumer electronics prod-
ucts, automobiles, etc., both design costs and unit costs
rmust be brought down to levels comparable with the
product into which they are to be incorporated. Mil-
lion dollar supercomputers, or even twenty thousand
diollar workstations are simply inappropriate for mass-
produced products intended to cost less than the com-
puter.
Horswill's approach is one of situated agents, whereby
vision systems can be made much simpler and cheaper
by specializing them to a specific task and environment.
A task-specific system need only extract the specific in-
formation needed to perform the task. As well. a task
provides performance constraints that can simplify the
design process by allowing the use of approximate so-
lutions which might not be appropriate for all tasks. A
system specialized to its environment can take advantage
of domain knowledge which can simplify computational
problems. For example, a complete stereo system can
sometimes be replaced by a system which uses height in
the image plane to determine rough distance from the
agent.
A critical problem in developing these systems is the
reusability of components. It is important that we be
able to apply experience gained in designing one system
to the design of other systems. For this reason, tasks
and environments must be analyzed at a theoretical level
so as to make explicit the ways in which they simplify
computational problems.
Low cost task-oriented vision systems could signifi-
cantly improve the performance and flexibility of au-
tonomous systems and reduce their cost. Such systems
have applications in transportation, surveillance, con-
struction, manufacturing, space applications, and haz-
ardous operations. Low cost vision technology could also
be extremely valuable in consumer electronics. Track-
ing systems could be incorporated into camcorders or
surveillance systems. Face recognttion, person detection.
stereo and motion algorithms could be incorporated into
intelligent nightscopes and binoculars. Low unit costs
would be particularly important in this area.
To date, we have developed systems for tracking and
following moving objects, detecting obstacles, proximity
detection using stereo (see Figures 10 and 11), following
corridors, and recognizing nods and shakes of the head.
All systems run in real time on inexpensive computers
such as Macintoshes. All systems use very low resolution
processing (54 x 48 or less). A number of optimizations
are shared between two or more systems, suggesting that
some amount of recycling of design time is possibble.
The particular prototype system is an indoor navig-
tion system for a mobile robot that runs at 15 frames
per second on stock hardware. A computer equivalent
to the one on board the robot can be purchased for $.3-
4K. At present, the robot is capable of following corri-
dors, avoiding obstacles, recognizing corridor junctions
navigating frorm point to point, and detecting the pres-
ence of people. The corridor follower is extremely well
tested, having seen hundreds of hours of service. T];e
other capabilites are newer and have not yet been flly
evaluated. An article by Horswill in these proceeding-
further describes the approach.
Although a primary focus has been on recognition and
navigation, we continue to develop methods for early
processing of visual information.
Ancona (CSATA, Italy) and Poggio have shown that a
new technique exploiting 1D correlation of 2D or even
ID patches between successive frames may be sufficient
to compute a satisfactory estimation of the optical flow
field. The algorithm is well-suited to VLSI implemen-
tations and a patent application is being filed by MIT
and CSATA. The sparse measurements provided by the
technique can be used to compute qualitative properties
of the flow for a number of different visual tasks. In
particular, they also showed shows how to combine the
1D correlation technique with a scheme for detecting ex-
pansion or rotation [37] in a simple algorithm which also
suggests interesting biological implications. The algo-
rithm provides a rough estimate of time-to-crash. It was
tested with good results on real image sequences.
CClay Thompson has recently completed a Ph.D. thesis
that considers the problem of fusing two computer vision
methods, using variational methods. The example algo-
rithms solve the photo-topography problem; that is, the
algorithms seek to determine planet topography given
two images taken from two different locations with two
different lighting conditions. The algorithms each em-
ploy a single cost function that combines the computer
vision methods of shape-from-shading and stereo in dif-
ferent ways. The algorithms are closely coupled and take
into account all the constraints of the photo-topography
problem. One such algorithm, the z-only algorithm, can
accurately and robustly estimate the height of a surface
from two given images.
Feature-based methods for recovering the motion of a
camera from a sequence of images have suffered from the
inabbility of the early vision processes to provide dense,
robust features. Typically, the features, such as Canny
edges, are very sparse in each image. Furthermore, most
features are unreliable in the sense that they often dis-
appear from one frame to the next. Similarly, sporadic
features often appear that are not associated with any
object in the scene. To address these problems, Ron
(CChaney has developed a framework for early vision pro-
cessing that leads to a dense, robust set of features. The
early vision framework is based on the interpretation of
the Laplacian of (iaussian (Lo4) filter as a matched filter
for features of a particular size as well as an edge locator.
An object in the image that has roughly the optimum
width associated with a particular Lo4 filter will typi-
cally be nearly surrounded by the zero-crossings of the
Lo4 filter. Hence, a naive approach would be to take
the regions bounded by the zero-crossings of the LoG
filter as the set of features. Of course, the regions asso-
ciated with nearby objects in the image tend to merge
or blend together. To alleviate this problem, we intro-
duce a stable, robust decomposition of such regions into
their salient subparts. These subregions, called simple
region features, serve as the feature set for higher level
processing. The decomposition is based on the medial
axis skeleton of the region. Each subregion corresponds
to a portion of a branch of the skeleton; each branch is
divided at positions where the distance from the skeleton
to the bounding contour is minimized. To facilitate the
computation of the decomposition, a novel scale-space
is introduced for contours and the medial axis skeleton.
The scale-space is parameteric with the complexity of
the contour or the skeleton. The complexity measure of
the skeleton is the number of branches. A related cotmw
plexity measure of a contour is the number of extrema
of curvature of the contour. This leads to a complexity
scale-space for the region decomposition. The result of
the early vision process is the set of simple region fea-
tures for each frame. These features are dense, stable,
and robust. To demonstrate the utility of the early vi-
sion process, we present a relatively simple motion and
structure-from-motion algorithm based on tracking sim-
ple region features at multiple resolutions of the Lo4
filter.
CComputing relative orientation is an important problem
for calculating depth from binocular stereo and for de-
termining general camera motion. Lisa Dron has been
exploring methods for establishing the complete design
of a small, autonomous system with specialized V LSI
hardware for computing relative orientation in real-time
[14]. Such a system would be suitable for mounting on
mobile or remote platforms that cannot be tethered to
a computer and for which the size, weight and power
consumption of the components are critical factors
There are two parts to this work. The first is theo-
retical and involves developing and adaptitng algorithmu-
for finding point correspondences and solving the motion
equations which are robust as well as simple enough to
be easily implemented in hardware. The second part is
engineering and involves the design, fabrication and test
of prototype chips for the specialized processors which
will be used to find the point correspondences. Two sep-
arate processors are needed; one which computes a bi-
nary edge map from the input image data, and the other
which determines translational offsets between patches
of the edge maps from two different images. Fabrication
of these circuits is done through MOSIS.
In support of such work, Dron has already developed
the edge detection algorithm known as the multi-scalr
veto (MSV) method [15]. During the past year. she
has completed the design of a two-dimensional proces-
sor which combines (CCCD and (CMOS technology to n-
plement the MSV algorithm. A test chip containing
4x4 two-dimensional array has been fabricated and s
currently being tested. Algorithms have been developed
both to perform matching with the binary edge signals
produced by the MSV chip, and to solve the motn
equations with a least-squares method suitable for im;lc-
mentation on a programmable digital microprocessoor. n
addition, Dron has developed a least-squares algorithm
to determine the internal camera calibration parameters,
which are required in order to compute motion from a set
of point matches, using a sequence of images for which
the translational motion is known. A preliminary de-
sign, comprising both analog and digital components,
has been completed for the second processor which will
compute point correspondences from the edge maps, and
have sent out for fabrication a set of test structures which
will form the basis of the matching circuit.
In related work, Gideon Stein has developed a simple
method for internal camera calibration for computer vi-
sion systems. It is intended for use with medium to wide
angle camera lenses. With modification it can be used
for longer focal lengths. This method is based on track-
ing image features through a sequence of images while
the camera undergoes pure rotation. This method does
not require a special calibration object. The location of
the features relative to the camera or to each other need
not be known. It is only required that the features can be
located accnrately in the image. This method can there-
fore be used both for laboratory calibration and for self
calibration in autonomous robots working in unstruc-
tured environments. The method works when features
can be located to single pixel accuracy but subpixel ac-
curacy should be used if available.
In the basic method the camera is mounted on a ro-
tary stage so that the angle of rotation can be measured
accurately and the axis of rotation is constant. A set of
image pairs is used with various angular displacements.
If the internal camera parameters and axis of rotation
were known one could predict where the feature points
from one image will appear in the second image of the
pair. If there is an error in the internal camera param-
eters the features in the second image will not coincide
with the feature locations computed using the first im-
age. (One can then perform a nonlinear search for camera
parameters that minimize the sum of distances between
the feature points in second image in each pair and those
computed from the first image in each pair, summed over
all the pairs.
The need to accurately measure the angular displace-
ments can be eliminated by rotating the camera through
a complete circle while taking an overlapping sequence
of images and using the constraint that the sum of the
angles nust equal 360 degrees.
The closer the feature objects are located to the cam-
era the more important it is that the camera does not
undergo any translation during the rotation. A method
is described which enables one to ensure that the axis
of rotation passes sufficiently close to the center of pro-
jection (or front nodal point in a thick lens) to obtain
accurate results.
Stein shows that by constraining the possible motions
of the camera in a simple manner it is possible to devise a
robust calibration technique that works in practice with
real images. Experimental results show that focal length.
aspect ratio and lens distortion parameters can be found
to within a fraction of a percent. The location of the
principal point and the location of the center of radial
distortion can each be found to within a few pixels.
In addition to the first method a second method of
calibration is presented. This method uses simple geo-
metric objects such as spheres and straight lines to find,
first the aspect ratio, then the lens distortion parameters
and finally the principal point and focal length. Cali-
bration is performed using both methods and the results
compared.
T'wo other recently complete theses, reported in detail
in earlier reports are Steve White's work in highly accu-
rate representations for early vision, especially edges and
stereo disparities [54[, and Subirana's work on recogni-
tion and representation of flexible obbjects [45].
Median window filtering is a simple non-linear tech-
nique for reducing image noise while preserving shharp
discontinuities. It works by replacing the current value
of each image pixel with the median value of the pixel's
local neighborhood. Although the technique has been
extensively used for smoothing scalar image data like
grey-level intensities, little work is known about median
filtering in multi-dimensional data domains like image
color, image texture and motion fields. Perhaps this is
because the sample median is an ill-defined concept for
multi-dimensional quantities. Recently, Sung has pro-
posed a novel interpretation of the median concept for
multi-dimensional metric spaces , The interpretation
follows from a mathematical prop erty of the scalar me-
dian, and the basic idea is to similarly define the muulti-
dimensional median as the sample member that mini-
mizes a mean absolute error term. Sung implemented
a multi-dimensional median filtering algorithm for color
images and showed that the operation indeed preserves
edges while reducing noise. He has also mathematically
derived that in the best case, the smoothing performmance
of multi-dimensional median filtering is comparable to
that of local averaging. More recently, he has also de-
veloped algorithms for approximating multi-dimensional
medians that run in linear time with respect to data di-
mension and sample size.
Over the past twenty to thirty years, a number of diffor.
ent techniques have been proposed for segmenting un-
ages into piecewise uniform regions. Like many other
computer vision tasks, most of these techniques contain
at least a few thresholds and operating parameters whose
values are crucial for producing reasonable results. (Ofen
however, these values are determined either empirically
or by guess. Kah Kay Sung has explored an alternative
approach to the threshold selection problem for a simple
but fairly general class of uniformity based region find-
ing paradigms. He proposed a statistical formulat ion
for uniformity based region finding as a series of ''coonfi-
ddlence'' and ''significance'' tests, where each test roughly
corresponds to a decision procedure in the original region
finding paradigm. The main advantage of his approach
is that it replaces typical region finding thresholds and
parameters with a new set of confidence and significance
thresholds and parameters that gives greater insight to
the system's pertinent characteristics. A color region al-
gorithm, based on his formulation, was implemented on
the parallel Connection Machine.
UUnder separate contract, Tomaso Poggio and colleagues
have been developing techniques for the application of
learning methods to vision problems. In particular,
building on extensive earlier work by Poggio and collabo-
rators on the use of ieneralized Radial Basis Functions,
they have been developing learning methods for use in
object recognition and computer graphics.
Under separate contract Berthold Horn and colleagues
have continued to developed VLSI implementations of
low level visual algorithms.
The problem that this chip solves is that of computing
the direction towards which a camera is moving, based
on the time-varying image it receives. There is no re-
striction on the shapes of the surfaces in the environ-
ment; only an assumption that the imaged surfaces have
some texture, that is, spatial variations in reflectance.
It is also assumed that the camera is stabilized so that
there is no rotational motion.
Once the translational motion has been determined,
it is possible to estimate distances to points in the scene
being imaged. While there is an ambiguity in scale, since
multiplying both distances and speed by some constant
factor does not change the time-varying image, it is pos-
sible to estimate the ratio of distance to speed. This
allows one to estimate the time-to-collision between the
camera and objects in the scene.
Applications for such a device include systems warn-
ing of imminent collision, obstacle avoidance in mobile
robotics, and aids for the blind.
The projection of the translational motion vector into
the image is called the focus of expansion (FOE). It is the
image of the point towards which the camera is moving,
and the point from which other image points appear to
be receding.
The method used is based on least squares analysis -
that is, find the point in the image that best fits the
observed time variations in brightness. The quantity
minimized is the sum of squares of the differences at
every picture cell between the observed time variation of
brightness and that predicted, given the assumed posi-
tion of the FOE and the observed spatial variations of
brightness.
The minimization is not straightforward, because the
relationship between the brightness derivatives depends
on distance to the surface being imaged and that dis-
tance is not only unknown, but varies from picture cell
to picture cell. It turns out that so-called stationary
points, where brightness is constant (instantaneously).
play a critical role. If there were no measurement errors,
quantization effects or noise, then the FOE would be
at the intersection of the tangents to the iso-brightness
contours at these stationary points.
In practice, image brightness derivatives are hard to
estimate accurately given that the image itself is quite
noisy. Hence the intersections of tangents from differ-
ent stationary points may be quite scattered, Reliable
results can nevertheless be obtained if the image con-
tains many stationary points and the point is found that
has the least weighted sum of squares of perpendicular
distances from the tangents at the stationary points.
This method was chosen from amongst a group of
competing approaches by considering both simulation
results of these methods and constraints of what can
reasonably be built in analog VLSI.
The amount of computation for every picture cell (in-
cluding a number of multiplications) is such that it is not
feasible today to perform the task in a totally unclocked
manner with the processing done at each picture cell.
Instead a row parallel scheme has been decided upon
where each row of the image has a single processor.
The first chip has been made by MOSIS and tested.
Minor revisions are being made.
A problem in motion vision that is somewhat more dif-
ficult than that of recovering the focus of expansion is
that of recovering both translational and rotational com-
ponents of motion of a camera from the time-varying
image. Presently there is no simple robust method for
solving this problem in general, but methods are known
in the special case that the surface being viewed is pla-
Applications for such a system include landing a ve-
hicle on a planar surface and station keeping of a sub-
mersible vehicle above the ocean floor. Also, such a sys-
tem could be used to recover the motion of a person by
aiming a camera at the flat ground in front of the per-
son. The motion estimates obtained from such a down-
ward looking camera could then be used to interpret the
time-varying image from a second camera aimed directly
forward. The resulting system could be an aid for the
blind that warns them of obstacles - even those that do
not have a support directly below - such as signs hanging
from beams supported off on the side.
Here also the proposed method involves a least squares
approach, although it is now considerably more complex
than in the case of simple translational motion. It is
known, for example, that there is an ambiguity in that
two quite different motions, and corresponding different
surface orientations, can yield the same time-varying im-
age.
Detailed design will have to await the results of ex-
tensive simuulations.
John Harris and Prof. Poggio are studying analog imple-
mentations of vision and learning algorithms. They are
interested in analog models because these models pro-
vide a novel mechanism for understanding and develop-
ing algorithms. Experimentation with these continuous-
time nonlinear circuits facilitates algorithm intuition and
leads to fundamental insights. Powerful analog algo-
rithms thus developed will prove useful even if a re-
searcher is limited to simulating the analog hardware on
a digital computer. In addition, biology has motivated
some of the circuits and, conversely, some of the VI,S[
modules may help develop a better intuition for solutions
that biology has found for the same class of early vision
problems.
A real-world vision system must be adaptive in order
to operate in a unconstrained environment. The system
must be smart enough to deal with such nonidealities
as changing light conditions or slight variations between
components. For example, the thresholds for detection
of edges in edge detectors should dynamically change
with the brightness of objects in the scene, Or the ap-
propriate space constant of resistive network could by
dynamically determined by an estimate of the noise in
the input signal. Analog hardware allows for adaptation
in many instances by relying on basic physics to perform
the necessary computations. One specific pro ject un-
der implementation is the time-to-contact motion sensor
proposed by Poggio (1991) and Poggio & Ancona (1992).
This sensor combines the outputs of 1D motion correla-
tion sensors to produce an estimate of the amount of time
until crash (assuming constant velocity). The small, low-
power implementation will be useful as a crash-warning
sensor for robots or automobiles.
In this paper we introduce a new approach to build-
ing models for main roads in aerial images and a
new computation approach to finding them. The
goal is a 4ompletely automated sysyei. The idea is
that this appr9ach could then be exfended to finding
gther types of objects in areal images of the ground.
Ip recen% years a number of papers have appeared in
the published literature dealing with semi-äutomatic
extraction of roads from aerial photos. In general a
buman gpprotor gives the road starting points and
the road'directioiis at te startiog poiifs. This is
a kuge belp to the road finding aloiithm. This in-
teraction has been necessary because road images
can be very cogplicated. Examination of just ie
two images in this paper, Figs 6 and 8, reveals that
94gge intensity accross a road can yary noticeably
Theie may bg a barrier running along the center
of g oa4. Road- width ean väiy sprecisbly, s+-
pecially when a barrier is present. Ifiage intensity
edgggs along s road boundäry may disappear, espe-
cially when there is a building entrance' with a very
ema pwrbns == songiäS'ie'W6äü. fi4 isns
mtensity stuctgre at road intersections can be veiy
complicated. There may be gps or markings o
roads, g sbadows cast by buildings pr trees,'etc.,
etc.. Three major types of road finder were used:
gdge linkers, correlation trackers, region based fol-
lowers.
Edge linkers, based on an edge operator output for
magnjtudg and angle for each pgint in the' image
and then linking the edges according to some crif4-
ria, were used first by [6] and later by [8] and by [2].
Correlation trackers based on the assumption that
there exists a pattern or texture on the road surface
was used first by [7], and later by [2] in combina-
tion with edge linkers. A region based method as-
suming constant intensity in the region and in the
background was used first by [4] with a correlation
follower.
In [l] and [3], a Bayesian approach to low-level
boundary estimation and object recognition was in-
troduced. The problem considered iii this paper is
the automatic extraction of main roads when road
curvature, width, image intensity and edge strength
can vary considerably and when a barrier algng the
road center may or may not be present. The ap-
proach is geperal, and we feel that it can be extended
g handle' the full range of road image variability.
The approach is to build geometric-stöchastic mod-
els for representing rpad images, and thep use pmaxi-
mum aposteriori probability estimation for estimat-
ing the road boundaries (and other important fea-
tures) in an image. The modeling approach forces
the designer to model all significant phenomena, and
the model is generative so that its representation
power can be assessed. The map estimation provides
for the most accurate road finding. Global map es-
timation is realised in a computationally reasonable
way by using dynamic programming to search small
windows to obtain initial road candidates, and then
dynamic prograrmming again with small windows in
order to obtain global esfimates.
We build a geometric-stochastic road model based
on the following assumptions:
A stochastic process model is built exhibiting the
preceding behavior. Specifically, autoregressive pro-
cesses are designed to model road center line, road
width, gtey level within the road, edge strength at
the road boundary,and regions gutside the roads and
adjacent to the boundraies. We refer to these re-
gions as backgroupd. Note that the road geometry
processes are hidden, i.e, they are not observed di-
through the use of dynamic programing (detailes are
given in [5]). Our approach also includes of desion
rules for starting points of a road in a window, and
stopping points of a road in a window.
In the low lgyel processing re are searching for seeds
of roads. The image is' divided into sqiuare win-
dows, N x N pixels in each window, apd the sys-
tem searches for road candidates that fit the road
model with high probability by using the dynamic
programmingstructure. We run the road search four
times, with a separate search starting at each of the
four sides of the window, to pass over all the possi-
bilities of road gggmetries. In Fig 3a we represent
those examples. There may be more than one can-
didate road within a window. This is handled by
searching a window for the best road, and then re-
peating a window search for the next best road. We
do not want the second-best road to be a variation
of the best road. This is handled by not perpmitting
boundary points for the best road also to be bound-
ary points for roads ig subsequent searches. In this
way, the system will find a pair of roads such as in
figure 1b. The procedure is repeated until all road
candidates are found. Fig 3b illustrate another ex-
ample, road junction where a main road forks into
twp roads. Ip a first search starting on, (he right
side gf the window roadl between points H, K,E,
F will be found, then road2 between points L, M,
F, G will be found, and road3 between points A, B,
C, D will be found, by a search startini, at the lefi
side of the wipdgw. The splitting area'represented
by points C, D, E, F will not be found at this level
. The hidden structure for every road candidate is
found and observed to the high- level processing.
A high level processing stage is now used to extend
each road candidate produced by the low level win-
ggw search ip order to obtain global road estimates,
This is done by using shifting windows, as illustrated
in Fig 4, where a new window is introduced at an end
of a toad candidate centered on one of the sides of
the wigdow. The best extension, of the road candi-
date, through the window is estimated by using the
dynamic programming algorithm. This process is
repeated until the stopping criterion stops the road
seargh or until the estimäted road hits the image
border. Ig the prgcess pf extending a road through
a new shifted wipdow, the dynmamic programming äl-
gorithm begins by using the last estimiated state of
the road and the associated road image data. Upon
termination of the road search, if the' length of'the
estimated road from the initial road caididate is
greater than a threshold, the estimated road is ac-
cepted. If the length is less than the threshold, the
estimated road is iejected, unless there is other sup-
porting evidence. Suppprting evidence we have used
in the experiments is that if three long roads enter
an intersection and the short road is adjacent to the
intersection and appears to be an extension of one
of them, i.e., has roighly the direction and width of
one of them, then accept the short road.
This section describes the results of road finding in
the synthetic images (Fig2), and two different real
images called, Rad1 (Fig6) and Rad2 (Fig8) that
were obtain from the Radius Program. The goal is
to find the roads in the real and' synthetic inages
using our approach. The image field is partitioied
into an array of square windows (32x32). The road
finder runs simultaneously within the windows to
find initial road seeds in the images (section 3). It
then combines all the local results to obtain the fi-
nal maig roads in the images by using the high-level
approach. Using the low level approach only for the
synthetic images, the results in Figure 5 were ob-
tained. The fecognized road boundary points are
indicated by black dots. The recognized roads for
Radl are indicated in (Fig7), (detailes are given in
[5]). The recognized barriers and roads for Rad2 are
indicated in (Fig9), The system starts first with rec-
pgnising the road barriers, using knowledge that the
barrjer intensity is lighter than the road surrounding
it. The initial road barrier seeds in the image were
obtain by low-level processing within the windows,
and with the high-level following stage it combines
the local results to obtain the final barrier (detailes
are given in [5]). Each side of the barrier is bordered
by a road; therefore, by knowing the boundaries of
the barriers, the system also knows the correspond-
9g boundaries along one side of each main road.
The other boundaries along the second side of each
road are estimated by using the high-level approach
for road finding.
The automated compilation of man-made and
natural terrain in urban or built-up areas has been
the focus of our research for a number of years.
Built-up areas provide some of the most difficult
and time consuming tasks for the cartographic
community, and provide a rich environment of
varied structures and natural terrain features to test
the robustness of new approaches to computer
vision, The theme of our research is to understand
the computational aspects of automated recovery
of three-dimensional scene information using a
variety of image domain cues. These cues include
the analysis of cast shadows, stereo matching,
geometric models, and structural descriptions
based upon analysis and combination of low-level
image-based features. We look for opportunities to
augment traditional computational vision
techniques with domain knowledge since it is
clearly used by both cartographers and imagery
analysts in a variety of tasks ranging from mapping
to environmental land use analysis to natural
resource Inventory.
In this paper we provide a status report in a variety
of research activities. Section 2 describes recent
work in the application of rigorous
photogrammetric methods to image orientation and
building extraction within the context of the
DARPA RADIUS program. We also describe recent
results using our stereo matching systems
[McKeown and Hsieh 92, Hsieh et al. 92] on
model board imagery. A companion paper in this
proceedings describes the incorporation of
vanishing point geometry into the BABE building
extraction system [McGlone and Shufelt 93]. Such
geometric analysis is a key requirement for
cartographic feature analysis for oblique imagery,
particularly in the fusion of results from multiple
views,
Section 3 describes new research in developing
symbolic and geometric descriptions for buildings
using multiple cues such as stereo, shadow
analysis, and monocular building detection. The
goal is to detect those regions in the disparity map
created by stereo matching that correspond to
buildings. Given that structures may appear on
rolling terrain, and that stereo analysis rarely
constructs an error-free model of the scene, simple
techniques based upon region analysis must be
augmented with other sources of information.
Section 4 details an experiment in measuring the
effectiveness of human-computer interaction to aid
in building detection. While most of our research
has focused upon automated end-to-end analysis,
there is a role for user interaction at various stages
of the cartographic process. Some preliminary
results are presented in interactive building
selection using a simple pointing method.
Section 5 describes a continuation of previously
reported research toward the development of
improved techniques for a terrain representation
called a triangular irregular network (TIN) [Polis
and McKeown 92]. A new point selection
algorithm is described and results for the
generation of a 2500 kilometer square area of the
National Training Center (NTC), Fort Irvin,
California, are shown.
Research on knowledge acquisition and refinement
for rule-based systems used for the interpretation
of aerial imagery is reported on in Section 6.
Using a large production system architecture,
SPAM, we report on ongoing research in the
evaluation of the utility of various sources of
knowledge for airport scene analysis.
Finally, in Section 7 we briefly describe current
research in multispectral analysis to determine
surface material properties as a knowledge source
of built-up area segmentation and cartographic
feature extraction. A detailed description of
performance evaluation for two classification
techniques can be found in a companion paper
[Ford et al. 93] in this volume.
Our goal is to apply rigorous photogrammetric
methods in several areas of research, particularly to
improve the extraction of geometric cues, and to
relate partial object descriptions across multiple
images. One of our first applications has been the
incorporation of vanishing point geometry into the
BABE building extraction system, as discussed in
[McGlone and Shufelt 93] in this volume.
In this section we discuss our research using the
RADIUS modelboard imagery. Under the RADIUS
program a set of images of a synthetic industrial
site, represented by a scale model, were created
and distributed to the RADIUS community. These
images differ from more typical mapping
photography used in cartographic research in that
they are taken from oblique angles rather than
near-nadir (down looking) mapping cameras.
Along with the imagery a set of ground control
points with known modelboard coordinates were
distributed.
Using this imagery we have addressed two major
areas. The first is the implementation of a rigorous
central projection camera model and the solution
for the camera parameters for the modelboard
imagery. The second is the evaluation of our
current stereo matching techniques developed for
traditional mapping imagery using the modelboard
imagery.
Our work to date has been focused on an initial set
of eight images, J1 through J8, of the modelboard
industrial site. Figures 1 and 2 are two overlapping
areas, from images J5 and J4 respectively, and
illustrate typical scene content. In order to obtain
valid position and orientation parameters for the
images we implemented a standard
photogrammetric resection procedure.
A relatively large number of modelboard control
points (70-80) were measured in each of five
images, J3 through J7. We scaled the RADIUS
modelboard control point coordinates into world
units using the modelboard scale information,
given as 1:500. In order to better integrate with
our existing landmark database software
[McKeown 87] we transformed the modelboard
coordinates into pseudo geodetic (latitude-
longitude) coordinates. The coordinate origin of
the modelboard imagery was taken to be
somewhere in central Kansas. For each of the
images we performed an individual image
resection to establish an error measure based upon
RMS image displacement of the measured points.
In addition we performed a simultaneous block
adjustment of the modelboard images using all of
the measured points. Simultaneous resection of the
images in the same adjustment allows better error
detection, due to the higher redundancy in the
solution, and gives orientation parameters that are
more consistent between images.
Results of the individual and simultaneous
adjustments of images J3 through J7 are shown in
Table 1. One can see a fairly consistent residual
error of about 2.3 pixels in these resections.
Further refinements of our camera model may
improve this situation, but at the current scale of
the modelboard photography these errors
correspond to about a three foot displacement in
ground position. Sources of error include
uncertainty in the modelboard ground control
locations, errors in the measurement of these points
in each of the modelboard images, and unmodeled
distortions resulting from the image formation
process. It remains to be seen how such errors will
effect the accuracy of cartographic feature
descriptions, such as buildings, whose models are
composed from partial object descriptions acquired
from multiple views of the scene.
An important output of the resection solution is the
precision information obtained on the orientation
parameters, which can be propagated to estimate
the precision on calculated ground coordinates,
distances, or heights. These precision estimates
will in turn allow us to more meaningfully control
and merge various operations.
The image resection parameters are used
pervasively in our research. For building
extraction from the oblique aerial imagery, the
vanishing point information is directly calculated
and exploited as described in [McGlone and
Shufelt 93]. In the stereo processing the image
orientation parameters are used to precisely
resample the images into epipolar geometry and to
calculate elevations and heights in the scene. The
incorporation of precise camera models, resection
information, and precision information into other
applications is now in progress.
The RADIUS modelboard imagery presents several
new complexities in the interpretation of aerial
imagery. The emphasis on oblique views breaks
some of the basic assumptions built into processes
that analyze and interpret near-vertical stereo pairs.
In order to establish a performance baseline for our
stereo analysis systems we processed the two
stereo pairs (J4-J5) and (J6-J7) found in the initial
release of the RADIUS modelboard dataset. We
used our standard orientation methods developed
for near-vertical imagery taken along a single
flightpath [Perlant and McKeown 90] as well as
our new image orientation system based upon the
resection results reported in the previous section.
Both pairs, (J4-J5) and (J6-J7), are relatively wide
angle stereo, with convergence angles of 60 and 25
degrees, respectively. For the examples in this
section we show results using the 60 degree pair
because it represents an extreme case with respect
to our previous work.
Most stereo systems in cartographic analysis
assume that the stereo pair is in a collinear epipolar
geometry. We use two independent stereo
matching systems of this type. The first, S1, is an
area-based method that provides good figural-
continuity and captures a sense of foreground and
background. It works in a hierarchical coarse-to-
fine fashion in order to capture as much global
continuity as possible while retaining a locally-
based process. Its results are best in smooth
textured areas, but tends to smooth (blur) abrupt
changes in depth.
The second, S2, is a feature-based method that
provides a more accurate estimate at a few
points---especially near depth discontinuities, but
requires interpolation to ''fill in the gaps.'' This
process also uses a hierarchical coarse-to-fine
approach, but matches ''waveform'' features
across (epipolar) scanlines rather than a correlation
window. To remove false matches this process
uses a inter-lintra-scanline consistency check
[McKeown and Hsieh 92].
The results of the two stereo processes are refined
using a monocular segmentation of the original
intensity image into homogeneous regions. This
process first merges the disparity results from each
stereo method using a common estimate of
''goodness'' to select the best match; however, if
there is a large disagreement between the two
methods, then both estimates are suppressed.
Within each region of the segmentation, which is
assumed to represent a single continuous patch of
surface, the disparity values are averaged and the
outliers are removed. Two different segmentations
are used to limit the formation of artifacts during
this process [McKeown and Perlant 92].
Epipolar resampling, that is, resampling a stereo
pair of images so that the epipolar lines run along
the rows of the image, is a requirement for our
stereo matchers, as it is for most existing computer
vision systems applied to aerial imagery.
Unfortunately, the resampling that is typically
performed uses approximate warping techniques
that may be adequate for vertical images but may
fail for imagery with severe obliquity. We have
implemented a rigorous epipolar reprojection
routine that transforms a given stereo pair into the
required geometry using the full orientation
parameters for the images.
As an experiment we generated epipolar aligned
imagery using two different techniques. First, we
established a baseline registration by performing a
relative orientation of the RADIUS modelboard
imagery by finding common scene points in each
of the stereo pairs. A polynomial orientation was
performed giving an approximately collinear
epipolar alignment [Perlant and McKeown 90].
The second orientation was performed using a
rigorous epipolar reprojection based upon the
modelboard resection.
Both S1 and S2 were run using both the polynomial
orientation and the resection reprojection on the
RADIUS J4 and J5 stereo pair. Figures 1 and 2
show the left and right image pairs. Figures 3 and
4 show the stereo disparity results after refinement
using the polynomial orientation. The disparity
results are encoded such that bright areas are
higher than dark areas.
Using the polynomial orientation one can easily
see areas of mismatch, especially for the area-
based S1 process. This was mostly due to the lack
of a precise alignment of the epipolar lines. In
addition, the large number of occluded regions
caused several mismatches by the feature-based S2
matcher. In many cases the correct match between
features had an opposite intensity contrast, which
violates one of the current S2 constraints. This was
less an issue of registration and due more to the
1magmg geometry.
Figure 5 shows the reprojection of the original left
image in Figure 1 such that the camera axis is
perpendicular to the stereo baseline. Both the left
and right images were reprojected into epipolar
alignment. One can notice a change in shape of
the scene due to the obliquity and the angle
between the original camera axis and the stereo
baseline. In this case the change was minimal
since the baseline was nearly horizontal.
Figure 6 shows the result of S2 matching and
refinement using the resection orientation.
Although the results in Figures 4 and 6 are not
directly comparable due to the reprojection, one
can see significantly more structure in the
buildings in the upper left corner of the scene.
Figures 7 and 8 show perspective views of the
modelboard reconstruction and also highlight some
of the differences between the two orientation
techniques. Quantitative analysis of the stereo
accuracy along the lines of [Hsieh et al. 92] will be
performed over a set of test cases.
As discussed in Section 2.1 we have applied a
rigorous photogrammetric approach to the problem
of obtaining a more exact collinear epipolar
alignment of the stereo images. We still need to
determine how much tolerance our stereo
algorithms exhibit with oblique imagery. With the
larger angle oblique images, we will need to
consider methods to deal with the large occluded
areas and the large baseline-to-range ratio.
We have observed that the stereo refinement
process greatly improved the disparity results in
both of the modelboard image tests. We plan to
introduce additional sources of information in the
stereo process, such as wall and roof hypotheses
generated by monocular analysis, in order to help
guide and refine the stereo matching.
The interpretation of stereo disparity maps to
detect and delineate manmade structures contained
within is a difficult problem. Our recent research
has been addressing the detection and extraction of
buildings using stereo analysis together with
monocular cues. The goal is to produce full three-
dimensional models of complex buildings for site
model construction and update. Our approach is to
apply the cooperative-methods paradigm starting
with the results generated by the stereo analysis of
a pair of aerial images and, together with
monocular cues, mark those areas of the image that
appear to be structures.
Our first step is to obtain a set of refined stereo
estimates of the scene. This is obtained by using
the S1 and S2 stereo matching systems coupled with
disparity map refinement as described by
[McKeown and Perlant 92]. In the course of the
stereo refinement process an intensity
segmentation is produced. This segmentation is
used as the basis for subsequent processing.
The second step is to merge those segmented
regions that have approximately the same disparity
and that are adjacent. Next, those (merged)
regions having a significantly greater disparity than
their neighbors are selected. The rule applied in
this step is liberal in the sense that we would rather
produce a few false positives that miss buildings at
this point.
In order to remove some or all of the false
positives, we apply heuristics and constraints
derived from monocular cues.
Finally, the remaining clusters of buildings are re-
analyzed by searching for a best fit building model
for each cluster. In addition, the shadow regions
are again used to hypothesize the location of the
shadow casting sides of each potential building,
acting as a final cluster hypothesis verification.
Some initial results are shown for a complex
industrial scene, DC38008. Figure 9 shows the
original intensity image of the left view of the
stereo pair of aerial images, while Figure 10 shows
the refined stereo result produced by the S2
matcher and used as the initial input to the building
extraction process.
Figure 11 shows the segmentation of the scene in
Figure 9 that was generated during the stereo
refinement process in which the range of intensity
within each region is 15.
These regions are used as an initial over-
segmented view of the aerial scene and adjacent
regions are merged using the mode stereo disparity
value within the region. The threshold for merger
of adjacent regions is tl pixel. Next individual
regions are marked as potential buildings based on
their relationship to adjacent regions. According to
the heuristic rule: (1) If the mode disparity within
the region is less than the lowest of its neighbors,
then it is not considered a building; (2) If its
disparity is greater than the highest of its
neighbors, then it is given a likelihood value of 1.0
(from a range of 0-1.6); (3) If the mode disparity is
equal to both the high and low values of its
neighbors, it is allowed to be considered a building
hypothesis and assigned a value of 1.6; Otherwise,
its likelihood is calculated by the formula:
where:
Figure 12 shows the result of accepting regions
rated at 0.5 or according to the above heuristic.
Clusters of regions that are very large (more than
6000 pixels) or small (less than 100 pixels) are
removed.- This is followed by a further restriction
that all clusters that do not have a hypothesized
shadow region to their non-sunward edges are
removed. Figure 13 shows the final result after
these restrictions.
As a result of this process many of the significant
buildings are detected, with various degrees of
accurate delineation. One way to visualize the
results is to look at the differences between a three-
dimensional ground truth description, the refined
stereo disparity map, and the three-dimensional
scene that results from using building extraction.
Figure 14 shows the original scene rendered using
a hand-generated stereo ground truth estimate
14(a), the shows the refined S2 stereo result 14(b),
and using the building hypotheses generated by
this technique 14(c).
Although our initial results are promising, we feel
that no approach will reliably detect and delineate
manmade structures solely by using stereo
disparity. As a part of the cooperative-methods
paradigm we plan to include other sources of
information such as BABE building hypotheses
[McKeown 90] and surface material classification
[Ford and McKeown 92b, Ford and McKeown
92a]. In rugged terrain or in areas with significant
tree canopy additional cues will be necessary for
both the selection and the filtering of building
hypotheses. In addition, we expect that such
monocular cues, such as those generated by BABE
will play an important role in the verification and
re-analysis of region clusters during the model
fitting and labeling process.
Automated feature extraction from aerial images is
a complex problem, and research in this domain
has illustrated the difficulties in reliably detecting
and verifying building structure. Although the
ultimate goals of our work in this area are systems
which will accurately detect and precisely
delineate man-made features in aerial photography
without human intervention, it is clear that a
combination of current extraction techniques with
some degree of user guidance has the potential to
exhibit improved performance on complex
magery.
To date, many of the semi-automated systems
require a large portion of the detection and
delineation tasks to be performed by the user of the
system. In such systems, the user interactively
manipulates a variety of models over features in
the image, fitting the models to the features
[Hanson et al. 87, Kass et al. 87]. An alternative
paradigm suggests that another approach for
developing a high-performance system is to allow
the user to lend a guiding hand during the
execution of the feature extraction algorithms.
We have been exploring possibilities for the
application of human interaction in the extraction
process. Our current testbed for this research is
BABE, a line-corner intensity based feature
extractor [McKeown 90]. In brief, BABE proceeds
through four major phases to incrementally
generate building hypotheses. The first phase
constructs corners from lines, under the
assumption that buildings can be modeled by
straight line segments linked by (nearly) right-
angled coners. The second phase constructs
chains of edges which are linked by corners, to
serve as partial structural hypotheses. The third
phase uses these line-corner structures to
hypothesize boxes, parallelopipeds which may
delineate man-made features in the scene. The
fourth phase evaluates the boxes in terms of size
and line intensity constraints, and the best boxes
for each chain are kept, subject to shadow intensity
constraints similar to those proposed by [Nicolin
and Gabler 87] and [Huertas and Nevatia 88].
In recent work, we have addressed the possibility
of replacing the hypothesis evaluation routine with
a simple form of user verification, in which a
person uses the mouse to drop points on each
individual structure in the scene. Then, hypothesis
evaluation reduces to determining which boxes
produced by BABE contain points placed by the
user. This level of interaction does not place great
demands on the user, and makes effective use of
the hypothesis generation capabilities of BABE;
thus, it serves as an interesting test for an
intermediate level of man-machine interaction in
this domain.
Figure 15 shows a ground-truth hand segmentation
of a suburban scene in Washington, DC. Figure 16
shows the complete set of hypotheses generated by
BABE for this scene, and Figure 17 shows the
hypotheses verified by the shadow intensity
constraint algorithms invoked in the fully
automatic version of BABE. Figure 18 illustrates
the results of a semi-automated BABE execution, in
which the shadow verification algorithm was
replaced by user selection of three points on each
building, followed by intersection of these points
with the full set of hypotheses in Figure 16. Each
of these results was then compared on a pixel-by-
pixel basis with the ground-truth hand
segmentation to generate the statistics in Table 2.
Note that we give data for a single-point user
selection example as well; we omit the
corresponding figure for brevity.
With the simple mechanism of multiple point
selection, a user interacting with BABE can achieve
a marked improvement in building detection, at the
slight expense of accumulating errors in
background classification. This is due to line
placement errors in BABE hypotheses that are
otherwise accurate descriptions of man-made
structure. Note also, however, that the total scene
classification rate remains essentially the same in
each of the three examples. 'This suggests that user
interaction at the verification level trades detection
rate against overall classification precision.
Given that the initial hypothesis data produced by
BABE still fails to detect 18% of the building
structures in the scene, it should be clear that more
work is necessary on the basic feature extraction
algorithms, and we intend to continue our pursuits
in this area. User interaction at an intermediate
level appears to be a fruitful avenue for further
exploration, however, and we intend to investigate
this topic further. One key issue is the
determination of the appropriate level of
interaction between a user and a feature extraction
algorithm. We also plan to experiment with user
input at other phases in the extraction algorithms,
such as corner detection, line linking, and structure
generation.
Terrain modeling is becoming an increasingly
important issue with the advent of large-scale
distributed simulations for training, mission
rehearsal, and mission planning. Such systems
rely on efficient representations for natural terrain,
as well as manmade features such as buildings,
roads, and bridges. Our recent work in this area
has focused on the development of visualization
tools for three-dimensional data, and in the
continuation of our research in triangular irregular
networks (TINs).
Witth the increasing availability of a variety of
digital spatial data ranging from map databases,
object model descriptions, digital elevation
models, and geo-referenced imagery, there is a
need to conveniently view image and vector data to
support various aspects of our research. In many
cases these datasets are best visualized in three
dimensions. Figure 19 demonstrates the difference
between viewing terrain as an intensity mapped
height field and as an overhead shaded relief
rendering. The latter process takes into account
shading from a light source and tends to make the
surface structure more apparent. Small changes in
terrain detail are enhanced and surface slope and
aspect appear more pronounced. To support our
need for 3D display of spatial data we have
developed an X/Motif application, XRELIEF, to
allow us to visualize digital elevation models and
TINs, manual ground truth segmentations,
automated stereo results, multi-spectral results, and
digital map data (ITD, DLMS) overlaid on terrain.
Figure 20 shows a sample control panel used to
specify imagery, terrain, map overlay, and viewing
parameters. Users can create and store multiple
ordered sets of camera parameters in order to
compare results from different stages of an
extraction process. They can also compare results
from different analysis methods from a single
known viewpoint. XRELIEF has an intuitive
graphical interface for control and creation of these
camera parameters, as well as positioning of an
illumination source used for shading calculations,
and simple animation support. This interface is
shown at the lower right of Figure 20. The large
and small circles represent camera lookfrom and
lookat points, and the image displayed underneath
corresponds to the image being overlaid on the
terrain.
A digital elevation model (DEM) is a terrain model
consisting of elevation data regularly spaced on a
grid. A triangular irregular network consists of
elevation data that are irregularly spaced and are
connected into triangular facets to form a surface.
The ability of the TIN to place points irregularly
permits point density to adapt to terrain
complexity, and allows points to be placed
precisely on peaks and valley floors. The TIN
terrain model is ideally suited to real-time
rendering, as it consists of a reduced set of
polygons tailored to the underlying terrain
complexity. Previous research in TIN generation
using point selection from the DEM was described
in an earlier paper [Polis and McKeown 92]. In
this section we give a brief update on our
development of a new (and improved) method for
point selection.
Our point selection method relies on the iterative
selection of points based upon successive
approximation to the actual terrain surface. At
each iteration a dense DEM is constructed by
interpolation from the current TIN, This
approximate TIN is compared to the actual DEM
and the point or points having the greatest error in
elevation are determined. These DEM points are
added to the TIN as correction points, and a new
triangulation is generated. The triangulation is
evaluated based on a global point budget and the
residual global error. If the point budget is not
exceeded and the RMS error is still greater than a
user specified error goal, then the process is
repeated. In practice the point budget controls the
stopping conditions for the TIN generation
process.
Our previous point selection method relied on the
generation of error contours and associated medial
axes, Points would be selected from the set of
maximal contours generated at each iteration. The
new point selection is based solely upon a measure
calculated at each point in the approximation
DEM. The new process chooses fewer correction
points per iteration and as a result many more
iterations are required. However, the points
chosen are of higher quality in terms of our RMS
error metric. As a result of this a fast triangulation
is necessary, so the modified greedy triangulation
has been replaced with a Delaunay triangulation.
However, the improvement in point selection
appears to outweigh the loss in triangulation
accuracy, especially since the iterative process will
naturally add points to correct poor triangulations.
In the following section we will describe the use of
this new triangulation method to generate large-
scale TINs suitable for use in SIMNET.
A digital elevation model constructed to support a
SIDMNET training exercise was provided to us by the
U.S. Army Topographic Engineering Center
(USATEC). The DEM covered an area 50
kilometers on a side (2500 square km) including
the National Training Center (NTC), Fort Irwin,
California. This area is primarily desert, with
some highly eroded mountainous areas and
intricate alluvial fans running to the desert floor.
The sheer size of the area presents significant
problems. The DEM consists of 1979S1979
points, nearly 4 million elevation posts. To
maintain the desired polygon density for the
SIIMNET computer image generation systems, only
90,000 points were to be selected for the TIN.
An additional complication for the TIN generation
process was the desire for reduced fidelity in the
mountainous areas with increased detail in the
areas of alluvial fans and on the desert floor. This
was primarily driven by the fact that mountainous
areas are not accessible to ground vehicles
(simulated or otherwise) yet, due to their height
and complexity, they tend to accumulate a large
number of TIN points. This decreases the budget
available for other areas of the terrain. An overlay
indicating the mountainous areas was provided by
USATEC, and was used to produce an importance
grid. Our initial experiment was to make the
maximum error in the mountains approximately
one fifth as large as that in the low lying areas. We
smoothed the importance grid to avoid problems
that might result from a discontinuity at the
boundary of the mountainous area. Since point
selection under our new method was based solely
upon a measure calculated at each point, it was
now possible to use the importance grid to apply a
weight to each point based upon its subjective
importance.
Figure 21 shows a shaded relief representation of
the western part of the National Training Center.
The left half shows the terrain relief using the
original digital elevation model. The right half
shows the same area using the TIN representation
for the underlying surface structure. The TIN was
generated using selective fidelity in the
mountainous areas. Using approximately 2.5% of
the original DEM points we were able to construct
a TIN with an RMS elevation error of 3.1 meters
when compared to the original DEM. The range of
elevations in the DEM was approximately 1500
meters. From a qualitative standpoint it appears
that the major topographic features are generally
preserved and that detail in the alluvial fans and
desert floor areas are also quite good. This
impression was confirmed using the SIDMNET
system at USATEC and driving an M1 tank
(simulated) through the terrain.
We have shown the utility of our new TIN
construction method for a large-scale digital
elevation model. Research issues remain in
determining how to factor more detailed mobility
information into the point selection process. We
are also interested in addressing how to integrate
small scale cartographic features, particularly roads
into a TIN, while maintaining a limited polygon
budget.
From a pragmatic standpoint, the generation of a
TIIN directly from the NTC digital elevation model
using our new point selection method would take
weeks, even on a fast (20mips) workstation. Our
initial solution was to divide the DEM into tiles
and then generate a TIN for each tile. We
maintained a restriction that TINs must match
along common boundaries. The execution time is
divided by the number of tiles, and can be further
reduced since tiles which have no common
boundary can be generated in parallel. Using this
method we were able to generate the NTC TIN
overnight using three workstations. There are
limits to this technique since as the number of tiles
is increased, the global behavior of point selection
is greatly reduced. This can defeat the overall goal
of placing points wherever their utility is the
greatest.
Knowledge refinement is a central problem in the
field of expert systems [Buchanan and Shortliffe
84]. It refers to the progressive refinement of the
initial knowledge-base of an expert system into a
high-performance knowledge-base. For rule-based
systems, refinement implies the addition, deletion
and modification of rules in the system so as to
improve the system's empirical adeuacy, ie., its
ability to reach correct conclusions in the problems
itis intended to solve [Ginsberg et al. 88].
The goal of our research effort is to understand the
methodology for refining large rule-based systems,
as well as to develop tools that will be useful in
refining such systems. The vehicle for our
investigation is SPAM, a production system (rule-
based system) for the interpretation of aerial
imagery [McKeown et al. 89, McKeown et al. 85].
It is a mature research system having over 600
productions, many of which interact with complex
geometric algorithms. A typical scene analysis
task requires between 50,00) to 400,000
production firings and an execution time of the
order of 2 to 4 cpu hours.'
Large, compute-intensive systems like SPAM
impose some unique constraints on knowledge
refinement. First, the problem of credit/blame-
assignment is complicated. It is extremely difficult
to isolate a single culprit production (or a set of
culprit productions) to blame for an error observed
in the output. Second, given the large run-time, it
is not possible to rely on extensive experimentation
for knowledge refinement.
As a result, the methodology adopted in well-
known systems such as SEEK and SEEK2 [Politakis
and Weiss 84, Ginsberg et al. 88], or KRUST [Craw
and Sleeman 91], cannot be directly employed to
refine knowledge in SPAM. Our approach is to
address this problem in a bottom-up fashion, ie.,
begin by understanding SPAM's individual phases,
and then attempt to understand the interactions
between the phases. A different set of tools is
required to allow the user to focus attention on
individual modules responsible for intermediate
results and refine them. In our work so far, we
have focused on the second phase in SPAM, local-
consistency (LCC), which applies constraints to a
set of plausible hypotheses and prunes the
hypotheses that are inconsistent with those
constraints. Furthermore, we have narrowed this
focus to refining SPAM's distance and orientation
constraints.
In working toward refining these constraints, we
posed several questions to help guide our analysis:
In the following sections we describe some of our
current efforts toward addressing these questions.
We have begun our investigation on knowledge
refinement by focusing on SPAM's second phase of
processing, LCC. This phase was chosen because
most of SPAM's time is spent in this phase, and it
showed the most potential for future growth. LCC
performs a modified constraint satisfaction
between hypotheses generated in SPAM's first
phase. In LCC, a successful application of a
constraint provides support for a pair of
hypotheses, and an unsuccessful application goes
towards filtering out that pair of hypotheses. The
distance constraint specifies allowable distance
ranges between different pairs of hypothesized
objects, e.g., two hangar buildings must occur
between 20 and 200 meters apart, while a parking
apron and a hangar building must be between 0
and 50 meters apart. In essence, each constraint in
the LCC phase classifies the pairs of hypotheses --
either the constraint supports that pair, or it does
not.
Our refinement methodology consists of three
parts: intermediate result evaluation, constraint
optimization, and embedded evaluation. The first
two methods allow the isolation and improvement
of individual constraints, while the third method
allow us to evaluate the performance of the new
knowledge in the context of the overall system
output.
In order to measure the effect of various spatial
constraints we needed to establish a database of
correct inputs and outputs for LCC. For each of the
sets of data that we run through SPAM we have a
ground-truth database containing all the objects in
the scene with their correct hypothesis labels. An
''ideal'' input to the LCC phase, a set of hypotheses
that are 100% correct, can easily be manually
generated and run through the system. Any errors
in the output are then directly attributable to the
constraints.
A set of constraint results can be generated by
allowing a user (the expert) to enumerate those
constraints that should exist between each pair of
objects in the ideal input. This is equivalent to an
''ideal'' output for LCc.
Once SPAM has processed the ideal input, the
generated output can then be compared to the ideal
output. Such a comparison is informative as it
allows a quantitative measure of error to be
computed. We can produce this comparison as a
set of confusion matrices where each matrix
represents the results for a single constraint and a
single pair of classes. These matrices contain the
usual cells (true-positives, false-positives, true-
negatives, false-negatives). An example confusion
matrix is shown in Figure 22.
A true-positive entry in the confusion matrix
indicates situations where the expert and LCC both
conclude that the constraint supports a pair of
hypotheses. A true-negative entry indicates
situations where the expert and LCC both conclude
that the constraint does not support a pair of
hypotheses. A false-positive entry is one where
LCC concludes support, while the expert does not.
A false-negative entry is one where the expert
concludes support, while LCC does not.
By examining the overlap of each histogram, we
can tell if SPAM's distance constraint is working
properly. The overlap of, for example, true
positives with false positives can tell us how the
constraint can be modified to achieve the greatest
number of true positives without introducing too
many false positives. For numeric constraints,
such as distance, we have developed an automatic
process for adjusting the constraint bounds to
generate an improved set of ranges.
Automated bounds selection is achieved by doing
an exhaustive search through the space of possible
bounds settings, evaluating each setting with an
objective function. Currently, this objective
function weighs all cells in the confusion matrix
equally and seeks to maximize the number of
elements in the diagonal cells of the matrix (true-
positives, true-negatives).
Both methods described above evaluate and
improve the performance of isolated constraints.
However, it is most important that the system's
overall output improve with the adjusted contraint
embedded within it.
We want to choose to evaluate embedded
performance at a place within the system where the
intermediate results have been used, but where
only a small amount of processing has been done
so that the credit assignment problem is avoided.
We chose to evaluate performance at the end of
SPAM's third phase, FA. This phase does grouping
based on the results of the constraints applied in
LCC. These groups of supporting hypotheses are
called fiunctional-areas (FAs).
SPAM's long run times prohibit iterative refinement
if the number of iterations required can be large.
This limitation can be avoided by appropriately
choosing experiments to run and observing the
system's behavior. In this way, we sample the
space of possible bounds settings and hence,
sample the system's output behavior.
We ran the LCC phase of SPAM with a set of hand-
labeled hypotheses and compared this to our
ground-truth. The resulting comparison histogram
was used by our bounds adjustment procedure
which generated a set of optimal settings for this
constraint. These experiments were performed on
four data sets, with each data set corresponding to
a different airport scene.
Next, we ran five experiments for each data set,
allowing SPAM to execute through it's third phase.
Each experiment corresponded to a modification of
the distance constraint bounds, as follows:
Those table entries labeled orient- are orientation
constraint modifications. For each run, we
compiled statistics on run-time, number of
production firings, number of functional-areas
generated, and number of correct and incorrect
hypotheses included in those functional-areas.
Evaluation was done by comparing each run to the
original bounds settings.
Our results are presented in Table 3. From this
table, we can make several observations. First,
from the increase in run time (from off to original),
it can be noted that the distance constraint is
having some impact on the results. The increase in
the number of correct hypotheses and the drop in
the number of incorrects reveals that this constraint
is playing a positive role.
Finding the best setting for the bounds of the
constraints is a more difficult problem. The
evaluation function for this task seems very
complex, taking into account relationships between
numbers of correctslincorrects, sizes of functional-
areas, and run time. For the Moffett data set, the
number of corrects increases, while the number of
incorrects increases, but at a slower pace. From
this we would conclude that the bounds should be
set to the maximum value. However, the same
analysis for DC National implies that the
optimized value would be best. Other larger data
sets, such as those for San Francisco National
Airport, show a similar trend. This suggests that
the bounds for the distance constraint should be
chosen on a case by case basis.
An interesting phenomena is observed as the
constraints are selectively turned off. The
generated functional-area groups get smaller, but
they do not radically change in area of coverage.
This implies that the distance constraint is
selectively applicable, i.e., it largely overlaps with
the other system constraints, but it is necessary for
the inclusion of some subset of hypotheses.
Because optimizing and then coupling these two
constraints does not produce a dramatic
improvement in results, it appears that more
constraints may be required to do a better job of
interpretation
With the recent emphasis on performance
evaluation of vision systems focused upon low and
intermediate level vision tasks, this work
establishes a data point in the area of high level
vision systems. Though our goal is to improve the
interpretations generated by SPAM, we have begun
by improving our understanding of how the
individual components of SPAM operate, and how
they interact. This will provide the foundation for
understanding the effects of modifying or adding
knowledge to the system.
We have been able to show that SPAM's distance
constraint plays a positive role in the interpretation
task. However, choosing an ''optimal'' value is
difficult, and seems to be scene dependent.
Finally, we have determined that, of the two
constraints considered thus far (distance and
orientation), the applicability of both overlaps a
great deal.
There is still much to be done. In the short term,
there are several obvious problems that we have
not addressed. First, we need to look more closely
at the applicability of the constraints and
characterize, if possible, the cases where each
constraint can be applied. Second, it is unclear if
the bounds optimization procedure we developed
will extend to non-numeric constraints. Finally,
we wish to extend the analysis to simultaneously
perform validation across multiple constraints.
Overall, we believe that it will be possible to build
a heuristic system that would automate the
knowledge refinement process, similar to the
automated system in [Ginsberg et al. 88, Politakis
and Weiss 84]. Within such a system, we would
like to discover ways not only to improve the
current constraints, but to automate methods for
determining what new knowledge may be needed.
Our work in multispectral analysis to determine
surface material properties has been focused on
basic research on demonstrating the utility of such
data for cartographic feature extraction. For many
tasks in traditional remote sensing it is clear that
having surface material information drives many
tasks in land use, environmental monitoring, and
natural resource management. Our hypothesis is
that such data can aid in manmade object
detection, delineation, and identification.
However, getting multispectral imagery at spatial
resolutions that are comparable with the high
resolution panchromatic imagery has been
difficult.
Initial work has demonstrated the utility of the
refinement of multispectral classification using
monocular panchromatic imagery, and the fusion
of stereo disparity maps with surface material
information [Ford and McKeown 92b, Ford and
McKeown 92a]. One issue is maintaining accurate
registration between the multispectral scanner data
(8 meter gsd) and the panchromatic imagery (1.3
meter gsd). Once this is accomplished a unique
hybrid three dimensional multispectral dataset can
be created and utilized for further analysis.
Our recent research has been to perform a
performance evaluation of two classification
techniques, gaussian maximum likelihood and
differential radial basis function, for surface
material classification. In order to do this
evaluation we have created several highly detailed
ground truth segmentations based upon manual
analysis of the multispectral imagery, as well as by
inspection of panchromatic imagery acquired over
the same area. Details of this work can be found in
a companion paper [Ford et al. 93] in this volume.
Our overall conclusions are that multispectral
imagery with moderate spatial resolution has great
potential to provide scene domain cues necessary
to improve the performance of cartographic feature
extraction based on panchromatic imagery with
high spatial resolution.
Our work in multispectral analysis to determine
surface material properties has been focused on
basic research on demonstrating the utility of such
data for cartographic feature extraction. For many
tasks in traditional remote sensing it is clear that
having surface material information drives many
tasks in land use, environmental monitoring, and
natural resource management. Our hypothesis is
that such data can aid in manmade object
detection, delineation, and identification.
However, getting multispectral imagery at spatial
resolutions that are comparable with the high
resolution panchromatic imagery has been
difficult.
Initial work has demonstrated the utility of the
refinement of multispectral classification using
monocular panchromatic imagery, and the fusion
of stereo disparity maps with surface material
information [Ford and McKeown 92b, Ford and
McKeown 92a]. One issue is maintaining accurate
registration between the multispectral scanner data
(8 meter gsd) and the panchromatic imagery (1.3
meter gsd). Once this is accomplished a unique
hybrid three dimensional multispectral dataset can
be created and utilized for further analysis.
Our recent research has been to perform a
performance evaluation of two classification
techniques, gaussian maximum likelihood and
differential radial basis function, for surface
material classification. In order to do this
evaluation we have created several highly detailed
ground truth segmentations based upon manual
analysis of the multispectral imagery, as well as by
inspection of panchromatic imagery acquired over
the same area. Details of this work can be found in
a companion paper [Ford et al. 93] in this volume.
Our overall conclusions are that multispectral
imagery with moderate spatial resolution has great
potential to provide scene domain cues necessary
to improve the performance of cartographic feature
extraction based on panchromatic imagery with
high spatial resolution.
We thank the unsung hackers of the Digital
Mapping Laboratory for their help in the research
reported in this paper. Steve Lacy pointed the way
toward user assisted building verification, Chris
Olson motifed his way through visualization
routines, Jeff McMahill performed multispectral
magic, and Scott Colville knows more than he'd
like about functional areas. Ed Allard, Karl
Fischer, and Mark Stemm joined the project too
recently to have done anything terribly interesting.
Computational Sensors combine computation and
signal acquisition to improve performance and pro-
vide new capabilities that were not previously pos-
sible.
They may attach analog or digital VLSI process-
ing circuits to each sensing element, exploit unique
optical design or geometrical arrangement of ele-
ments, or use the physics of the underlying material
for computation. Typically, a computational sensor
implements a distributed computing model of the
sensory data, including the case where the data are
sensed or preprocessed elsewhere.
Recognizing the importance and potential of
computational sensors, Oscar Firschein, DARPA
SISTO, requested us to organize a workshop to bring
together developers and users of computational sen-
sors. The workshop was to define the state of the
art, discuss the issues, and identify promising ap-
proaches and applications for this new technology.
The workshop was held at The University of Penn-
sylvania on May 11-12, 1992. Approximately 40
people attended from academia, government, and
industry. The workshop hosted several key presen-
tations and followed them with group discussion and
summary sessions. This workshop report presents
a summary of the state of the art in computational
sensors and recommendations for future research
programs.
In Section 2 we discuss opportunities for compu-
tational sensors. Some computational sensor exam-
ples are reviewed in Section 3. Technologies, issues,
and limitations are considered in Section 4. Section
5 discusses algorithms for computational sensors.
Recommendations for future programs are given in
the concluding section. The appendix includes a
bibliography of computational sensing created with
input from the workshop participants.
Traditionally, sensory information processing pro-
ceeds in three steps: transducing (detection), read-
out (or digitization), and processing (interpretation).
Micro-electronics technologies will spawn a new
generation of sensors which combine transducing
and processing on a single chip - a computational
Sensor.
In machine vision, the basic approach has been
to use a TV camera for sensing, to digitize the im-
age data into a frame buffer and then to process
the data with a digital computer. Apart from be-
ing expensive, large, heavy, and power-hungry, this
sense-digitize-and-then-process paradigm has fun-
damental performance disadvantages. A high band-
width is required to transfer data from the sensor to
the processor. The parallel nature of operands cap-
tured in a 2D image plane is not exploited. Also,
high latencies caused by this method, due to image
transfer times, limit the usefulness of this method
for high-speed, real-time applications. Combining
processing on silicon wafers together with detectors
will eliminate these limitations, and have the po-
tential to produce a visual sensor of low-cost, and
low-power with high-throughput and low latency.
The potential for integrating the transducing and
processing of signals has been recognized for some
time, but in the past, research and development in
this area was driven mostly by curiosity or special
use. Today, however, the advancement of VLSI
and related technologies provides opportunities for
us to harness this potential in new, broad, practi-
cal applications in image understanding, robotics,
and human-computer interfaces. Most importantly,
VLSI technologies have become available and ac-
cessible to the sensor application community where
we have recently observed a growing body of re-
search in computational sensors.
Several computational sensors have been fabri-
cated and demonstrated to perform effectively. Ana-
log vision chips have been demonstrated which can
detect a motion field, or continuously compute the
size and orientation of an object. Three dimen-
sional range sensing has been performed at a rate
of 1000 frames per second using a chip containing
an array of cells each capable of detecting and cal-
culating the timing of an intensity profile. Sensor
chips that mimic the human's fovea and peripheral
vision have been fabricated and used for pattern
recognition. Tiny lenses can be etched on silicon
to focus light efficiently on a photosensitive area,
or even to perform a geometrical transformation of
images. Resistive networks and associated circuits
on a chip can solve optimization problems for shape
interpolation.
Computational sensors are not limited to vision
use, but have applications in mechanical, chemical,
medical and other sensors. Development of mi-
cromechanical pressure sensors and accelerometers
has been underway for some time. An air-bag sen-
sor for automobiles could become one of the first
successful, mass-produced, low-cost computational
sensors. It contains a miniature accelerometer and
processing circuits in a chip. Processing could also
be combined with micro-chemical sensors to de-
tect water contamination, air pollution, and smells,
while micro-medical sensors could measure blood
chemistry, flow, and pressure.
Potential applications/markets of computational
sensors are abundant:
Development of a computational sensor does not
simply mean combining sensing capability with pro-
cessing algorithms. It requires new thinking. Most
of the current vision algorithms, for example, are
strongly influenced by the fact that image data is
provided in a stream and processed by instructions.
Also, the concept of frame rate (ie., considering
a certain number of discrete frames per second) is
dominant in dealing with time varying events. How-
ever, a computational sensor can take advantage of
the inherent, two-dimensional nature of the sensory
data arrangement, the continuous time-domain sig-
nal, and the physics of the media (eg. silicon) it-
self for processing. This type of new thinking of-
ten results in a completely different, more efficient,
orders-of-magnitude faster ''algorithm''. Many of
the successful examples mentioned above and in
section 3 are the results of such new algorithms.
Finally, computational sensors can create a funda-
mental change in the approach to the sensor system
as a whole. When a sensor is bulky, expensive and
slow, it is not affordable, both economically and
technically, to place many of them within a system.
The sensor system is forced to be centralized. If
computational sensors can provide cheaper, smaller,
and faster sensing units, we can place a large num-
ber of sensors throughout a system, such as covering
the whole surface of a submersible vehicle. A new
opportunity exists to make sensor systems more dis-
tributed, reliable, and responsive.
This section reviews computational sensor architec-
tures that have emerged in recent years:
Many existing systems would fall into several of
the above categories. Representative examples of
each category are presented here.
Although most examples we give are of visual
information processing, these considerations and
techniques extend directly to measurement over the
whole spectrum of electromagnetic radiation. In
general, any other ''imaging sensors'' such as me-
chanical (e.g. tactile) or magnetic sensors, could
also benefit from lessons learned when considering
and designing computational sensors for vision ap-
plications.
The focal plane architecture tightly couples process-
ing and sensing hardware-each sensing site has a
dedicated processing element. The sensor and the
processing element (PE) are located in close phys-
ical proximity, thus reducing data transfer time to
PE's. Each PE operates on the signal of its sen-
sor. However, depending on the algorithm, each PE
may need the signals of neighboring sensors or PE's.
This concept corresponds to the SIMID paradigm of
parallel computer architectures. In computational
sensors, the operands are readily distributed over an
array of PE's as they are being sensed.
Gruss and Kanade [26][27][40] at Carnegie Mellon
have developed a computational sensor for range
detection based on light-stripe triangulation. The
sensor consists of an array of cells, each cell having
both a light detector and a dedicated analog-circuit
PE. The light stripe is swept continuously across the
scene to be measured. The PE in each cell monitors
the output of its associated photoreceptor, recording
a time-stamp when the incident intensity peaks. The
processing circuitry uses peak detection to identify
the stripe and an analog sample-and-hold to record
time-stamp data. Each time-stamp fixes the position
of the stripe plane as it illuminates the line-of-sight
of that cell. The geometry of the projected light
stripe is known as a function of time, as is the line-
of-sight geometry of all cells. Thus, the 3-D location
of the imaged object points (''range pixels'') can be
determined through triangulation. The cells operate
in a completely parallel manner to acquire a frame of
3-D range data, so the spatial resolution of the range
image is determined solely by the size of the array.
In the current CMOS implementation, an array of 28
x 32 cells has been fabricated on a 7.9mm x 9.2mm
die.
Keast and Sodini [41] at MIT have designed and
fabricated a focal plane processor for image acqui-
sition, smoothing, and segmentation. The processor
is based on clocked analog CCDICMOS technol-
ogy. The light signal is acquired as an accumulated
charge. The neighboring PE's share their operands
in order to smooth data. In one iteration, each PE
sends one quarter of its charge to each of its four
neighbors. The charge meets halfway between the
pixels and mixes in a single potential well. After
mixing, the charge is split in half and returned to
the original PE, approximating Gaussian smooth-
ing. However, the segmenting circuit will prevent
this mixing if the absolute difference between the
neighboring pixels is greater than a given threshold.
A 40 x 40 array with a cell size of about 150 x 150
microns is currently being fabricated.
Some algorithms can exploit the physics of the VLSI
layers to achieve ''processing'' in a computational
sensor. Carver Mead at Caltech has developed a
set of subthreshold CMOS circuits for implement-
ing a variety of vision circuits. The best known
design is the ''Silicon'' retina, a device which com-
putes the spatial and temporal derivative of an im-
age projected onto its phototransistor array. The
photoreceptor consists of a phototransistor feeding
current into a node of a 48 by 48 element hexagonal
resistive grid with uniform resistance values R. The
photoreceptor is linked to the grid by a conductance
of value G. An amplifier senses the voltage between
the receptor output and the network potential. The
circuit computes the Laplacian of an image, while
temporal derivatives are obtained by adding a ca-
pacitor to each node.
Another example which exploits resistive grids
to achieve signal processing is the blob position and
orientation circuit developed by Standley, Horn, and
WyattatMIT [83][84]. Light detectors are placed at
the nodes of a rectangular grid made of polysilicon
resistors. The photo-current is injected into these
nodes and the current flowing out of the perimeter
of the grid is monitored. The injected photocurrent
and the grid perimeter current are related through
Green's theorem; based on sensed perimeter cur-
rent, information to compute the first and second
moments of the blob is extracted at 5000 frameslsec.
An array of 29 x 29 cells has been fabricated on a
9.2mm x 7.9mm die.
Some computational sensors are based on the ''com-
putation'' performed by virtue of the special geom-
etry or optical material of the sensor array.
The University of Pennsylvania's log-polar
sensor developed by Kreider and Van der
Spiegel [47] [48] [73] [77] in collaboration with
Sandini of University of Genova and researchers at
IMEC in Belgium has a radially-varying spatial res-
olution. A high resolution center is surrounded with
a lower resolution periphery in a design resembling
a human retina. A sensor that has a high spatial
resolution area, like a fovea in a human retina, is
often termed a foveating sensor. The image is first
mapped from log-polar to the Cartesian plane. There
is evidence that in biological systems this type of
mapping takes place from eye to brain. The authors
have shown that transformations involving perspec-
tive, such as optical flow and rotation, are simplified
with such a mapping. This sensor must be mechan-
ically foveated for a specific region of interest, and
current research concentrates on applying this chip
to robotics.
Bederson, Wallace, and Schwartz [7]at New York
University and Vision Application, Inc. designed a
log-polar sensor as well. The VLSI sensor itself is
in the process of being fabricated. An additional
interesting part of their system is a miniature pan-
tilt actuator called Spherical Pointing Motor (SPM)
shown. The SPM is capable of carrying and ori-
enting the sensor. It is an accurate, fast, small, and
inexpensive device with low power requirements
and is suitable for active vision applications.
Another foveating sensor has been designed by
Kosonocky, Wilder and Misra at Rutgers Univer-
sity. The objective was to design a sensor whose
foveal region(s) will be able to expand, contract and
roam in the field-of-view. The chip is, in essence,
a 512x512 square array with the ability to ''merge''
its pixels into regions, and output only one value for
each such rectangular ''super pixel''. The largest su-
per pixel is an 8x8 region. There are three modes of
operation. In Variable Resolution Mode, the resolu-
tion of the entire chip can be selected from highest
to lowest, or anywhere inbetween. The Multiple
Region of Interest mode provides multiple active
windows, possibly with different resolutions, while
reading data out from the rest of the array is inhib-
ited. The third mode is a combination of the first
two modes. This third mode would resemble the
sampling of a human retina if so programmed. The
design permits multiple foveae within the retina.
The authors demonstrated significant speed-up in
data acquisition for a variety of tasks from indus-
trial inspection to target tracking.
Hexagonal sampling tessellates the frequency plane
more efficiently than rectangular sampling.' Pous-
sart and Trembley [91] at Laval designed a 200 x
200 array with a hexagonal grid. This chip facil-
itates parallel access to the data in a particular lo-
cal neighborhood. For rapid convolution, this local
neighborhood is subsampled along three principal
axes of the grid, thus reducing the data needed for
convolution in the local neighborhood of each pixel.
Their MAR (Multi-port Array Photo-Receptor sys-
tem) performs zero-crossing detection at seven spa-
tial frequencies in 16 milliseconds. Edge detection
is computed in real time.
By etching desired geometrical shapes directly into
the surface of an optical material, a designer can pro-
duce optical elements with properties that were pre-
viously impossible to achieve. This method, called
binary optics, can perform simple optical processing
before the light is detected.
As VLSI microlithographic techniques have ad-
vanced, inexpensive fabrication of binary optical
devices has become possible [93]. Veldkemp of
Lincoln Lab at MIT has developed a micro lens ar-
ray in which each lens is only 200 microns in diam-
eter. One application of such an array would be to
focus light onto tiny photodetectors thus saving sil-
icon area for processing hardware. Some of the first
applications of the idea are already on the market:
Hitachi FP-C 10 HI-8 video coders use a micro-lens
array CCD, and the Sony XC-75 video camera dou-
bles the sensitivity to f8 @ 2000Lux using their Hy-
perHAD CCD structure which uses micro lenses. In
addition, binary optics devices have been applied to
automatic target recognition and space applications.
McHugh of Hughes Danbury Optical Systems ex-
perimented withbinary optical techniques and found
that they can generate virtually any transformation
of an optical wave front. The first application that
used this new capability was a binary optical com-
ponent that optically mapped the log-polar plane to
the Cartesian plane. This device, in effect, samples
images at log-polar resolution and optically trans-
forms them for sensing on a Cartesian grid. This
way an optical log-polar foveating sensor is pro-
duced, while the mapping to the Cartesian plane has
become ''free of charge''.
Wolff at Johns Hopkins University uses liquid crys-
tal polarizers whose polarization angles are elec-
tronically controlled [100]. It has been reported that
by eliminating mechanical rotation of filters, switch-
ing time between different polarization angles is re-
duced, and accuracy of results is improved. Wolff
hopes to build polarization cameras with polarizers
in each element of the CCD array for acquisition of
polarized images in real-time. For specularity detec-
tion, material classification and object recognition,
color and polarization carry independent and com-
plementary information: polarization for specular-
ity, and color for diffuse surfaces and light sources.
Sensors for real-time combination of both color and
polarization images will add rich information to vi-
sion systems.
While not strictly a computational''sensor'', there is
a class of computational modules for sensory infor-
mation processing which exploit VLSI technologies
in a similar manner as computational sensors.
These computational modules are useful when
there is not enough space on a single chip to accom-
modate complex PE's, or the data to be processed
comes from other modules.
At Caltech, several regularization techniques have
been implemented on-chip. For example, consider
the problem of fitting a 2D surface to a set of sparse,
noisy depth measurements by imposing a ''smooth-
ness'' constraint. This method produces quadraticly
varying functions. This can be solved using simple
linear resistive networks by virtue of the fact that
the electrical power dissipated in linear networks is
quadratic in the current or voltage [71].
Mapping 2D motion algorithms onto analog chips
has turned out to be surprisingly difficult. A ro-
bust motion detection circuit implemented in ana-
log VLSIhas yet to be demonstrated, but early effort
has been made by Tanner at Caltech [88] [89]. He
successfully built and tested an 8x8 pixel chip that
outputs a single uniform velocity averaged over the
entire image. His chip reports values of x and y ve-
locity which minimize the least square error in the
image brightness constraint equation.
Bair and Koch have successfully built an ana-
log VLSI chip that computes zero crossings of the
difference of Gaussians. It takes the difference be-
tween two copies of an image, supplied by a 1-D
array of 64 photoreceptors, each smoothed by a sep-
arate linear first-order resistive network, and reports
the zero-crossings in this difference [6]. This imple-
mentation has the particular advantage of exploiting
the smoothing operation naturally performed by re-
sistive networks, and therefore avoids the burden
of additional circuitry. The network resistance and
the confidence of the photoreceptor input are inde-
pendently adjustable for each network. Also, an
adjustable threshold on the slope of zero-crossings
can be set to cause the chip to ignore weak edges
due to noise.
Binary line processes which model discontinu-
ities in intensity within the stochastic framework of
Markov Random Fields provide a method to detect
discontinuities in motion, intensity, and depth. This
is achieved by selectively imposing the smoothness
assumption. Harris and Koch have invented the
''resistive fuse'', which is the first hardware circuit
that explicitly implements line processes in a con-
trolled fashion [31]. Like a normal house fuse, a
resistive fuse operates as a linear resistor for small
voltage drop and as an open-circuit for large voltage
drops. A 20x20 rectangular grid network of fuses
has been demonstrated for smoothing and segment-
ing test images which are scanned onto the chip.
Van der Wal and Burt at David Sarnoff Research
Center developed a VLSI pyramid chip PYR [94].
Combined with external framestore, the PYR chip
is capable of computing Gaussian and Laplacian
pyramid transforms simultaneously. These trans-
forms consist of Gaussian filtering and consecutive
subsampling, and, for Laplacian, image subtraction.
The Chip has a separable 5 by 5 filter and four
1024-sample-long delay lines. Each filter tap has
a preassigned set of possible values. Coefficient
values from this set can be changed under software
control. PYR has special features such as double
precision, double sample density, image border ex-
tension and automatic timing control. At 15MHz
a single chip can compute Gaussian and Laplacian
pyramids at 44 frameslsecond for 512 by 480 im-
ages. PYR is implemented in digital VLSI using the
CMOS standard cell library from VLSITechnology.
Inc. Digitized image samples pass through the chip
sequentially, in raster scan order.
Successful development of a computation sensor re-
lies on careful consideration of several issues includ-
ing:
All of these issues are discussed in the following
sections.
Both digital and analog circuits can be implemented
using VLSI technology. The analog approach can
be conceptually divided into continuous-time (un-
clocked) and discrete-time (clocked) processing.
The choice of technology depends on the particu-
lar application, but several general remarks are in
order. Compared to digital, the traditional disad-
vantage of analog electronics is its susceptibility to
noise, yielding low precision. The source of this
noise can be on-chip switching electronics which
require special considerations for hybrid designs.
Also, analog electronics do not provide efficient
long-term storage; typical storage times are about
one second. On the other hand, digital processing
requires A/D and D/A conversion, which usually
imposes limitations on total circuit speed. Analog
electronics are characterized by:
In general, analog hardware takes less chip area
than digital mechanisms of the same functionality.
Most participants at the workshop were experts in
analog circuitry which seems to be preferred; how-
ever, many recognized the importance of digital
electronics for computational sensing.
Analog VLSI offers two interesting advantages
for computational sensor design. First, the physical
properties of the solid-state layers and devices can
sometimes be exploited to yield elegant, new solu-
tions, One such example is to exploit the physics of
a resistive sheet (or dense grid) to compute desired
quantities.
The second interesting advantage of analog VLSI
is charge-domain processing, best exemplified by
CCD technology. which offers an area-efficient
mechanism for transferring data. In addition, cre-
ative processing schemes can be developed to pro-
cess the data in charge-domain as it is transferred.
CCD technology has already provided several useful
examples of integrated sensing and signal process-
ing.
While the VLSI computational sensor offers excit-
ing opportunities, one must be careful in deciding
which algorithms or applications will benefit from
such an implementation. At the present state of tech-
nology, successful design of working VLSI circuits,
especially analog ones, is a lengthy process.
Algorithms must be carefully selected or invented
to match the architecture to the circuitry for max-
imum performance - there are definite limitations
on circuitry and architectures. Circuitry has lim-
ited precision and storage. Until technology allows
much denser circuits (or 3D structures) for example,
there is not enough room to fabricate a complex PE
at each photo site.
Simple cell-parallel algorithms that detect local
cues or integrate local information over time or mul-
tiple channels (eg. spectrum) at each cell are most
ideal.
When a complex PE is required, processing and
sensing can take place on separate, but tightly cou-
pled (preferably on-chip) modules. The cost of
transferring data must be minimized in order to jus-
tify the use of VLSI over conventional computer
systems. CCD row-parallel transfer is one way to
perform the transfer at a reasonable speed. Also,
some algorithms do not directly exhibit parallelism
in the focal plane; they often require significant lo-
cal data storage at each PE. In stereo algorithms, for
example, optical signals are to be combined from
two different focal planes. In this case, data are
read out and processed on a separate computational
module.
There are optimizations and other techniques that
map naturally to physical processes in silicon; such
as relaxation processes implemented on resistive
grids. The advantage of these physics-based pro-
cessors over computer implementation is that they
minimize a multi-dimensional energy function by
reaching a stable state of a continuous-time system,
potentially reducing round-off error and numerical
instability from which an iterative solution by a dig-
ital computer may suffer.
In summary, the following are some general char-
acteristics of algorithms which are good candidates
for computational sensors implementation:
CMOS, Bipolar, and BiCMOS are the most avail-
able VLSI technologies. CMOS is characterized by
very dense packaging, low power consumption, and
high input impedance. Good switching properties
make it well suited for digital, switching, and hy-
brid circuits. It is widely accessible and relatively
inexpensive technology. CCD's are implemented in
MOS technology.
Bipolar technology is characterized by low noise
and fast circuitry, but consumes more power and
takes more substrate real estate. It is not as accessi-
ble to the wider research community as it probably
should be.
BiCMOS combines the advantages of both
CMOS and Bipolar technologies.
Semiconductor material other than silicon is also
available. GaAs compounds yield very high speed
circuitry and are well suited to electro-optical ap-
plications. GaAs technology is less available, how-
ever, and is considerably more expensive.
The trend in VLSI is toward smaller device ge-
ometries. This produces both smaller and faster
digital circuits and hence more functionality per unit
area, This scaling, however, is not as beneficial to
analog circuitry as to digital. Most active devices
are designed at a given size and scaling and would
not preserve desired functional features after a scale
change. Analog MOS circuits benefit more from
improvements in fabrication process quality. Fac-
tors such as oxide quality and thickness, or tighter
control of threshold voltages would greatly benefit
analog circuit performance.
Great interest has been shown in 3D VLSI. One
possibility is optical signal communication between
stacked chips. This could be accomplished with
the availability of silicon-compatible semiconduc-
tor emitters and IR detectors [90]. This technique
would also require and exploit integrated optics
capability such as binary optics. Alternatively,
a conducting feedthrough could be developed for
making distributed point-to-point electrical connec-
tions [70].
Micro fiber-optics could be used to route data in
parallel from module to module. The optical ap-
proach has the advantage of possible optical pro-
cessing during the data transmission itself, but has
the disadvantage of high power consumption and
heat dissipation, This technology has not been
developed far enough to become accessible to the
wider research community.
As VLSI technology advances and becomes acces-
sible to a wider research community, a number of
ideas that combine sensing and processing on a chip
are emerging. Many attempts, however, are too
quick to postulate miraculous chips and systems
which have little chance of ever working.
Several successful examples of computational
sensors have been driven by applications, and the
workshop participants have agreed that this will re-
main true for most successful developments. A truly
successful ''marriage'' of sensing and computation
can be done only by careful analysis of application
requirements in conjunction with implementation
technologies.
While a wide variety of applications are conceiv-
able, the following are potential applications that
have been suggested during the workshop:
An issue which received unanimous agreement
among workshop participants is the lack of ana-
log VLSI design tools equivalent to those for digital
design. These tools include design aids from lay-
out to testing, including extraction, verification and
simulation. Analog circuits are more sensitive to
parasitics than digital circuits. Accurate techniques
for including these parasitics in the extracted files
would reduce the number of design iterations due to
unexpected circuit behavior.
Analog modeling and simulation capabilities are
still inadequate. Much of the attention in modeling
is directed at the effects of extremely short chan-
nel lengths on MOS transistor operation. Analog
design rarely uses minimum size transistors, but is
more critically dependent upon operating under a
different bias condition: subthreshold and saturation
regions, The proper modeling of bias-dependent ca-
pacitances is critical for modeling circuit dynamics
and stability. There is little or no supportfor simulat-
ing charge-domain devices like CCD's. Statistical
modeling is an important predictive element of ana-
log design, providing assurance that the resulting
circuits will meet the prescribed design constraints.
Without it, a circuit may be functional and within
specifications for a given process model, but actual
process variation may result in an out-of-spec or
inoperable circuit.
It has been noted that a data book for standard
analog cells would be very useful. While it will be
more difficult than the digital domain, it is necessary
to develop a library of standard building blocks of
compatible electronic and sensor components with
which one can design a new computational sensor.
The MOSIS Service is a prototyping service offer-
ing fast-turnaround standard cell and full-custom
VLSI circuit development at very low cost. The
MOSIS Service, begun in 1980, provides fabrica-
tion services to government contractors, agencies,
and university classes under the sponsorship of
the Defense Advanced Research Projects Agency
(DARPA) with assistance from the National Sci-
ence Foundation (NSF). MOSIS has developed a
methodology that allows the merging of many dif-
ferent projects from various organizations onto a
single wafer. Instead of paying for the cost of mask-
making, fabrication, and packaging for a complete
run (currently between $50,000 and $80,000) MO-
SIS users pay only for the fraction of the silicon
that they use, which can cost as little as $400. Ini-
tially, the MOSIS user-base was primarily university
and government users. MOSIS' success in serving
this group of users led, in recent years, to a natu-
ral expansion into the industrial sector, with rapidly
growing use of MOSIS by commercial companies.
MOSIS foundries have also taken advantage of the
frequent prototype runs for their own needs as well
as those of their clients. MOSIS is located at the
Information Sciences Institute of the University of
Southern California (USCIISI) in Marina del Rey,
California.
The MOSIS program has been a successful mech-
anism for promoting VLSI applications. MO-
SIS' ease of access, quick turnaround, and cost-
effectiveness have afforded designers opportuni-
ties for frequent prototype iterations that otherwise
might not even have been considered. With MOSIS'
low cost for ''tiny-chip'' fabrication, silicon can be
used as a rapid prototyping vehicle. Small func-
tional building blocks can be easily fabricated and
tested before too much time is invested in building
and integrating a full system. Furthermore, many
ideas and needed intuition can be gained through
''playing'' withthese actual working chips. Success-
ful designers of existing functional computational
sensors have reported that silicon prototyping, com-
bined with higher level algorithm simulation, has
proven to be a useful system-building approach in
computational sensors.
MOSIS offers two monthly runs of a standard
2um, double-layer metal, CMOS process. One of
these runs usually includes a second layer of polysil-
icon. Typically these designs are fabricated, bonded
and returned in about two months. In addition to
these standard runs, a 1.2um CMOS run goes out
about once every month and there are more infre-
quent runs at 0.8um. Every other month includes
a low-noise 2um analog CMOS run which has op-
tions for second poly, a NPN bipolar transistor in
the n-well, and a buried channel CCD.
MOSIS's capability, however, is limited for the
research and development of computational sensors.
Quality bipolar and depletion-mode MOS devices
are unavailable. MOSIS is beginning to offer GaAs
(instead of the more usual Silicon) process runs on
a regular basis.
At this point, MOSIS does not provide a capa-
bility for optical electronics fabrication. University
researchers must rely on teaming with industries
which have the fabrication capability in this area. It
is noteworthy that both the European research com-
munity and the Japanese micro-sensor project will
have a common facilities including capabilities for
optical electronics fabrication.
Understanding semiconductor and device physics
as well as techniques for marketing custom-made
integrated circuits are essential prerequisites to de-
veloping a successful computational sensor. For
the complete success of a computational sensor, av-
enues of communication between VLSI designers,
computer vision researchers, and product develop-
ers must be developed. These groups would ex-
change information about the opportunities and dif-
ficulties in each others' fields. Vision (and other
sensor) researchers must be made aware of what is
available in VLSI technology, and VLSI designers
must understand the problems of machine vision.
This workshop was very productive. It was recom-
mended that follow-on workshops or conferences
be held.
It was proposed that universities and industries
team-up to allow students to obtain more hands-on
experience. This is an old idea that still has diffi-
culty working in practice. Namely, most students
and university professors are more likely to under-
take theoretical research than to work on the ''real
thing''. This is primarily due to the fact that deal-
ing with hardware tends to extend time in graduate
school for students, and reduce the publishing rate of
professors. This problem received some attention,
and reviews of academic standards were suggested.
It was suggested that more credit should be given to
efforts which produce working prototype devices or
systems.
The body of experience and knowledge of com-
putational sensors is currently scattered over a large
number of disciplines and corresponding publica-
tions. Publications range from journals on elec-
tronic circuits and signal processing to publications
on neural networks and vision research. To ef-
fectively communicate knowledge about computa-
tional sensors, it was suggested that a new journal
be created.
Another type of cooperation is to distribute work-
ing prototype sensors in among the user community.
An excellent example is the log-polar camera proto-
type that University of Pennsylvania has offered to
share with interested researchers. This type of co-
operation is of mutual benefit to the sensor designers
as well as to application developers. Designers of
the computational sensor receive much needed feed-
back about the actual need and practical value of the
sensor, while application researchers can investigate
new areas previously limited by the absence of these
specialized devices.
In light of the previous analysis, the workshop has
recommended the following:
THE FOLLOWING BIBLIOGRAPHY CONTAINS PA-
PERS COLLECTED DURING AND AFTER THE WORK-
SHOP BY THE CONTRIBUTIONS OF PARTICIPANTS.
In many real world applications, there is a
need to perform alignment tasks between
two objects, Two simple, generic tasks
are inserting a peg into a hole and align-
ing objects into arbitrary geometric config-
urations (e.g. robotic assembly tasks,) A
key component of this problem is position-
ing where there is little room for mechan-
ical error, The idea of precision measure-
ment (in our example, alignment) using a
mechanical device, photographic emulsions
or photo-electric sensors, has been exam-
ined in great detail by the researchers in
non-topographic photogrammetry, By us-
ing models which account for most of the
aberration and lens defects in modern lenses,
they obtain highly precise calibrations of
their camera systems. For more informa-
tion see Karara[6]. These methods are often
difficult to understand and inconvenient to
use in most robotics environments. They
usually require the minimization of several,
complex, non-linear equations of multiple
variables (of which the results are not guar-
anteed to be robust.) Other methods for
performing camera calibration for robots in-
clude the works of Tsai [16, 15], Young et.
al. [18], Bennett et, al, 1], and Holt et, al.
[4] for example.
To give the reader an idea of the align-
ment/insertion task, figure l shows our ex-
perimental setup. Off the end of the end
effector of our robot is a probe with a sharp
tip (the ''peg''.) The target in this scene is a
2mm hole in the machined aluminum block
located almost directly below the probe,
Figure 2 shows a view of the target objects
taken from the camera system. In this fig-
ure, the holes in the machined block are
more easily seen, The goal of the task is
to maneuver the probe to a position where
it is directly above the target, and then to
insert the probe into the target.
Another class of methods revolves
around the depth from motion paradigm.
This body of research tries to recover the
absolute pixel velocity for objects in image
space. Here too the researchers are search-
ing for an absolute transformation from a
known reference (the velocity of a known ob-
ject) and an unknown system (the actual,
time-varying, intensity data). The method
we propose does not require the absolute po-
sitional information that both of the afore-
mentioned systems require, It uses simple
image displacement data (generated from
the movement of the camera system) to gen-
erate an estimated position where it expects
that the object motion will be minimized
with respect to the camera movement.
Our technique takes the typical map-
ping from 3-D positions to image coordi-
nates, and instead of finding this mapping,
it recovers a property of the image coor-
dinates. The traditional mapping problem
(known as the calibration problem) deter-
mines the position of objects based on rela-
tive scale difference, perspective distortion,
and/or several other properties which ex-
ist between a calibrated system and an ob-
served system, These positional values can
be obtained from both static and dynamic
systems. These methods do not exploit the
fact that a known movement in the camera
system can result in useful motion informa-
tion in the image system without knowing
the exact calibration between the systems.
We approached the problem by asking
the following question: How can I get a
robot to perform a given task using only un-
calibrated visual input to direct the robot's
actionsT In many cases, it is not neces-
sary for the robot to have a completely cal-
ibrated work area, (It is not necessary to
know the exact positions of everything in
the robotic workspace. It may be more im-
portant to know only the exact position of
certain items.) We propose a new tech-
nique, similar to the work of Sawhney [l1,
12], which will allow the robot system to
maintain an arbitrary, geometric relation-
ship with an object system, and as a result of
certain operations, the robot-object system
can calibrate'' itself to or ''can define its lo-
cation with respect to'' the unknown camera
system, The newness of our technique arises
from the fact that our system performs the
useful task of moving to the goal position
without ever really knowing the true loca-
tion of the camera system.
In order to perform the peg-in-hole insertion
task, we broke the task into two parts: the
alignment task and the actual insertion task.
The alignment task servos the end effector
in a plane in robot space until the alignment
condition occurs (that being when the ob-
ject to be servoed to and the end effector
lie on the same axis,) The insertion task
relies on the fact that the alignment stage
has constrained the solution to lie along a
line (thus making the insertion task simply
a one degree of freedom search.)
A simplified setup is shown in figure 3.
The task is to maneuver the end effector to
a position directly over the target position.
We started our investigation by examin-
ing what would happen if we attached some
sensing system to the rotational axis, such
that the system could image the rotational
axis, We noticed the following effect as we
servoed the rotational joint over a small an-
gle (see figure 4.) Those objects which were
further away from the axis of rotation moved
a greater distance than those points closer to
the axis, This effect is not new and is very
similar to the work done by researchers on
the analysis of the Focus of Expansion for
time to impact studies.
We make the simplifying assumption
that the objects do not change their appear-
ance as we perform the rotation, One simple
way of doing this was to use point-like tar-
gets, The point-like targets rely on the fact
that the perspective distortion is highly lo-
calized due to the fact that the targets have
a high level of spatial coherence.
Using the effect noticed above, we trans-
formed the alignment problem into one of a
positioning problem in a plane. The simpli-
fication is justified by the following observa-
tions:
To perform our peg-in-hole insertions
we also make the following assumptions:
The following constraints were not nec-
essary:
In the figure 3, the robot-camera sys-
tem is constrained to move in the plane A,
where A is defined by the circle swept by the
camera around the rotational axis, R. We
have simplified the alignment task to one of
a 2 DOF problem. The goal state is one
where the object simply rotates in the im-
age plane without translating, hence satis-
fying the alignment condition which is that
the object lies on the rotational axis, The
rotational degree of freedom is used as a free
variable for the alignment task and does not
contribute to the final alignment state (for
circularly symmetric objects).
Once an object has been selected in the
camera's view, the robot rotates the camera
around its rotational axis, R. By slowly ro-
tating the camera around its rotational axis,
we remove the correspondence problem (the
object moves only a slight bit between con-
secutive shots, therefore making the corre-
spondence between two shots trivial to com-
pute.) If the only movement in the robot-
camera system is caused by the rotation, the
object will trace out a conic section, an el-
lipse under certain conditions ', in the cam-
era system. We propose to use these ellipti-
cal parameters to recover the alignment con-
dition, One simple method requires that we
move about in the plane A, sweeping out
ellipses in camera space, The further away
the object is from the rotational axis, the
larger area is swept out by its ellipse pro-
jection into camera space. The closer we
come to aligning the object to the rotational
axis, the smaller the projected ellipses will
become. The goal in this scenario is to de-
vise a method for maneuvering the end ef-
fector's position in plane A to the position
which causes the object to project to an el-
lipse with the smallest area.
The majority of this work was inspired by
Safaee-Rad et. al [10, 9, 14], Haralick [2],
Magee et. al, [7], Sawhney et. al. [11,
12] and Shiu et, al,[13l.
While inspired by these methods, we
have developed a new formulation for de-
riving ellipses from scattered point data, In
our current scenario, we accumulate the (g,
X projection, Y projection) triplet derived
from combining the angle made between the
end effectors zero position and the current
position of the end effector and the pro-
jection of the tracked feature into camera
space, We then parameterized the curve
traced out by the feature as:
The area enclosed by this curve (computed
using Green's Theorem) is
The full proof that the parametric curves
generated by these equations are ellipses is
contained in [17].
The problem of fitting raw data points
to elliptical data was covered in both the
Sawhney and Safaee-Rad works cited earlier,
We were concerned primarily with develop-
ing a method which did not require data
points be taken from the entire ellipse and
which could be solved linearly. In our exper-
iments, the elliptical data was taken over a
90 degree sector of the ellipse. Using only
this data, we were able to fit ellipses quite
well (see figure 5.)
This method uses a version of the simplex
method for finding local minima. We were
motivated by the fact that the solution sur-
face was fairly smooth and by the idea that
even a simple ''walking'' algorithm should
be able to find the solution. We proposed
creating a ''walker'' with three legs: a sim-
plex (for two dimensional ''walking'') requir-
ing three starting points, A simplex was
created in X-Y space from the set of three
arbitrary, non-collinear positions ((0.0, 0.0),
(0.0, 50.0), and (50.0,50.0)). The search
method using the simplex simply ''walks''
down the surface by tossing the ''leg'' which
is furthest uphill an equal amount down-
hill. Once it constrains the solution to
lie between its ''legs'' it shrinks itself and
tries ''walking'' down the surface using its
new position and new, smaller ''legs.'' This
method is similar to the Simplex method of
Nelder and Mead[8].
The implemented version of the algo-
rithm for the simplex search runs as follows:
The above algorithm tries to trap the global
minimum using large simplex movements to
surround the minima and when the minima
is trapped, it reduces its search space by
moving the point with the largest area to
the middle of the simplex (roughly reduc-
ing the bounding area by one-third). The
algorithm is repeated until the area of the
ellipse computed for a position is falls below
the area threshold.
In figure 3, we show a schematic of the sys-
tem set up for testing the new alignment
method. We mounted a Sony XC-77 CCD
camera in a bracket system off the end ef-
fector of a Puma 560 robot. The camera
was not calibrated or position constrained
when initially placed. The system was con-
trolled using RCCL and RCI [3]. The im-
ages were digitized at 256x242 resolution
and 8 bits gray scale at standard NTSC
frame rates using the PIPE parallel image
processing engine [5]. The resulting images
were thresholded to recover a simple black
object on a white background. In general,
any recovery method can be used to generi-
cally extract object information from an im-
age array, The object was positioned so
the robot would not encounter singularities
when moving to the new control positions.
Given that the only information neces-
sary to constrain the alignment is the area
of the projected ellipse on the image plane,
it is not necessary to know anything about
the geometry of the sensor setup.
In the experiment, we used the modified
simplex method (see section 4) with an ini-
tial simplex of ((0.0,0.0), (0.0,50.0), and
(50.0,50.0)).
We built a feature tracker which as-
sumes velocity constrained object motion in
image space. At the beginning of the ex-
periment, a scene was extracted by the im-
age processor and the user was prompted to
move a pointing device to the location of the
feature. The feature extractor used a Sobel
operator with a fixed threshold to extract
the predominant feature in the selected re-
gion, The tracker would follow the feature
over consecutive image frames as long as the
feature moved only small distances.
After establishing the feature tracker,
the robot was instructed to move to the first
position, stop , and rotate its last joint 90
degrees over the course of which it would ex-
tract 16 images spaced equi-angularly with
respect to the robot's rotation.
The feature tracker tracked the move-
ment of the selected target position over the
complete 90 degrees (reporting to the con-
troller the position of the object at 16 equi-
angular positions over the duration of the
movement.)
The centroid of feature (in image coor-
dinates) was then fed to a least squares esti-
mator to recover the ellipse parameters asso-
ciated with the moving features's trajectory.
These parameters were then fed into formula
3 for computing the area of the ellipse.
The process was repeated for the re-
maining two points in the simplex. We ini-
tialized the simple simplex algorithm using
these three areas and allowed it to step its
way to the minima.
The halting condition was when the
area of the ellipse formed from a position
was 5 lpirels% The following table tabu-
lates the results of this experiment:
In the figure, the raw data is displayed
as point data while the predicted ellipses are
drawn in as solid lines,
Figure 6, we show the ellipses generated
by all 25 positions investigated. Note that
the system is not guaranteed to be mono-
tonically convergent (in terms of the number
of evaluations) but the system is convergent
none the less,
The system also can be fooled by ellipses
generated at points which are sampled very
close to one another. In the case of the fi-
nal few ellipses, noise pixels resulted in the
oddish ellipsoid calculations. A more intelli-
gent system would detect this condition and
would hypothesize about the area of the el-
lipse taking this into account. But even with
noisy data, the system reconstructs an ellip-
soid which reflects the general behavior of
the points,
In the experiment above and in figure 7,
the tracked feature was a 2mm diameter
hole. The robot system was able to place the
''peg'' (a tapered probe) within 3mm of the
hole (this using uncalibrated camera data!)
In addition, trying the same experiment 3
more times resulted in about the same re-
sult, that is: an error of about 3mm for the
insertion task. When using a l0mm diam-
eter hole, the robot system almost always
succeeds in placing the probe in the hole.
U pon closer examination, the modified sim-
plex method does converge as well as a
method should taking into account the
amount of knowledge we have given it about
this system. (See figure 8.) The modified
simplex method does suffer from the fault
of reexamining points analyzed previously.
This can be seen in the overlapping num-
bers in figure 8. The only way the simplex
can shrink itself is by covering all possible
point reflections and then after exhaustively
examining all possibilities, it determines the
best step for proceeding to the solution state
is to contract.
Notice that the simplex method suffers
from the fact that it must ''overextend'' the
simplex in all directions before coming to
the conclusion that the simplex should be
shrunk, This ability allows a simplex to
normally ''jump'' over a local minima and
continue its search in a more fruitful valley.
In the case of our system, there exists only
one minima and simplex need not evaluate
all positions when it sees an increase in the
area of an ellipse after picking a new point.
This observation brings up several pos-
sible places where the algorithm for com-
puting the next position can be improved
and makes two insightful observations which
are crucial to understanding the alignment
problem. The first observation is the fact
that we are tracking the centroid of the mov-
ing object rather than the true center of the
object. In the case where the object is point-
like, the center of the object and the centroid
of the object are very close together, so the
algorithm works. But, in the case of a fairly
large object observed through a fairly wide
angle lens (take for instance: 12.5mm focal
length), the distortion of the center of an ob-
ject can be significant (on the order of > 1/2
the radius of the object) depending on the
angle the camera takes with respect to the
rotational axis.
The second observation is the fact that
by starting with several observations where
the rotational axis is far from the object po-
sition, resulting in large ellipses, we can start
with accurate estimates of the ''family'' of
ellipses over small rotations. This is in con-
trast to the smaller ellipses which (because
of numerical inaccuracies in estimating the
center of the object and problems caused by
quantization) need larger movement arcs to
adequately recover the parameters of the el-
lipse. This sweep function is a function of
the resolution of the imaging device as well
as the size of the object and position of the
object.
One possibility for increasing the effec-
tiveness of this process is to use more of the
innate properties of the ellipses generated
by the process, A more sophisticated search
procedure, one based on the physical model
of the parabolic surface which is formed by
the ellipses areas, would give more satisfac-
tory results,
In addition, productive results will
probably be gained from the analysis of
other properties of the conic sections. If the
component values of the conic sections are
traced out as a function of the rotation, the
sinusoids generated will show a phase an-
gle difference with respect to the rotation
of the end-effector. The magnitude of the
sinusoids will determine the net amount of
translation of the object with respect to the
rotational axis. These four values can prob-
ably be used as a control signal to effect a
net change to drive all four values to zero
which is a position where the sinusoids are
both in phase and at zero amplitude with
respect to the rotations: the alignment con-
dition. This technique needs to be examined
in further detail.
Another problem which must be faced
is the problem of small ellipses. When im-
age noise is of the same magnitude as the
centroid data the Least Squares fit no longer
captures the true centroid information of the
object. Remember that the object itself is
perspective transformed and the true object
center can actually be a great distance from
the objects projected center. It may be pos-
sible to use the centroid information, if we
are able to recover the varying amounts of
skew caused by perspective. It may also be
possible to recover the centroidal informa-
tion by using the parametric description of
the object and divining the focal points of
the object, the generating lines, and/or the
eccentricity of the ellipses,
The final positioning error may be im-
proved by using a set of movement primitive
vectors defined by a spiral like the logarith-
mic spiral or some member of the family of
spirals, which can exploit the properties of
containment and possibly approach with an
incremental goodness-of-fit function (which
may be a property of the spiral).
We have demonstrated a method for per-
forming a three dimensional task in essen-
tially two dimensions. The peg-in-hole ser-
voing task and the the vernier alignment
task both benefit from a method which can
constrain the initial position of the object
(to a high degree) and which can essen-
tially turn a three dimensional search prob-
lem into a two dimensional search in uni-
modal space, We have presented such a
method which converges to a solution state
even when using a very simple convergence
algorithm.
The key features/contributions of our
system:
Active camera motion that recovers image
space properties of tracked objects has
shown itself to be useful in performing
alignment tasks without the need to
calibrate the camera systems,
Changes in the relative orientation of a surface with
respect to a camera cause deformations in the im-
age of the surface. Deformations can be used to in-
fer local surface geometry from motion [Koenderink
and van Doorn, 1987; Sawhney and Hanson, 1991;
Cipolla and Blake, 1992; Jones and Malik, 1992b].
Since a repeating texture pattern can be thought of
as a pattern in motion, shape from texture can also be
derived from deformations [Kanade and Kender, 1983;
Super and Bovik, 1992]. Constraints on the shape of
the undeformed structure also allow the computation
of shape from texture [Brown and Shyvaster, 1990;
Garding, 1990].
To first order, the image deformation and translation
due to relative motion can be described using a si
parameter affine transformation Awhere
r, and r, are the image coordinates and uy and v4
the image translation. This is a valid approximation
assuming local planarity and weak perspective projec-
tion [Kanade and Kender, 1983]. Even in situations
where full-perspective projection must be used, it can
be shown that if the change in relative orientation of
the surface patches is small, the image projections can
again be related by an affine transform IAdiv, 1985].
The recovery of 3-D structure from the affine trans-
form requires robust local estimates of the affine pa-
rameters. Consider the case where an image patch F4
is deformed into a patch Fy (either in the same im-
age or in another image) by an unknown affine trans-
form. The problem of measuring the affine transform
is to first find the corresponding patch Fy given F; and
second to recover the affine parameters from the two
patches. Even if the centroids of the image patches
are matched, the precise sise and shape of Fy is diffi-
cult to determine since it is a function of the unknown
deformation. If this correspondence is not precisely
done, the affine parameters will be determined incor-
rectly, Thus the problem is more difficult than in stan-
dard correspondence problems e.g. the determination
of optical fiow.
Existing methods using image patches usually ignore
this problem. A number of techniques assume that
the affine parameters are small and then linearize the
brightness function or filtered versions of it with re-
spect to the spatial coordinates [Bergen et al., 1992;
Koenderink and van Doorn, 1987; Campani and Verri,
1992; Werkhoven and Koenderinck, 1990]. Thus these
methods are restricted to cases where the affine trans-
form is small which in turn requires that the 3-D mo-
tion be small. [Jones and Malik, 1992b] do not assume
that the affine transform is small, Their method, how-
ever, uses brute force search techniques and again ig-
nores the determination of precise correspondence. A
natural way to find correspondence is to use straight
lines [Sawhney and Hanson, 1991] or closed boundary
contours ICipolla and Blake, 1992], with the change in
the sise and shape of the enclosed area defining the
affine tranform. These methods, however, fail when
such structures are absent as in many richly textured
scenes. Further, their use has only been demonstrated
on homogeneous image regions with closed boundaries.
This paper presents a technique for reliably measur-
ing affine transforms that correctly handles the diffi-
culty of corresponding deformed image patches. The
image patches are filtered using ggaussians and deriva-
tives of gaussians. Measuring the affine transform is
then recast as a problem of finding the deformation
parameters of the filters rather than the patches. For
example, let F4 and F4 be related by a scale change
s. Then the output of F; filtered with a gaussian of
o will be equal to the output of Fy filtered with a
gaussian of so. Similar relationships hold for arbitrary
affine transforms and filters described by derivatives of
gaussians. These equations are exact for any arbitrary
affine transform in arbitrary dimensions.
The second part of the paper focuses on solving for
the affine transform when it can be written as the
product of a scale change and a rotation (the solution
for the general case will be considered in future pa-
pers). For example, this situation arises in the case of
mostly translational camera motion and shallow struc-
tures (i.e. structures whose extent in depth is small
compared to their distance from the camera [Sawhney
and Hanson, 1991]).
The equation can be solved by sampling the o space.
Rather than use a brute force search technique, the
search space is sampled for a few different o' and one
of the o' is picked as the operating point. The scale is
recovered by linearising the gaussian filter with respect
to o about this operating point using the diffusion
equation. Consistency is used to establish the correct
operating point. Note that linearization is done uith
respect to o' as opposed to linearisation with respect
to the image coordinates done by other methods. As
a result, scale changes of arbitrary magnitude can be
dealt with by choosing different operating points. The
rotations can also be arbitrary. In contrast, linearis-
ing with respect to the image coordinates is a valid
approximation only for small affine transforms.
The gaussian (seroth moment) equation is linear in
the scale parameter. By sampling at several scales,
an overconstrained linear system is obtained. This
is solved for scale using singular value decomposition.
Using the first moment an equation which is nonlin-
ear with respect to scale is obtained. Again, this may
be sampled at multiple scales to provide an overcon-
strained system of equations. This non-linear system
is solved using the Gauss-Newton technique. The first
moment equation also allows the computation of the
rotation. Both the formulation and the solution are
done for arbitrary dimensions, not just 2. Experimen-
tal results are shown on both synthetic and real im-
ages attesting to the robustness and simplicity of the
method.
Notation Vectors will be represented by lowercase
letters in boldface while matrices will be represented
by uppercase letters in boldface.
We will assume that the image translation is known
and has been set to sero. Methods for finding the im-
age translation are briefly discussed in section 4.2.4.
Then the affine transform has only four deformation
parameters. It is also assumed that shading and illu-
mination effects can be ignored. These can, however,
be taken care of by incorporating an additional con-
stant factor in the equations. For simplicity, we focus
on the 2-D case although the discussion is dimension-
independent.
Our discussion is based on two observations. First, the
result of a filtering operation on two image patches will
be different in general unless the filter is appropriately
deformed for the second image patch-the deforma-
tion being a function of the affine transform. Second,
moments of the image patches are related by simple
functions of the affine transforms, and this can be ex-
ploited to compute the affine transform.
Consider two functions Fg and Fy related by an affine
transform of the underlying coordinate system. Then
Their integrals over some finite interval are related by:
expressed succinctly as
Let s; be the scale change along the i'h dimension and
n the number of dimensions. Then det(A) = II?s;.
This can be intuitively understood as follows, Con-
sider the 1-D case (n s 1), where the affine trans-
form reduces to a scale change. Let the function F;
be graphed on a rubber sheet. The graph of Fy is
obtained by stretching the sheet and attached coor-
dinate system. The determinant term is equal to the
stretching undergone by the coordinates. Note that
the integral of a function may also be viewed as its
eroth moment.
(3) cannot be used directly because the limits on the
right-hand side depend on the affine transform and are
therefore unknown. This crucial point has not been
handled correctly before. On the other hand, taking
the llimits from -oo to oo would not preserve local-
isation. The solution to this problem is to weight the
function by another which decays rapidly-here the
gaussian is used.
We present the weighted equations analogous to (3)
first for the case where A = aR. (i,e. the affine trans-
form equals a scale change s times a rotation R.), fol
lowed by the general case.
Denote the unnormalised gaussian by
Multiply both si4es of (2) by B(r,o') to obtain
From the orthonormality of rotations it follows that
which allows (6) to be rewritten as
The weighted seroth moment is therefore
where the limits are taken from -oo to oo. The factor
ae detA* can be eliminated by using normalised
gaussians
in place of H. The moment equation then becomes
The integral may be interpreted as a gaussian convo-
lution or filtering at the origin. Thus we write (11)
where r s Ar.
(12), the weighted analog of (3), is exact and valid for
arbitrary dimensions. The problem of recovering the
affine parameters has been reduced to finding the de-
formation of a known function, the gaussian, rather
than that of the unknown brightness functions. How-
ever since (12) is invariant to rotation it can only be
used for recovering the scale (the recovery of rotation
by other means is discussed later). Note that although
the llimits are infinite, since the gaussian is a rapidly
decaying function, it suffices in practice to take limits
from -4o to 4o (and correspondingly from -4so to
4so on the right-hand side).
A similar equation can be shown to hold for arbitrary
affine transforms, provided generalised gaussians are
used. Define a generalised gaussian as
where M is a symmetric positive semi-definite matriz.
Then
Thus the weighted moment equation may be written
where the i&entity det(AAF')MA e det(A) has been
used. The matrix AA is a symmetric, positive semi-
definite matriz and may therefore be written
where R. is a rotation matriz and E a diagonal matrix
with entries io',s4o'..is,o' (si 2 0). Thus
Again, to show the connections to convolution and fil-
tering, this may be written as
(18) is the analog of the sero moment equation (3),
and can be used for determining the affine transform.
The level contours of the generalised gaussian are el-
lipsoids rather than spheres. The tilt of the ellipsoid
is given by the rotation matrix while its eccentricity
is given by the matrix E, which is actually a func-
tion of the scales along each dimension. (18) clearly
shows that to recover affine transforms by filtering,
one must deform the filter appropriately; a point ig-
nored in previous work [Bergen et al., 1992; Koen-
derink and van Doorn, 1987; Campani and Verri, 1992;
Werkhoven and Koenderinck, 1990; Jones and Malik,
1992b]. The sero moment equation (18) alone does
not permit the recovery of the complete affine matriz-
only the scales and the tilt. To find the complete affine
transform, higher order moments need to be consid-
ered. Using higher order moments also permits the
use of more overconstrained equations.
The first order moments of F4 and Fy are related by
The second order moments are given by
and this may be expressed as
Note that the zeroth moment equation is a scalar equa-
tion, (20) a vector one, and (21) is a matri equation.
As before, the moment equations (20) and (21) are not
directly usable due to the difficulty that the limits of
the patches integrated over depend on the deforma-
tion, Therefore we again employ gaussian weighted
moments, using the fact that the derivatives of gaus-
sians are closely related to moments weighted with
gaussians.
The effect of filtering with derivatives of gaussians can
be obtained by differentiating the gaussian (13). First
write rs = Ar. Differentiating (13) gives
where
and
This equation looks different from the first moment
(20) because the first derivative of the gaussian has
been normalized. Convolving with second derivatives
of a gaussian gives
where
and
(25) and (21) are seen to be closely related; the differ-
ences are the additional term due to the gaussian in
(25) and due to normalisation.
Since convolutions with gaussians and derivatives of
gaussians are so closely related to the original weighted
moment equations, they will often be referred to as
moments in the rest of the paper.
If the value of the moments is sero, (or near zero in
practice), the moment equations are ill-conditioned
and cannot be solved. This can occur in two ways;
either the signal strength is too low (i,e. the magni-
tude of F is small) or the function F is purely even or
purely odd causing some of the moment equations to
be sero. There is little that can be done in the first
case. The latter case, however, provides insight into
the number of moment equations required to solve for
the affine parameters.
Consider first the 1-D case. It is easy to see that the
even moments of any odd function will be sero while
the odd moments of any even function are zero. Since
the zeroth moment is even, and the first moment is
odd and only one parameter (the scale) needs to be
determined, these two equations suffice to find it.
The situation is a little more complicated in higher
dimensions. One way of stating the problem is to con-
sider each dimension separately. Then if a function is
odd along any dimension, its contribution to the even
moment from that dimension will be sero and hence
inferences along that dimension cannot be made. Sim-
ilarly, if a function is even along any dimension, its
contribution is sero to the odd moments along that
dimension. Note that typically a function is even or
odd only at a few points over its domain, so this may
not be a significant problem.
How many moments are required in 2-D to solve for
the affine transform? In general four affine parame-
ters need to be determined. Straightforward equation
counting seems to show that there are four even equa-
tions (l from the seroth moment and 3 from the second
moment), and there are six odd equations (2 from the
first moment and 4 if the third moment is used). Thus
even if the function is purely even or odd, moments up
to third order suffice to solve for the affine parameters.
However, the third moment is actually not required,
since the previous analysis ignores the information
available from the deformation of a gaussian. Consider
the zeroth moment when an arbitrary affine transform
A needs to be measured. In this case, the zeroth mo-
ment may be used to find the matrix AA' (i,e. 3
parameters may be computed). Accounting for the
additional information available from the deformation
of the gaussian, there are at least 6 even equations [3
from the zeroth moment and at least 3 from the sec-
ond moment) and at least 4 odd equations (from the
first moment).
The function F may also be transformed so that some
of the moments are always non-sero. For example, if
instead of the function F, the magnitude of its auto-
correlation is used, the seroth and the second moment
are always nonsero. This follows from the radial sym-
metry of the auto-correlation function--which implies
that the odd moments are all zero while the even mo-
ments are nonsero.
A different transformation uses certain algebraic tricks
to convert any function to an odd or an even func-
tion thus ensuring that every moment equation is well-
defined. For example, consider the 1-D case again.
Every function F can be written as the sum of an
even part EF and an odd part OP. The odd part OF
can be converted into an odd function by taking its
magnitude. The even part EF can be converted to
an odd function by fiipping one half of the function.
The problem is somewhat more complicated in higher
dimensions. The 2-D case will be dealt with in the
solution section.
In the remainder of this paper, only the case where
the affine transform A = aR (i.e. a scale change and
a rotation) will be considered (see section 2.1); the
general affine transform will be considered in a later
paper. The seroth moment equation will be dealt with
first followed by the first moment equation.
When the affine transform is described by A == aR,
(12) can be written as
where o' s so. The important point here is that
the rotation matrix does not figure in o'. The
problem of finding the scale parameter has therefore
been converted into the problem of finding the value
of o'. Older methods [Bergen et al., 1992; Koen-
derink and van Doorn, 1987; Campani and Verri, 1992;
Werkhoven and Koenderinck, 1990; Jones and Malik,
1992b] have instead concentrated on the much more
difficult problem of trying to correspond the functions
F and F.
The equation can be solved by sampling the space of
possible values of o', filtering for each sampled value
and declaring the solution to be that value of o' for
which the above equation has smallest residual error
according to some norm. A more elegant approach
uses the fact that the affine transform can be analyti-
cally interpolated. The idea is to sample over a small
set of o' and then interpolate using a Taylor series
approximation. Consider first a given o'. The Taylor
series approximation to first order gives
where a = 1+ a. The last equality follows from the
diffusion eqwation =: ov G. This allows the con-
volution (28) to be written as
The above equation is linear in a. To find s, three
filtering operations need to be performed: two gaus-
sian filtering operations and one laplacian operation.
Note that the above equation expresses the well-known
result that a laplacian can be approximated by a dif-
ference of gaussians.
Information in an image is scale dependent. There
may be information present at several different scales
or at only one of them. A method which does not
take this into account is not likely to be robust. Thus
it is desirable to solve the above equation at several
different scales (o). Let a set of o, be chosen. For each
such o, an equation of the form (31) may be written
giing the following system of equations
This is an overconstrained set of equations in the
unknown a. The redundancy offered by the over-
constrained problem also makes it more robust with
respect to noise.
The particular choice of the o; is to some extent ar-
bitrary although some general criteria may be speci-
fied. Too small a o; will make the system sensitive
to noise while localisation requires that o; not be too
large. The actual values are not very crucial. In prac-
tice, a set of eight different o, were chosen. They
were all spaced apart by half an octave (a factor of
1,4). The filter width = 8 o, (since the filters need to
range from -4o, to 4o;). The widths actually chosen
were (3,5,7,10,14,20,28,40) (see also [Jones and Malik,
1992al).
The above system of equations was cast into the fol
lowing linear least squares problem
and was solved using Singular Value Decomposition
(SVD). It was found that the lowest filters (widths
s 3,5,7) were noisy and hence they were disregarded
(one reason may be that the laplacian is noisy when
the filter sise is small). The scale was recovered fairly
accurately using the other widths (see the experimen-
tal section for details). This set of filter widths worked
better than another one where 8 filters were used with
their os spaced apart by a factor of 1.2; presumably
because with a larger variation in scale, there is more
information available at multiple scales.
For large s (say s 3 1.3) the recovered scale sometimes
tends to be poor. This is because the Taylor series
approximation is good only for a small change in o.
The problem arises because in (31) the right-hand side
is expanded around the same o, as on the left-hand
side. A better approximation is obtained by expanding
o' as close to the correct scale as possible. An example
should clarify this point. Assume that the left-hand
side uses o s o and that the scale change s is 1.3,
then it is better to expand the right-hand side around
a4 = 1.4oo (i.e. the half-octave step closest to the
actual scale) rather than expanding at oa. In this case,
(31) may therefore be modified to
where s = 1.4(14a'). Since filtering by a set ofo is al-
ready being performed for the overconstrained system
no additional filtering operations are required. Again,
an overconstrained system may be implemented eas-
ily. For each value of o; on the left-hand side of (34),
expand around dau41 on the right-hand side.
A similar scheme may be implemented if the scale s
g 0.8 by expanding around a o, which is smaller by a
factor of 1.4.
A priori the o, around which the expansion should be
done is not known since the value of the scale is not
available. The solution is to expand around all three
of them (i.e os, 1.4oa and o;/1.4) and then pick the
correct answer to be the one which gives o' close to
the operating point. Again an example will clarify this
point. Assume that the correct scale is again 1.3 and
that the three different operating points return the
following values of a (1.25, 1.32, 1.18). Consistency
decides the correct answer here. 1.18 is inconsistent
with expanding around o /1.4and can be rejected. The
other answers are both between l and 1.4 and closer to
1.4. Therefore, the appropriate operating point to pick
is 1.4o, = c441. Experimentally, this method seems to
work well. An alternative is to compare the residual
error after SVD minimisation; this does not seem to
work as well, partly becanse the different errors are
not really comparable-they have different numbers
of equations. Another technique that has been tried is
to make all the equations into a single overconstrained
system and solve it using SVD -based on the answer
obtained, some of the equations may be dropped and
the system resolved.
In principle the same technique can be used to ex-
pand around nonnearest neighbor operating points ds,
[j - i|> 1, if the scale gets very large (or small). The
range of scales to be expected depends on the applica-
tion. For structure for motion, a scale change of more
than 1.4 almost never happens in practice. In find-
ing shape from texture, in principle any scale change
can occur. If the surface is smooth, it is expected that
there is likely to be a neighbouring texture patch whose
scale change is less than 2.5. IIn this case one should
also expand around 2.0a and 0.5o in addition to o, l.4o
and 1f1.4o. Very high scale changes are probably dif-
ficult to measure in any case because of the extreme
foreshortening that this implies. We reemphasise that,
apart from such inherent limitations, our approach can
in principle handle large magnitude affine transforms
with little approximation, whereas previous methods
were limited to small transforms.
The method does not work if the output of the gaus-
sian convolution is sero (or close to sero in practice).
This can happen either if the signal is weak or if the
signal shows odd symmetry along any dimension.
The 1-D case was dealt with in section 4.1. Here it
is shown how a function in 2-D may always be con-
verted into an even function. One cannot consider each
dimension separately for this would destroy the rota-
tional symmetry of the gaussian. Instead, the function
is decomposed into parts which are radially even E,F;
and tadially odd O,Fi where
The magnitude of both fanctions (] E, Fi ] ] O,Fi ]) is
then taken. The resulting functions are both even and
the gaussian convolution is nonsero for both. The SVD
is performed on each set of these functions separately
and the one with the lower error is then used (this
ensures that if either the even or odd components is
really small, it is ignored).
Before scale can be recovered, the two patches must
be aligned by finding the image translation. These can
be found using traditional optical flow or displacement
schemes Anandan, 1989]. Alternatively, the residual
of the SVD error can be used to localise the image
translation to S0.5 pixels. This is done in the fol-
lowing manner. The first image patch is filtered with
the set of gaussians. The second patch is filtered with
gaussians at every pizel in a small window centered
at the first patch's location and the SVD computed.
That pixel for which the SVD residual is minimised is
declared to be the correct image translation. Experi-
mentally, this method was found to work satisfactorily.
Note that in general no additional filtering operations
are required since the filtering operations are done at
every point in the image anyway.
Experiments were carried out both on synthetic images
as well as a pair of real images. The first synthetic
image (Figure 1) shows a cosine wave generated by
the equation F(,y) = 127 cos(wyV+ 4,). The cosine
was picked for the following interesting properties. It
can be made even or odd at any point depending on
the value of @,, Further, there is no information along
the y direction (the so-called aperture problem). In
spite of that the scale can be recovered.
For the first experiment, f, as chosen to be sero, so
that the function was even. 4, = 0.2 was chosen. A
second cosine function was generated using the follow-
ing function Fs(z,y) = 127cos(44 + 4,) (Figure 2).
Fy is rotated 90% with respect to F4 and also scaled by
the factor a. For various values of a, the scale was re-
covered using the seroth moment. The results are tab-
ulated in Table 1. The experiment was repeated with
noise added. First, uniform noise ranging from -10
to 10 was added to F4. Second, gaussian noise with
a standard deviation of 10 was added to Fy. These
results are also tabulated in Table 1. Two operating
points were used: a and 1.4o. The appropriate oper-
ating point was picked as discussed in the text.
Table 1 is to be read as follows. The first column in
Table 1 is the actual scale while column 2 shows the
recovered scale in the noise-free case. Two different
percentage errors are tabulated and they arise from the
following considerations. Assume that an object is at a
depth of za and after a translation T, in the s direction,
its new depth is s = A + T,. Then the percentage
error in finding the quantity ai/so i given by @ +
100 and this is tabulated in column 3 for the noise-
free case. On the other hand, the percentage error in
finding the quantity T,/sa i given by %++100 and this
is tabulated for the noise-free case in column 4. Which
of these quantities is more importantT Since the depth
4 is a priori unknown, the quantity of relevance at
least in the motion case is T,fso and the corresponding
percentage error is more significant. Similar values are
tabulated when gaussian noise (columns 5,5 and 7) and
uniform noise (columns 8,9, and 10) are added.
The results show that even with noise depth recon-
struction effectively has an accuracy on the order of
several percent. The results are excellent in the noise-
free case. The percentage errors in column 3 are all less
than about 3% while even in column 4 the percentage
errors do not exceed 7%, Note that the method recov-
ers scale accurately in spite of the large rotation.
With noise added, the results are as good exxcept for
the lowest scales (1.05 and 1.10, corresponding to the
largest depths). These results for the lower scales
might be improved by using operating points separated
by ratios smaller than 1.4.
The experiment was repeated using W4 = x/2 for both
images. The method failed because the function now
becomes an odd function at the origin and thus the
result of gaussian filtering is sero. However, if the
function is transformed into an even function using
the methods discussed in the text, the seroth moment
can once again be applied and the results are similar.
The experiment was repeated with random dot images.
A random dot image of sise 64 by 64 was generated
(Figure 3). The image was then affine transformed and
smoothed using a cubic interpolation scheme. For var-
ious values of the scale factor s, the scale was recovered
using the seroth moment method. The results are tab-
ulated in Table 2. The highest error in column 4 (rel-
ative depth error) is less than 9% if the smallest scale
(1.05)is ignored. Again the relative error in s (column
3) is much lower. The error is somewhat larger in this
case because the program that affine transforms the
image does interpolation which tends to destroy im-
age structure. This is more serious at the lower scales.
Finally the algorithm was tested on a pair of real im-
ages from a sequence [Sawhney and Hanson, 1991].
The images were taken with a Sony ccd camera us-
ing a robot moving straight ahead. The robot moved
about 1.4 ft between frames. Since the original images
were taken with the intent of using a line based algo-
rithm, most of the objects have little intensity varia-
tion in their interior. However, the posters on the back
wall show some intensity variation and can therefore
be used. Points 1 and 2 were picked by hand in the first
image (Figure 4). The corresponding points in the sec-
ond image (Figure 5) were determined using the SVD
residual error. For point 1, the recovered scale was
1.07 which corresponds to a distance of 1.4/(1.07 - 1)
s 20 ft. For point 2 the recovered scale was 1.06 which
corresponds to a distance of 1.4/(1.065 - 1) = 21.5 ft.
The measured distance to the back wall is 20.3 ft. The
accuracy is thus within 6%,
Again for the case where the affine transform is de-
scribed by A = aB, the first moment equation (22)
may be written
The diffusion equation applied to the derivatives of the
gaussian gives the following identity
where G,, denotes the derivative of the gaussian with
respect to the rP' coordinate. Using this identity,
the right-hand side of (37) is expanded around o and
rewritten as
where R; is the PP'% row of R and a = 1+ a. Note
that there are 2 such equations. This may be more
conveniently written as
where
and
The rotation matrix can be eliminated by taking the
dot-product (i.e. the magnitude) of both sides of [40)
and equating them. This gives
This is a polynomial equation in the unknown a. As
before several different scales o4 are used to give an
overconstrained system. The resulting system can be
solved using the Gauss-Newton technique [Gill et al.,
1981]. The Gauss-Newton procedure works by lin-
earising the system around the current estimate of the
solution reducing the problem to a linear least-squares
problem. Define a vector function c(om) where the PM
component is given by
Then if ag is the current estimate of c, then oa41 s
as + ps where ps is the solution of the linear least
squares problem
where a quantity subscripted by k denotes that quan-
tity evaluated at as and J(o) is the Jacobian matriz
of e(o).
The least s;uares problem was solved using SVD and
convergence was found to be rapid--within a couple
of iterations. The method was tested on a sine-wave
pattern.
In two dimensions, the rotation may be computed in
the following manner. Consider (40) again. This may
be rewritten as
where
is a known quantity (since aa is now known). Let
b = (b,%). Then (48) can be transformed into the
following form
where
u = (cos,sin 8) and 8 is the rotation angle. Us-
ins the i4enue co4 = y$C(i, aua B-*
-B/(detB), (48) can be transformed into the follow-
ing pair of equations each linear in the unknown sin 8,
where w1 (e4,eg). Such pairs of equations can be
written for every o, and the resulting linear system of
overconstrained equations can be solved using SVD for
the rotation angle 8.
Future work includes the solution for the case of the
general affine transform as well as the use of the second
moment equation. Other possibilities include the au-
tomation of the process over the entire image and the
detection of occlusions. Finally, the use of the affine
transform to find surface orientation from both texture
and motion cues will be explored.
Acknowledgements We wish to thank Al Ban-
son for his useful comments on early drafts of this pa-
per. The first author also wishes to thank Harpreet
Sawhney for many fruitful discussions and for his con-
stant encouragement.
ALVINN (Autonomous Land Vehicle In A
Neural Network) [Pomerleau, 1992] has
shown that neural techniques hold much prom-
ise for the field of autonomous road following.
Using simple color image preprocessing to
create a grayscale input image and a 3 layer
neural network architecture consisting of 960
input units, 4 hidden units, and 50 output units,
ALVINN can quickly learn, using back-propa-
gation, the correct mapping from input image
to output steering direction. See Figure 1. This
steering direction can then be used to control
our testbed vehicles, the Navlab 1 [Thorpe,
1991] and a converted U.S. Army HMMWV
called the Navlab 2.
ALVINN has many characteristics which make
it desirable as a robust, general purpose road
following system. They include:
These features make ALVINN an excellent
candidate as the building block of a neural sys-
tem which can overcome some of the problems
which limit its use. The major problem this
research addresses is ALVINN's lack of ability
to learn features which would allow the system
to drive on road types other than that on which
it was trained. In addition to overcoming this
problem, the system must meet the current
needs of the autonomous vehicle community
which include:
From these requirements we have begun
developing a modular neural system, called
MANAC for Multiple ALVIINN Networks In
Autonomous Control. MANIAC is composed
of several ALVINN networks, each trained for
a single road type that is expected to be
encountered during driving. See Figure 1.This
system will allow for transparent navigation
between roads of different types by using these
pretrained ALVINN networks along with a
connectionist integrating superstructure. Our
hope is that the superstructure will learn to
combine data from each of the ALVINN net-
works and not simply select the best one.
Additionally, this system may be able to
achieve better performance than a single
ALVINN network because of the extra data
available from the different AILVINN net-
works,
The MANIAC system consists of multiple
ALVINN networks, each of which has been
pretrained for a particular road type. They
serve as road feature detectors. Output from
each of the ALVINN networks is combined
into one vector which is placed on the input
units of the MANILAC network. The output
from the ALVINN networks can be taken from
either their output or hidden units, We have
found that using activation levels from hidden
units provides better generalization results and
have conducted all of our experiments with
this connectivity. The MANIAC system is
trained off-line using the back-propagation
leaning algorithm [Rumelhart, 1986] on
imagelsteering direction pairs stored from
prior ALVINN training sessions.
The architecture of a MANIAC system which
incorporates multiple ALVINN networks con-
sists of a 30x32 input unit retina which is con-
nected to two or more sets of four hidden units.
(The M1 connections in Figure 2.)This hidden
layer is connected to a second hidden layer by
the M2 connections. The second hidden layer
contains four units for every AILVINN network
that the system is integrating. Finally, the sec-
ond hidden layer is connected to an output
layer of 50 units through the M3 connections.
All units in a particular layer are fully con-
nected to the units in the layer below it and use
the hyperbolic tangent function as their activa-
tion function. Also, a bias unit with constant
activation of 1.0 is connected to every hidden
and output unit, The architecture of a
MANAC system incorporating two ALVIINN
networks is shown in Figure 2.
The topology of the input retina and M1 con-
nections of MANIAC system is identical to
that of the Al connection topology of an
ALVINN network. See Figure 1. This allows
us to incorporate an entire MANIAC system
into one compact network because the Al con-
nection weights can be directly loaded onto the
M1 connections for a particular set of first
layer hidden units of the MANIAC network.
Simulating the entire MANIAC system, then,
does not entail data transfer from ALVINN
hidden units to MANIAC input units, but only
a basic forward propagation through the net-
work,
It is the Al connection weights of the
ALVINN network that extract vital features
from the input image for accurate driving. So
in addition to allowing easy implementation of
the MANAC network, the network topology
of the M1 connections allows us to capture
important weight information in the MANIAC
system that the AILVINN hidden units have
learned. These features can be interpreted
graphically in two dimensional views of the
A1 connection weight values. Typically, a net-
work trained for one lane roads learns a
matched filter that looks for the road body,
while a network trained on multi-lane roads is
sensitive to painted lines and shoulders.
To train the MANAC network, stored imagel
steering direction pairs from ALVINN training
runs are collated into a large training sequence.
These pairs consist of a preprocessed 30x32
image which has been shifted and rotated to
create multiple views of the original image
along with the appropriate steering direction as
derived by monitoring the human driver during
ALVINN training. See [Pomerleau, 92] for an
in-depth discussion of the image preprocessing
and transformation techniques. After collation,
the sequence of pairs is randomly permuted so
that all exemplars of a particular road type are
not seen consecutively. The current size of this
training sequence for a two ALVINN
MANIAC network is 6000. IIf additional
ALVIINN networks are used, 300 images per
new ALVINN network are added to the train-
ing sequence. This sequence is stored for use
in our neural network simulator.
Next, weights on each of the connections in
the MANIAC network must be initialized.
Because the MANIAC M1 connections consist
of precomputed ALVINN A1 connection
weights, they must be loaded from stored
weight files. After this is done, the M2 and M3
connection weights in the MANILAC network
are randomized. This weight set is then ready
for use as the initial starting point for learning.
To do the actual training, the network architec-
ture along with the weight set created as dis-
cussed in the previous paragraph and the
stored training sequence, are loaded into our
neural network simulator. Because the
MANILAC M1 connection weights are actually
the pretrained ALVINN weights who serve as
feature detectors, the M1 connections are fro-
zen so that no modification during training can
occur to them. See Figure 2.
Initially, training is done using small learning
and momentum rates. These values are used
for 10 epochs. At this point they are increased
(approximately doubled) for the remainder of
training. This technique seems to prevent the
network from getting stuck in local minima
and is an adaption of a technique used in
ALVINN training.
The back-propagation learning algorithm is
used to train the network. The stored images
are placed on the input units of the MANAC
network while a gaussian peak of activation is
centered at the correct steering direction on the
50 output units of the network. After about 60
epochs, the network has converged to an
acceptable state and its weights are saved. This
takes approximately10 minutes on a Sun
Sparcstation 2.
It should be noted that MANIAC uses the
same output vector representation as AILVINN,
This allows the output of the MANIAC net-
work to easily be compared with that of
ALVINN for quantitative study and also
allows for the use of existing software in the
MANIAC-vehicle interface.
Once the network has been trained, we use it in
our existing neural network road following
system to produce output steering directions at
approximately 10 Hz.
Empirical results of a MANIAC system com-
posed of two ALVINN networks have been
encouraging. For this system, one AILVINN
network was trained to drive the vehicle on a
one lane path while the other learned to drive
on a two lane, lined, city street. The resultant
MANIAC network was able to drive on both
of these road types satisfactorily.
To determine more quantitative results, imagel
steering direction pairs from the same two road
types as well as from a four lane, lined, city
street were captured. See Figure 3. Using these
stored images, ALVINN networks were
trained in the lab to drive on the one lane
paved path and the two lane, lined city street.
Also, a MANIAC network integrating the
same two ALVINN networks was trained. The
results of these experiments are summarized in
Table 1. In Table 1 the columns represent the
average error per test image for a particular
road type and the rows represent the type of
network that is being used. The errors com-
puted are of two types, SSD error and Output
Peak error. SSD error is the sum of squared
differences error while Output Peak error is the
absolute distance between the position of the
gaussian peak in the desired output activation
and the peak in the actual output activation.
SSD error can be thought of as a measure of
the network's ability to accurately reproduce
the target vector while Output Peak error is a
measure of the ability of the network to pro-
duce the correct steering direction.
The initial comparison to notice in the table is
that the ALVINN network trained for a partic-
ular road type always performs significantly
better (> 50%) than the ALVINN network
trained for the other road type when presented
test images of the type of road on which it is
trained. This is to be expected. Also notice that
the single MANIAC network, which has been
trained to respond properly to both road types,
typically compares well to the correct
ALVINN network (within 11% in all cases).
As mentioned earlier, this amount of error is
acceptable to properly drive the vehicle.
The case of the four lane road is unique in that
neither of the AILVINN networks nor the
MANILAC network saw a road of this type. In
this case, the response of the one lane path
ALVINN network is nearly identical to when it
was presented a two lane, lined, city street.
Because this type of network typically
responds to the body of the road and the fact
that the two and four lane roads are both sig-
nificantly wider than the one lane path, ie.
have a larger body area, this response was
expected. A more interesting response is that
of the two lane road AILVINN network. It
seems to respond better to the four lane road
images than it does to the two lane road test
images. A possible explanation of why this is
occurring can be seen in Figure 3. The four
lane road and the two lane road look almost
identical. One slight difference, though, is that
the contrast of the road/offroad boundary is
slightly higher in the four lane road case than it
is in the two lane road case. This difference
could help the network localize the road better,
and because we want the vehicle to drive in the
left lane, close to the yellow line, the correct
output is identical to the two lane road case.
The most interesting result, though, is that
when presented with four lane road images,
the MANILAC network actually performs better
than either the one lane path ALVINN network
or the two lane road ALVINN network. In both
the prior cases, the MANIAC network per-
formed slightly worse than the best AILVINN
network for a particular road. This could imply
that the MANILAC network is using informa-
tion from both networks to create a reasonable
steering direction at its output. This will be
discussed more in the following section.
A central idea that this research is trying to
examine is that of improving performance and
making connectionist systems more robust by
using multiple networks - some of which
might be producing incorrect results. In our
system the key point to notice is that although
a particular ALVINN network may not be able
to drive accurately in a given situation, its hid-
den units still detect useful features in the input
image. For example, consider an ALVINN net-
work that was trained to drive on a two lane,
lined road. The features that it learns are
important for accurate driving are the lines on
the road and the road/non-road division. Now
present this network with a paved, unlined
bike path. The ALVINN network will respond
in its output vector with two steering direction
peaks. The reason for this is that one of the
features that the network is looking for in the
input image is the delineation between road
and non-road. Because this occurs at two
places in the image of the paved bike path, the
feature detecting hidden units produce a
response which indicates that the road/non-
road edge is present at two locations. If these
hidden unit activations were allowed to propa-
gate to the output of the network, the charac-
teristic two peak response would appear.
Although in reality this is the incorrect
response, it is a consistent response to this
input stimulus. A similar scenario holds for
other ALVINN networks given input images of
road types for which they haven't been trained.
Because the response of particular AILVINN
network is consistent when presented with
similar images, the MANIAC network can use
this 'extra' data to produce a correct, perhaps
better, steering direction than a single
ALVINN network. It is possible that this is
what is happening in the case of MANIAC
driving better on the four lane road than either
of the ALVIINN networks.
There are many directions this research can
take but perhaps the most interesting is that of
developing se[f-training systems. In the cur-
rent implementation of the MANIAC system,
ALVINN networks must be trained separately
on their respective roads types and then the
MANILAC system must be trained using stored
exemplars from the AILVINN training runs. If
a new ALVINN network is added to the sys-
tem, MANIAC must be retrained. It would be
desirable to have a system that, when given
initial or new ALVINN networks, created its
own training exemplars and was able to auto-
matically learn the correct MANILAC network
weights. Creating training exemplars from
existing network weights is essentially the net-
work inversion problem. Techniques such as
those developed by [Linden, 1989] may pro-
vide clues of how to do this one to many map-
ping that can create an input exemplar from an
output target. It can be argued that this task is
extremely difficult, even impossible, due to the
high dimensionality of most networks, but per-
haps it is worth taking a hard look at imple-
menting some network inversion techniques
because of the benefits that can be obtained by
having self training modular neural networks.
Another area in which modular neural systems
such as MANIAC may be useful is that of
incorporating information from different
sources. An example of this idea is to use
MANIAC as a framework in which to add
sensing modalities other than video. In addi-
tion to a video camera, our testbed vehicle, the
Navlab 2, is equipped with an infrared camera
and two laser rangefinders. If these devices can
be used as input to ALVINN-like systems
which produce a steering angle as output, it is
reasonable to assume that a training technique
similar to the one used in the current video-
only MANIAC system will result in a network
which will be robust in all of the component
network domains. This could lead to highly
robust autonomous systems which could oper-
ate in a variety of situations in which current
systems fail. Driving with the same system in
both daylight and at night is an example. In
this scenario video images provide sufficient
information to drive in the daytime but at night
sensors such as infrared cameras would be
necessary. The infrared cameras need not go
unused in the day though, as their output
would provide addition information to the
modular network.
In addition to the previous areas of work, there
is much to be done with developing systems
which can allocate their resources and group
relevant features together. It has been shown
that modular neural networks can learn to allo-
cate their resources to match a given problem,
such as locating and identifying objects in an
input retina [Jacobs, 1990], while the cascade
correlation algorithm provides a way to pro-
duce appropriately sized networks. [Fahlman,
1990] By using similar techniques in a
MANILAC-like system, the need to pretrain
ALVINN networks would be eliminated. It is
not clear, though, how new information would
be incorporated into this type of system once it
has been trained.
This research has focused on developing a
modular neural system which can transpar-
ently navigate different road types by incorpo-
rating knowledge stored in pretrained
networks. IInitial results from the autonomous
navigation domain are promising. Although
the system is simplistic, it provides a starting
point from which we can explore many differ-
ent areas of the connectionist paradigm such as
self-training modular networks and network
resource allocation. In addition to these areas,
autonomous navigation tasks such as multi-
modal perception can be studied.
This research was partly sponsored by
DARPA, under contracts ''Perception for Out-
door Navigation'' (contract number DACA76-
89-C-0014, monitored by the US Army Topo-
graphic Engineering Center) and ''Unmanned
Ground Vehicle System'' (contract number
DAAE07-90-C-R059, monitored by TACOM)
as well as a DARPA Research Assistantship in
Parallel Processing administered by the Insti-
tute for Advanced Computer Studies, Univer-
sity of Maryland. Many thanks also go to the
Semiautonomous Surrogate Vehicle group at
Martin Marietta, Denver, where this research
began.
ecently, there has been much research in the
field of sensor planning [Cowan and Bergman,
1989, Hutchinson and Kak, 1989, Ikeuchi and
Kanade, 1989, Tarabanis et al., 1991a]. The ba-
sic problem is that in setting up an automated
system for monitoring some process, the effec-
tiveness of the system can largely be determined
by the locations, types and configurations of the
sensors used, To manually determine these pa-
rameters on a case by case basis may not be cost
effective or accurate, and the resulting system
may not be optimal in any sense, It may be
better to have an automated system for deter-
mining the sensor locations and parameters for
monitoring a given task,
To that end, many systems have been and are
being developed which, based on geometric mod-
els of an environment and models of the sensors,
can generate sensor locations and settings which
provide a robust view of specific features so that
the features are detectable, recognizable, mea-
surable, or meet some other task constraints, In
general, the sensors are cameras and a robust
view implies that the camera must have an un-
obstructed view of the entire feature set, which
must lie within the depth-of-field of the camera
and must be magnified to a given specification.
Sensor planning systems can then generate cam-
era locations, orientations, lens settings (focus-
ring adjustment, focal length, aperture), and in
some cases lighting plans to insure a robust view
of the features.
It is interesting to note that while research in
robot motion planning abounds, research in sen-
sor planning has focused on sensor planning for
static scenes, It is our belief that an intelligent
robot system capable of planning its own actions
should be capable of planning its own sensing
strategies. With a dynamic sensor planning sys-
tem, this goal is closer to a reality. Robots in-
volved in manufacturing or assembly can deter-
mine appropriate sensor locations, Teleopera-
tors can have the robot system guarantee robust
viewpoints during the operation. The intelligent
motion plans which researchers spend so much
effort computing can be monitored in an intelli-
gent fashion.
To that end, we have been exploring methods
of extending the sensor planning abilities of the
'MVP'' Machine Vision Planning [Tarabanis,
1991, Tarabanis et al., 1991a] system to func-
tion in environments where objects are moving.
In particular, we focus on sensor planning in a
dynamic robotic work cell environment.
In previous work, we described a technique
for sensor planning in a dynamic environ-
ment [Abrams and Allen, 1991], which was im-
plemented using a simulated model of a simple
moving object. Here, we present a detailed anal-
ysis of the dynamic sensor planning problem and
improved versions of the original algorithms, In
addition, experimental results using a model of
a dual-robot work cell are presented in which we
automatically monitor a task in the work cell,
A complete description of the MVP system is
beyond the scope of this paper, For details,
see [Tarabanis, 1991, Tarabanis et al,, 1991a,
Tarabanis et al., 1991b]. In brief, MVP takes
a constraint based description of the vision task
requirements and synthesizes what has been
termed a generalized vieupoint, which is an eight-
dimensional vector incorporating sensor loca-
tion, orientation, and lens parameters including
aperture and effective focal length. The con-
straints MVP considered in determining view-
points are depth-of-field, field-of-view, resolu-
tion, and unoccluded visibility.
MVP contains analytical relationships for the
optical task constraints (resolution, focus, field-
of-view), and uses 3-D solid geometric models
of the environment to formulate visibility con-
straints, (The geometric models are polyhedra,
both convex and concave,) The constraint equa-
tions can be thought of as defining hypersurfaces
bounding feasible regions in the 8-dimensional
parameter space of the generalized viewpoint.
These constraints are combined in an optimiza-
tion setting to produce a generalized viewpoint
which meets all task constraints with as much
margin for error in sensor placement and set-
ting as possible (i,e., as far away from all hyper-
surfaces as possible). Using CAD descriptions
of the object to be viewed and its environment,
MVP generates the visibility region for viewing
the desired features. This region is calculated
to be the total volume in space from which the
features are viewable without obstruction. This
volume is used in the optimization stage of MVP
for finding the best viewpoint.'
There are two basic cases which must be dealt
with separately in the dynamic sensor planning
problem. First is the case where the target ob-
jects, i.e, those features which must be viewed,
remain stationary and other objects, such as the
robot which is performing some operation on the
stationary part, moves. This case can arises in
teleoperation and in many manufacturing tasks
(ie. spray-painting, spot-welding, etc.) Sec-
ond is the case where the targets to be viewed
are moving. This can also arise in teleoperation
and in other manufacturing tasks (i,e, pick-and-
place, part insertion, etc.).
The main difference between these two cases is
that in the first case, if a viewpoint is found
to be valid at some point during the task, it is
guaranteed to be valid with respect to all op-
tical constraints at all times during the task.
This is because the functions defining the op-
tical constraints only depend on the target fea-
ture locations and the sensor parameters, and
not on the positions or orientations of obstacles
in the environment. This fairly obvious, but im-
portant property allows us to ignore changes in
the optical constraints over time and focus only
on changes in the geometric parameters, i,e, the
visibility constraint.
The second case is more difficult because it re-
quires an examination of how changes in the po-
sition and orientation of the target features effect
the optical parameters, particularly focus and
resolution, However, if the viewpoint is consid-
ered in terms of a coordinate frame attached to
the feature set, the target can always be con-
sidered stationary with the entire environment
considered as moving. The only limitation is
that the entire feature set must be moving as
a single rigid body, i,e, features can not move
independently, While extremely important, in-
dependently moving features are not yet handled
in this work, although it is being examined as
part of ongoing research.
To summarize, the exact problem we are deal-
ing with is one in which an accurately movable
camera is being used to monitor a task, In this
task, the actual target we are monitoring does
not move, but other objects in the environment,
such as a robot arm, or other mechanical parts,
move in a way which is known a priori, The
problem is to find where to place the camera, and
when and where to move the camera, so that at
all times during the task, we have a good view-
point for monitoring the task,
At a first glance, it may seem that the dynamic
sensor planning problem can be solved trivially,
T'he naive algorithm for computing a series of
viewpoints is as follows:
There are several problems with this approach.
First, it makes no attempt to reduce the number
of sensor placements required. Second, a view-
point is used up until the moment it becomes
invalid, or at least up until the point at which
the margin for error becomes very small, This
defeats the purpose of MVP, which is to find a
viewpoint which has as large a margin for er-
ror as possible. Worse, by the time a viewpoint
is deemed unacceptable, due to errors in sensor
placement, etc., the viewpoint may have been
invalid for some time.
TThe basic problem is that this technique does not
use knowledge of the motion in computing view-
points which will be valid for a long period of
time. It is conceivable that a new viewpoint will
be needed at every 2t, since objects are moving
in unaccounted for paths. A better approach,
such as the one presented below, uses its knowl-
edge of how objects in the environment move to
plan better viewpoints.
The approach being taken is a Temporal Inter-
val Search method, which is is based on the use
of swept volumes. The geometric models of the
moving objects are swept through their paths to
compute the regions in space which, during some
interval, are occupied by some moving object in
the environment. The MVP algorithms are then
run using the swept volumes for the occluding
bodies as opposed to the actual models, thus re-
ducing the dynamic sensor planning problem to
a static problem. If no viewpoint is found consid-
ering these swept objects over a time interval, a
temporal interval search is performed to find the
largest time intervals which can be monitored by
a single viewpoint. This allows us to plan a se-
ries of viewpoints and the times at which they
become feasible.
Given that we have an object O whose motion is
known over a time interval T, we define 7(T,O)
to be the volume swept out by O during T. For
example, in 2 dimensions, if O is an axis-aligned
unit square moving one unit per second in the
positive a direction, and T'is 3 seconds, 7(T, O)
is a 1 x 4 square. The key to using swept ob jects
for sensor planning (or, in fact, for any collision
avoidance problem) is that in planning around
an obstacle given by 7(T', O), you guarantee that
you have avoided the actual obstacle O at any
instant in interval T, This observation was made
by Cameron in [Cameron, 1984] for the ''clash
detection'' (robot collision avoidance) problem.
Let V represent visibility volume for T(T. O).
VV is the set of all points (in 3-space) which give
views of the target which have no obstructions
(due to O) for the entire time interval T. If V is
a null volume, there is no single viewpoint which
would be valid for all of T. Even if V is not null,
there is no guarantee that there are viewpoints
within V which satisfy the optical constraints of
MVP.
A possible problem when using swept volumes
for collision avoidance type problems is that
sweeping an object discards all information re-
garding where the object is at any particular
moment. We present a technique for recover-
ing sufficient temporal information to plan sen-
sor locations. If using V as a visibility volume,
MVP is unable to find a viewpoint which meets
all constraints, we conclude that T is too large
an interval to plan a single viewpoint for, given
the motion of O. We have no information con-
cerning when any particular viewpoint becomes
invalid; we only know that we can not find a
single viewpoint which is valid for the entire in-
terval. Recomputing T(T, O) for a shorter time
interval T will yield a smaller obstacle, a larger
V, and MVP may now be able to find a view-
point.
We can now present the algorithm formally. As-
sume we have a polygonal target r which we wish
to monitor during the time interval I' = [a,4].
During T, there is a set of known obstacles O
through O,4, which move in known paths. The
goal is to plan a single viewpoint valid for the
entire interval, if such a point exists, or to de-
termine a sequence of viewpoints which, when
executed at the appropriate times, allow the fea-
tures to be monitored for the entire interval,
Note, this is not strictly a binary search. Step
4 above only looks at the first half of the time
interval, i,e. T; = [o.4;2]. The algorithm
searches for the endpoint of the first time interval
for which MVP can find one viewpoint. It does
this by esamining [t6,], then [o.4.; ].o.4;d.
and so on, Once a single viewpoint is found for,
say, the interval [ts,t,], step 5 sees to it that the
interval [t;,tG] is examined. If no viewpoint is
found for this whole interval, [G,44(-j/2] is ex-
amined, and so on, until a single viewpoint is
found for, say, the interval [t;,t;], This proess
continues until a viewpoint has been found which
is valid until t,4, The critical times are the end-
points of the intervals, i,e, the times at which
the sensor must be moved.
The computation of swept volumes is central to
this algorithm. Depending upon the format in
which the motion is known, the computation of
swept volumes may not be expensive, If piece-
wise linear translational motion is all that is al-
lowed, then the computation of swept volumes is
certainly tractable [Weld and Leu, 1990]. How-
ever, if more general types of motion are allowed,
as in the motions which would be executed bv
a typical articulated manipulator (rotations in
particular ), the exact computation of swept vol-
umes is more expensive, but not impossible. Un-
fortunately, sweeping is not closed over the set of
polyhedra when rotational motion is permitted.
An articulated robot arm moves strictly in rota-
tions about its joint axes, so the resulting swept
volumes are not polyhedral (they would contain
circular arcs, spherical patches, and other curved
surfaces). These objects would not be useable
in MVP. Korein gives an algorithm for comput-
ing polyhedral approximations [Korein, 1985] of
the swept volumes formed by the motion of ar-
ticulated robot links. These techniques can be
used to simplify the computation of the swept
volumes.
Strictly speaking, MVP directly computes vol-
umes of occlusion, not volumes of visibility, In
theory, the complement of a volume of occlusion
is a volume of visibility, In practice, the comple-
ment of a volume of occlusion with respect to the
workspace of the manipulator placing the sensor
yields the usable visibility volume. In the cur-
rent dynamic sensor planning implementation,
instead of computing a swept volume and then
computing its occlusion volume, we compute a
set of volumes of occlusion at discrete points
along the trajectory, These volumes of occlusion
are then unioned to form the volume of occlusion
for the entire interval. This is possible because
the volume of occlusion generated by the union
of a set of obstacles (for viewing a particular tar-
get) is equal to the union of the volumes of oc-
clusion generated by each obstacle. One benefit
of this approach is that subdivisions of the time
interval do not require recomputing new swept
volumes, Instead, the appropriate subset of the
instantaneous occlusion volumes can be unioned
to approximate the volume of occlusion for any
given interval,
T'he result of the temporal interval search will be
a set of viewpoints and critical times at which
to execute them, However, an explicit represen-
tation of time is not required for the temporal
interval search, in which case the critical times
are not times at all but, rather, critical events,
If, for example, the motions of a robot have been
planned as a series of joint-space moves, the crit-
ical events would be joint angle values, If the
motion was planned in cartesian space, the criti-
cal events would be cartesian positions. Finally,
if the robot motion was planned on some global
time scale (perhaps avoiding other moving ob-
stacles), the critical events would be actual times
on this scale, As long as at task execution time
there is a way to determine when the critical
events arise, (i,e. by waiting for the robot to be
within some distance of the prescribed position ),
the viewpoints can be realized.
We have modelled our laboratory environment
using a CAD system (see figure 1). The model
includes two PU MA 560 robots and the object
to be monitored during the task. The first robot
(I) executes tasks, while the second robot (II)
has a camera mounted on it. In the simulated
experiment, robot I passes over the object as if
it were performing an operation on it, such as
spray-painting. During the task, robot II needs
to monitor a feature inside the object. A CAD
model of the object and the feature is shown
in figure 2. The target (i,e, the feature to be
viewed) is the top face of the inner cube.
In order to compute viewpoints for monitoring
robot I's task, we need to compute the visibil-
ity volume for the object as the robot moves in
the vicinity of the object, i,e, the volume from
which the object is visible during the entire task.
In other words, we need to compute the visibility
volume for 7( TaskIntervalRobotI, .) The visibil-
ity volume is computed by first computing the
volume of occlusion, and subtracting it from the
reachable work-space of robot II, in order to pre-
vent the computation of a viewpoint which is ei-
ther unreachable or has an occluded view. The
volume of occlusion is approximated using the
discrete union algorithm described earlier.
In the experiment, the robot model is stepped
through a series of positions along its planned
trajectory. At each step, the volume of occlu-
sion is computed as in the static sensor plan-
ning problem. The individual volumes of oc-
clusion are unioned together to form the vol-
ume of occlusion for the entire trajectory, In
this way, we approximate the volume of occlu-
sion for T( TaskInterval, RobotI ) without explic-
itly computing T(TaskInterval, RobotI ). In fig-
ure 4 we show a discrete approximation to the
volume swept out by Robot I during its task (i,e,
7(TaskInterval, RobotI)). The volume of occlu-
sion resulting from this motion is shown in fig-
ure 5. The volume of occlusion resulting from
the walls of the part (i,e, due to self-occlusions)
is shown in figure 3. These two volumes were
unioned to form the total volume of occlusion,
An approximation to the workspace of Robot II,
the camera-carrying robot, (called the robot's
reachability volume) was generated, The total
occlusion volume was subtracted from this reach-
ability volume giving the reachable/visible vol-
ume. This volume, which containS all points in
space where the robot can position the camera
such that the target can be seen without occlu-
sion, was used in the optimization stage of MVP
in order to compute a viewpoint.
Since MVP was unable to find a valid view-
point for the entire task, the temporal interval
search was used to find subintervals for which
we can find valid viewpoints, Instead of recom-
puting the swept volumes for each subinterval
examined, the discrete approximation allows us
to union the appropriate subset of volumes of
occlusion. The subintervals found for this task
are shown in figures 6 and 7. The generated
volumes of occlusion due to the robot's motion
during each sub-interval are shown in figures 8
and 9. These volumes were again unioned with
the self-occlusion volume and subtracted from
the reachability volume forming the volumes of
reachability/visibility shown in figure 10 and 11,
These volumes were used in the optimization,
and MVP was able to compute a viewpoint for
each interval, Simulated views from these view-
points are shown in figures 12 and 13,
In this section we describe some alternate ways
of examining the both the static and dynamic
sensor planning problems. The observations and
discussions of this section are the motivation for
additional research which is currently being car-
ried out.
One can view the static sensor planning prob-
lem as a configuration space problem. Using this
view, the sensor's possible configurations are de-
scribed by the generalized viewpoint. The valid
configurations are bounded by the constrain-
ing hypersurfaces in the 8-dimensional parame-
ter space of the generalized viewpoint. However,
the combination of the highly nonlinear fashion
of the sensor constraining equations, plus the
high dimensionality of the generalized viewpoint,
standard techniques for searching configuration
spaces appear to be unpractical. This is one of
the reasons why MVP takes a numerical opti-
mization approach to searching the sensor's pa-
rameter space, However, the configuration-space
analogy will be useful in motivating other ideas
below.
Dynamic sensor planning is to static sensor plan-
ning what path-planning with stationary ob-
stacles is to path-planning with moving obsta-
cles. Erdmann and Lozano-Perez [Erdmann and
Lozano-Perez, 1987] proposed a configuration
space-time for solving such problems in two di-
mensions. They presented two approaches, one
for translating polygons and one for two-link ar-
ticulated planar arms. Their approaches focused
on the efficient construction of slices of configu-
ration space-time. The slices were chosen so as
to include easily computable time-varying con-
straints, simplifying the search from the start
configuration to the goal configuration.
In dynamic sensor planning with stationary tar-
gets, the only constraints in configuration space
which move are the boundaries of the visibil-
ity volume, Even if the obstacles are only al-
lowed restricted classes of motion, their volumes
of occlusion not only move but warp, due to the
fact that the volume of occlusion between an ob-
ject and a target depends on the relative orien-
tation of the two. Thus, the constraints which
are moving in configuration space-time are non-
rigid. This makes it very difficult to determine a
convenient way of slicing a configuration space-
time
Another way of viewing the dynamic sensor plan-
ning problem is to segregate the positioning of
the sensor from the orienting and adjusting of
the sensor. This allows the computation of a
3-dimensional region from which all constraints
can be met (i,e, the projection into 3-space of
the set of valid 8-dimensional sensor configura-
tions). The moving polyhedral volumes of occlu-
sion generated by the moving obstacles in the en-
vironment can be considered as obstacles which
the sensor must avoid while moving in the free-
space, This reduces the sensor planning problem
to that of keeping a single point away from the
boundaries of a set of moving polyhedra. Then,
after the sensor path through 3-space has been
planned, the other 5 (optical) parameters can be
planned accordingly.
This suffers from the same problem as the pre-
vious approach, namely that the set of mov-
ing polyhedra (the volumes of occlusion ), are
non-rigid bodies, Although moving polyhedra
have been modelled and examined (i,e, [Canny,
1986, Cameron, 1984[), non-rigidly moving bod-
ies have not been examined in detail, It appears
that an examination of how the volumes of oc-
clusion change with respect to movements of the
obstacles will allow these approaches to be more
useful and appears very promising for future re-
search.
In conclusion, we have successfully extended our
MMVP system to plan sensor locations in a time-
varying environment. This is notable in that to
the best of our knowledge, motion has not been
widely addressed in the sensor planning litera-
ture, The use of swept volumes which provides
a useful way to extend static planning problems
to dynamic domains. We have presented a con-
venient way to recover enough temporal infor-
mation from swept volumes to use them in plan-
ning tasks. Our immediate research plans are
to bring the results of this paper into our lab-
oratory and execute the task with the planned
viewpoints. Also, we will be examining the al-
ternative sweeping techniques presented to see if
they offer any performance improvements.
There are several open issues in dynamic sensor
planning. There is work to be done in compu-
tational geometry to characterize the changes in
a volume of occlusion as the target and occlud-
ing bodies move with respect to each other. A
similar characterization of how the optical con-
straints vary with the target's motion is also
important. Finally, it is hoped that these vari-
ous characterizations can be combined to plan a
continuous path through the sensor's parameter-
space, rather than computing a series of view-
points and critical times, This would complete
the analogy between sensor planning and config-
uration space-time based motion planning, and
allow more useful solutions tobe found to dy-
namic sensor planning problems.
This work was supported in part by DARPPA
contract DACA-76-92-C-007, NSF grants IRI-
86-57151 and CDA-90-24735, North American
Philips Laboratories, Siemens Corporation and
Rockwell International.
A cell-parallel implementation greatly improves the perfor-
mance of a light-stripe range-imaging sensor[1, 2, 3]. Though
equivalent to conventional light-striping from optical and ge-
ometrical standpoints, cell-parallel light-stripe sensors incor-
porate a fundamental improvement in the range measurement
process. As a result, the acquired range data is more robust and
more accurate. Furthermore, range image acquisition time is
made independent of the number of data points in each frame.
By fully exploiting the capability of VLSI to both sense and
process information, we have built a smart sensor that acquires
a complete frame of 10-bit range image data in a millisecond.
Range information is crucial to many robotic applications.
A range image is a 2-D array of pixels, each of which rep-
resents the distance to a point in the imaged scene. Many
techniques for the direct measurement of range images have
been developed[4]. Of these, the light-stripe methods have
proven to be among the most robust and practical.
Fig. 1 illustrates the principle on which a light-stripe sensor
is based. The scene to be imaged is lit by a stripe - a plane of
light formed by fanning a collimated source in one dimension.
The stripe is projected in a known direction using a precisely
controlled mirror. When viewed by an imaging sensor, it ap-
pears as a contour which follows the profile of objects. The
shape of this contour encodes range information. In particu-
lar, if projector and imaging sensor geometry are known, the
distance to every point lit by the stripe can be determined via
triangulation.
A conventional light-striperange sensor builds a range image
using a ''step-and-repeat'' procedure. A stripe is projected onto
a scene, as described above, and one column of range image
data is measured. The stripe is stepped to a new position and
the process is repeated until the entire scene has been scanned.
Unfortunately, step-and-repeat implementations are slow. In
order to build a complete range image using data from N stripe
positions, N intensity images are required. The total time TP*?
to acquire the range frame is
Assuming TN' e 1/30second and N s 100, 7P%
3.3seconds is required.
The frame time of a step-and-repeat sensor has been im-
proved by imposing additional structure on the light source.
For example, the gray-coded sources used by Inokuchi[5] re-
duce the factor of N in (1) to log, N. However, achievable
frame rates are still too slow and the fundamental problem
remains - range frame time increases with spatial resolution.
The cell-parallel technique is an elegant modification of
the basic light-stripe algorithm. The technique is a dynamic
one, with time an important aspect of the range measurement
process[6].
Consider the geometry of a three-pixel, single-row cell-
parallel range sensor, seen from above in Fig. 2. In the fig-
ure, the stripe plane is perpendicular to the page. The stripe is
quickly swept across the scene from right to left, briefly illumi-
nating object features. A sensing element, say Sy, monitors the
light intensity Iy returned to it along a fixed line-of-sight ray
R4. When the position of the stripe is such that it intersects Ry
at a point on the surface of an object, a ''flash'' will be observed
by the sensing element.
Range to the object is measured by recording the time t;
at which the flash is seen. The location of the stripe as a
function of time is known because its projection angle 8; (t)
is controlled by the system. The ''time-stamp'' t; acquired by
the sensing element measures the position of the stripe when
its light is reflected back to the sensor. The three-dimensional
coordinates of one object point are uniquely determined at the
intersection of the line-of-sight ray R with the stripe plane at
6;. (t;) on the surface of the object.
A sensor which collects a dense range image is formed by
arranging identical sensing elements into a two-dimensional
array. The cells of the array work in parallel, gathering a
range image during a single pass of the light stripe. The time
required to acquire the range frame is independent of its spatial
resolution -
The frame time TP 'P% 6f a cell-parallel sensor is set by the
bandwidth of the photo-receptor used in its sensing elements.
Very high frame rates (1/7P'P)can be achieved. The photodi-
odes used in our cell design have bandwidth into the megahertz.
They can detect a stripe moving at angular velocities in excess
of 6,000rpm.
Cell-parallel system geometry can be described using homo-
geneous coordinate transformations[7, 8]. Referring to Fig. 3,
the origin of the frame Og is placed at the optical center of
the imager. The stripe is a half-plane which radiates out from
an axis-of-rotation aligned with the y-axis of the frame and
passing through the point
Stripe rotation 8; is measured counter-clockwise about its axis
when viewed from the positive y direction and defined to be
zero when the stripe lies in the yz-plane. In a homogeneous
representation, a plane is described in terms of a column vector
P that satisfies the scalar product xP E 0, where x is a ho-
mogeneous point that lies in P. In the sensor coordinate frame
defined above, the stripe plane is modeled in terms of b and 8;
The position xg = (4s,9E, 4s) of a sensing element on the
sensor image plane defines the line-of-sight ray Rg. The para-
metric equation for a line in three dimensions is used to repre-
sent Rg as
wbere n = lssl1 = Ve[. The ne parameer
r, when normalized by r;, is simply the distance along Rg
measured from Og heading toward the object.
The point of intersection xs, between the stripe and the
line-of-sight, is found by solving xP; = 0 for r:
In the coordinate frame of the sensor, this point is
Thus, the 3-D position xo of imaged object points can be
recovered from the scalar distance measurement r.
A practical implementation of the cell-parallel range imag-
ing algorithm requires a smart sensor - one in which optical
sensing is local to the required processing. Silicon VLSI tech-
nology provided the means for building such a sensor.
Fig.4summarizes the operation of elements in the smart cell-
parallel sensor array. Functionally, each must convert light
energy into an analog voltage, determine the time at which
the voltage peaks and remember the time at which the peak
occurred.
The multi-pixelcell-parallel range sensor we have developed
is shown in Fig. 5. This chip consists of 896 sensing elements
arranged in a 28 x 32 array. It was fabricated using a 2uum
p-well CMOS, double-metal, double-poly process and mea-
sures 9.2mm x 7.9mm (width x height). Of the total 73 mm'
chip area, the sensing element array takes up 59 mmf, read-
out column-select circuitry 0.37 mm' and the output integrator
O.06mm%. The remaining 14mmf is used for power bussing,
signal wiring, and die pad sites.
The architecture chosen for the range sensing elements
is shown in Fig. 6. Areas of interest in the diagram in-
clude the photo-receptor (PDiode), the photo-current trans-
impedance amplifier (PhotoAmp), threshold comparison stage
(n2Comp), stripe event memory (RSFlop), time-stamp track-
and-hold circuitry (PGateI/CCell) and cell read-out logic
(PGateO/TokenCell).
In operation, sensing elements cycle between two phases
aCuisition and read out.
During the acquisition phase, each sensing element imple-
ments the cell-parallel procedure of Fig. 4. The photodiode
within a cell monitors light energy reflected back from the
scene. Photocurrent output is amplified and continuously com-
pared to an external threshold voltage Vth. When photorecep-
tor output exceeds this threshold, the ''stripe-detected'' latch in
the cell is tripped. The value of the time-stamp voltage at that
instant is held on the capacitor CCell, recording the time of
the stripe detection.
The acquisition phase is synchronized with stripe motion and
ends when the stripe completes its scan. At that time, the array
sensing elements recorded a range image in the form of held
time-stamp values. This raw range data must now be read from
the chip.
A time-multiplexed read-out scheme off loads range image
data in raster order through a single chip pin. One bit of token
state is passed through the sensing element array, selecting
cells for output. Dual nlp-transistor pass gate structures are
used throughout the time-stamp data path. They permit the use
of rail-to-rail time-stamp voltages, maximizing the dynamic
range of the analog time-stamp data.
One of the more challenging aspects of the cell design in-
volved the circuitry which detected the stripe.
A photodiode forms the light sensitive area within each cell.
This diode is a vertical structure, built using the n-substrate
as the cathode and the p-well of the CMOS process as the
anode. An additional p' implant, driven into the well, reduces
the surface resistivity of the anode and increases the device
bandwidth.
The non-linear transimpedance amplifier of Fig. 7 was a key
element of the sensor cell design. Reflected light from the
swept stripe source generates nano-amp photo-current pulses
and thus a very high-gain amplifier is required to convert this
current into a usable voltage. In addition, very little die area
could be devoted to photo-current amplification if cell area
was to be kept small. The three transistor amplifier design
of Fig. 7 satisfies both requirements. Its logarithmic transfer
characteristic provides freedom from output saturation even
when input light levels vary over several orders of magnitude.
The output rise-time of photodiodelamplifier test structures in
response to a stripe was measured to be a few microseconds.
Analog signal processing techniques played an important
role in the design of this smart sensor. As shown in Fig. 6,
sensing elements use analog circuitry to amplify the photo-
current, to detect the stripe and to record the per-cell time-
stamp information. Stripe timing is represented in analog form
as a 0-5V sawtooth broadcast to all cells of the array. This
allowed the time-stamp value to be stored as charge on the l pf
capacitor within each cell. The digital equivalent of latching
a count into a multi-bit register would be significantly larger
in area and would require that the digital time-stamp counters
run during the acquisition phase. Thus, analog processing kept
cell area small and minimized digital switching noise during
photo-current measurements in the acquisition phase.
The 28 x 32 element VLSI sensor prototype described in the
previous section was incorporated into the light-stripe range
system shown in Fig. 8. System components visible in the pho-
tograph include (from the left) the stripe generation assembly,
the VLSI sensor chip and its interface electronics, a calibration
target and the 3-DOF positioning system. Table I provides
details of the configuration shown.
Calibration providesthe complete specification of system ge-
ometry necessary for converting cell time-stamp data into range
images. Two sets of calibration parameters must be measured.
First, 3-D sensor chip geometry and optical parameters must
be measured - the imager model. Next, a mapping between
time-stamp values 8g and distance r for all sensing elements is
developed - the stripe model.
This method measures component model geometry using
reference objects, manipulated in the sensor's field of view with
an accurate 3-DOF (degree of freedom)positioningdevice. The
following two-step procedure is used (Fig. 3):
A planer target out of which a triangular hole has been cut as
shown in Fig.9 is used to map out sensing element line-of-sight
rays. The target is mounted on the positioner so that its surface
is parallel to the world-ay plane.
A single 3-D point on the line-of-sight of a particular sensing
element is found as follows. The target is moved to some z-
position in world coordinates and held. The bottom edge of the
triangular hole is located by moving the target around in z and
y as indicated in Fig. 9. When a small motion in either z or y
causes a large change in the time-stamp value reported by the
cell, occlusion of the line-of-sight at an edge of the triangular
cut is indicated.
Once many points along the bottom edge are located, a line,
known to lie in the plane of the target, is fit. The location of
the top edge is found in a similar fashion. The intersection of
the top and bottom edge lines define one 3-D point that lies on
the cell's line-of-sight. A number of these points are located
by moving the target in z and repeating the process. The line-
of-sight for a single cell can then be identified by fitting a 3-D
line to these points. Experimental data from the calibration of
one sensing element's line-of-sight is shown in Fig. 10.
Mapping the line-of-sight rays for all 896 sensing elements
in this manner is too time consuming. In practice, line-of-sight
information is measured for 25 cells, evenly spaced in a 5 grid.
The geometry of the remaining cells is approximated using a
pinhole-camera model.
The pinhole-camera model[ 11] constrains all sensing ele-
ment line-of-sight rays to pass through a single point focus of
expansion at the optical center of the camera. Fig. 1l graph-
ically illustrates the process. Sensing element locations are
assumed to lie in some sensor plane, at locations evenly spaced
in a 2-D grid on the plane. Eleven model parameters must be
determined that identify the transformation matrix Tgw and the
geometry of the the sensor plane. A least-squares procedure
is used to fit pinhole-model parameters to line-of-sight infor-
mation measured in the first calibration step. Imager model
geometry is now fully calibrated.
Unfortunately, calibration of the imager model via line-of-
sight measurement is not suitable for use outside of the labo-
ratory environment. ''One-at-a-time'' measurement of sensing
element geometry, as outlined above, is slow and cumbersome.
We are developing a faster, more precise method for imager
model calibration. In this new calibration method, the 3-DOF
positioning system is replaced with a liquid crystal display
(LCD) mask that need only be accurately positioned along one
degree of freedom. The LCD mask is used to define precise
black-and-white images that are ''seen'' by the range sensor.
The method relies on intensity image information, measuring
geometry through analysis of reference object images[9].
The LCD mask is placed between a diffuse planer target
and sensor chip at a known position and is backlit by shining
the system stripe source on the planer target. The pattern
displayed on the LCD forms a black-and-white image on the
sensor. Only illuminated sensing elements will latch the stripe-
detected condition (Section III-B). A single-bit intensity image
is derived by identifying the time-stamp output of illuminated
sensing elements.
Sensing element line-of-sight geometry is found by varying
the LCD mask pattern in a controlled fashion. For example, a
circular pattern, whose 3-D center is known, can be projected.
A calibration point is found by measuring the 2-D location of
this circle's center in the intensity image returned by sensor.
Additional calibration data is measured by varying the position
of the circle on the LCD mask and the position of the LCD
along ss. Also, by measuring the center different radii of the
circle at a fixed position, we can compensate for the low spatial
resolution of the current sensor. The new sensor chip design,
discussed in Section VII, returns multi-bit intensity image data
which further assists imager geometry calibration.
Use of the LCD mask significantly reduces the time required
to perform imager-model calibration. In the previous method,
two edges of a triangular hole had to be mapped out, viaaccurate
back-and-forth movement, in order to yield a single calibration
point. In the new method, one calibration point is measured
from a single LCD-generated pattern without mechanical X .Y
movement. Precise calibration of the low-spatial resolution
range sensor is possible because high-precision patterns are
generated by the LCD mask.
The use of an LCD mask to project precise 2-D patterns
has application beyond the calibration of our light-stripe range
sensor. For example, this technique could be used to assist
more traditional camera calibration procedures or to present
training data to image-based neural net systems. LCD displays
have several advantages over CRT displays for applications
like these - they are fast, they are static (not refreshed), and
they form images which are stable and well defined.
The second part of the calibration procedure determines the
mapping between time-stamp data and range along all sensing
element line-of-sightrays. As shown in Fig. 12, a planer target
with no hole replaces the target used in step one. The new
target is held at a known world-z position, parallel to the zy
plane, and time-stamp readings f8g from all sensors are recorded.
This process is repeated for many z positions. Using this
information, the function which maps cell time-stamp values
8; into line-of-sight distance 7 for each sensing element is
approximated by fitting a parabola to each. Experimental data,
showing the fitted r verses 8; functions for several sensing
elements, is shown in Fig. 13. Calibration of the cell-parallel
range sensor is now complete.
The quality of the range data produced by the cell-parallel
range sensor was measured by holding a planer target at a
known world-z position with the 3-DOF positioning device. In
the experimental setup, the world-z axis heads almost directly
toward the sensor with the zw s 0 pointroughly 500 mm away.
Analog time-stamp values from the sensor array were digitized,
using a 12-bit analog-to-digital converter (A/D), and recorded
for 1, 000 trials. Light-stripe sweep (acquisition phase) time
for each scan was 3 msec.
A histogram of the range data reported by one cell is plotted
in Fig. 14. The horizontal axis represents the digitized time-
stamp value, converted to world-z distance via the calibration
model. Data for six world-z positions are combined in this
plot. The vertical axis shows the number of times (plotted
logarithmically), out of the 1, 000 trials, that the sensing ele-
ment reported that world-z distance. The sharpness of each
peak is an indication of the stability (repeatability) of the range
measurements.
Averaged statistical data for 25 evenly-spaced sensing ele-
ments is plotted in Fig. 15. In order to measure accuracy and
repeatability, the position of the target, as reported by the cell-
parallel sensor, is compared to the actual target z position. The
'boxed'' points in the plot represent the mean absolute error,
expressed as a fraction of the world-z position and averaged
for the 25 elements at sw. One standard deviation of ''spread'',
also normalized with zw, is shown ($) above and below each
box.
The experiments show the mean measured range value to be
within 0.5 mm at the maximum 500 mm z -- an accuracy of
O.1%. The aggregate distance discrepancy between world and
measured range values remains less than (0.5 mm over the entire
360 mm to 500 mm z range. The cell-parallel sensor repeatabil-
ity is found by computing the standard deviation of the distance
measurements. The measured repeatability of histogram data
is less than 0.5 mm - 0.1% at the maximum 500 mm posi-
tioner translation. The 0.5 mm repeatability decreases with
the distance to the sensor - essentially with the slope of the
time-stamp to distance mapping function (Fig. 13).
Fig. 16 shows a wire-frame representation of one 28 s 32
range image produced by the sensor. The imaged object is the
cup shown in the figure, approximately 80 mm in diameter at its
opening and 80 mm high. The range sensor islookingdirectly at
the object from a distance of 500 mm. The viewpointof the plot
is at a point directly above the optical center of the sensor. The
complete range image was acquired during a 3 msec stripe scan.
The intersection points of the wire-frame plot are positioned on
cell line-of-sight rays at the measured distance along the ray
and the focus of expansion is located in front of the cup. Thus,
the smaller ''squares'' represent object surface patches closer
to the sensor. This is opposite the manner in which straight
perspective would make an object with a grid painted on it
appear, and at first glance gives the false impression that the
''mold'' used to make the cup has been imaged.
The curved smooth front surface of the object is clearly
visible in the range data. The 20 mm handle of the cup is
readily distinguished, as is the planer background behind the
cup. The curved surface of the object halfway down the cup
directly across from the bottom of its handle includes a slight
shift of the wire-frame. The imaged cup is slightly narrower at
its base by about 2 mm. The cell-parallel sensor is measuring
this small 3-D feature at the 500 mm object distance.
A summary of the cell-parallel sensor system performance
is given in Table II.
A second-generation implementation of the light-stripe sen-
sor array has been fabricated. This new chip, seen in Fig. 17,
incorporates several advantages over the first design. The die
area of the new cell, shown in Fig. 18, is 216um x 216um,
40% smaller than that of the cells of the first-generation sensor
(photoreceptor area has been kept constant). Stripe detection is
done in a more robust manner and range data read-out circuitry
has been simplified. In addition, the new cell provides a means
to record and read out the value of the peak intensity seen when
it acquires a range data sample. The peak intensity informa-
tion provides a direct measure of scene reflectance because
stripe output power is known and distance to the object point is
measured. In addition, the availability of intensity information
allows for efficient sensor calibration (Section V-B).
Peak detection is done using the circuit of Fig. 19. Operation
of the circuit is straightforward. The source following transis-
tor 2, enables capacitor C; to track the rising intensity input
voltage transitions. No path is provided for C,, to discharge
when photoreceptor output transitions downward. At the end
of a scan, the largest intensity reading observed will be held.
Stripe detection is easily accomplished by comparing the peak-
intensity value V; with the amplified photodiode output V,.
When V, falls below the V;, the output from the comparator
is used to record a time-stamp value.
Using Spice[ 10], operation of of the second-generation sens-
ing element design was simulated. The simulation results are
plotted in Fig. 20. The output from the peak-following cir-
cuit ILSCELL .30 acts as a dynamic threshold for each cell,
replacing the externally applied global threshold of the first-
generation design (Section III-B). Comparator input offset
mismatch made setting a global threshold level, valid for all
cells in the array, difficult. Thus, stripe detection is made more
robust by this modification. In addition, the ''true'' peak de-
tection of the new design provides better quality range data
because the new stripe detection scheme identifies the location
of the peak in time more accurately than simple thresholding.
The peak-intensity value held within the second-generation
cell is an important artifact of the ranging process and, in the
new design, is provided as an additional sensing element out-
put. The illumination source in the system, the stripe, is of
known power. Intensity reduction from 1/r-type losses can be
accounted for because range to the object is measured. The
intensity value therefore provides a direct measure of scene
reflectance properties at the stripe wavelength. It is an image
aligned perfectly with range readings from the cell array.
The area in each cell dedicated to time-stamp read out is
much smaller in the new design. Direct addressing of the cell
to be read, using row and column selects, eliminates the token
state necessary in the first-generation design. The N x M array
is read using N row select lines and M column select lines.
A given cell is enabled for read out by asserting the row and
column select lines that correspond to the location of the cell
in the array. The two-level bus hierarchy has been maintained,
however, to keep bus loading at a minimum. The area savings
of the new read selection method has made cell area of the
second-generation design smaller despite the additional peak
detection circuitry.
We have presented the design and construction of a very
high-performance range-imaging sensor. This sensor acquires
a complete 28 x 32 range-data frame in a few milliseconds. Its
range accuracy and repeatability were measured to be less than
O.5 mm on average at half-meter distances. The success of this
implementation can be attributed to the use of a VLSI smart
sensor methodology that allowed a practical implementation of
the cell-parallel technique.
While the advantages of processing at the pointsensing have
been advocated by many, few practical smart-sensor imple-
mentations have been demonstrated. The cell-parallel range
imager presented here bridges the gap between smart sensor
theory and practice, demonstrating the impact that the smart
sensor methodology can have on robotic perception systems,
like automated inspection and assembly tasks.
Smart VLSI-based sensors, like the high-speed range im-
age sensor presented here, will be key components in future
industrial applications of sensor-based robotics.
EEasing the dichotomy of two-level stores is currently
the most challenging problem in persistent object
stores, since transferring and translating data be-
tween the in-memory and the backing store degrades
the overall performance of persistent applications.
One way whereby we can ease this problem is clus-
tering. Clustering algorithms attempt to store co-
related objects within the same or in a neighbouring
unit (of transfer) on the backing store and thus re-
duce the number of accesses when such objects are
retrieved.
In recent years the problem of optimal clustering
has attracted a lot of attention. Most of the new
clustering algorithms are aimed at supporting object-
oriented database systems [Ben90] [Cha89]. Bow-
ever, the emergence of new complex data models and
database systems such as hypertext [Nie90] present
new requirements from the persistent object store.
In particular, the main requirement is to support a
dynamic graph object space. The underlying persis-
tent object store, in a hypertext system for example,
is represented by a graph whose nodes represent ob-
jects and links (i.e edges) between the nodes repre-
sent references between objects. This graph is highly
dynamic, is. nodes (objects) and edges (references)
may be added, deleted and updated arbitrarily and
frequently. Clustering of such a dynamic graph is
a complicated task. Moreover, the access patterns to
the underlying persistent object store can be changed
frorm one application to another.
Our main goal in searching for an optimal clustering
algorithm is to support a dynarmic persistent object
store environment such as a hypertext database en-
vironment. In [Tuv92] we present the main require-
ments for such a store to support hypertext. Naviga-
tion is no longer the only access to the said store but
it is augmented with support for associative queries
and free text retrieval operations. Thus the access
patterns to the underlying persistent object store are
not based solely on the inter-structure of the persis-
tent object store (i.e. a digraph object space), but
also on the content of the objects.
Existing clustering algorithms use different types of
information as input for finding the clusters in persis-
tent object stores. Some algorithms, such as [Hor87],
use a static strategy whereby objects of the same type
are grouped together in the same cluster. In [Ban88]
the algorithm relies upon the directed acyclic graph
structure of the underlying object store and on the
expected traversal operations on it. More seman-
tic information is provided in clustering algorithms
which aim at supporting object-oriented databases
such as [Cha89] [Ben90]. These algorithms exploit
the knowledge of the class hierarchy of the database
and the operations performed on these classes. The
semantic clustering [Sha90] proposes to use knowl-
edge of the programmer in the clustering algorithm.
ObServer [Hor87] supports dynamic clustering by us-
ing the actual values of the objects. In this algorithm
an object remains in its original cluster as long as its
content satisfies the cluster's specification. In Cac-
tis [Dre90] statistics are kept which count how many
times each relationship between two objects is tra-
versed. Frequently traversed objects are candidates
to be clustered together. The relationships between
objects is also the input in [Tsa91] and [Ker70] but
these algorithms ignore the way this information is
gathered and reclustering is required as a result of
changes in the relationships between the objects in
the underlying persistent object store. None of the
above-mentioned algorithms provides for incremen-
tal learning of the clustering algorithm based on the
actual usage of the persistent object store.
Herein we present a new approach to clustering in
the context of dynarmic changes in the access pat-
terns to the persistent object store. In particular, we
concentrate on incremental learning of the clustering
algorithm that allows us to capture the dynamic as-
pects of the persistent object store, and on learning
from the actual usage of the persistent object store,
i.e, the actual access patterns to the persistent object
store, This approach provides the support to employ
dynamic clustering whereby the optimal persistent
object placement is decided at the time the object is
to be transferred from the in-merrmory to the backing
store.
The rest of the paper is organized as follows.
In Section 2 we present an overview of the Self.
Organising Feature Map (SOFM) algorithm and dis-
cuss the motivation behind the use of the SOFM for
optimal clustering in a persistent bject store. In
Section 3 we show how SOFM can be applied to the
object placement problem in a persistent object store.
Experimental results of Kohonen's SOFM are given
in Section 4. Finally, in Section 5, we conclude the
paper and discuss directions for further research.
A typical topology of the Kohonen network is
presented in Figure 1. The architecture of
the network consists of two layers, The in-
put layer is a one-dimensional array of N nodes,
and the output layer is a two-dimensional array
of M output nodes which are used to form fea-
ture maps. Every input node is fully connected
to every output node via a variable connection
weight.
The basic idea behind the SOFM learning is as fol-
lows : Assume an input data set X, whereby each
a E K is a vector of N dimensionality, R, and
assume a set of connections [mr : m: E R',i s
1,2,.-.,M} between the input layer and the output
layer whereby each connection vector m; is associated
with the node iin the output layer. Let d(,mc) be
some distance measurement between z and m;, such
as the Euclidean distance, and let N, be the neigh-
bourhood of the output node c, which consists of the
node c together with a set of nodes around the node
SOFM is an iterative algorithm whereby each iter-
ation is a two-step process. In the first step, which
is called selection of the best matching output node,
the algorithm compares a new selected input vector
a E X7 with each m, and finds the best matching
node c such that d(z,rm.) S d(, mQ,i= 1,2,...,M.
In the second step, which is called learning se,
the connections m; which are associated with the
nodes in the neighbourhood N, are updated such
that d(z,rm4), i N,, is decreased, i.e. the up-
dates increase the matching between the nodes in the
neighbourhood N, toward the input vector z, At the
end of this algorithm the output nodes are organized
into neighbourhoods (i.e clusters) which act as fea-
ture classifiers on the input data set.
As discussed earlier in the paper our aim is to find
an algorithm which identifies clusters in a dynamic
persistent object store such as a hypertext database
system. The algorithm must employ incremental
learning where the computational cost is minimized,
and which takes into account the actual usage of the
persistent object store. We decided to use Kohonen's
SOFM whose properties we believe satisfy the above
requirements.
Herein, we enumerate some of the main properties of
Kohonen's SOFM and the way in which they support
the clustering problem.
We now describe the way by which we model the
clustering problem in a persistent object store envi-
ronment and how Kohonen's SOFM algorithm can be
applied to it. We assume that each persistent object
in the persistent object store is associated with an
invariant unique identity, called object identity. The
object identity is the means by which objects refer to
each other and allows sharing of objects. In [Tuv92]
we realize the concept of object identity by using log-
ical identities rather than physical addresses in order
to allow mobility of objects within the store; this al-
lows clustering of any set of objects.
As mentioned earlier, the underlying persistent ob-
ject store can be modelled as a graph. Based on this
graph, many researchers, model the clustering prob-
lem as a graph partitioning problem. The advan-
tage of this approach is that one can use existing
algorithms, which solve the graph partitioning prob-
lem, such as the heuristic algorithm of Kernighan-
Lin [Ker70] and optimisation algorithm using sim-
ulated annealing [Joh89]. However, in an example
given later in the paper, we show that it is more accu-
rate to view the persistent object store together with
the accesses to that store as a hypergraph, since the
accesses to the persistent object store are not based
solely on the structure of the said graph but also can
be set-oriented, whereby a set of retrieved objects do
not necessarily follow the said graph structure.
Before we give a formal description of the cluster-
ing problem we first introduce the concepts of feature
and co-reference with regard to the persistent object
store,
A feature is any concept which is used for learning
by the clustering algorithm. A persistent object is a
feature. However, a feature can be any other concept
such as a type in a type system or even special values
or terms which are expected to be important with
regard to clustering organization.
A co-reference is a a set of features. Mostly co-
references represent actual answers to queries posed
to the persistent object store but they can also rep-
resent answers to expected queries. An example of
co-reference can be the query to retrieve a father and
his children or even retrieval of the whole family.
We next model the problem of clustering in a per-
sistent object store as the problem of node parti-
tioning in a hypergraph. Let P (F,V, Q,I) be
a weighted hypergraph, where F is the set of nodes
whereby each node corresponds to a different feature
over P, Vis the set of all persistent objects in P with
t g F, Q is the set of hyperedges of P, where each
g 6 2 is a non-empty subset of F (i.e. g F)
and each hyperedge corresponds to a co-reference (or
access) over P, and I is an infinite set of identities,
Each feature (i.e. node) is associated with an invari-
ant unique id i E I, which is the means by which
features can be referenced, and each hyperedge, (4
is associated with a weight, denoted by uu;, which
counts the number of times that the corresponding
co-reference, i, is the answer to queries which have
been posed to P.
For a given P we then define the node partitioning
as M non-empty disjoints subsets of V, V,.-., Yr)
(ie, V, n V, = 6, i w j), and which cover the set V
(i,e, V = UD,; V). We further define the cost of a
co-reference 4i bY 4(40 = wc3C[,;M4, where h i 1
if ;; n Vj y 6, and 0 otherwise.
For a given P and a positive integer K( ]V], in the
optimal clustering we are looking for a node parti-
tioning of P whereby ]V,] gK, ( = 1,..,M, and such
that it minimizes the total cost of all co-references in
2,e. mnme 4)= 52750
Kohonen's SOFM algorithm applied to the above
problem now follows :
We define O to be a special set of answers to queries
which are used for the classification of objects in a
given organized feature map. In each o' E O only
one object (i,e feature) j is '1' (i,e o[ = 0 V y
j, and o} = 1). Each feature corresponds to one
o' @ O. At the end of the learning process we apply
the set of access patterns O in order to classify the
different objects into their clusters. Moreover, after
this learning session, the neural network can continue
learning from new incoming queries by applying steps
3,4 and 5 in the above algorithm. This provides for
incremental learning of the clustering algorithm in a
persistent object store.
The following is an example which illustrates the
representation of the clustering problem as a hyper-
graph and which shows that the hypergraph repre-
sentation of the clustering problem is more accurate
than the graph representation of the said problem.
Let P be a persistent object store consisting of six
objects labelled from 1 to 6, and let Q be a set of
seven different accesses to these objects which are :
where each tuple is an answer to a query and the num-
bers represent the retrieved objects. The set Q in Fig-
ure 2a in its vectorial representation for the use in Ko-
honen's soFM is : '(a) = (1,1,1,0,0,0), ;(b) =
(1.9,9,1,0,o), (e) = (0,1,o,0,1,o), *(4) =
(0,4,1,0,0,1)i *() = (,4,o,1,1,o), Pe(/) =
(0,4,0,0,1,1)i and'(0) = (0,0,0,1,0,1). Figure
2a depicts the hypergraph representation of the above
persistent object store where the nodes correspond to
the objects and the hyperedges correspond to the dif-
ferent accesses, and Figure 2b depicts the graph rep-
resentation of the said persistent object store, where
the access (1,2,3) is replaced by the three edges (1,2),
(1,3) and (2,3). In an environment with the con-
straint that a cluster can contain 3 objects (nodes)
at most (i,e K = 3), it is easy to figure out that the
best partitioning is (1,2,3) and (4,5,6) which is the
solution for the hypergraph partitioning with Ke3.
However, the graph partitioning fails to give the best
clustering, since the graph in Figure 2b is cubic, which
implies that any partitioning can be applied.
Since there are no known benchmarks for node parti-
tioning of large and dynamic hypergraphs, we use the
following example in order to illustrate the use of our
algorithm. Figure 3 depicts a hypergraph represen-
tation of part of the London underground travel map
consisting of 25 nodes, labelled from 1 to 25, each of
which corresponds to a different train station, and 9
hyperedges, labelled from 1 to 9, which correspond
to the different train lines. Table 1 presents the hy-
peredges of the said hypergraph together with their
labels, their corresponding train line names and fi-
nally with their incident node sets.
In the said hypergraph we are looking for a node
partitioning subject to the constraint that the maxi-
mum number of nodes in each partition is 5, i.e. Kz:5. ,
We experimented different initial configurations in or-
der to ensure a stable solution for the required clus-
tering. The total cost of all given hyperedges (i.e.
co-references), c(Q), is used as an indicator for the
efficiency of any resulting clustering. In the above
problem we are looking for a solution with c(Q) g 24.
Since the cardinality of the node set and the hyper-
edge set of the said hypergraph are both small, we
use a small network topology for our experiments,
e.g. the output layer consists of a linear array of 10
nodes,
As mentioned earlier, after the learning phase we next
employ a classification phase based upon the mea-
surement of the distance between each of the features
to the output layer of the trained network and orga-
nising the features in an ordered list. The resulting
features ordered list is then used for partitioning the
25 features into 5 disjoint partitions, whereby the first
5 features are assigned to the first partition followed
by the next 5 features assigned to the second parti-
tion, and so on.
Table 2 presents the resulting clustering of an ex-
periment with the following initial configuration :
= 0.1, r = 5, and the number of iterations is 8100.
The goals of this paper were two-fold; firstly, we
showed the potential of using neural network learning
paradigms such as Kohonen's SOFM in a database
systerm and, secondly, we presented a new approach
for incremental learning of clustering in a persistent
object store that captures the dynamic aspects of the
underlying persistent object store.
The aim of the presented algorithm is to identify
classes (i,e. clusters) of objects from a set of objects
based on the access patterns to these objects. Fur-
thermore, our algorithm can use in its initial stage a
priori knowledge on the underlying persistent object
store, such as the database schema or inter-relations
between types of objects, in order to employ more
effective clustering.
In our model of the clustering problem we focused
on the dynamic aspects of the access patterns to the
persistent object store and ignored many other im-
portant factors, such as the size of the objects, which
may affect the efficiency of the resulting clustering.
However, at the final stage of our algorithrm, after the
clusters are defined and found, one can employ any
known technique, such as the heuristic graph parti-
tioning of Kernighan-Lin [Ker70], for further refine-
ment of a given cluster.
A promising direction in solving general combina-
torial optimization problems such as graph partition-
ing and, in particular, in solving the optimal cluster-
ing problem is to combine the neural network model
with a simulated annealing algorithm. For example,
in Kohonen's SOFM we can modify the best match-
ing step by using an energy function (cost function),
which takes into account arguments such as the num-
ber of elements in the different clusters, rather than
just use the Euclidean distance measurement.
Finally, we suggest the use of a neural network
model to improve performance in a database system.
Clustering is only one aspect of a wide range of po-
tential problems that the above model can solve. For
example, an efficient algorithm for replication of ob-
jects in a persistent object store can use an overlap-
ping clustering algorithm similar to our proposed al-
gorithm. Moreover, the knowledge of the underlying
clusters in the persistent object store is important in
order to employ efficient buffering policies, such as
prefetching of objects as well as clever indexing tech-
niques.
The authors wish to express their appreciation to the
referees for their useful comments
For the past three decades, there has been a
mounting interest among researchers in the
problems related to machine simulation of
human reading. Character recognition has
attracted an immense research interest not only
because of the very challenging nature of the
problem, but also because it provides a means
for automatic processing of large volumes of
data such as postal codes [1], office automa-
tion [2, 3], and other business and scientific
applications [4, 5].
The different areas covered under the
general term character recognition fall into
eitther On-line or Off-line categories, each
having its own hardware and recognition
algorithms. In On-line character recognition,
the computer recognizes the symbols as they
are drawn [6, 7, 8]. The most common writing
surface is the digitizing tablet, which typically
has a resolution of 200 points per inch and a
sampling rate of 100 points per second.
Off-line recognition, is performed after the
writing or printed is completed. Optical
Character Recognition (OCR) deals with the
recognition of optically processed character
rather than magnetically processed ones. In a
typical OCR system, input characters are read
and digitized by an optical scanner. Each
character is then located, segmented and
resulting matrix is fed into a preprocesssor for
smoothing, noise reduction, and size
normalization [9, 10, 11, 12, 13].
This paper proposes a new structural
technique for the recognition of printed Latin
text. The technique can be divided into five
major steps. First, is the digitization step, in
which the original image is transformed into a
binary image utilizing a 300 dpi scanner.
Second, is a preprocessing step in which the
thinning algorithm. Third, the skeleton of the
binary image is thinned using a parallel image
is traced and a binary tree is constructed.
Features are extracted from the binary tree as
pattern primitives, such as straight lines,
curves, and others. A partitioned dictionary is
then used to identify the character, and merged
characters are segmented. Figure 1 depicts a
block diagram of this system.
Extensive work has been conducted on the
digitization or binarization process [ 14, 15].
The binarization algorithm employed in this
work, however, is similar to the method
appearing in [16]. A 300 dpi scanner is used to
digitize the image using an 8-bit grey scale.
The majority of printed text is approximately 8
point size and this gives an image frame of
between 30 and 35 pixels in hight, a
satisfactory resolution for this system.
Individual pixels are then converted to a binary
value by a thresholding technique.
OId documents, carbon copies, and text
printed on recycled paper generally suffer grey
scale shading problems and a more complex
approach can be employed which adjusts the
threshold level according to the background
white level of the image [17], both for the
entire document and localised areas.
The preprocessing step involves operations on
the digitized image that are intended to reduce
noise and increase the ease of extracting
structural features. This involves cleaning the
image, extraction of individual character
images, and thinning of these images.
This is a reduction in the noise that the process
of binarization produces. The sharp edge
which the human eye sees between the black
printed letter and white page usually translates
upon scanning into a blurred grey scale edge of
several pixels wide. Thresholding can then
create a ragged edge in this region,
contributing to noise in the image.
Each pixel, P, has eight neighbor pixels
which are numbered P0 to P7 in a clockwise
fashion starting from the north neighbor. The
sum of the eight pixels surrounding P is
termed B(P) and is defined below.
To lessen the effects of this noise, any
black pixel P witth B(P) c 4 is whitened and a
white pixel P with B(P) > 6 is blackened.
There is a large volume of literature on
thinning algorithms of both the sequential and
parallel nature. Parallel approaches allow a
thinning of the entire image simultaneously
while sequential algorithms use a process on
the pixels which are dependent on previous
operations. The algorithm implemented in this
system is a parallel method based on the Guo
and Hall [18] A1 method. Figure 2 shows an
original scanned image and the resulting
skeleton following application of the thinning
algorithm.
Figure 3 gives an example of some
problems that occur during thinning. Due to
the nature of the thinning algorithm, lline
segments of typically short length can
sometimes be formed which do not contribute
to the structure of the character (Figure 3a.).
Without knowledge of the overall topology of
the character, it is difficult to ascertain
whether these 'hairs' are important to the
structure of the image. Figure 3.b shows some
of the difficulties which occur with serif fonts
following the thinning process.
The aim of this step is to extract the individual
character images from the document image.
The algorithm involves a separation of an
entire printed line by horizontally scanning for
bounding white lines. Individual character
images are then extracted by similar vertical
projection. The problem of merged characters
is dealt with in a later step.
This step builds, from the pre-processed
image, a binary tree which contains
information describing the structure of the
image.
An algorithm implementing a 3x3 window is
used to trace along the path of the skeleton,
recording the structural information of the
traced path. A path is described as a
tracingbetween junction or end points, where
an end point has a single neighbor and a
junction point 3 or more neighbors (and
multiple paths exist).(Figure 4)
This path is stored in a node of the binary
tree; where a choice of paths to trace exists, a
left and right node are formed beneath the
current one and their respective paths traced
out. A priority system is used which favours
certain directions over others (without this, the
window would trace the skeleton in random
directions). It is obtained by experience and
experimental results.
The starting point for tracing the skeleton
is based on several criteria. The image is
divided into 3 horizontal regions and the top
and bottom region are searched for end points
or junction points. This ensures that the
starting point does not split a path into two
subpaths. If no such points are found, such as
with the letter o', then the left most pixel of tthe
entire image is selected as starting point.
This path is stored in a node of the binary
tree; where a choice of paths to trace exists, a
left and right node are formed beneath the
current one and their respective paths traced
Out.
The structural information saved for each path
traced is the following:
Additional information stored concerning the
entire character image includes:
The completed tracing results in the
segmentation of the character into paths or
strokes which will latter be formed into
primitives. For the thinned image given earlier,
experimental results produced the binary tree
in figure 7.
Upon completion of the binary tree, a
smoothing step allows redundancies and noise
to be removed from the tree. The smoothing of
the binary tree is designed to minimize the
number of nodes in the tree and minimize the
Freeman code chain. Loops whose paths
contain multiple nodes are identified and then
compressed to single node. Redundancies in
the Freeman chain code are smoothed and
noise is reduced. At points of change in the
Freeman code, a vector averaging algorithm
produces results as shown in figure 7. The
result is a minimal smoothing function which
produces segment-like code with a minimum
unit length of 3 and eliminates paths of length
The structural information in the binary tree
allows the formation of pattern primitives, or
subpatterns, which are used to describe the
original image. There are two main primitives
described in this system: straight lines and
curves. A path may be described by a single
primitive or by multiple primitives. Thhe
structural information in the tree is converted
to these primitives using the following
definitions.
Breakpoint (separator): divides a path into
subpaths more easily described by primitives.
A breakpoint has at least one of two possible
conditions:
Straight Line: has its usual geometric
definition as two points in sequence within a
path. A point in a freeman chain can be
defined as a change in the freeman code. Lines
can be distinguished from curves in two ways:
the length of a line segment is significant in
comparison to the length of the path, or the
path contains only two points.
Figure 8c illustrates the sectioning of a line
drawing into its primitives.
The recognition phase involves the use of a
partitioned dictionary to obtain a match
between the image tree, which describes an
unknown symbol, and a specific Latin
character.
The dictionary consists of three main partitions
which allow easier classification of the
characters; those characters which contain a
loop(s), curve(s), and those with straight line
segments only. These partitions are further
sub-divided into sub-partitions depending on
particular structural features. The lowest
partition consists of a a linked list of word
trees which conform to the partition features
where each word tree contains a structural
description of a single character in the Latin
alphabet. The word tree is constructed in a
similar fashion to the tree built to represent the
unknown character. Each node, hence,
contains a primitive pattern along with other
structural information. The links in the tree
indicates connectivity of primitives. Once the
image tree (the description of the unidentified
character) has been classified into a partition,
a match is achieved by a comparison of each
word tree with the image tree.
The matching occurs in this manner. A
traversal of the image tree is performed,
searching for a match to the structural
information contained in head node of the word
tree. Figure 10 shows an example of a match
for the unidentified tree describing the letter
'A'.
For the head node only, primitive line
segments may be matched to line segments of
opposite direction (e.g. ER may be matched
witth W4). If a match is found, the equivalent
node in the image tree becomes the temporary
head of that tree.
A preorder traversal is then performed on
the word tree. Each node encountered in the
traversal of the word tree must equate to a
node in the unidentified tree; the match
includes primitives, certain structural
information, as well as connectivity, but with
the following re-definitions to the image tree.
The example image tree would be
restructured according to the above rules as in
tigure 1.
Every node in the word tree must have an
equivalent in the imagetree in order for a
recognition to occur. Extra nodes left
unmatched in the image tree indicate that the
recognized letter is a sub-pattern of the image
tree. This indicates several possibilities:
The segmentation of merged or touching
characters remains a difficult process in any
OCR systems. The technique used for
recognizing characters in this paper simplifies
the problem considerably. Merged characters
which are singly connected and whose contact
point occurs at natural junction points of the
letters will be separated and recognized
according to rule 2 in the above section
without any further processing. An example
would be the two letters 'ar', connected by the
bottom serifs. Letters whose contact point does
not occur at natural junction points creates a
splitting of primitives in one or both
characters. The characters 'lu' merged at the
left most serif of 'u' would split the description
of letter 'l into two short line segments instead
of a single long one. To overcome this
problem, equivalence relationships can be used
in the traversal of the word tree (e.g. two
connected short line segments of the same
direction are equivalent to a single long
segment).
Several other more difficult problems need
to be dealt with. Multiply-connected characters
(two or more contact points) will create an
incorrect loop primitive which must be
segmented. Merged characters whose image
forms a new character are also difficult to
counter. The merging of 'rn', for instance,
cannot be separated on lower level information
alone.
The advantage of the proposed
segmentation method lies in the fact that
characters are first identified before being split
from the merged image. Many current
techniques rely on statistical methods which
indicate several likely segmentation points
based upon selected topological information.
The image is segmented, an attempt is made to
recognize the image, and upon failure requires
re-segmentation. Such methods are
computationally expensive because they are
essentially blind to the entire topological
information of the image. By recognizing a
character before segmentation, it is ensured
that the segmentation point is in fact correct.
This paper presented a combined structural
and classification approach for the recognition
of multi-font Latin texts. The intuitive idea
behind this technique is to extract certain
features which are purely structural such as
curves, straight lines, etc. in a manner similar
to that which human beings describe
characters geometrically and recognize them
based on a classification which uses partitions
in a dictionary. In addition, this approach
adopted is rapid for feature extraction,
recognition, and segmentation.
Results to date with a small set of
characters in different fonts have teen
encouraging. More extensive testing and
results will be presented during the conference.
All algorithms in this paper were written in
C++ and run on an IBM pc.
Throughout the information processing
industry there is a need for rapid, effective
access to information in databases containing
text, formatted data, graphics, and other mul-
timedia (e.g., engineering drawings, sound,
and video). Furthermore, as conversion of
large volumes of paper documents and techni-
cal manuals into electronic raster format
accelerates, the need for fast, accurate index-
ing of optically stored raster images becomes
increasingly important. Typically such files
are indexed manually at great expense, while
search methods are crude. Improved retrieval
methods are needed. The Automated
Document I Image Indexing and Retrieval
(ADIIR) research project seeks to address
these needs with: new techniques for captur-
ing and indexing images; new algorithms for
effective retrieval from large text and image
databases; and intelligent interface design,
including integrated text and image retrieval.
In this paper we give a short description of the
project as a whole with an emphasis on the
document image analysis portion and a brief
account of our performance at the Text
Retrieval Conference (TREC).
The first long-term objective of the ADIIR
project is to develop effective, computation-
ally implemented retrieval algorithms that can
efficiently scale up for very large text and
image retrieval systems. The second is to
develop an automatic indexing engine that
will capture key elements of a scanned docu-
ment based on document class and common
zone patterns, eliminating costly manual
indexing. The third is to develop indexing
and retrieval algorithms that can take advan-
tage of structural information contained in
documents, c.g., those formatted in the
Standard Generalized Markup Language
(SGML).
In support of these objectives we are
developing a retrieval system/testbed incor-
porattng;
The ideal text retrieval system retrieves doc-
uments based on an understanding of the
meaning of the query and the document. Due
to ambiguity in the use of words and concepts,
keyword retrieval falls far short of such an
ideal [1, 2]. Furthermore, scanned documents,
or those in SGML format, provide structural
and layout information of which a retrieval
system should take advantage. All such
Insert Fig. 1]
The first stage of this research has
involved: a) analytical work developing and
implementing the individual retrieval algo-
rithms and the methodology for combining
them in an overall ranking algorithm; b)
acquisition of large test collections of paper,
image, SGML-tagged, and ASCII documents;
c) evaluation and acquisition of commercial
products for scanning/OCR and SGMIL pars-
ing, and of university research prototypes for
document retrieval [5, 6]. The second stage,
presently underway, is to build the proto-
information, however, provides at best clues
to document relevance. Thus, a probabilistic
approach is warranted.
Figure 1 shows the prototype ADIIR
retrieval system. The two arcs from the
incoming document stream indicate that some
documents are in ASCII format, others are
paper, requiring scanning and optical charac-
ter recognition (OCR). The analysis system
provides automatic indexing. Natural lan-
guage processing (NLP) is provided by an
NLP module created using PRC'4 NLP shell,
PAKTUS [3, 4]. The intent is to provide con-
cept-based indexing with broad, shallow,
domain-independent NLP without extensive
handcrafting. Analyzed documents are stored
in an object-oriented repository customized
for document retrieval, LEND [5]. LEND
supports various types of data including a
comprehensive computerized lexicon for
English derived from machine readable dic-
tionaries, factual domain knowledge, large
digital, hyper-linked, and the multimedia
archives from which retrieval takes place.
Finally, the retrieval system matches docu-
ment representations derived by the analysis
System to query representations derived from
the user.
typeltestbed system. These stages include the
following tasks.
Research has shown that different retrieval
models retrieve different sets, only slightly
overlapping, of more or less equally relevant
documents [7, 8]. Accordingly, a major thrust
of this project is to implement the
Combination of Expert Opinion (CEO)
methodology [9] for combining the results of
multiple probabilistic retrieval models into an
optimal ranking of documents. Each retrieval
model can be viewed as a retrieval expert.
The initial models examined will be the prob-
abilistic indexing model of Maron and Kuhns
[10] and the inverse document frequency
model !11, 12]
In the Bayesian formulation of the CEO
problem [ 13] a decision maker is interested in
some parameter or event; and he/she has a
prior, or initial, distribution or probability for
that parameter or event. The decision maker
revises the distribution upon consulting sev-
eral experts, each with his/her own distribu-
tion or probability for the parameter or event.
To effect this revision, the decision maker
must assess the relative expertise of the
experts and their interdependence, both with
each other and the decision maker. The
experts' distributions are considered as data by
the decision maker, which is used to update
the prior distribution.
For automatic document retrieval, the
retrieval system is the decision maker and
different retrieval algorithms, or models, are
the experts [9, 14]. This is referred to as the
upper level CEO. At the lower level the prob-
abilities of individual features, e.g., terms,
within a particular retrieval model can be
combined using CEO. In lower level CEO Uhne
retrieval model is the decision maker and the
term probabilities are viewed as lower level
experts. The probability distributions sup-
plied by these lower level experts can be
updated, according to Bayes theorem, by user
relevance judgments for retrieved documents.
These same relevance judgments also give the
system a way to evaluate the performance of
each model, both in the context of a single
search of several iterations and over all
searches to date. These results can be used in
a statistically sound way to weight the contri-
butions of the models in the combined proba-
bility distribution used to rank the retrieved
documents.
Current retrieval systems provide little sup-
port in query formulation. User models allow
a system to adapt to each user, enabling
mixed-initiative interaction [15]. In this pro-
ject relevance feedback algorithms [16] will
be investigated.
Fox, in collaboration with this research, is
investigating techniques for rapid processing
of queries against very large text databases
using minimal perfect hash functions [ 17].
These are especially important for accessing
data on optical storage, since they guarantee
collision-free hashing, thus reducing the num-
ber of seeks required on inherently slower
optical storage devices. These hashing tech-
niques are embedded in LEND. The integra-
tion of LEND witth ADIIR is planned later this
year.
Standard test collections are small compared
to today's terabyte databases. It is uncertain
how retrieval techniques will scale up.
Furthermore, these collections are not well
suited to testing iterative, feedback-based
retrieval, nor to image retrieval. We are
obtaining access to large document text and
image collections for our testbed. Through
participation in the Text Retrieval Conference
(TREC), we have acquired an approximately
half million document, 2 gigabyte corpus of
ASCII text [18].
As previously explained, documents enter the
ADIIR system as paper or as electronic files,
such as, SGML-tagged ASCII text or raster
image files. If a document enters the system
in paper format, the first step is to scan it into
raster format and store the image files into an
appropriate directory structure. Because each
page of the paper document is treated as a
single image file after the scanning process,
the directory structure should logically repre-
sent these page files as a document. The next
step is to index each document image.
Document image indexing is the ability to
segment, identify, and tag specific zones on
an image. These zones could be document
titles, abstracts, summaries, or any objects on
a page that require identification. After each
zone is located it is used to create a tagged
text representation of the document, through
the use of optical character recognition and
other techniques. At this point the paper doc-
ument will be transformed into a representa-
tion similar to an SGML-tagged ASCII docu-
ment. The algorithms to perform the pro-
cesses are currently the focus of much
research at various sites. The process currently
utilized to index an image requires manual
intervention at three points: 1) indices
creation; 2) indices to image link creation; 3)
OCR correction. The goal of automatic doc-
ument image indexing in ADIIR is to reduce
manual involvement as much as possible. The
remainder of this section will focus on several
subtasks: evaluation of common-off-the-shelf
(COTS) OCR products, automatic indexing
of images and ASCII text, establishment of an
expert thesaurus database, and
implementation of reference tables to improve
OCR accuracy.
The first subtask has been to evaluate optical
character recognition products based upon
specific criteria. The recognition engine has
to operate on a UNIX platform, be capable of
performing OCR on zones of an image, and
provide a robust application programmer's
interface. After literature review, product
demonstrations, and meetings with several
companies; the Calera Recognition Systems
product was selected. This engine operates on
many platforms, including the Sun SPARC,
and has a Sun developers kit for application
programmers. In addition, Calera performed
at the highest accuracy rate according to a
report on the accuracy of OCR devices, based
on a study conducted by the Information
Science Research Institute at the University of
Nevada, Las Vegas [19].
The second subtask is to automatically index
image documents and ASCII text. Most
recent work on task 5 has focused on this
subtask. For explanation purposes, this task is
broken down into 7 steps:
The PRC Digital Image Library (DIlL) is at the
heart of the automatic document image index-
ing process. It provides the ability to control
and manipulate image files without concern
for the physical attributes of image files or
with special imaging hardware. The library
file services provide application routines for
reading, writing, and manipulating a wide
range of image formats including: CALS
28002 Type 1 and 2, TIFF, PCX, DCX, patent
trademark (proprietary to the United States
Patent and Trademark Office) and encapsu-
lated postscript. The library conversion ser-
vices support conversion to and from CCITT
Group IV(G4), CCITT Group IV tiled (G4T),
Run Length List (RLL) and Binary Bit map.
The library also provides transformation ser-
vices such as: scaling and rotating, inversion
and mirror; copy, cut and paste; and drawing
operations. The ability to deskew, despeckle,
erase, fill, crop, and annotate are also part of
the library.
Our initial large collection of document
images comes from the United States Patent
and Trademark Office (USPTO). As the
contractor for management of the USPTO's
text and imaging system, the Automated
Patent System, the selection of a patent image
collection was sensible. Patent images consist
of large amounts of text, drawings and tables.
They have a cover page, listing key informa-
tion about the patent such as, title, abstract,
inventors, and filing date. The general layout
is dual column with numbers imbedded
between the columns to ease reading. They
can be provided in 150 dpi or 300 dpi, but file
formats are not standard. The image file con-
sists of Group IV compressed data, but with
no file header to identify dpi. The PRC
Digital Image Library supports the PTO file
format.
Once a document image has entered the
ADIIR system, by scanning or from an on-line
collection, a deskewing and despeckling
algorithm is initiated. The algorithm removes
noise and corrects the skew of the image. A
skewed image will cause incorrect zoning or a
failure in OCR. Zone segmentation begins
once despeckling and deskewing are com-
plete. This is a three step process, known as
X-Y Tree partitioning, beginning with vertical
segmentation, horizontal segmentation, and
finally resegmentation of identified zones
I201.
To begin the process each page of a doc-
ument is opened by the PRC Digital Image
Library and decompressed into a memory
buffer. A vertical pass (phase 1) of the image
is performed to mark the y coordinates of each
zone. These coordinates are determined by
ANDing each run length list (rll) line with a
rll mask. If several consecutive white lines
are found, and they fall outside a collection-
dependent threshold, then the y2 coordinate of
the previous zone is recorded as well as the yl
coordinate for the new zone. For example, in
a typical newspaper article, stories are sepa-
rated by several lines. If the lines that sepa-
rate the articles are larger then a defined
height, then the end of the previous article is
marked and the beginning of the new one
recorded.
Once the vertical pass of the image is
complete, the zones are processed through the
horizontal segmenter (phase 2). The process
is similar to the vertical pass, except the indi-
vidual zones identified in phase 1, are seg-
mented to determine the x coordinates. If a
gap larger then the horizontal threshold is
found, then the x2 coordinate of the zone is
recorded. Following the previous example,
once the vertical pass has identified the indi-
vidual articles in a column of newspaper text,
the horizontal pass will mark the width of
each column. During this process a zone may
be further segmented (phase 3) to reveal addi-
tional zones. This is analogous to scanning a
newspaper page from left to right and finding
two columns of text on a newspaper page
instead of one column. If a new zone is iden-
tified (column), it must reenter the vertical
segmentation step to find its height.
When this process is complete the x and y
zone coordinates for each page image are
placed in memory. Finally, the density of
each zone, the ratio of black to white pixels, is
calculated and stored with each zone. These
densities will be used to assist in the identifi-
cation of text, table, and drawing zones. Once
all zones have been located and their densities
determined, optical character recognition
(OCR) is applied.
Caleras OCR engine consists of four phases:
segmentation, image recognition, ambiguity
resolution, and document analysis [21].
Segmentation is the process of locating indi-
vidual characters, while image recognition
takes a look at each individual character and
classifies it without the use of contextual
information. Ambiguity resolution attempts
to resolve any image recognition errors with
the use of contextual information (i.e. known
word forms). Document analysis attempts to
assign tags to entities such as paragraphs and
columns on an image. Once OCR is applied
to each zone, the resulting text is returned.
This text becomes another attribute of the
respective zone. The text is used to assist in
assigning a type to each zone; and in the cre-
ation of tags for the text document.
The next step in document image indexing is
the assignment of a zone type such as, abstract
or title, to each zone, which can be done, for
example, by utilizing a basic keyword search
on the text [22]. If a keyword is found, then
the zone can be tagged with the appropriate
identifier. For example, a search on the text
of a zone may result in the match on the key-
word ''Abstract'', thus the zone will be give
the identifier ''ABSTRACT''. This process
alone will not always result in a correctly
tgged zone, thus structural knowledge is nec-
essary. Structural knowledge consists of
rules, such as: a title is always in upper case; a
title is always a specific pitch; and a title usu-
ally occurs at the top of the page. Processing
using structural knowledge is referred to as a
publication-specific, because the rules for
document layout are predefined [23]. Zone
density can also assist in the typing of zones.
When all zones have been identified and
assigned types, the process of generating a
text document and an outline document is ini-
tiated. The text document includes all of the
ASCII text that was extracted during the OCR
process along with the tags such as
''ABSTRACT'', hat identify the text. This
text document will be the input for the auto-
matic indexing process. The outline docu-
ment is a PRC proprietary file that provides
the ability to navigate through the logical
document image. The document is organized
much like an outline in that it includes items
such as: a table of contents, list of figures, list
of effective pages, and pages of the image.
The table of contents could be expanded,
when selected, to reveal key sections such as
the title, abstract, and summary. By selecting
the abstract, the image file containing the
abstract is displayed. The PRC Outline
Documentor is the application that provides
the ability to manipulate and navigate through
the outline. lt is described in more detail in
section 2.1.5.A4.
Text documents that are outputs of the zone
process (step 6) are utilized to create the
indices for the various retrieval models dis-
cussed in Task 1 (Multiple Retrieval Models).
The exact indexing process used depends
upon the text retrieval system performing the
indexing. A typical system begins by break-
ing the text document into tokens or concepts.
Each concepts is examined to determine its
type or classification (proper noun, word,
date, number) and marked accordingly. When
complete, each term is compared against a
stop-word list. Words identified as being on
the list are not stored. Once stop-words are
removed, terms are stemmed to their root
form. The stemmed words are then assigned
weights, generally determined by the term
frequency in the document[24]. Once this is
complete an index to the text collection is
built. This index will be used for the retrieval
of relevant documents based on a user query.
Different models (vector space, boolean,
probabilistic) use varying indexing schemes
therefore, the multiple retrieval models
explained in Task 1 will differ in their index-
ing procedures. Each indexing engine will
store a special key (document ID) used to
identify the related document image during
retrieval.
The third subtask is to establish an expert the-
saurus database and implement reference
tables to improve OCR accuracy. The key
terms of a particular domain form the basis for
the thesaurus. Reference tables are also based
on expert knowledge of the domain. They are
used to verify andlor correct the OCR inter-
pretation of special names and titles. Typical
reference tables may be parts lists, glossaries,
phone directories, or organizational charts.
This subtask will be implemented once the
second subtask has been completed.
When a user requests to display a document,
the PRC Outline Documentor is initialized. It
provides an object-like approach to the man-
agement, manipulation, and presentation of
diverse data types and locations of documents
in a rule-based outline. Within one structure,
the user has easy access and control of
images, text, and fonts. It provides the ability
to navigate through an outline (created in the
automatic document image indexing task), by
promoting, demoting, or adding and deleting
items. The user can control the presentation
of the outline by expanding or collapsing
entries. If an item, c.g., abstract or main text,,
is selected for viewing, the Outline
Documentor will interface with the PRC
Digital Image Editor to display the item. The
Digital Image Editor offers a complete range
of image viewing and manipulation functions
for documents including display of multiple
images, scaling, pan, scroll, rotate, zoom, and
reverse video. Annotations can be added to
the image with the ability to add 16 layers of
color markup without modification of the
original image. Each layer supports text and
graphics markup (lines, freehand line, arrows,
circle, rectangles, polygons). Finally, raster
editing features are available to copy, cut and
paste full or partial images, erase, crop, and
for image cleanup with despeckle and deskew.
An object-oriented design has been com-
pleted and implemented for the Combination
of Expert Opinion (CEO) module. The
SMART retrieval system, which has provided
the framework for implementing the retrieval
models, was acquired from Cornell
University. We have also acquired Fulcrum's
Ful/fText system, which may be used in addi-
tion to the SMART system. We have
obtained the LEND system from Professor
Edward Fox of Virginia Polytechnic
University and State University and the
INQUERY system from Professor Bruce
Croft of the University of Massachusetts,
Amherst, but have not yet integrated them
with our prototype system. The Calera optical
character recognition engine has been pur-
chased and installed. The PRC Digital Image
Library has been implemented and the United
States Patent image collection has been pro-
cured. The capability to identify zones of
interest using the X-Y tree partitioning algo-
rithm and the ability to OCR those zones has
been accomplished. The algorithm to identify
the types of zones (abstract or title) has been
partially completed, but it only employs
textual clues for identification and not struc-
tural information. Likewise, the classifying of
zones (text, drayting, table) by usage of the
density routine has been achieved to a limited
extent. However, the use of other structural
clues to assist in this process has not been
implemented. The creation of a PRC outline
document and a tagged text document has
been attained.
On the retrieval side, we plan to integrate the
various retrieval systems mentioned above, as
well as the natural language understanding
system, PAKTUS (PRC Adaptive
Knowledge-based Text Understanding
System) with the PRC document imaging
products described in section 2.1.5 to com-
plete the ADIIR system. The ranked outputs
of the different retrieval systems will be
combined using the CEO algorithm. On the
document image analysis side, the zone iden-
tification process will be enhanced to include
a run length smoothing algorithm (RLSA)
[25] and a connected pixel analysis [26].
These enhancements will assist with the iden-
tification of imbedded zones and improve the
accuracy of the X-Y tree partitioning routine.
The use of structural data to improve zone
identification and classification is also
planned. The despeckling and deskewing
algorithm currently requires user assistance,
thus the ability to automatically perform these
functions is a desirable feature. The tagged
text document created after zone identification
and typing will eventually be replaced with a
SGML-tagged document. Lastly, the estab-
lishment of an expert thesaurus database and
implementation of reference tables to improVe
OCR accuracy will be implemented after the
zOne identification routine has been opti-
mized.
PRC in collaboration with Professor Edward
Fox and his colleagues at Virginia Polytechnic
Institute and State University (VPI&SU) par-
ticipated in the first TREC conference [27,
28]. The CEO algorithm developed for
ADIIR was used by PRC to combine the
results of different retrieval methods supplied
by VPI&SU. Since these methods, such as p-
norm, are expressed in terms of correlations
rather than probability distributions, it was
necessary to extend the CEO algorithm to
handle correlations. So far this extension has
been handled in a heuristic fashion. Due to
delays in obtaining necessary equipment we
were not able to process the full set of TREC
queries against the full set of data.
Nevertheless our ll-point average (based on
averaging precision at 1l points of recall, the
standard overall TREC measure), was within
the range of the top three scores of systems
completing the full task using manually-gen-
erated ad hoc queries. It should be pointed
out that the TREC task was experimental in
nature and that there were many factors, such
as the delays many sites experienced in
obtaining equipment, which preclude too
much weight being placed on the scores
received by the various systems.
Since TREC, we have experimented with
weighting the different methods combined
based on their performance with the TREC
data, i.e., to determine an upper bound for per-
formance based on knowledge of each
method's performance on the actual test data.
We have used four different weighting
schemes: the 1l-point average, precision at
0.00 recall, precision at 0. 10 recall, and
unweighted (i.e., our official TREC results).
So far none of the weighting schemes has
produced better results than the unweighted
scheme. Two immediate explanations sug-
gest themselves. First, using overall averages
may not be too useful. Second, our simple
implementation of CEO assumes indepen-
dence among the methods. To examine the
first problem we intend to try weighting the
methods on a topic by topic basis rather than
by overall averages. Again this would be a
retrospective upper bound experiment. In
terms of the CEO approach [9] using only
overall averages would be analogous to using
only feedback from past searches, while using
topic-specific weights would correspond to
receiving feedback over several iterations of
the same search. We propose to investigate
the second problem by analyzing the overlap
of pairs of runs of the various methods to
determine dependence and thus perform CEO
without the independence
The ADIIR system provides a proto-
typeltestbed for research in information
retrieval systems which can take documents
input in paper or electronic format and that
can utilize layout or other structural informa-
tion, e.g-. given by SGML tags. It is designed
to work effectively and efficiently for very
large databases. Further it seeks to address
the problem of different retrieval models
retrieving very different sets of more or less
equally relevant documents, by providing a
methodology, CEO, for the combination of
the results of different weighted retrieval
models into an overall optimal ranking.
Nearest neighbor (NN) classifiers with su-
perior recognition accuracy have at last be-
come practicable due to advances in com-
puting technology. Although their excellent
asymptotic properties are well known, and
both optimal and heuristic pruning algo-
rithms have been extensively investigated,
to the authors' knowledge this is the first
report on using NN in a large-scale OCR
application.
Recognition of scanned handprinted
digits is the objective of continuing re-
search. For up-to-date surveys, see [l-
5]. Applications include zip-code recog-
nition, facsimile coversheet interpretation,
form reading, and data entry,
A recent conference [6] sponsored by
the United States Bureau of the Census
and conducted by the National Institute of
Standards and Technology reported a wide
variation in recognition performance: one
participant reported a 98.2% accuracy, with
most in the 96.0% to 96.4% range. Most of
these digit recognition systems were trained
on the same NIST database used in the ex-
periments reported here,
The following work will focus exclu-
sively on the classification of isolated hand-
printed digits, Several versions of the NN
classifier are considered. The motivation for
using N N is its low error rate, simplicity,
and minimal training requirements, How-
ever, NN classifiers are slow and require a
large memory, Fortunately, several meth-
ods exist to prune the NN classifier (Section
3), reduce memory requirements by 80%,
and increase classification speed by a factor
of 5.
We first investigate the dependence of
the error rate on the number of training
samples and on I, the number of nearest
neighbours, performing experiments using
two distinct sets of features: contour tan-
gents [7] (derived by smoothing and sam-
pling the chain code description of the char-
acter), and Zernike moments [8] (computed
by determining the inner product of the
character bitmap with the Zernike kernel
of the form k,a4(p,8) = R,,4,(p)eP* where
=-- - S'E4EE
The NN classifier utilizing tangents is more
accurate, with a substitution error rate of
1.64%.
Next we turn our attention to the error-
reject trade-off. The highest possible accep-
tance rate may be valuable in some appli-
cations such as information retrieval with
robust word-matching, but substitution er-
rors are undesirable since they are diffi-
cult to detect or correct in the absence of
numeric context, If throughput is com-
puted to include both automated classifi-
cation and human proofreading and correc-
tion, then a very low substitution rate with
a relatively high rejection rate may actu-
ally increase throughput [9]. By combin-
ing the two NN classifiers (one using tan-
gents, the other using Zernike moments) a
substitution error rate of 0.27% is obtained.
An even lower substitution rate of 0.035%
can be obtained by using a unanimous vote
acceptance criterion (Section 4) at the ex-
pense of a rejection rate of 18.41%.
The data considered in these experiments is
from the NIST'-3 database of hand-printed
characters [10]. The database was gen-
erated using 2100 writers distributed geo-
graphically in proportion to the population
density of the United States to compensate
for geographic variations in hand-printing
style. The writers are from the Bureau
of the Census and can be considered mo-
tivated (i,e,, this database represents fair
hand-printing). No restriction was placed
on the writing implement, which ranges
from wide felt-tipped pens to hard sharp-
pointed pencils. This leads to a consid-
erable variation in stroke thickness, which
sometimes causes characters to touch (these
experiments consider only well-segmented,
isolated characters). There is also a wide
range in character size, tilt, and skew. This
database contains approximately 250,000
hand-printed digits scanned at 300 dots per
inch. Of this total, we will use 118,000 sam-
ples for training and 119,121 samples for
testing. Care was taken to ensure that the
test set was not compromised in the follow-
ing experiments.
The database was partitioned to form
disjoint training and evaluation sets: no
writer has samples in both sets, A small
portion of the database was reserved for fu-
ture research. Approximately 10,500 touch-
ing characters were not included in the ex-
periments. We partitioned the database as
follows:
Database: 252,000 digits (2100 writers)
The reference sets used in our experi
ments are subsets of the Training Set, and
the test sets are subsets of the Evaluation
Set.
The NN rule, introduced in 1967 [11], as-
signs the label of the nearest pattern in the
reference set to the unknown pattern, Let
ar @ i1 be a labelled reference pattern, se-
lected from the reference set A = a,},,
where 3I is the number of reference pat-
terns, and 92 is the pattern space, Let y 9l
be the unknown pattern. Then the nearest
neighbour of y, denoted -4. Is given by
where d() is a distance measure, In other
words, 4/4i4 is the pattern in A which min-
imizes d(r, y), The nearest neighbour rule
assigns the label of az,,4 to unknown y.
where l( -) is the label assignment.
The most interesting theoretical prop-
erty of the NN rule [12] is that under very
mild regularity assumptions on the under-
lying statistics, the large-sample error rate
incurred is less than twice the Baves er-
ror rate. The Bayes decision rule achieves
minimum error rate but requires complete
knowledge of the underlying statistics [11],
[13]. More precisely [12], the error rate of
the NN denoted PNN, is bounded by
where c is the number of categories, and Fg
is the Bayesian error, This bound applies to
the asymptotic case, and little can be said
about the non-asymptotic behaviour of the
N N classifier (i,e., real pattern recognition
applications).
The NN classifier uses each sample in
the reference set as a model. Unlike clus-
tering techniques, the NN does not attempt
to capture global characteristics of the data,
Rather, the boundaries between classes are
defined locallv as the Voronoi boundarv
bbetween adjacent reference patterns, NN
classifiers do not require any training (]ust
store the patterns), but they do require a
large memory and are slow at query-time,
In sparse spaces, a large number of refer-
ence patterns are often required to capture
the variations in patterns within a category.
The NN classifier has high accuracy in com-
parison to other techniques [14], and is of-
ten used to benchmark classification perfor-
ITiance.
The KNN classifier is similar to the NN
classifier, except that the nearest K neigh-
bours are determined, and the unknown
pattern is assigned the label most prevalent
among them. Let [4.,}[L, be the K pat-
terns in reference set , which are nearest
to unknown y, The nearest neighbour set
+G, is defined recursively by
where P, = [ae,ha; are the i nearest
neighbours to y, and A/P; is the reference
set A less P;. The K-nearest-neighbour
rule assigns to y the label most frequently
represented among the I nearest samples
(majority vote).
T'he KN N rule reduces the influence of
local category information in favour of in-
formation contained in a wider neighbour-
hood of the unknown pattern. This dimin-
ishes the importance of an individual ref-
erence sample in favour of its neighbours,
The asymptotic error rate of the KNN rule
is nearer to the Bayes error rate than that
of NN [12]. As K - oc, the performance
of KNN approaches that of the Bayes de-
cision rule. For sparse data, however, few
theoretical results apply, In dense spaces,
the KN N is a good approximation to the
Bayes decision rule, while in sparse spaces,
category averaging will often decrease the
accuracv of the KN N classifier.
In order to gauge the relative merits
of the NN and KN N classifiers, a series of
small-scale experiments were conducted us-
ing hand-printed digits, The reference set
consists of 10. (000 patterns, and the test set
consists of 4,000 patterns, Each category
is equally represented. The reference set
contains samples from 100 writers, and the
test set contains samples from 40 writers
not used in building the reference set, i,e.,
a 'blind test''.
Table 1 shows the performance of the
K N N using tangents and Zernike moments.
Each feature set is considered in isolation.
In both cases, accuracv on the test set de-
creased as K increased, This result in-
dicates that the small-sample performance
of KNN is inconsistent with its asymptotic
performance (which predicts an increase in
KNN accuracy with increasing ). The
K N N can be viewed as an attempt to es-
timate the a posteriori probabilities from
samples [12, page 104]. It is therefore not
surprising that it falters in sparse pattern
spaces which are insufficient to model the
class probability densities. The results in
Table 1 support the use of K = 1 for clas-
sifving hand-printed digits, We have ob-
served that several of the errors of N N-
Tangents classifier are correctly identified
bv the NN-Zernike moments classifier, and
vice versa, motivating the combination of
the classifier outputs (Section 5).
Another experiment examines how NN
classifier performance changes with an ex-
panding reference set (Table 2). A test set
of 4,000 samples was used, while the size of
the reference set was varied from 1, (000 to
50. 000 samples. The classification accuracy
improves as the sample size increases, indi-
cating that the category boundaries are be-
coming better defined with increasing sam-
ple size, This experiment also lends cre-
dence to the notion that the single most im-
portant variable in hand-printed digit clas-
sification is the size of the training set.
A drawback of NN classifiers is the large
amount of storage and computation in-
volved duue to the appparent need to store
all the sample data, Pruning (also known
as thinning or editing) is an attempt to
store only a fraction of the data without
substantially degrading classifier accuracy.
Pruning removes superfluous patterns from
the training set, specifically those patterns
which do not affect the decision boundaries
(i.e,, patterns interior to homogeneous re-
gions of the pattern space). Pruning ac-
celerates classification and reduces memorv
requirements.
Two methods of pruning are consid-
ered here: (1) a geometric method (Voronoi
pruning), which preserves the NN deci-
sion boundaries but is computationally
prohibitive for higher-dimensional pattern
spaces, and (2) a heuristic method (based
op Hart's condensed nearest neighbour rule
[13]), which is computationally tractable al-
though it does not always preserve the orig-
inal decision boundaries. Experiments in-
dicate that 79% of the training set can be
pruned with only a 0.07% deterioration in
recognition accuracy,
A pruning algorithm based on the geometry
of the pattern space is now discussed [15],
[16]. This method uses the Voronoi diagram
of the sample data,
Definition. Voronoi Diagram [17'
The Voronoi diagram of a set of points,
called sites, is a partition of R' that assigns
a surrounding region of ''nearby'' points to
each of the sites (see Figure 1(b)). Each re-
gion is the d-polytope containing all points
lying nearer to one site than to any other,
Formally, the (nearest-set) Voronoi diagram
of the set A,, = i41. 2---44} of n sites in
R'' is the set of n convex regions,
for 1 5 i 5 n.
Algorithm. Voronoi Pruning (V P)
The VP algorithm preserves the N N de-
cision boundary precisely, since the NN de-
cision boundary is contained in the Voronoi
diagram, and only those data points which
do not affect the NN decision boundary are
removed from the data set (see Figure 1).
U nfortunately, there are several problems
associated with VP. The VP algorithm does
not always produce a minimal representa-
tion. The algorithmic complexity is on the
order of O(447 log n), where n is the sam-
ple size, and d is the dimensionality [15],
[16]. For higher dimensional spaces, and
for large sample sizes, the complexity is
prohibitive. As well, VP treats all regions
equally and does not give precedence to
hard-to-recognize nonhomogeneous regions
in the pattern space,
In order to reduce the computational
complexity of the VP algorithm, an op-
timization is considered in [17]. The
worst-case computational complexity is
O(+3)/]og n), while the expected com-
plexity is O(ES,,n log n), where S,, is the
number of dual simplexes in the Voronoi
4r=. N. = (,. ,). .
the probability of a sample being interior
to the Voronoi decision region (for further
explanation, see [17]). This complexity is
still intractable for large data sets in high-
dimensional spaces, Sub-optimal geometric
approximations to VP are somewhat faster.
These methods do not preserve the NN de-
cision boundaries, but do have a lower com-
putational complexity, For example, the
Gabriel Algorithm [15], has a worst-case
complexity of O(dn*).
The data sets considered here consist of
approximately 10% samples, and the dimen-
sionality is d = 64 for tangents, and d = 56
for Zernike moments, The computational
complexity of partitioning this set of data is
pgP for the VP algorithm, 10%% (estimate)
for the optimized VP algorithm, and 10
for the Gabriel algorithm. All these meth-
ods are computationally too intensive for
the available computer resources, which are
capable of approximately 10% computations
per second. Note that even the Gabriel al-
gorithm would take 10? seconds (approxi-
mately 30 years) to complete! Therefore,
we next examine heuristic pruning algo-
rithms which are more tractable.
The condensed nearest-neighbour (CNN)
rule is a heuristic method of reducing the
size of the training set without significantly
affecting the performance of the NN classi-
fier. Popularized by Hart [13], this method
has several interesting variations [18], [19],
[20]. Our own variation yields a more
conservative pruning algorithm which pre-
serves nearly all of the NN decision informa-
tion. For clarity, we will first outline Hart's
original CNN rule.
A lgorithm. Condensed Nearest Neigh-
bour Rule. [13]
T]e CN N rule attempts to preserve the
NN decision boundaries by identifying ho-
mogeneous regions of the pattern space,
where homogeneity is measured usig a
nearest neighbour rule (step 2). The CNN
rule is sequential and order-dependent. In
general, the condensed data set is not able
to preserve the NN decision boundaries,
as substantiated by the results in Table 3.
A modified CN N rule uses an interesting
method to determine the decision bound-
aries: for each pattern z, the nearest neigh-
bour to a from another class, denoted y, is
found, Pattern y must be close to the deci-
sion boundary [18], although the set of all
y is not sufficient to define the N N decision
boundarv.
We now outline a novel variation of the
(CN N which considers the K nearest neigh-
bours of each data pattern, A pattern ar
is pruned from the data set only when all
I of its neighibours are of the same cate-
gory (unanimous vote criterion ). This al-
gorithm updates the reference set until all
the pruned samples are correctly classified.
Algorithm. Modified Condensed Nearest
Neighbour Rule. (MCN)
The modified condensed nearest neigh-
bour rule is illustrated in Figure 2. The
computational complexity of MCN is ap-
proximately O(dn*) (note: an application
of MCN using 118,000 samples takes ap-
proximately 48 (CPI hours). The MCN rule
retains a larger portion of the reference set
than does the (CN N rule, and it preserves
most of the NN decision boundaries (see Ta-
ble 3). A modification to the MCN based
on [18], which we call TCN, is derived by
changing step (2): delete from the refer-
ence set every sample which is not the near-
est neighbour of a different class for some
other sample. The TCN is shown to reduce
the data set by a larger fraction than either
CNN or MCN. but TCN does not perform
as well in test conditions. Since we are in-
terested in maintaining a high recognition
rate, we will use the MCN rule in further
pruning experiments,
Table 4 shows the performance of MCN
for various sized training sets with K = 15.
(The contour tangents feature set is used.)
Other MCN experiments, conducted using
I = 5 and = 10, suggest that smaller
values of I do not lead to as good a repre-
sentation of the decision boundarv. A value
of = 15 provides a reasonable tradeoff
between size of condensed set and test per-
formance. Similar results were noted for
Zernike moments, with a pruning rate of
76.73% for K = 15 when 118,000 samples
were used, Note that the classification of
the test set was conducted with K = 1
using either the full training sets or the
pruned reference sets,
As the sample size increases, the prun-
ing performance improves. For 118.000
data samples, the training set is reduced by
79% withh a loss of accuracy of only 0.07%.
The data considered in these experiments is
from the NIST-3 database of hand-printed
characters, described in Section 1.1. The
uriters in the test set have not been used
in ay manner whatsoever in the train-
ing of these classifiers. The reference and
test patterns are the feature vectors derived
from the original bit maps of the hand-
printed digits. Two feature sets are con-
sidered: contour tangents, and Zernike mo-
TTieintS.
An NN classifier for each feature set was
tested in isolation (see Table 5). When
using a training set of 118, 000 patterns
(condensed using the MCN algorithm with
I = 15), the NN classifier with = 1
has a recognition rate of 98.36% when using
tangents, compared to 96.71% when using
Zernike moments.
The results show that the pruned ref-
erence set derived from a large number of
writers outperforms a larger reference set
derived from fewer writers,
A simple rejection criterion is now de-
scribed: If the K-nearest neighbours are of
the same class, then the result is accepted,
otherwise the result is rejected. For suitable
values of I, very low substitution rates can
be achieved in the relatively hommogeneous
portions of pattern space isolated by this
rejection rule (see Table 6). For instance,
we can achieve a substitution rate of (0.33%
with 4.71% rejections using a K = 5 unan-
imous vote criterion (tangents ).
WVe now consider a classification method
that exploits the availability of several NN
classifiers, each using a different set of fea-
tures. The output of the classifiers are
combined using the following rule: If thc
K-nearest neighbours for all the classifiers
are of the same class, then the result is ac-
cepted, othe rutise the result is rejected,
Combining the tangent based and
Zernike moment based classifiers leads to
a test performance of (0.27% substitutions
with a rejection rate of 4.11% for = 1
(i.e. the combined classifier has an accu-
racy of 99.73% on non-rejected characters).
For K s 5, this method leads to a test per-
formance of 0.035% substitutions with a re-
jection rate of 18.41% (i,e, the combined
classifier has an accuracv of 99.965% on ac-
cepted characters). The results for K = 1
to 8 appear in Table 6.
The tradeoff between rejection and sub-
stitution errors is illustrated in Figure 3.
The interesting properties of the error-
reject curve, including convexity, initial
slope, and the integral relation between the
error rate and the reject rate in optimal
classifiers, are explored in [21].
It is interesting to note that both near-
est neighbour classifiers (with I = 1) are
wrong 0.65% of the time, If neither classi-
fier is correct, it is difficult to determine the
true output identity (but see [22] on com-
bined classification using rankings ). There-
fore, 0.65% of the characters are hard to
recognize. This may be due to character
shape ambiguities, inadequate features, in-
sufficiently robust classifier, or the limited
size of the reference set.
N N classification is as powerful as it is
simple. Very high recognition accuracy
(98.36% ) is possible using a sufficiently
large training set. Pruning of the NN data
set can lead to a speed-up by a factor of 5.
A substitution rate lower than 0.04% can be
achieved by combining two nearest neigh-
bour classifiers. Four errors per 10,000 dig-
its surpasses the accuracy of most human
proof-readers,
These experiments demonstrate the ad-
vantages of using the NN classification rule.
The main disadvantage is operating speed.
This disadvantage is quickly disappearing.
and it is certain that in the foreseeable fu-
ture computing machines of greater speed
will increase throughput (currently 50 char-
acters per minute) to perhaps 1000 char-
acters per minute. Hence, in the situa-
tion where extremely high performance is
required in the absence of context, it is clear
the Nearest Neighbour rule is best (NN
1).
This work is supported in part by NSERC
grant yOGP0004234. The authors wish
to thank Theodore C. Yapo of Rensse-
laer Polytechnic Institute for providing the
Voronoi diagram display software used in
Figures 1 and 2.
We are building an experimental page
reader [l] that is easily adaptable to various
languages and writing systems. A strongly
language-free page reader must cope with
variability of many kinds, such as sym-
bol sets, font styles, page layouts, linguis-
tic context, output encodings, and so forth.
We have adopted a strategy in which geo-
metric page layout [2, 3] is performed early,
before symbol recognition and contextual
analysis, While in many Western writing
systems (e.g. Latin and Greek), textlines
are generally horizontal, some East Asian
writing systems (e.g. Japanese and Korean)
allow vertical textlines and even allow both
orientations to occur in the same page.
Thus, we felt the need for an algorithm
to infer the textline orientation within iso-
lated blocks of text, if possible without
prior knowledge of the language, symbol
set, text sizes, or the number or location
of the textlines.
Layout analysis occurs as follows. Black
8-connected components are first extracted,
the image is corrected for skew and shear
[4], and blocks of text are isolated by ana-
lyzing the structure of the white space [5, 6].
Then for each text block, the subject algo-
rithm is used to infer the textline orienta-
tion, Once orientation is known, connected
components are formed into textlines and
organized into the logical reading order ap-
propriate for the language and orientation.
A typesetting convention which appears
to be universal across writing systems is
that characters are printed more tightly
within a textline than between textlines.
The heuristic to be described in Section 2
attempts to exploit this convention by ex-
amining distance relationships among con-
nected components in the image. The tech-
nique uses computational geometry algo-
rithms to achieve a worst-case asymptotic
runtime of O(n log n), where n is the num-
ber of black connected components in the
block image.
Few papers in the research literature
address this problem directly; most pa-
pers covering such writing systems assume
a fixed orientation [7, 8]. Tsuji [9] uses
a top-down strategy for image segmenta-
tion using recursive projection profiles with
hand-crafted rules for stopping and back-
tracking from a depth-first search. Leaves
of the resulting structure tree are elements
such as text lines, ruled lines, and non-
text areas, Once individual text lines are
found, their orientation is determined, pre-
sumably by a simple rule such as aspect
ratio. Spitz [10] describes a system capa-
ble of processing pages containing both En-
glish and Japanese text. Page segmentation
is accomplished by searching for horizontal
followed by vertical rivers of white space.
Lines of Japanese text with vertical orienta-
tion are represented as individual columns.
One approach to inferring textline ori-
entation which comes to mind is to make
use of pixel projections. After accurate
skew-correction, the image artwork could
be projected in both directions and the
energy computed. The projection direc-
tion producing the larger energy should be
the text orientation. However, this sim-
ple method is not robust, for example it
fails on a single tall column of horizontal
fixed-pitch text. Alternatively, the projec-
tions could be used to estimate the domi-
nant spacing, with the larger spacing taken
to be the textline orientation. Two prob-
lems are immediate - non-regular spacing
makes finding the dominant spacing diffi-
cult, and a reasonable sample size is needed
(say, 5 textlines). The latter problem leads
to special case tests and analysis for smaller
blocks.
The algorithm to infer textline orientation
accepts a list of connected components. It
is assumed these connected components are
printed in a single orientation, and that the
characters of the writing system are nomi-
nally detached (for example Japanese, Chi-
nese, Korean). It is not necessary that
the components be previously corrected for
skew or that a character of the writing sys-
tem be comprised of a single glyph.
The steps in the heuristic are:
The intent of the first step is to ignore
fragments that are much too large or small
to be characters, based on a rough estimate
of expected point sizes, This reduces prob-
lems caused by small spatially correlated
noise such as lines of dirt fragments along
page borders and gutters,
The next step reduces each connected
component to a single point in the plane
- we use the center of its bounding box.
These points define vertices in a fully con-
nected undirected graph; the edges of the
graph are labeled with the Euclidean dis-
tance between its endpoints, As we will
show, this fully-connected graph need not
be explicitly constructed.
Most of the computation of the heuris-
tic is due to step 3. The objective is to
construct a tree from the fully connected
graph containing all vertices such that the
total length of the tree is minimum. The
MST tends to connect vertices (i,e, con-
nected components) from within the same
textline, with a few edges joining compo-
nents between textlines. Efficient construc-
tion of the MST is described in Section 2.1.
Inferring textline orientation is then a
matter of finding the dominant orientation
of the edges in the MST. A coarse histogram
of edge orientation from 0' to 180% is main-
tained as edges are added to the tree (Sec-
tion 2.1). Once complete, we sweep over
the histogram to find the window contain-
ing the largest energy (details later in Sec-
tion 3). If sufficient energy is contained
in a window with its center near (0', it
is decided the text is organized into hor-
izontal textlines; likewise, if the window
is centered near 90, vertical textlines are
assumed; otherwise, the algorithm returns
uncertain''. Note that the technique is tol-
erant of skew since it is not required that
the edges be oriented exactly horizontal or
vertical.
The MST problem for general graphs was
first solved independently by Prim [l1], Di-
jkstra [12], and Kruskal [13], each demon-
strating a polynomial time algorithm. The
best algorithm to date has a run time of
O(e), where e is the number of edges in
the graph[14]. Solving the Euclidean MST
(EMST') problem would then require O(n')
time, since e = (n'- n)f2. Fortunately, the
EMST problem has metric properties which
can be exploited. (The following facts are
well-established in the computational ge-
ometry literature, but may be unfamiliar to
some readers in the document image anal-
ysis community.)
The Voronoi diagram is a fundamen-
tal structure in computational geometry.
Given a set of points in the plane, the
Voronoi diagram produces a partition of
the plane into regions such that for each
point p;, there is a region defining the lo-
cus of points closer to p; than to any other
point. Adding a straight-line segment be-
tween points sharing an edge in the Voronoi
diagram produces a triangulation' of the
points known as the Delaunay triangula-
tion. Figure 1 shows a few points in the
plane with the Voronoi diagram and the De-
launay triangulation.
TThe Delaunay triangulation contains all
edges of the EMST and no more than 3n-6
edges altogether (by planarity). This sim-
plifies the problem of finding the MST of a
fully connected graph with (n'- n)/2 edges
to finding the MST of the Delaunay trian-
gulation which contains no more than 3n-6
edges. Algorithms to compute the Voronoi
diagram and Delaunay triangulation have
been studied for two decades, An asymp-
totically efficient algorithm due to Fortune
[15] uses the sweepline paradigm and pro-
duces the Delaunay triangulation of a set
of n points in the plane in time O(n log n).
T'he algorithm is also efficient with respect
to storage, requiring only O(n) space.
Cheriton and Tarjan [14] proposed an
MST algorithm using a data structure rep-
resenting a forest of subtrees which are
merged until a single tree remains (the
MST). They further proposed a strategy for
the selection of the subtrees in such a way
that when applied to the Delaunay triangu-
lation, the MST can be found in time linear
in n. The initial queue of subtrees is the list
of vertices, so it is easy to show the storage
requirement of the algorithm is O(n).
These results together give an EMST
algorithm running in O(n log n) time and
O(n) space.
The algorithm described in the previous
section has been implemented and inte-
grated into our experimental page reader.
Figure 2 shows an image of a page of Chi-
nese. The textlines in the two main blocks
are oriented vertically, while the page num-
ber in the upper left is horizontal.
Our block segmentation algorithm pro-
duces these 3 blocks of text. The block
containing only the page number is clearly
determined to be horizontal since all (two)
edges of the MST are near 0%. The other
blocks are more interesting. Figure 3 il.
lustrates the EMST for the upper block of
text, along with the histogram of MST edge
orientation. As can be seen from the his-
togram, a large fraction of the energy is
centered around 90%; this block of text is
clearly oriented vertically.
Chinese and Japanese orthographies do
not delimit words with white space (Fig-
ure 2). This actually helps our algorithm
since there are no large word breaks to
)ump. Figure 3 is representative of the ef-
fectiveness of the technique on these writ-
ing systems. The Korean system, however,
is more challenging. Figure 4 shows a small
block of Korean text written in horizontal
lines.
Hangul is the native alphabet used to
write the Korean language. Hangul has
only 24 letters which are combined in a
two-dimensional fashion to form composite
Hangul symbols. Each composite symbol
corresponds roughly to a syllable of the lan-
guage. Often the individual letters of a syl-
lable are detached and are therefore seen
as multiple connected components. Words
in the language are delineated with white
space and since a composite is an entire
syllable, words tend to be short (with re-
spect to the composites). This results in a
relatively large number of inter-word gaps
which may be as large as inter-textline dis-
tances.
Figure 5 (a) shows the Delaunay trian-
gulation for the fully connected graph rep-
resenting the Hangul text. The triangu-
lation contains 473 edges; the fully con-
nected graph contains more than 13,000
edges. Figure 5 (b) shows the resulting
EMST while (c) gives the distribution of
edge orientations.
The evidence provided by the edge dis-
tribution is not nearly so clear in this case.
Experiments have shown it is helpful to
weight an edge's contribution by the area
(i,e, number of black pixels) of its connected
components, In this way the distribution is
less influenced by small fragments of sym-
bols which may result in edges at wild ori-
entations. The ''energy'' at histogram bin
i is defined as (/ 32; ;)', where 6, is the
count at bin i, This has the effect of sharp-
ening peaks and dampening noise in the
histogram. Our implementation sweeps a
window of 320 over the histogram and re-
quires that 65% of the energy be contained
in a single window in order to believe the
associated decision. Given this implemen-
tation, over 72% of the histogram energy
shown in (c) is contained within the window
around (0; Therefore the correct decision of
horizontal textlines is made.
Figure 6 shows a small block of set-solid
English text. The right justification and
constant pitch font cause large, irregular
inter-word gaps, Figure 7 shows the EMST
for this block of text, along with the his-
togram of edge orientation. The large inter-
word gaps, resulting in a high percentage
of inter-textline edges, is offset somewhat
by the longer words of English. Over 80%
of the histogram energy is contained within
the window around (0%;
Figure 8 shows a block produced from a
Japanese newspaper by the page segmenta-
tion algorithms. (Our system does not at-
tempt to separate text from non-text prior
to this stage.) Also shown are the resulting
EMST and edge histogram. Here the pre-
processing step is helpful as it throws out
several of the very large connected compo-
nents. The window with maximum energy
is centered near 60 and contains 45% of the
energy. The algorithm returns ''uncertain'',
leaving the decision of what to do with this
block to higher level control.
The algorithm has been tested on a
database of over 100 pages covering the
following writing systems: Chinese, Dan-
ish, English, Japanese, Korean, Russian,
Sinhalese, Thai, Tibetan, Ukranian, Viet-
namese, and Yiddish. Pages were selected
with preference given to complex, multi-
column layouts (e.g. newspapers). Pages
with tabular data and ine-graphics were
accepted. Table 1 shows a confusion ma-
trix summarizing results over all blocks
produced by our page segmentation algo-
rithms. The correct orientation was deter-
mined by hand on a block basis. An answer
of ''uncertain'' was the desired result when
the algorithm was given a block of noise or
fragments of a non-text region. Overall, the
algorithm deduced the correct orientation
95% of the time.
The majority of the uncertain blocks
incorrectly labeled vertical actually con-
tained nothing but noise, typically located
at the left and right margins of the page.
These blocks tended to be tall and narrow,
producing EMST edges oriented near 90%.
Most of the horizontal blocks for which the
algorithm was uncertain contained narrow
columns of tabular data. The EMST edges
between the few characters in each textline
could not offset the edges joining textlines.
Ignoring for the moment blocks of noise,
purely tabular data, and the rare case of a
block of mixed orientation, there were a to-
tal of 638 horizontal and 447 vertical blocks
of text within the 100 page images. There
were no confusions among the orientations
but 8 of the horizontal blocks and 1 vertical
block were labeled uncertain (7 of the 8 hor-
izontal errors were on horean text). Over-
all, the algorithm correctly labeled 99% of
the clean, non-tabular, text blocks.
Figure 9 shows the CPIDU time spent ex-
ecuting this algorithm over a range of block
sizes. The times were measured using stan-
dard UNID prof tools on a Silicon Graph-
ics Computer Systems Power Series Model
4D/480S minicomputer with a 40 MHz IP7
processor. For reference, the block of Fig-
ure 3 has 400 points and the Hangul text
of Figure 4 contains 160 connected compo-
nents. The average block size over the 100
page test database was 141 points, while
the largest block had 1315 connected com-
ponents (the data points above this were
produced by turning off block segmentation
and running the algorithm over the entire
page).
While the EMST describes a convenient set
of distance relationships among the con-
nected components, other relationships are
available. The knearest neighbors (k-NN)
is a long-popular approach in pattern recog-
nition. Rather than finding the MST in
step 3 of our algorithm, we could find the
K-NN for each point and examine the dis-
tribution of these edges in step 4.
Experiments with this approach have
shown that k 1 does not provide enough
information and that k ? 4 provides too
much data due to the edges which almost
certainly connect distinct textlines. k = 2
or k 3 produces reasonable results, in
most cases comparable to the MST. Inter-
estingly, the MST gave better results on
proportionally spaced alphabetic text. The
kk-NN seemed to cross textlines often due to
the large fraction of wide connected com-
ponents resulting from touching characters.
This is an indication of the robustness of
the MST approach.
Hashizume et al. [16] use the 1-NN pairs
between connected components to estimate
the dominant skew angle. In a method sim-
ilar to ours, a histogram of the direction
vectors defined by the 1-NN is constructed.
The histogram peak is found and the local
weighted average taken to be the dominant
skew angle.
O'Gorman uses the k-NN information
for a variety of page layout tasks[17]. He
analyzes the structure and edge length and
angle of the k-NN (k typically 5) graph to
merge connected components into words,
lines, and blocks, estimating document
skew, point size, and line spacing along the
way. For our more specialized application,
we have been able to avoid an analysis of
the MST structure, relying instead on the
orientation of individual edges only.
This paper has described a simple heuristic
to infer textline orientation given a block of
image artwork. The technique makes use
of constructs from computational geometry
for speed. The algorithm has been imple-
mented and integrated into our experimen-
tal page reader, It has been tested on a
database of over 100 pages in a dozen writ-
ing systems with an accuracy rate of over
95%, failing primarily on non-text blocks
and tabular data where spacing is not reg-
ular.
Our algorithm assumes that the charac-
ters of the target writing system are nomi-
nally detached from one another. The algo-
rithm has also been tested on a few pages
of Arabic and Nepali (written using the De-
vanagari script), two writing systems where
characters making up a word are typically
connected. Performance on Arabic was us-
able, but not so on Nepali text (attached
units are even longer than in Arabic, with
more white space between units). Perhaps
in this case the metric on which the MST
is constructed should be the minimum dis-
tance between bounding boxes, rather then
the Euclidean distance between their cen-
ters
A limitation of this technique is that it
relies entirely on the page segmentation al-
gorithm to provide blocks of a single ori-
entation. If page segmentation produces a
block of mixed orientation, the algorithm
happily returns the dominant orientation,
or at best, an indication of uncertainty. It
may be necessary to analyze the structure
of the MST to verify all text is of the same
orientation and to split text blocks when
Inecessary.
Our downstream geometric page lay-
out algorithms, such as line finding, pitch
estimation, and word finding, have been
generalized to support vertical text in a
completely analogous manner to horizontal
text. Text is maintained internally accord-
ing to the logical reading order for the par-
ticular language. System output is in this
order, regardless of the orientation or read-
ing direction of the original material.
Steven Fortune provided code for his
sweepline algorithm to construct the Delau-
nay triangulation of points in the plane. I
am very grateful to Henry Baird for helpful
comments on this paper.
Several methods to program a robot have been pro-
posed. Such methods include: teach-by-showing, teleop-
eration [15, 12, 5], textual programming[3],and automatic
programming [10, 8]. In teach-by-showing methods, an
engineer stores, using a teach pendant in teaching mode,
a path along which a robot should move repeatedly. This
methodrequires thatan engineer is in the same environment
as the robot. Thus, we cannot use this method in hazardous
environments such as in nuclear plants, underwater, or in
outer space.
Tb remedy this problem, teleoperation methods have
been proposed. This method uses a master manipulator
for teaching and a slave manipulator for execution. An
engineer controls the master manipulator in a safe environ-
ment while monitoring the hazardous environment with a
remote TV camera and display. The slave manipulator in
the hazardous environment executes real operations based
on control signals from its master manipulator. By using
this method, we can only teach a robot trajectory informa-
tion, It is difficult to build a flexible robot system able to
use force control with error recovery capabilities. It is also
true that we have to reconstruct entire programs,even when
a very minor change in the program is desired.
Textual programmingisoften used in academic environ-
ments. A programmer stores a robot command sequence
in a computer as a textual program. It requires a long
development period and expert programmers.
In order to speed up the programming process, auto-
matic programming has been proposed. The method tries
to develop geometric reasoning systems which can gen-
erate textual programs to control a robot from geometric
information given by geometric models and task specifica-
tions. This direction is quite promising, however, there are
many issues to be addressed before we have acomplete au-
tomatic programming system; It is quite difficult to build a
complete automatic programming system, though perhaps
not impossible.
We propose a novel method that combines automatic
programming and teleoperation. We propose to add a vi-
sion capability that will observe human operations to an
automatic programming system (a geometric reasoner). In
particular, we propose a system that observes a human
performing an assembly tasks while a geometric reasoner
analyzes and recognizes such tasks from observation, and
generates the same assembly sequence for a robot. We will
refer to this paradigm as Assembly Plan from Observation
(APO).
Due to thegeometric reasoning capability, the APO sys-
tem understands the operations that the operator isperform-
ing. Thus, the system for example can discard unnecessary
motions which are often introduced by a human teleoper-
ator. The system can also insert error recovery routines
into the generated assembly plans. In this regard, APO is
superior to the teleoperation method.
Due to the vision capability, the system can solve sev-
eral otherwise extremely difficult problems, such as path
planning and determining the optimal assembly sequence,
by simply observing a human performing the operation. In
thisregard, APO is superior to the automatic programming
method.
Several systems have been proposed to recognize hu-
man (robot) hand movements for various purposes. Hirai
and Sato proposes a system to recognize manipulator mo-
tions for maintaining the consistency between an internal
world model and the real world [5]. Herve et al developed
a system to recognize robot hand movements for visual
feedback [4]. These systems are, however,for monitoring
purposes not for program generation purposes.
Kuniyoshi et al [7] developed a system which tracks
movements of a human hand for program generations.
Their goal was similar to us. The system is, however,
restricted to pick-and-place operations. This is because the
system only analyzes apparent hand movements and does
not employ the knowledge given by analyzing geometric
models of objects.
Inan APO system,a human operatorperforms assembly
tasks in front of a video camera. From the camera, the
system obtains a continuous sequence of images recording
the assembly tasks. In order for the system to recognize
assembly tasks from the sequence of images, the system
has to perform the following six operations (See Figure 1.):
In thispaper, we will concentrate on the task recognition
and task instantiation modules, because these two parts
form themain loop for the assembly plan from observation.
The outline of the modules are as follows:
Our object recognition module identifies each object
using the object models from a given image segment. The
module represents the recognition results in a world model,
as shown in Figure 1, by using the geometric modeler,
Vantage.
Our task recognition module recognizes objectrelations
in two image segments and extracts the transition between
twoobjectrelations from thetwo segments. The task recog-
nition system has abstract task models in a data base. Each
abstract task in the data base describes a transition between
two different object relations. From the task models in
the data base, the system identifies a task model that de-
scribe the transition needed to achieve the observed object
relations, as shown in Figure 1.
Our task instantiation module represents the recogni-
tion result as an instantiated task model. An instantiated
task model associates a transition with an action capable
of causing the transition. It also includes appropriate pa-
rameters to achieve the action based on the given scenes.
Such parameters include object locations and the grasping
locations for the action. The instantiated task model also
includes the global path along which to move an object.
The system, then, inserts the obtained grasp and stack lo-
cations into the command sequence. Finally, the command
Sequence is sent to the robot.
In order to develop task models for an APO system, we
have to define representations to describe assembly tasks.
In this section, we will define assembly relations for such
representations. Then, we will consider how to define
assembly task models using the assembly relations.
In each assembly task, at least one object is manipu-
lated. We will refer to the object as the manipulated object.
The manipulated object is attached to other stationary ob-
jects, which we refer to as environmental objects, so that
the manipulated object achieves a particular relation with
environmental objects.
We will define assembly relations with respect to face
contacts between amanipulated objectand itsstationaryen-
vironmental objects. The essential goal of an assembly task
is to establish a new face contact between a manipulated
objectand environmental objects. For example, the goal of
a peg-insertion is to achieve face contacts at the side and
bottom faces of the peg against the side and bottom faces
of the hole. Thus, we will use face contact relations as the
central representation for defining assembly task models,
To make the overall problem manageable, we concen-
trate on a world of polyhedral objects in which only one
polyhedron may be moved by one assembly task. An as-
sembly relation will be defined between a manipulated
polyhedron and several stationary environmental polyhe-
dra, This restriction still leaves a diverse range of interest-
ing relationships, actions, and resulting assemblies.
Using such face contact relations as the basic repre-
sentations, we will describe an assembly task with a tran-
sition between pre-assembly relations and post-assembly
relations. Based on the description, we will build an APO
system in the following steps:
For geometric objects in a polyhedral world,our taxon-
omy identifies all possible assembly relations based on the
directions of contact surface normals. The taxonomy has
classes of uni-, bi-, tri-,tetra-,and hexadirectional contacts.
Nine different contact patterns are extracted.
We will represent the contact directions and possible
movement directions on the Gaussian sphere as shown in
Figure 2. The shaded areas indicate the prohibited move-
ment directions of the object with respect to the environ-
ment. The non-shaded areas indicate the possible move-
ment directions. Similar representations have been pro-
posed for automatic programming purposes [11, 2].
We will consider a.sequence of manipulator operations
to achieve each assembly relation from assembly relation
3d-s, Such asequence of manipulatoroperationsis grouped
into a motion macro, ie., a template of manipulator opera-
tions, which, when applied to an object, yields the desired
assembly relation. This is possible because each assembly
relation is defined so that we can apply the same manip-
ulator control strategy to achieve the relation by changing
only controller parameters, not the strategy.
In order to reduce the number of necessary templates,
we will analyze each assembly relation in an iterative man-
ner. We will analyze simpler relations earlier and more
complicated relations later. Also, instead of considering
a template to directly achieve a complicated relation from
3d-s, we will consider an intermediate relation, and then
try to achieve the complicated relation. First, we try to
achieve an intermediate relation from 3d-s by using the
templates already considered. Then we try to achieve the
final relation from the intermediate relation using a newly
considered template.
In order to find an appropriate intermediate relation, for
each assembly relation, we consider disassembly actions
from the assembly relation, and extract all possible im-
mediate intermediate assembly relations just prior to the
assembly relation. We do this because considering disas-
sembly actions is easier than considering assembly actions.
Several intermediaterelations sometimes occur from the
same assembly relation due to 1) the variation in shapes of
contact faces, and 2) the variety of possible disassembly
operations,
In case that due to variations in the shapes of contact
faces, we have to analyze all intermediate relations and
assign appropriate motion templates to all transitions from
the intermediate relations to the desired relation.
In case that due to the variety of possible disassembly
operations, we can choose one appropriate intermediate re-
lation among the several intermediate relations. We choose
the one which is achieved by the simplest and most robust
operation under uncertainty in positional information.
By using these criteria, we will analyze each assembly
relation, extract all possible assembly relation transitions,
and prune unnecessary relation transitions.
We can represent relation transitions as a tree structure.
Each node in the tree represents one particular assembly
relation, and each arc represents corresponding assembly
relation transitions.
A procedure tree is created by placing a template of ma-
nipulator operations (motion macro) at each arc separating
the assembly relation nodes. The manipulator operations
chosen are those which can correctly achieves an assembly
relation on one node from theassembly relation on the other
node. Figure 3 represents a completed procedure tree. See
Ikeuchi and Suehiro [6] for more details.
A task model consists of an assembly relation transition,
a motion macro, and the necessary parameters required to
expand the motion macro into a sequence of manipulator
commands. For example, Figure 4 shows the task model
corresponding to the transition from 3d-s to 3d-a. The
startingand end relation slotscontain 3d-s and 3d-a,respec-
tively, The action slotcontains the move-to-contactmotion
macro. In order to achieve the motion, it is necessary to
know the previous configuration and end configuration of
the manipulated object. The corresponding parameters are
prepared as task parameters. The values corresponding
to these parameters are obtained by the task instantiation
module at run time.
Thirteen task models corresponding to all arcs in the tree
are prepared. They are attached to the procedure tree.
How are task models used to recover human assembly
tasks in the APO system? The task recognition mechanism
will be explained in the following examples. The example
system consists of three classes of objects, (any of which
can appear in the scene): castle, block, and stick. See
Figure 5.
The system assumes that at the beginning of each as-
sembly task human intervention occurs in the scene and
at end of the assembly task the human disappears from the
scene. Byusing thisassumption,the APOsystem segments
a continuous image sequence given by a TV camera from
the scene into a finite number of meaningful chunks.
By using the level change in the brightness difference,
the system can detect human intervention. Before human
intervention,the scene consists of only stillobjects,thus the
difference between two consecutive images is at the quite
level. When human intervention occurs, the brightness
difference is large due to the motion of human and manip-
ulated object in the scene. This disturbance continues until
the end of the assembly operation. After the human hand
disappears, the scene consists of only still objects, Thus,
the brightness difference returns to the quite level.
Objects in the scene are recognized from range data.
In our current implementation, bfw images are used only
for detecting the completion of one assembly task. More
reliable range data are used for analyzing the scene. After
acertain period after the detection of the completion of one
assembly task,the APO system invokes therange finderand
measures range information in the scene. The APO system
then generates a difference image between the range image
from the previous step (before the assembly task) and the
range image from the current step (after the assembly task).
The system applies a segmentation program to the dif-
ference image and obtains any newly appearing regions.
These new regions correspond to the faces of the manipu-
lated object by the assembly task.
The system maintains the configurations of other envi-
ronmental objects. The system represents these manipu-
lated and environmental objects in the Vantage geometric
modeler [1] as shown in Figure 6.
By using the transformation from body coordinate sys-
tems to face coordinate systems, (available from the Van-
tage geometric modeler), the configurations of the faces of
the manipulated and environmental objects are obtained.
The system extracts contacting face pairs from the face
configurations. Here, a contacting face pair is a face from
the manipulated objects and a face from an environmental
object, which have the same face equations and whose
surface normals are opposite to each other.
The system determines the assembly relation based on
thecontactingface pairs by analyzing the contact directions
ofpairs. Here, thecontact direction isdefined as the normal
direction from the environment faces to the manipulated
object faces as previously defined. The contact pairs are
grouped intoa set of contact directional groups so that each
group has face pairs with the same contact direction. By
examining the occurrence of directions, we can determine
which assembly relation occurs by the assembly task.
The system recognizes the contact faces and contact
directions as shown in Figure 6. From the contact faces in
Figure 6, the system determines that the current assembly
relation is 3d-a.
Before the assembly task, the castle does not exist in
the scene. Thus, before the assembly task, the assembly
relation between the stick and the castle was 3d-s. After
performance of the assembly task, the manipulated castle
establisheda 3d-aassembly relation withtheenvironmental
object.
From this observation, the system recognizes that the
assembly relation transition, 3d-s to 3d-a, occurs due to the
assembly task. The corresponding task mode 3d-s to 3d-a
isextracted from the corresponding arc along theprocedure
tree.
In this example, at the previous step, the castle was
stored on the warehouse table. Thus, the assembly relation
transitions during the entire assembly task are
Thus,the corresponding three task models are instantiated:
a-to-s, s-to-s, and s-to-a,
The followingprocedure is executed to instantiatea task
model:
The instantiation of task models occurs in the reverse
order, s-to-a,s-to-s,and a-t0-s.
The s-to-a task model has a move-to-contact motion
macro in the action slot. The task model examines each
object model and determines grasp configurations, how to
grasp the object withrespect to the body coordinatesystem,
and the specified grasping method. In the current imple-
mentation, each object model has predetermined grasping
configurations. The task model chooses an appropriate
grasping configuration and recalculates it based on the cur-
rent body configurations. The task model determines the
grasping configuration of the castle based on the observed
castle configuration. The task model also determines the
stack configuration of the castle on the table in a similar
manner. The system then inserts these parameters to the
corresponding slots in the instantiated task model.
The global motion is also implemented as a task model,
s-to-s, This task model has a motion macro, move. The
disassembly task is also implemented as a task model.
The system finally performs the operations given by the
three task models sequentially: a-to-s, s-to-s, and s-to-a.
Figure 7 shows the final move-to-contact operation by a
manipulator
Figure 8(a) shows a human operation for inserting a
stick in a hole of the block. The system recognizes the
contact faces (Figure 8(b)). From the normal direction of
contactfaces,the system generates tetradirectional contact.
By examining the directions of the contacts, the system
determines that the observed assembly relation is 3d-e. By
examining the shape of contactpairs,the system infers s-to-
e path occurs. The s-to-e task model has a motion macro,
insert-intoin theaction slot. Using thepredetermined grasp
configuration and the observed stick position, the system
performs the insert operation as shown in Figure 8(c).
Figure 9 shows an example, having the 3d-d assembly
relation between the stick and the two castles, constructed
successfully throughs-to-btask model (insert-between)and
b-to-d task model (move-to-contact) by the system.
We have described a method that can recognize an as-
sembly task performed by a human and produce corre-
sponding operational plans for a robot. The current system
works among polyhedral objects. Future directions include
how to extend this method to handle general objects, how
to generate grasp plans and global motion plans from ob-
servation.
RajReddyand'Takeo Kanade provided useful comments
and encouragements. Bradley Nelson proofread the draft
of this manuscript and provided useful comments.
Takashi Suehiro was on leave of absence from Elec-
trotechnical Laboratory, Tsukuba, Japan.
.,,the whole of analysis discovered is dependent in
great part upon modified algorithms of certain fixed
quantities...
Leonhard Euler, 1764!
his opinion was quoted in 1822 by Babbage, in a paper
on notation which was related to functional equations
(p. 1/344).'In thisarticle Iput forward a general thesis about
Babbage's work as a mathematician,engineer, and scientist,
of which several elements are exemplified by the quotation.
My claim is as follows:
The content of this thesis will become clearer in and after
the discussion (in the next section) of certain mathematical
trends in Babbage's time, but some preliminary expansion
on the word ''algorithm'' is necessary here. I intend it to
refer, in a very general range of contexts, to ideas, theories,
or procedures in which prominence is given to successive
repetitions of a process or maneuver, its reversal, its com-
pounding with other processes, andlor its substitution into
itself. In mathematical contexts the words ''iteration'' and
''combination'' will also be used (and indeed, quoted).
In addition, two related notions have to be included.
First, ''algebra'' refers both to the branch of mathematics
in which Babbage worked (specifically, functional equa-
tions) and to certain features of algebraic thinking and
proof which also arise elsewhere in his activities. Second,
''semiotics'' denotes theories of signs, symbols, and nota-
tions as such (in mathematics and elsewhere), in which is
stressed their importance in a theory and in its philosoph-
ical basis. The word was not used in Babbage's time,' but
it can be applied to several of his concerns and those of
some contemporaries.
The thesis, then,is that Babbage consciously followed an
algorithmiclalgebraiclsemiotic approach in his choice and
solution of many of his problems and deployment of analo-
gies, and that his historians should give it proper attention.
For convenience I shall coin the word ''algorithmism'' to
refer in general to this characteristic.
This thesis is explored in approximate chronological
order of the development of Babbage's pertinent interests.
The section ''Mathematical orientations'' covers the part
that Babbage played in the reform of mathematics at Cam-
bridge and his researches in functional equations and some
other areas of mathematics. The section ''Calculations by
hand and by handle'' starts with mathematical tables and
moves on to the Difference and Analytical Engines.''Indus-
try and science'' notes a miscellany of other examples in
manufacturing,cryptography,and physics. The final section
draws some conclusions about the importance of algorithm-
ism in Babbage (including a contrast with Boole) and spec-
ulates upon its origins. Reference is made in places to ''the
figure,'' which appears in the last section.
I give details of the principal pertinent events in or
related to Babbage's life in the box. For more details,I refer
the reader to A. Hyman's excellent biography.'I rely almost
entirely upon Babbage's published writings and on certain
manuscripts that appeared posthumously, for enough mate-
rial is to be found there for my purpose. Many unpublished
documents reinforce the thesis, and a few have been cited.
The references will be found in the references list, but in the
text I cite by volume and page number from M. Campbell-
Kelly's fine new edition.' For example, in the reference at
thestartof thisarticle to an 1822publication,''p. 1/344''cites
page 344of Volume l of the new edition, but the superscript
-2'' cites the original publication. Dates associated in the
text with items are normally those of first publication. A
page range not preceded by a volume number and a slash
refers to the work indicated by the superscript number that
precedes it.
.,,the dotsof Newton,the d's of Leibnitz,or the dashes
of Lagrange.
Babbage, 1864 (p. 11I19P
One part of Babbage's life is well known; he played a
major part in the conversion of British mathematics by the
Analytical Society from Newton's fluxional calculus to the
Continental notation. However, this''fact'' is not a fact; it is
also misleading in connotation. A revised account will be
briefly summarized here.'
First, before that Society set to work in 1812, reforms in
calculus teaching had been under way, at least among the
staff, in various British institutions: in Scotland, in the circle
around J. Playfair and also W. Spence; in Ireland, at Trinity
College, Dublin,in moves initiated in 1812by H. Lloyd;and
in the Home Counties of England, at the Royal Military
College and the Royal Military Academy (with P. Barlow,
O. Gregory,C. Hutton, J. Ivory, W. Leybourn, and W. Wal-
lace). At Cambridge itself, R. Woodhouse had become
acquainted with, and even the current occupant of Newton's
chair of mathematics, I. Milnor (a quite insignificant math-
ematician ). had been buying Continental mathematical
books.* The single most important stimulus for change had
been the pubiication of the first four volumes of P.S.
Laplace's Traite du mecanigue celeste (1799-1805). While
the young men who founded the Analytical Society in 1812
made themselves remarkably familiar with Continental
mathematics, they may not have been aware of all of these
developments in Britain. Thus, while their movement cer-
tainly led to the most profound changes in British teaching
and research in mathematics, it was not the first such initia-
tve.
Second, the Society lasted as such only for a little over a
year, while its founders (principally Babbage, J.F.W. Her-
schel, and G. Peacock) were undergraduates at Cam-
bridge.'' However, their intentions were maintained after-
ward, and Iuse the word ''Society'' in this looser sense when
referring to the later activities of its former members.
Third, the change was not simply of notation but princi-
pally of theory. Fourth, there was no single Continental
theory into which change could be effected; on the contrary,
as Babbage himself indicated when referring to the pertain-
ing notations in the quotation above, three theories were in
competition:'' limits (although not usually formulated in
Newton's manner) the differential and integral version (dx,
l,]v dx as an area, and so on), proposed by G.W. Leibniz
but then used in the developed version largely created by
Euler; and an algebraic approach introduced by J.L. La-
grange.
One did not necessarily have to make a choice. In partic-
ular, S.F. Lacroix, the chief textbook writer of the day, was
a disciple of M.J. Condorcet and followed the eighteenth-
century ''encyclopediste'' philosophical tradition of present-
ing all available traditions in his writings. His principal text
was the great Traite du calcul differentiel et du calcul
integral,'* which had appeared in three volumes at the end
of the eighteenth century. Babbage had bought a copy of it
in 1811 (p. 11I19)' and he and his colleagues would have
become very well acquainted with Continental traditions
from it alone.** At all events, he and Herschelshowed their
erudition when publishing in 1813 their preface'? to the first
(and only) volume of the Memoirs of the Society; it is quite
a comprehensive survey of Continental calculus over the
previous 30 years.
Out of the possibilities available to them the Analytical
Society chose the Lagrangian approach (the word ''Analyt-
ical''wasthen often used in mathematics to refer to algebraic
principles). The origin of this decision is not clear; it seems
most likely that a general consensus among the members
was made. The main principles of this approach are ex-
plained in the next subsection. The strength of their adhe-
sion to it was made evident in the preface to the Society's
translation of the second (1802) edition of Lacroix's shorter
treatise on the calculus, published in 1816. As a wandering
encvclopediste, Lacroix had allowed himself to shift his pen-
chant somewhat to limits: but the Young Turks from Cam-
bridge admonished their senior citoven for this sad prefer-
ence ''in place of the more correct and natural method of
lLagrange.'''*In an 1827 paper on notation in mathematics,
Babbage praised in a similar vein the (attempted) expres-
sion of mechanics in algebraic theories that Lagrange had
attempted to effect in his 1788 treatise Mechanigue an-
alitigue (p. 1397).%
The ''analytical'' algebraization of mathematical theories
in France in the late eighteenth century related to a growing
interest there in semiotics. The abbe Condillac and his
semifollowers, the ''ideologues,'' had stressed the impor-
tance of signs, especially in or from his textbook, the (so-
called) Logigue (1780). Indeed, the word ''ideologie'' origi-
nally denoted ideas, their reference, and means of signifying
them. Further, for Condillac (common) algebra was the
(semi-) formal language par excellence; a posthumous book
on it called La langue des calculs was published in 1808.''
While Condillac did not influence Lagrange personally to a
notable extent. the general connection with algebra was
then important - and not only in France, as we shall soon
lLagrange had developed an algebraic version of the
calculus, based on the assumption that every function
f( + h) of a real variable x could be expanded in a Taylor
series for every value of (apart from values of when f
misbehaved, such as taking an infinite value):
He also claimed that the derivatives could be defined, by
purely algebraic means, as the coefficients of the appropri-
ate powers of the incremental variable h. He introduced thc
dash notation for derivatives, which was mentioned by
Babbage at the head of the previous subsection; it denoted
a functorial operator, going from the function f() to the
function f'(x).
This approach is based on a clear program -- doubtless
one of the sources of attraction to the members of the
Analytical Society in 1812. Babbage retained his enthusiasm
for it even after belief in the Taylor series (Equation 1) was
refuted by A.L. Cauchy in 1820 with counterexamples such
as exp(-1lxf) when x = 0. Indeed, Babbage did not realize
and perhaps did not know of Cauchy's work, for in 1827 he
spoke of the time that''has been required to fix permanently
the foundations on which the calculus of Newton and
Leibnitz shall rest,'' with a clear allusion to Lagrange's
approach (p. 11371).}' (The independent discovery of this
counterexample by W.R. Hamilton in the 1830s also es-
caped Babbage's attention.) However, according to J.M.
Dubbey' (p. 90), Babbage had sent three of his papers on
functional equations to Cauchy in 1820, and to that topicwe
now turn.
In the course of pursuing his approach, especially from
the late 1790s, Lagrange gave considerable impetus to the
development of two new algebras: differential operators,
using D (:= dldx) as an algebraic object; and functional
equations, in which the function was itself treated as the
object. These algebras flowered especially with a group
around the Alsatian L.F.A Arbogast, who developed the
operational aspects by ''separating the symbols'' (a phrase
of the time). They detached dldx from y in the derivative
dyldx, and ffromx in f(r). Thereby they extended the realm
of algebra by considering objects which were not numbers
or geometrical sizes.
lLaws and rules of manipulation of these new algebras
had to be found. A notable contribution was made by F.J.
Servois in 1814;'' Seeking the fundamental properties of
both algebras, especially functions, he characterized f as
''distributive'' and fand g as''commutative with each other''
if, respectively,
This is the origin of these standard words in algebra.
Functional equations, in one and several variables, were
Babbage's main mathematical interest from 1813 until the
early 1820s. He wrote nine papers in or around them, which
were published between 1813 and 1827. (They are repub-
lished in Babbage's works, Volume 1.') Herschel also
worked in this area, mostly on the special cases of difference
equations, and with related summation of series; the two
men corresponded intensively. It is not my intention to
describe Babbage's methods in detail;* suffice it to indicate
some principal concerns, especially the algebraic and semi-
otic aspects.
A simple example of a functional equation is
(which is an equation in two variables, because of the form
of the left-hand side ). The task is to find functions f which
satisfy the stated property for all values of x and y. or some
specified range of them. (The definitions in Equation 2
could be reinterpreted as functional equations.) Babbage
and Herschel were drawn to functional equations not only
by lLagrange's program but also by certain solution methods
developed by G. Monge and Laplace in the 1770s.
In his first paper (published in 1815) and later, Babbage
gave special attention to
to solve for f in terms of a given g. (In the important
circumstance when g(x) = x in the second equation of Equa-
tions 4, f was said to be ''periodic'' of order n.) His most
general equation in one variable was
to solve for f given F and the lg,] (p. 1I120).'* Apart from
some examples from the geometry of curves, he did not
consider many applications, explaining in 1816 that ''my
object has been to direct the attention of the analyst to a new
branch of the science'' and stressing that ''the doctrine of
functions is of so general a nature, that it is applicable to
every part of mathematical enquiry'' (p. 1I193).P8
At the time, the subject was often called ''the calculus of
functions,''referring to the determination of particular func-
tions (f''(r), say) even if no equations were involved; for
them the phrase ''functional equations'' was used.
Babbage's methods of determination and solution, which
followed the French to some extent, were rather freewheel-
ing. He manipulated functions and series, used self-substi-
tutions of functions into equations, and deployed cunning
changes of variable. He tried to study the difficult question
of the complete solution of an equation, and made very
clever use of symmetric functions (where h(x, y) = h(y, x)
for allx and y)to build up an iterative sequence of particular
solutions. These methods show very well his enthusiasm for
algorithmic and iterative procedures. Like his predecessors,
he did not normally consider conditions for existence and
uniqueness; Cauchy was soon to focus on such questions.
As the quotation at the head of this subsection shows,
Babbage was also very concerned with the inverse function.
In this connection he made much use of the form g''fg of
solution* -- perhaps the first example of this significant
''conjugate'' form (as it became known) in mathematics.
Although he was more chary than Herschel of treating
functions as objects, he exploited well the algorithmicpower
of this algebra; to handle a function f. its iterations ff (= f),
f', ..., the inverse function(s) f'' and their iterations f'. ..,
compounds with other functions fg, .-., and so on.
Babbage also tackled related topics, which had been little
studied, such as ordinary and partial differential functional
equations, integral equations, and fractional differentia-
tion,** In 1817 he explicitly compared ''the calculus of func-
tions with other modes of calculation with which mathema-
ticians have been long acquainted'' (p. 1/216).' Summation
of (in)finite series was a special concern, in which he followed
Euler and others in seeking formal relationships between
functions and their series expansions - indeed, the kind of
procedure which Euler designated as an algorithm in the
quotation at the head of this article.t One of his methods is
described in a special algorithmic context in the last section.
These methods belonged to a tradition in British algebras
which started principally with Woodhouse and was to be-
come best known with Peacock. In an 1830 book on alge-
bra,P% and elsewhere, Peacock proposed ''the principle of
permanence of equivalent forms.'' It put forward conditions
under which a mathematical expression or equation could
be interpreted outside its''respectable'' domain of interpre-
tation (such as the sum of a divergent series, or a relation in
which negative numbers were accepted as legitimate math-
ematical objects). In its emphasis on form, the principle
carried something of a semiotic ring. But the principle was
applied mostly to common algebra, and Babbage did not
discuss it in his papers on functional equations, although he
came close to it in some of his manuscripts.tt
In 1816 Babbage introduced some good notations in
developing his methods: underbars for homogeneous func-
tions, so that ''v(. . ],,,,'' indicated a function of degrees
p inx and ytogether and q in vand z together(in a somewhat
unclear passage (p. 1/144)'); and superscripts for iterative
substitutions, with overbars for symmetric cases, such as (p.
1125y'
lLater he even took the general case g'''(x, y) as a mathe-
matical problem of notations, for he found the numbers of
occurrences of y and of r within it by forming and solving
simple difference equations (pp. 1/348-349).%
Such points are evidence only of a (well-developed)
normal desire of a mathematician to use good notations; but
Babbage went much further to show his semiotic side also,
considering families of symbols, and symbolism in general.
Unlike its algebraic mathematics, French semiotics did not
come over strongly to Britain, either in the revival of math-
ematics or in that of logic (which dated from the mid-1820s.
with the publication of R. Whately's book The Elements of
Logic). Nevertheless, Babbage was aware of it. In the early
1820s he wrote an encyclopedia article on ''Notation'''' and,
more importantly, a paper on ''the influence of signs in
mathematical reasoning,'''? which were published in 1830
and 1827 respectively. There is much material in common
between the two pieces.
Babbage noted the work of boththe French philosophers
and mathematicians: He quoted from the book lDes signes
et de lart de penser (1819) written by the ideologue J.M.
Degerando (pp. 1/374, 376).'' and also explained the over-
bar and overarcnotaionsA B anaAB used byL. Carnot o
represent respectively directed straight and curved lines (p.
11404).'? He did not develop the ''ideological'' link, but
variants of Carnot's notations were to be used in computing.
as we shall see later.
Babbage also laid out various desiderata for notations,
including one of almost iconic character: ''all notation
should be so contrived as to have its parts capable of being
employed separately'' (p. 11418).'' He gave as an example
possible notations for the sine-squared function: He pre-
ferred ''(sin 8)''' -- as found in the ''excellent work'' of
Arbogast, or even better ''sin 8' for avoiding brackets -
by contrast. ''sin,%8'' (including the period) was ''by far the
most objectionable of any, and is completely at variance
with strong analogies'' (p. 11422).'
In holding this opinion, Babbage was close to his friend
Herschel. who had discussed in 1813' (p. 25) the use of the
indices to denote powers of functions, suggesting the nov-
elty ''cos. ''e'' for the inverse trigonometric function. This
notation (without the period) has become standard, but the
positive powers of these functions are always normally rep-
resented by the form which Babbage criticized -- rightly.
The same fate awaited most of Babbage's (and
Herschel's) work on functional equations and on notations.
The French took some note of it; in 1821 J.D. Gergonne
translated part of Babbage's paper''in his own mathematics
journal,'' while the Baron Ferrusac's abstracting Bulletin for
science included routine short notices of his papers pub-
lished in the period 1824 to 1831 of its run. Lacroix noted
Babbage's 1816 paper' in 1819, in the second edition of his
large Traie'' (p. 595, and a note of Herschel on p. 732).
Surprisingly, Babbage seems never to have consulted this
new edition of a work that had helped him so much in his
youth, and so, for example, seems never to have discovered
Servois's 1814paper,''to which lLacroix also gave publicity?P
pp. 726-127).
Functional equations fell rather into the doldrums for
several decades, and the only substantial use of Babbage's
contributions was made in 1836 by De Morgan,'' in the first
systematic study of the subject. However, although pub-
lished as an article in an important encyclopedia of the time,
unfortunately it did not make the impact that it deserved.*
Similarly, Babbage's concern with notations failed to raise
the interest it deserved, although again De Morgan was a
commentator.
Some evidence of algorithmic concerns can be found
elsewhere in Babbage's mathematics. His occasional writ-
ings on probability were concerned with combinatorial
cases, including an 1821 paper'' on the ''martingal'' in con-
nection with successive betting, the estimation of mortality
for the calculation of annuities in an 1826 book (pp. 6/91-
96).'' and the interpretation of apparent miracles in terms
of some higher law unknown to man in his unofficial 1838
Bridgewater treatise (pp. 9/T73-80).'* In the latter case his
algorithmic and analogical inclinations stood him in espe-
cially good stead. He gave as examples an iteration executed
by his engine which followed a mathematical law and then
''miraculously'' contravened it at some stage, which, how-
ever, had been prepared deliberately by the operator (pp.
952.55).' (See also Babbage's Passages from the Life of a
Philosopherf pp. 11292-293.)
A striking example of algorithmism comes from the
mathematics of chess: ''During the first part of my residence
atCambridge,I played atchessveryfrequently,''he recalled
(p. 11125)7 and in an 1817 paper''' Babbage followed his
hero Euler in studying the iteration of the knight's move so
as to cover every square of the board. A further feature of
this paper is displayed in the figure on pages 42-43.
In this section,I treat Babbage's concerns with compu-
tation and computing.
For about five years from 1826 Babbage concerned him-
self with the production of logarithmic tables.'' He drew on
existing tables for the basic numerical data and did not
introduce any major new idea about their calculation. But
concerning their physical appearance he showed his semi-
otic side. For ease of reading he spaced out the arrays of
digits into five-row bands** and chose a font where all
numerals were of the same height, yielding clean rows of
digits. For clarityand compactness,he printed out digitsonly
after the second decimal place, indicating by small zeros in
the third place those cases when ''1'' had to be added to the
second;and atthe finalseventh place he indicated rounding-
up by setting a dot under the digit (explanations and sample
pages from 1827 are given in Babbage's Tables of Loga-
rithms of the Natural Numbers (pp. 2/72-107)'% Babbage
used a few of these principles also for the tables in his 1826
book on assurance (pp. 6/104-127).P In 1831 he printed
some of the logarithmic tables with a variety of colored inks
on papers of different colors, to compare various possibili-
ties for clarity of reading (pp. 2/115-117).'*Part of a page is
contained in the figure on pages 42-43.
Jn the end, Babbage's findings were not ot major signif-
icance, But they were unusual, and reflect clearly the semi-
otic cast of his mind.
Prior to these semiotic ruminations on tables, Babbage
had brought to their algorithmic side a remarkable insight
which was to influence his whole life: that logarithmic tables
''might be calculated by machinery.'' In his autobiography
he gave two occasions for this reflection - in 1812-1813 and
1819(p. 11/31)'--and the latter is of particular significance.
During the 1790s a large set of logarithmic and trigono-
metric tables had been produced in Paris under the direction
of the French engineer scientist G. Riche de Prony. The
work was planned out according to Adam Smith's principles
of the division of labor, and a large number of unemployed
hairdressers were used to fill out the numbers on the sheets
by adding and subtracting. Although the tables were com-
pleted in 1801, their size made publication a costly task and
it was never done, despite the fact that printing was started
more than once and various efforts were made over the
years to find finance.'''
Now one of these occasions occurred during the late
1810s, when the detente between Britain and France after
the fall of Napoleon opened up the possibility that the
British government might share the expenses. Although the
plans did not come to fruit, they were proposed just at the
time when Babbage was thinking about mechanical compu-
tation for the second time and must have remained in his
mind, for he described de Prony's project in his ''letter'' of
1822 which secured the original governmental grant (pp.
2/10-12).'' It seems that that project, with its extension ad
absurdum of manual computation, helped Babbage to con-
ceive of the need for an automated alternative.* Further, de
Prony so conceived his tables that the hairdressers only had
to add and subtract over differences of various orders.
Babbage's variant was his projected Difference Engine no.
1 (as he later named it), working the same way mechanically
up to S' and back again, ''either proceeding backwards or
forwards,''as Dionysius Lardner putitin 1834(p. 2/167)%+
The analogies here extend not only to de Prony; f and f
(see the earlier subsection on functional equations) also
readily come to mind. Another analogy is shown in the
figure (in the last section). concerning the layout of the
wheels. Babbage himself reported yet another, in a paperon
mechanical computation. He posed there a question con-
cerning the determination of the digit in any place in the
array (rather like the numbers of x's and v's in Equation 7).
and in 1822 he came up with a difference equation ''which
had impeded my progress several years since, in attempting
the solution of a problem connected with the game of chess''
p.233)7
In 1848 Babbage thought out a simplified version of this
engine. He denoted it in his autobiography as ''Difference
Engine no. 2'' (pp. 11/75-85)' It has recently been con-
structed. as a new-old machine.'' Little of the relevant
paperwork has been published so far; I would expect it to
provide further evidence supporting the claim.
In 1834 Babbage came to his next great idea: a machine
that would give commands as well as execute them (p.
11146).'' The quotation above suggests that with this ex-
tended algorithmism he had a glimmering of mechanical
recursion, as it was to be conceived (in electrical and elec-
tronic contexts) a century later, the whole of arithmetic.
numbers and operations with them. Thus was born the
Analytical Engine.''
Many analogies were brought into play when Babbage
developed this engine. For example, the Jacquard loom
cards which ran it were of three kinds, for numbers, vari-
ables,and algebraic operations - as one would expect from
a mathematician who distinguished f from z.f They were
used to give compound instructions (like functions fg. .-).
and they were able ''to revolve back wards instead of for-
wards,'' in the words of Lady Lovelace in 1843 (p. 3/135y*
so close to Lardner's quoted in the previous subsection.
Again, presumably drawing on the experience of printing
tables, in 1837 Babbage represented the four basic arithme-
tic operations on cards of four different colors (p. 3/52)P%
In the mechanism Babbage distinguished between the
''store,'' which held operands and results between opera-
tions, and the ''mill,'' where they were sent to execute arith-
metic operations (see A.G. Bromley's article,'' p. 198).
Another mathematical analogy comes readily to mind:
Babbage did not miss this analogy. On the contrary, in
his autobiography he foresaw actual applicability of the
engine to functional equations (p. 11/325)%
Calculus of Functions
This was my earliest step, and is still one to which
I would willingly recur if other demands on my time
permitted... It is very remarkable that the Analytical
Engine adapts itself withsingular facility to the devel-
opment and numerical working out of this vast de-
partment of analysis.
With her usual acuteness, Lady Lovelace had also stressed
this possibility in 1843 (p. 3!116);'
In studying the action of the Analytical Engine, we
find that the peculiar and independent nature of the
considerations which in all mathematical analysis be-
long to operations, as distinguished from the objects
operated upon and from the results of the operations
performed upon those objects, is very strikingly de-
fined and separated.*
Menabrea had gone a little too far in his 1842 account of
the engine. In a fit of outdatedness he invoked Lagrange's
belief(Equation l)in the Taylor series to stress the (alleged)
generality of its range (p. 376).'* Neither Lovelace nor
Babbage picked up this detail in her transiation (p. 3/1071'%
and earlier we saw that Babbage may not have known of
Cauchy's refutation of the belief.
In an 1826 paper on ''expressing by signs the action of
machinery'' (akin to the paper on mathematical notation'?
cited earlier, incidentally), Babbage varied his use of
Carnot's curved and straight overbars by deploying vertical
lines and curled left brackets to distinguish different types
of motion of parts of Difference Engine no. I (pp. 3215-
216)''' Later, in his account of the Analytical Engine, he
used Lagrange-like predashes as in Equation 1 to distin-
guish the axes; for example, in 1837, F, 'F, and''F were used
(p.317).Later,in the pamphlet'''of 1851, he lettered parts
of the engines on his working drawings in a manner extend-
ing this practice to sub-, super-, and all-over-the place indi-
ces, which indicated the type of part involved and its rela-
tionship to other parts,
Most important of all for semiotics, Babbage developed
a ''mechanical Notation'' for all his engines, by means of
which ''the drawings, the times of action, and the trains for
the transmission of force, are expressed in a language at
once simple and concise'' (p. 11179) It included rules on
using upright,italic,and small-font letters for different kinds
of referents (pp. 11I107-110)' Had he managed to construct
his engines as envisaged, he might well have developed
these ideas further in producing the envisaged printing
mechanisms. There are obvious cross-influences between
these concerns and his printing of mathematical tables (dis-
cussed in the earlier subsection on mathematical tables).
Babbage's style is evident in concerns other than math-
ematics and computing, as we shall now see.
It is not a bad definition of man to describe him as a
tool-making animal.
Babbage, 1851 (p. 10/104yy
To the modern view it is an irony that Babbage, much
concerned as he was with various applications of probability
and statistics, failed to notice their place in production
engineering. Instead, the semioticist won:''Nothing is more
remarkable, and yet less unexpected, than the perfect iden-
tity of things manufactured by the same tool,'' he wrote in
1835 (p. 8/47, italics inserted).P' We may have a clue here
about his failure to complete any of his engines: A lack of
understanding of production processes led him to waste
time and money on the excessively precise manufacture of
some of their parts.
Babbage's statement was made in the most influential
book that he published, On the Economy of Machinery and
Manufactures (the 1835 edition is cited here). There was
much concern at that time,especially among engineers, with
the mathematical analysis of economicquestions,especially
concerning optimization' and equilibriurmP (Chaps. 2-3).
However, in his usual lateral and algorithmic way, Babbage
focused instead upon the processes that take place: To the
extent that optimization is treated, it is usually in the form
oftime-saving or time-consuming. A wide variety of produc-
tion procedures and problems was given in the book, of
which one is worth noting here: an extensive account of
Adam Smith's principles of the division of labor and their
use by de Prony to manufacture his logarithmic and trigo-
nometric tables (pp. 8/124-126, 135-139)P
Algorithmicthinkingisevidentin Babbage's ideas on the
postal services. His proposal ''for transmitting letters en-
closed in small cylinders, along wires suspended from posts,
and from towers or from church steeples'' (p. 111447) has
the characteristics of compounding and reversal, and the
little model that he made around the mid-1820s in his own
house shows that he took it seriously. Again, in his book on
manufactures, he criticized the poor way in which letter
boxes were indicated and advocated a semiotically much
superior system: ''at each letter-box, to have a light frame of
iron projecting from the house over the pavement, and
carrying the letters G.P., or T.P., or any other distinctive
sign''(pp. 832-33)M
'*I was much struck with the announcement'' of F,
Arago's researches of 1824 ''on the magnetism manifested
by various substances during rotation,'' recalled Babbage in
his autobiography (p. 11/339)7 and he and Herschel re-
ported their own researches in a joint paper'' of 1825 and
Babbage in one of his own''? a year later. One may presume
that the repeated oscillations inherent in the phenomena
attracted the attention of this natural algorithmist.
In 1851 Babbage published his study of another case of
repetition, this time an optical one: He proposed occulting
systems for lighthouses,in which ''Itwould only be necessary
to apply a mechanism which should periodically pull down
an opaque shade over the glass cylinders of the argand [sic]
burners.'' Further, a lighthouse could identify itself by ex-
hibiting its identification number in a temporally semiotic
manner,in terms of the appropriate numbers of occultations
for each digit. For example, the lighthouse numbered 253
would signal
[time --]
0000000000.0 . 0000.00.0000.0-0.0000000000...
where the raised points indicate the (2, then 5. then 3)
interruptions of shining of the light (pp. 10/62-65).
Another late interest of Babbage lay in cryptography; it
exhibits algorithmism and semiotics very clearly, especially
in the transposition and rearrangement of the letters of the
alphabet. In attending to the coding and decoding, he
thought once again of going forward and backward. He did
notpublish much on it(principally an 1854article'').but O.I.
Franksen''' has shown recently from the manuscripts that he
gave it a great deal of attention. It also provides another
feature for the figure.
The next morning I breakfasted with Humboldt. On
the previous day I had mentioned that I was making
a collection of the signs employed in map-making.
Babbage, 1864 (p. 11I149y
Babbage's eclecticism is not as random as it might ap-
pear: The constant concern with algorithms, semiotics, and
algebraicthinking functioned together and gave his work far
more interconnections than are obvious at first. This is the
thesis proposed in the first section of this article, and it is
strengthened not only by the content of the cases exhibited
but also by their choice: That is, Babbage preferred to work
on problems and contexts in which they were prominent
rather than on the numerous other situations in which they
were not (so) evident.
A further common factor can be pointed out, as I fulfill
the promise of providing the figure. It shows four illustra-
tions of what I call an ''iterative array'': that is, some kind of
repeatable process or its product. Further, one passage from
a paper of 1819 on infinite series (see the subsection on
functional equations) proposed a mathematical analog:
namely, ''the method of expanding horizontally and sum-
ming vertically'' when a sequence of series were to be
summed together (pp. 1/248-249, 268).-*The wide range of
contexts highlights the strength of his liking for algorithmic
thought.
Babbage's position can be clarified by contrasting it with
that of his contemporary George Boole, another major
algebraist and a pioneer of logic. One might expect to find
that the two enjoyed fruitful contact, but in fact it never
developed, a point that has surprised some historians (see,
for example. D. MacHale's book,'' pp. 234235).
The two men did meet, in 1862, probably at the Septem-
ber meeting at Cambridge of the British Association for the
Advancement of Science. Babbage explained the Differ-
ence Engine and recommended that Boole study
Menabrea's 1842 article'* and learn about the loom-card
system. Boole hoped to meet Babbage in London after the
Cambridge meeting, but he was called back urgently to his
institution (Oueen's College, Cork). No significant con-
tact developed between them, but Boole enclosed an off-
print of a paper on probability theory, and this may have
been the stimulus on Babbage around that time to read
Boole's major paper'' of 1844 on differential operators. ''It
related to the separation of symbols of operation from those
of quantity,a question peculiarly interesting to me. since the
Analytical Engine contains the embodiment of that
method,'' Babbage noted (p. 11/105)'Nevertheless,despite
the quality of the paper,''There was no ready,sufficient,and
simple mode of distinguishing letters which represented
quantityfrom those which indicated operation''(p. 11/105)''
Boole's work on logic began to appear with a short
book'' of 1847. Babbage read it and annotated it with the
marginal remark ''This is the work of a real thinker''' (p.
244). This praise is itself noteworthy; for unlike Boole's
contributions to differential operators, his work on logic did
notarouse strong interest until the early 1860s, when Stanley
Jevons was the first to give it detailed scrutiny.t However,
Babbage did not make use of Boole's ideas developing his
engines. To a modern view this is a pity, for his thinking,
especially on the Analytical Engine, was rather weak on
logical matters.''' However, there are intimate and explicit
links between Boole's work on differential operators and on
logic,'? and it seems likely that Babbage would have found
(or did find) similar and thus unsatisfactory conflations in
Boole's logic, with the same symbol (deliberately) serving
both for the operation of selecting of objects to form a class
and for the (objectual) class itself. Indeed, while his criti-
cisms of Boole's 1844 paper are rather overstated, they are
quite accurate (when meantas criticisms)on Boole's algebra
of logic.
Conversely, although Boole praised in 1860 Babbage's
contributions to functional equations'' (p. 208), he would
have found repellent the mechanical aspects of Babbage's
engines. He had stated in his 1847 book'' (p. 2)
Algorithmic theories and methods occupied Babbage
from his youth to his dying days; they constitute a most
tuNnustual grotup of concerns for a scientist. Their origins and
drives must have been powerful.
The case of algebra is particularly instructive,for it came
to him very young. He recalled that he was ''passionately
fond of algebra'' when still a schoolboy (p. 11/18)'' and soon
afterward he was advocating Lagrange's approach at Cam-
bridge. For a research area he chose functional equations, a
perfectly reasonable choice but hy no means a frontline topic
at the time. Much more orthodox would have been, say,
differential equations and applications to mechanics. Yet he
went to that algorithmic theory and stayed there for several
years of productive work. Indeed, he never lost or regretted
his interest in functional equations, as we saw in his testi-
mony in the subsection on the Analytical Engine and his
attention to Boole.
What kind of explanation can we offer for the strong
place of algorithmism in Babbage? Sociological elements
make some contribution, in that from Woodhouse
through Babbage and Boole right to the end of the cen-
tury with A. Cayley and J.J. Sylvester, English mathemat-
ics showed a marked concern with algebras of one kind
or another. But such factors are very limited, for an equal
number of nonalgebraic English (near-) contemporaries
and successors can be specified: the later work of Her-
schel, for example, W. Whewell, G.B. Airy, G.G. Stokes,
and so on. The most similar case is De Morgan, whose
fondness for algebras started out in the common versions
and then passed through functional equations (see the
subsection on semiotics in Babbage's mathematics) to the
mathematical analysis of syllogistic logic.tt Peacock is
another, less significant, example.
Thus a personal explanation seems to be required, cen-
tered primarily on Babbage himself - 'psychological.'
maybe, though the scare quotes are there to scare off the
psychohistorians.* For some reason algebra came naturally
to Babbage, and the contexts of the time extended that
inclination into a lifelong interest in matters algorithmic and
semiotic.* In the quotation set at the head of this subsec-
tion, he stressed the importance of the semiotic aspects of
his early orientations. The quotation ''What is there in a
name?'' is the firstsentence of his autobiography.' And the
quotation on ''tool-making animal'' -- serving as not a bad
definition of man - is certainly a very good definition of
Babbage himself. Can we see him as a naturally algorith-
miclalgebraiclsemiotic mathematical thinker patched into a
practically oriented personality? Relative to the French
background,was he a fusion of the interests of Lagrange and
de Prony?
Material related to this article was presented at the Babb-
ageiFaraday Bicentenary Conference, held at Cambridge,
England, in July 1991: and at the International Conference
on the History and Prehistory of Informatics, which took
place in Siena, Italy, in the September following. Ouestions
asked on these occasions led to valuable additions. Com-
ments on a draft were received from M. Panteki and O.I.
Franksen, and two anonymous referees. For permission to
quote from Lovelace's letter I am indebted to Taylor &
Francis Ltd., and the St. Bride Printing Library.
Traditionally, there have been two general approaches
to performance measurement of computer networks:
modeling [1] and monitoring [2-4]. The modeling ap-
proach is appropriate when the network is not yet
operational. Two modeling techniques, analytical and
simulation, apply a workload to a model of the network
in order to derive performance parameters. An analytical
model is a static, mathematical approximation, whereas a
simulation is a computer program that models a network
and provides data about its dynamic behavior. There are
inherent problems in any modeling technique, primarily
because the model is a simplification of the real system.
Simplifying assumptions may compromise the extent to
which the model represents the actual behavior of the
network under real operating conditions. Another
problem is verifying the accuracy of the model and the
results. For the emerging distributed systems, modeling
techniques may provide only limited results.
The alternative to modeling is the monitoring ap-
proach, which provides real-time surveillance and con-
trol of an operational network. A monitor observes the
network, collects data, and presents results in a usable
form. Monitors can provide traces or profiles of network
transactions. Although useful for debugging purposes,
traces, or complete records of network traffic, impose
excessive storage and processing requirements on a
monitor. Thus, a profile made up of certain statistics
about network traffic is often more practical.
There are three basic types of statistical network
monitoring [5]: performance analysis, performance
verification, and network management. Through perfor-
mance analysis, the actual network performance is
measured and can be compared with that predicted by
analytical or simulation studies. Performance verification
ensures that a network meets its design specifications.
Finally, network management involves determining
whether a network is operating properly and efficiently.
These functions of network monitoring are outside of
the scope of the modeling techniques. Hence, monitors
become a vital part of the network in order to effectively
characterize its performance. A network monitor serves
as the agent for making measurements of appropriate
network parameters and summarizing the results.
This article discusses a network monitoring measure-
ment technique and tool in the context of the CSMA/CD
protocol for local area networks. The second section
briefly reviews this protocol and its performance im-
plications. The next section describes existing monitor
measurement techniqucs. In the fourth section, a mas-
ter/slave monitor measurement technique and the func-
tions and measurements that the monitor supports are
presented. Finally, in the last section, an experimental
implementation of the monitoring technique is described.
Carrier Sense Multiple Access with Collision Detec-
tion (CSMA/CD) is a medium access contention method
that defines the means by which two or more nodes
share a common bus medium. The CSMA/CD protocol is
described fully by the IEEE Standard 802.5. A brief
overview of its operation is provided in this section.
A node wishing to transmit listens to the medium and
acts according to the following rules [5].
Selection and measurement of the network parameters
are often dictated by the protocol of the operating
network. Certain characteristics of the CSMA/CD protocol
are significant, in particular, the slot time and the
retransmission scheduling algorithm. The transceiver
cable interface defined by the Ethernet specification is
also important for the measurement technique proposed
in the fourth Section.
The CSMA/CD standard requires a certain minimum
packet length in order to ensure reliable collision detec-
tion by all the nodes. The minimum packet length is the
number of bytes that can be transmitted during the
round trip propagation delay between the two furthest
points in the network. This delay is called the slot time.
Transmitting a packet for a slot time ensures that a
collision signal has time to propagate back to all nodes
involed in the collision while the nodes are still transmit-
ting
The retransmission scheduling algorithm defined by
the IEEE Standard 802.3 is binary exponential backoff.
When a collision occurs during transmission, the node
backs off and schedules a retransmission attempt. If
there are successive collisions, the node continues to
back off and reschedules transmissions until either the
transmission is successful or a maximum number of
attempts have been made. Thus, CSMA/CD specifies a
non-deterministic delay for accessing the channell
Finally, the transceiver cable interface is an important
interface in the network specification. When used in a
configuration, it ensures compatibility between physical
interfaces and the Ethernet. The transceiver cable, also
referred to as the Attachment Unit Interface (AUI),
interconnects the transceiver (which taps directly into
the coaxial cable) and the Ethernet controller of the
network node. The transceiver cable consists of five
twisted pair wires, each of which carries one of five
signals: transmit, receive, collision presence, control, and
power. In the measurement technique described in the
fifth section, the transceiver cable serves as the interface
between a monitor and a network node. Certain local
parameters at each node can be derived by monitoring
these signals.
Various monitor measurement techniques have been
proposed and implemented [2,5]. The following issues
should be considered when selecting an appropriate
technique for a given network.
Artifact-is the interference in a target system caused
by the introduction of a monitoring device. Besides
minimizing artifact, its magnitude should be known, so
that it can be removed from the measurements. This
ensures an unbiased indication of performance.
Artfical tmfficgenemton.-Traffic generators produce
traffic loads with known characteristics, thus facilitating
network testing and debugging. Artificial traffic genera-
tion can be used. to emulate high load. conditions,
produce repeatable and variable traffic patterns, and
extract timing information. For example, a traffic
generator can issue packets with periodic or Poisson
arrival rates, which facilitates comparison investigations.
Also, time-stamping of packets provides measures of
network traffic speed and node response time. If
generators communicate with a monitoring system, traf-
fic generation and data collection can be synchronized.
AAealdme analyss-The data collected by the monitor
can be either analyzed immediately (inn real time) or
stored for analysis later. When packets are transmitted at
a rapid rate, there is limited time to collect and analyze
data, which makes real-time analysis difficult.
Paarzmeters of interest-In order to decide which
monitoring approach to adopt (centralized, distributed,
or hybrid, which are described below), the parameters
to be measured must be defined. For a given network,
the best approach may change depending on the
parameters of interest.
The monitor measurement techniques may be divided
into three categories, depending on the location of
measurement: centralized, distributed, and hybrid.
The centralized approach is natural for broadcast net-
works. Typically, a modified interface taps onto the
channel to support the functions of the central monitor.
Two major types of centralized measurement techniques
have been proposed: the probe monitor and the spy
monitor [5]. These are depicted in Fig. 1.
The probe monitor injects packets into the network at
specified intervals and can record network parameters
for each injected packet, such as the channel acquisition
delay, transmission delay, and number of collisions. It is
an active monitor and thus introduces monitoring ar-
tifact. The monitoring resolution is dependent on the
frequency at which the traffic is injected and observed. If
the resolution is increased, the artifact increases as a
result.
The spy monitor is a special node dedicated to moni-
toring the network passively. It receives and analyzes all
of the packets on the network, and introduces no artifact.
For complete measurement, the spy monitor must be
capable of processing packets as they arrive. Hence, the
monitor must be equipped with sufficient processing
power and data storage space.
Although probe and spy monitors tend to be simpler
and less costly to implement than other types of monitors,
they cannot provide all of the desired information. Infor-
mation pertaining to a particular node interface cannot
be obtained. Also, central measurement biases some
timings, such as the arrival time of a packet onto the
network. An arrival recorded bv a central monitor does
not represent when the packet entered the channel. It is
offset by the propagation delay of a signal over the
channel. The monitor needs to account for this bias to
provide accurate measurements.
In the distributed approach, complete information
about network traffic at all nodes is available. Each node
captures data and periodically transmits its information
to a central analyzer. The central analyzer only analyzes
data received from the distributed monitors, since it
does not monitor the network. Two broad approaches
are adopted for distributed measurement: hardware
monitoring and software tmonitoring.
Hardware monitoring requires that each node monitors
traffic. This approach is more suitable for future network
designs, since it is expensive to modify the existing node
interfaces. Software monitoring, on the other hand, is
more flexible, because the software can be modified.
However, since nodes are typically of different types,
different software may be needed for each node. Also,
monitoring software may increase the processing load
and memory requirements of the nodes, and some types
of nodes may not be capable of meeting these needs.
Distributed monitoring, in general, has certain draw-
backs. Communication overhead results when transmit-
ting lrge amounts of data to a central location. Unless a
dedicated channel is used, each node must send data
over the network to the central analyzer, which intro-
duces artifact. Besides introducing artifact, using the
target network as an integral part of the monitoring
system could cause loss of the monitoring functions if
the network fails. Finally, synchronization of real-time
clocks at each node interface to coordinate timing
measurements poses a classical problem. The distributed
approach is depicted in Fig. 2.
The hybrid approach combines the essential features
of the centralized and distributed approaches, as shown
in Fig. 5. It allows accurate and comprehensive measure-
ment, while reducing modifications to node interfaces
and the amount of data sent to the central monitor. The
central monitor directly observes the network to collect
its own data and also receives data from each of the
node interfaces. It analyzes all data to calculate the
parameters of interest. Although artifact is introduced
onto the network if a dedicated channel is not es-
tablished, the artifact should be less than in the dis-
tributed approach since the central monitor locally col-
lects some of the necessary data.
Tables I and II summarize the features, advantages, and
disadvantages of the monitor measurement techniques
described above. The remaining sections of this article
describe a variation of the hybrid monitor measurement
technique that has certain novel features.
The previous section pointed out that the hybrid
technique is the most advantageous measurement tech-
nique in the sense that most of the measurements of
interest can be obtained without introducing severe
traffic overhead. However, in spite of its advantages, the
hybrid technique is impractical to apply in an operating
network, primarily because substantial changcs are re-
quired in both hardware and software for each node on
the network. To counter the drawbacks of existing hybrid
measurement techniques, this paper proposes a mas-
ter/slave measurement technique. Compared to existing
techniques, the master/slave approach differs primarily
in the following three areas: the location of the interface
between monitor and node, the use of an effective and
economical monitor bus, and the relationship between
master and slave monitors.
A major difficulty of applying an existing monitoring
technique in an operating network is establishing an
interface between the node and the monitor. Figure 4
depicts possible locations for this interface. Current
techniques typically place the monitor at locations
denoted by a, b, or c. However, in the master/slave
measurement techniquc, a monitor resides at the loca-
tion denoted by a'. As a result, this approach does not
introduce changes in the network nodes or in the
software running on the nodes.
The master/slave measurement system consists of four
basic components, which are illustrated in Fig. 5: 1)
master monitor, 2) slave monitor, 5) tapping cable, and
4) monitor communication bus. In this system, slave
monitors are connected to the transceiver cable (Attach-
ment Unit Interface, or AUI) through the tapping cable.
The slave monitors function passively (that is, they do
not interfere with the normal operation of each node ),
and they only receive data from the tranceiver cable. The
data include collision signal, received data, and transmit-
ted data. All received data are processed and reduced by
the slave monitor to minimize the monitor's memory
storage requirements and also the load on the monitor
bus. The processed statistics of each slave monitor are
collected by the master monitor via the monitor bus. No
statistics are transferred to the master monitor over the
medium. The monitor bus is a serial, asynchronous bus
whhich interconnects the master and slave monitors.
A master monitor collects statistical information from
the slave monitors via the dedicated communication bus.
A network user or manager can interact with the master
monitor to obtain network information. The information
available from the master monitor includes channel
utilization, total offered traffic, network delays, and in-
dividual node information. Analysis of the network delays
using artificial traffic generation and statistical informa-
tion is an essential function of the master monitor. The
functional block diagram of the master monitor is given
in Fg. 6.
A slave monitor passively monitors signals in the AUI
of an Ethernet node. The signals available for inspection
by a slave monitor are transmit data, receive data, and
collision detect. From these signals, performance infor-
mation is collected, processed, and transferred to the
master monitor by the slave monitor. The types of infor-
mation include the following:
A network user can then access this information via the
user interface in the master monitor. Figure 7 shows the
functional block diagram of a slave monitor.
Two different configurations can be established for
the master/slave measurement technique depending on
the network size and purpose of the measurement: 1)
fully distributed monitor system, and 2) locally dis-
tributed monitor system. Figures 8 and 9 depict the two
different architectures, respectively.
If the network size is small or the purpose of the
measurement is for system set-up or exhaustive testing,
then the fully distributed monitor system is the best
approach. In this configuration, every node is associated
with a slave monitor, and only one master monitor
exists.
When the network is lArge (for example, hundreds of
nodes), the fully distributed monitor system may not be
practical to implement. In this case, the locally dis-
tributed monitor system can be configured so that the
monitoring of the nodes is partitioned into smaller, local
areas as shown in Fig. 9. Only certain nodes are allocated
slave monitors. Since the slave monitors are designed as
plug-in modules, they can be easily moved from one area
to another. This configuration can provide a representa-
tive view of overall network performance if the nodes
with monitors are selected appropriately. Monitors should
be placed at nodes which are known to possess ypical
or desired types of traffic.
The hierarchical of structure of this system permits it
to be used in a multiple network environment. Multiple
master monitors can be set up, one per network, each
with its own set of slave monitors. Figure 10 shows a
configuration with three master monitors: MM1, MM2,
and MM3. MM3 is the main master monitor and receives
network information from MM1 and MM2.
An implementation of the master/slave measurement
technique is currently under development for use with
an operating AT&T 3BNet network [6] at Iowa State
University. The 3BNet interconnects certain AT&T Unix-
based computers and network-compatible peripheral
devices to form a local area computer network. The ISU
3BNet currently consists of one 3B20, two 3B5, and
twelve 5B2 computers. All are located in the same
building and connected over a 50-ohm coaxial cable
operating at ten megabits per second. The topology of
the network is shown in Fig. 11. A 3BNet communica-
tion protocol is implemented on top of the Ethernet
protocol.
The 3BNet network provides an experimental environ-
ment in which to implement and evaluate the mas-
ter/slave measurement technique. However, the tech-
nique itself is independent of any particular Ethernet
configuration. The distinctive components are the slave
and master monitors.
The slave monitor is a custom-built hardware device
consisting of the functional blocks shown in Fig. 12. It
passively taps onto the network at a node interface. As
shown in the figure, a simple plugin ''tee'' connector
attaches the slave monitor to the Medium Access Unit
(MAU) in the node interface. This tap is compatible with
the IEEE Standard 802.3 Attachment Unit Interface (AUI)
specifications. The slave monitor can be connected and
disconnected without interfering with the network opera-
tion,
The slave monitor has three primary functional com-
ponents: 1) an isolator unit, 2) a phase-locked loop
circuit, and 5) a processing unit. A standard, low-cost
processing unit which provides a small memory and
Supports serial communications was selected, namely
the MCS-8051 processor [7]. Since the storage require-
ments of the monitor are not very extensive, no addi-
tional memory unit is needed, although memory can be
added with minimal cost. All monitor software can be
stored in the four Kbytes of program memory on the
MCS-8051. An RS-422 driver interfaces the processing
unit with the serial inter-monitor bus. The serial port is
used to transfer data between the slave monitor and the
master monitor. RS-422 was selected for several reasons.
RS-422 differential line drivers provide good distance
and data rate characteristics. A multiple drop topology
for the inter-monitor bus is supported by the ''wired-
OR'' logical connection capability of the RS-422 inter-
face. Finally, communication over twisted pair calbles is
inexpensive.
The isolator unit performs two functions: 1) provides
isolation between the slave monitor and MAU, and 2)
improves the fanout of the monitored signals. The phase-
locked loop offers timing-related information to the
processing unit. It can detect the start and end of a
packet, which defines the packet length. The state of the
collision presence signal can be used to derive a count
of the number of collisions. The processing unit calcu-
lates and records the following local parameters: transmit-
ted load, packet size, maximum and minimum packet
lengths, inter-packet transmission time, number of pack-
ets transmitted, number of packets received, packet
reception rate, and number of collisions. Statistics are
transmitted to the master monitor when the slave
receives a request from the master.
Control information provided by the master monitor
assists the processing unit in its calculations. Control
information includes network data rate, the required
format of the data packets sent by the slave monitors to
the master monitor, and tuning parameters. The tunitng
parameters correct for measurement discrepancies found
by the master monitor when the statistics provided by
the slave monitors do not match the statistics kept by
the master monitor.
The master monitor is an off-the-shelf hardware device
based on a single-board Ethernet controller. It serves
four essential functions: 1) monitoring data on the net-
work, 2) generating traffic on the network, 5) setting up
and polling the slave monitors over the serial inter-
monitor bus, and 4) presenting network parameters to
the user.
Figure 12 depicts the modules comprising the master
monitor. The main components are an Intel 82586 LAN
Coprocessor [8], an Intel 82501 Ethernet Communica-
tions Interface [8], a standard Ethernet transceiver, a
processing unit, a memory unit, and two serial interfaces.
The shared memory stores any data transmitted or
received by the master over the network or the inter-
monitor bus. An RS-422 serial port serves as the interface
between the processing unit and the inter-monitor bus
while an RS-232 serial port sets up communication with
the user. The master monitor can be attached to a
display terminal or a personal computer, depending on
the user's needs.
An 82501 Ethernet Communications chip provides the
interface between an 82586 LAN Coprocessor and the
network. The 82586 Coprocessor manages the medium
aCcess mechanism for the master monitor. This scheme
relieves the processing unit from packet reception and
transmission overhead. However, the processing unit
handles the communication with the slave monitors over
the inter-monitor bus.
The processing unit is responsible for configuring the
slave monitors and resolving any traffic-mismatch measure-
ment problems. That is, when statistics kept by the
master are not consistent with statistics received from
the slaves, any slave monitor can be dynamically recon-
figured so that the necessary requirements are met. For
example, each slave monitor reports the number of
collisions at its station to the master monitor. The mas-
ter, however, observes the network and maintains its
own count of the total number of collisions. If the
master detects a discrepancy between its count and the
sum of the received slave counts, then it can invoke
diagnostics to check the operation of the slaves. Other
discrepancies might occur in the measurements of colli-
sion time or transmission time recorded by the master
and slaves, which can be corrected by fine-tuning the
slaves to improve the accuracy of local parameter
measurement.
The master monitor can calculate approximate net-
work delays by injecting its own packets onto the net-
work bus and asking receiving nodes to send back the
same packets. It time-stamps injected packets within a
resolution of ten microseconds. The delay is calculated
based on the time stamp of a packet (the time at which
the packet was sent) and the time at which the packet is
received back. Delay measured in this way is representa-
tive of actual delays only if a large user population is
asSumed. If the number of users is small, the traffic
generated by the delay monitoring packets becomes
significant and introduces an error in the delay calcula-
tions. However, when there is a large number of active
users, delay packets have little effect on total network
traffic, and any error is small enough to be neglected.
To handle the computational load of the master
monitor, an Intel 8086-based single board computer can
be used with the Ethernet controller board. An Intel
Multibus (IEEE Standard 796 bus) interface, included on
the boards, supports multiprocessing and DMA transfers.
The extra processing unit can perform statistical com-
putations, data analysis, system control, and user in-
put/output for the master monitor.
In section three, several existing monitoring tech-
niques were examined and contrasted. This examination
led to the development of a master/slave monitoring
system. This monitoring system has several advantages
over the existing systems. By using a passive tap, the
slave monitors provide distributed measurement without
introducing changes in the network nodes or in the
software running on the nodes.
The slave monitors communicate with a master
monitor via an economical multidrop twisted pair net-
work. This network provides sufficient bandwidth without
the overhead and cost associated with a standard net-
work. The slave monitors consist of a small number of
offthe-shelf components. These monitors process the
data locally and send only the processed results to the
master monitor. The slave monitors can be reconfigured
by the master monitor to provide maximum flexibility.
The master monitor provides all the functions of a
central monitoring system. The monitor can generate
traffic with time stamps and can use the information
obtained from the slave monitors to provide detailed
node traffic information.
This system is under development at Iowa State Un-
iversity and will be used for both teaching and research.
The monitor system will assist in the development of a
distributed file system based on a large collection of
UNID machines connected via Ethernet.
Doug Jacobson is an Assistant Professor of Electrical En-
gineering and Computer Engineering at Iowa State University,
Ames. He received his B.S. in Computer Engineering in 1980,
his M.S. in Electrical Engineering in 1982, and his P.D. in
Computer Engineering in 1985 fom ISU. From 1981 to 1985,
he was a Senior Design Engineer at the Iowa State University
Computation Center. Hc is currently teaching in the area of
networking and data communication, including courses with
NTU. Areas of research include network performance and
protocol verification and spccification.
Sunll S. Gahomde is a Pn.D. student in Electrical Engineer-
ing and Computer Engineering at Iowa State University, Ames.
He recetved his BTech. in Electrical Eagineering from the
ndian Institute of Technology, Ksragpur, India, in 1985, and
his M.S. in Computer Engineering from ISU iin 1985. His re-
search interests are in the area of voice/data integration on
computer networks.
Jon-Nyun Klm is a Phh.D. candidate in Electrical Engineer-
ing and Computer Engineering at lowa State University, Ames.
He received his B.S. in Electronic Engineering from Seoul Na-
tional University, Seoul, Korea, in 1978, and his M.S. in Com-
puter Engineering from ISU in 1986. He is currently invotved in
the performance analysis of local area networks and protocol
specification and verification.
Jai Yong Lee is a P.D. candidate in Electrical Engineering
and Computer Engineering at Iowa State University, Ames. He
received his B.S. in Electronics Engineering from Yon-Sei Uni-
versity, Seoul, Korea, in 1977, and his M.S. in Electrical En-
gineering from ISU in 1984. From 1977 to 1982, he was a
research engineer at the Agency for Defense Development of
Korea. He is now a temporary Assistant Professor of EE/CprE at
ISU. His current research interests include local atea networks,
integrated voice/data communication systems, and protocol
design and analysis.
Dlane Thlede Rover is a Ph.D. student in Electrical Engi-
neering and Computer Engineering at Iowa State University,
Ames. She received her B.S. in Computer Science from ISU in
1984, and her M.S. in Computer Engineering from ISU in 1986.
From 1984 to 1986, she was an Ames Lb Associate at the
Microelectronics Research Center, and since 1985, she has
been an IBM Graduate Fellow. Her research interests include
parallel computer architectures and performance of computer
systems.
Mansoor Sarwar is a Ph.D. student in Electrical Engineering
and Computer Engineering at Iowa State University, Ames. He
received his B.Sc. in Electrical Engineering from the University
of Engineering and Technology, Lbore, Pakistan, in 1981, and
his M.S. in Computer Engineering from ISU in 1985. His inter-
ests include functional programming l4guages, parallel pro-
cessing, computer architecture, and local area networks.
Muhammad Shafiqq is a Ph.D. student in Electrical Engineer-
Ing and Computer Engineering at lowa State University, Ames.
He received his B.E. in Electrical Engineering from N.E.D.
University of Engineering and Technology, Karachi, Pakistan, in
1982, and his M.S. in Computer Engineering from ISU in 1986.
His interests include local area networks, protocol develop-
ment, and embedded computer systems.
HE motivation for this paper is the observation that a
scene containing more than one object most of the time
cannot be segmented only by vision or in general by any
noncontact sensing method. Visual information may be suf-
ficient to accurately segment simple objects and nonoverlap-
ping scenes. However, in general, it is not sufficient for
random heaps of unknown objects.
If no a priori knowledge is available, the vision system
cannot reliably distinguish between overlaps caused by two
different objects in the scene and overlaps caused by a single
self-occluding object. A flat rigid object supported by and
totally occluding another smaller object may be recognized as
a large box-shaped object. Similarly, a flat nonrigid object
supported in the middle by a smaller object may be recog-
nized as convex, while if it is supported at the edges by more
than one object, it may be recognized as concave.
Therefore, machine vision alone (or any noncontact sens-
ing method) is not sufficient for segmentation and recogni-
tion. An exception to this may be the case when the objects
are physically separated so that the noncontact sensor can
measure this separation or one knows a priori a great deal
about the objects (their geometry, material, etc.).
The traditional approach is to segment the noncontact
sensory information (range, intensity, etc.) regardless of
scene complexity. Then, based on the outcome of segmenta-
tion, to interpret the scene and recognize the objects. The
problem with this approach is that reliability decreases when
scenes become more complex and when a priori assumptions
are removed.
Our approach is different. Instead of trying to deal with an
ever increasing visual scene complexity, we use the manipu-
lator to make the scene simpler for the vision system. Our
paradigm is analogous to having the hand help the eye when
interpretation of visual information is ambiguous, or when
the scene is visually complex.
Our system is iterative because random arrangements of
objects form layers. Due to our noncontact sensor arrange-
ment, only the top layer of the heap is visible at any given
time and the objects are removed from the scene one at a
time. In general, the system must sense and manipulate more
than once for every random scene.
The system is interactive because the vision system may
request a manipulatory action to resolve an interpretation
ambiguity, to reduce visual complexity, or to grasp and
remove an object from the scene. The manipulatory action
must be monitored by the noncontact sensor (vision system)
as well as the contact sensors (force/torque) in a closed loop.
Our assumptions are:
The domain is the class of irregular parcels and pieces
(IPP) found in a post office environment. The class consists
of rigid and nonrigid flats, boxes, tubes, and rolls. The
objects have different weights, sizes, colors, visual surface
textures (address labels, stamps, and other markings), vary-
ing porocity, coefficients of friction, and rigidity. Because
many of these objects are not rigid, their true geometric
shape cannot be measured; it is rather a function of where the
object is in a random heap, how it is supported by its
neighboring objects and other objects that it supports. The
heaps are formed by emptying a sack of an unknown mixture
of IPP's on a conveyor.
Our immediate goal is to physically segment and sort a
random heap of IPP's into several output streams of single
similar-shape objects. In other words, our first goal is to
disassemble random heaps of separable objects (held together
by gravity and friction) into three output streams. The first
stream contains two-sided planar objects (flats). The second
stream contains three-sided nonplanar objects (tubes /rolls).
The third stream contains six-sided planar objects (boxes).
We view this physical segmentation of disassembly prob-
lem as a subclass of the more general disassembly problem,
which we will address in the future. We believe that the
solution to the general disassembly problem is active sensing
[3], as opposed to the traditional static analysis of passively
sampled data. The problem of active sensing can be stated as
a problem of intelligent control strategies applied to the data
acquisition process that will depend on the current state of the
data interpretation including recognition. This approach is
gaining more and more recognition in the literature, [2], [5],
[10], [21]. In this paper we shall describe our model of
segmentation via interaction between vision and manipula-
tion. We will generate several segmentation strategies. We
will describe the experimental system and the experiments.
The model of segmentation has the following components:
models of sensors, models of actions, a task/utility model, a
world model, and a control model. The segmentation process
is formulated in terms of graph-theoretic operations that are
mapped into corresponding manipulatory actions.
Sensor models include the characterization of the noncon-
tact sensor such as the spatial resolution, signal-to-noise
ratio, and their like; the physical parameters of the different
end effectors, such as a vacuum suction cup; the size of a
spatula for pushing objects; the span of a gripper; and the
maximum allowable forces and torques. Models of objects
are specified in terms of their geometry, size, and substance.
Our world consists of random arrangements of objects
called heaps. Object models are boxes, flats, and tubes/rolls.
A heap is represented by a directed graph. Objects in the
heap are represented by vertices, and the on-top-of relations
among objects are represented by directed edges. A scene is
a partial view of a heap as sensed by the noncontact sensor.
A scene is represented by a directed graph, where surface
segments are represented by vertices and the on-top-of rela-
tions among the surface segments are represented by directed
edges.
It is important to emphasize that, in general, the diagraph
representing the heap is different from the graph representing
a scene. This is because the scene diagraph represents spatial
relations of only the visible surface segments, i.e., as they
appear through the visual sensor, which may not always be
the same as the physical objects. The true physical arrange-
ments of objects in the heap (i.e., the heap diagraph) is not
known, unless given a priori. Only the scene diagraph is
measurable and constructable from noncontact sensory infor-
mation.
In this paper, our task is to measure and construct the
scene diagraph and to use the manipulator to decompose it. In
future work we will show how to use manipulatory and
exploratory actions to recover the true part-whole relation-
ships (i.e., to compose an object diagraph from its scene
diagraph).
Task models include the final goal of the process. An
example of a final goal may be the empty scene. Intermediate
goals may be those scenes that are simply measured by a
cost/benefit function. This cost/benefit function entails the
cost of performing the particular manipulation, and the bene-
fit is measured via the estimate of the outcome of the
manipulation with respect to the final goal, i.e., emptying the
scene.
There are two types of actions: sensing actions (i.e., data
acquisition) and interpretation actions (such as: look, and /or
feel), and manipulatory actions, such as: pick, push, pull,
and shake. The purpose of the manipulatory actions is to
exert physical disturbances into the scene, being either global
(shake) or (push, pull). In view of our formulation of the
segmentation problem as a graph generation /decomposition
problem, we classify the manipulatory action in relation to
the operation that applies on the diagraph. There are two
such operations: the vertex removal, which means, in terms
of manipulation, removal of an object from the scene, and
edge removal, which in turn translates into object displace-
ment in the scene so that the on-top-of relationship does not
hold any more between the two objects. An isomorphism
exits between the manipulation actions and graph decomposi-
tion operations [23].
Our approach is to close the loop between sensing and
manipulation. The manipulator is used to simplify the scene
by decomposing the scene into visually simpler scenes. The
manipulator carries the contact sensors and the manipulation
tools to the region of interest and performs the necessary
manipulatory movements that will result in a visually simpler
scene. The control model deserves special attention and is
described next.
The control model is a nondeterministic finite-state Turing
machine (NDTM) and is shown in Fig. 1. The physical world
(scene) is the tape of the machine, the read actions are the
sensing actions, and the write actions are the manipulatory
actions. The model is a Turing machine because the manipu-
lation actions constantly change the physical environment
(tape) and hence its own input. The model is nondeterministic
because of the nonpredictable state of the scene after each
manipulatory step. From this of course follows also the
nondeterministic control of actions. In addition to the nonde-
terminism of the control strategies, the machine has finite
states, which are determined by the finite numbers of recog-
nizable scenes and the finite number of available actions.
This model is quite general, providing that one can quantize
the scene descriptions and the sensory outputs into unique
and mutually exclusive states, and of course one has only a
finite number of manipulatory actions.
As is well known, the nondeterministic finite-state automa-
ton (NDFSA) that controls the Turing machine is defined as a
quadruple (I, O, S,T) where:
Fig. 1 describes the sensing and manipulation interaction
for segmentation. Relating this diagram to the NDFSA, we
shall describe in subsequent subsections the inputs, outputs,
states, and the transition function, i.e., the control, respec-
tively. There are several advantages to the formalism of the
nondeterministic finite-state Turing machine.
I) Inputs: As indicated above, the inputs come from
sensors. In our current implementation, the sensor is a laser
range imaging system (noncontact sensor). The scene is
segmented into spatially connected surface regions. For each
region, we compute the position of the center of gravity, the
orientation of the surface normal at the center of gravity, an
estimate of the size of the smallest parallelepiped bounding
the region, and an estimate of the maximum curvature. From
these measurements, the objects are initially classified into
one of three generic shapes such as: flat, box, and tube/roll.
These are four object models.
The on-top-of relation between all pairs of visible regions
in the scene is computed and the directed graph representing
this relation is constructed. Vertices represent visible, con-
nected, surface regions. Directed edges represent the spatial
relations between the vertices. See Figs. 2-5.
Top-most surface segments are important in physical scene
segmentation because they may belong to top-most objects in
the scene. Top-most objects are important because they usu-
ally have more surfaces exposed (more ways to be grasped).
The forces required to extract them from the scene are less,
and therefore the chances of loosing positional information
after the object is being grasped are minimized. Furthermore,
manipulating the top-most object keeps scene disturbances to
a minimum.
A partially dispersed scene corresponds to a disconnected
diagraph. An efficient algorithm based on ''fusion'' of adja-
cent vertices is given in [8]. A totally dispersed scene (as
well as a singulated scene) corresponds to a null graph (a
graph with vertices and no edges). Efficient graph theoretic
algorithms exist (testing the diagraph's adjacency matrix for
all zero entires) for singulation verification. Finding the
top-most objects in the scene corresponds to topological
sorting of the diagraph.
2) Outputs: There are two types of outputs. These are
sensing actions, (look, feel) and manipulatory actions (pick,
push, pull, shake, and stop). In this implementation, the look
and feel actions are only commands to take data. In our
future work, these actions will be more complex, i.e., the
system will choose its view point, sampling rate, resolution,
and other data-acquisition parameters. In addition, the cost of
the sensing actions will be included in the overall control
schema.
The manipulation actions are composed hierarchically from
simpler actions. Shake is the simplest action; it provides
global disturbance and displacement to the work place. On
the other hand, push and pick exert local disturbance and
cause local displacement of an object. In fact in our imple-
mentation, both the push and pick actions have two forms:
''push with spatula,'' ''push with suction tool,'' ''pick with
gripper,'' ''pick with suction tool.'' See Fig. 12 below for an
example of a ''pick with suction tool'' action. In addition,
each of these manipulatory actions is associated with an
''error recovery'' action.
The hierarchy of actions is in terms of composition of
complex actions from simpler actions and does not apply to
the execution of these actions. The hierarchy of action com-
position is given in [23]. An example of such hierarchy is
shown for the action: ''pick with gripper'' in Fig. 6. Each
node in the graph in Fig. 6 is a manipulatory action. Some of
these actions are modeled as deterministic finite state au-
tomata (FSA), while others are modeled as nondeterministic
finite-state automata (NDFSA). The lowest level in the hier-
archy of actions consists of very simple actions, such as:
''robot move to'' (RMT), ''robot move to while sensing''
(RMTS), ''gripper move to'' (GMT), ''gripper move to
while sensing'' (GMTS), and their like.
The advantages of hierarchical construction are modular-
ity, testability, and incremental growth. These actions (as
expected) use additional information from contact sensors.
Some of the contact sensors are as follows. Two force /torque
sensors (mounted on the gripper jaws) are used in closed loop
feedback during manipulation. Force feedback is used to
provide force servoing to the gripper, to sense collisions, to
measure the weight of objects, and to determine if an object
or tool is properly grasped. A finger position sensor is used
in a closed-loop feedback manner during manipulation. Posi-
tion feedback is used to provide basic position servoing to a
gripper and to refine size estimates of objects (computed
from vision). A vacuum sensor is used to verify proper grasp
and to differentiate small-size nonpenetrating cavities from
holes that penetrate an object. Note that all the contact
sensory feedback is carried out in a local reflexive mode
rather than in a planned mode with one exception, that is,
when a pathological state is detected.
3) States: This is a finite set of states describing the
environment of the Turing machine as perceived by the
sensors. If new sensors are added, the set of states is parti-
tioned to describe the scene as perceived by the additional
sensors. For example, if a sensor capable of determining the
''touch'' relations of objects in the scene is added, then the
set of five states, can be partitioned (a finer partition) to
describe both the ''touch'' and ''on-top-of'' relations. The
states of the machine are:
The goal of scene segmentation is the empty state. This
state must be not only reachable but also measurable with the
current sensors. In other words, for the machine to halt, the
system must have sensors to sense that the goal state has been
entered. In this work, the empty state is both reachable (see
section on strategies) and easily measurable (all range values
in the scene are zero, which means that no surface segments
and thereby no objects exist in the scene).
A specific place must be given to error states. They are
prioritized in order of severity (most severe first). For more
details, see [23]. The pathological states are: ''sensor dam-
aged,'' ''unable to get tool,'' ''lost tool,'' ''lost object and
tool,'' ''lost object in the scene, '' ''lost object away from
the scene,'' ''unable to reach object,'' ''unable to pick,'' and
''unable to push.'' As more sensors and actions are added
into the system, more, yet finite, pathological states must be
defined. When the machine enters one of these states, error
recovery actions are evoked.
4) State Transition Function: The control problem is
transformed into the problem of topological sorting of object
arrangements. The manipulation actions of object acquisition
(pick) and local displacement (push) are defined as decompo-
sition operations on diagraphs representing the on-top-of
relation of objects in the arrangement. The pick action is used
to break the vertex connectivity of the diagraph by removing
vertices. Several tools may be used to implement this action.
An object may be picked and removed from the scene using
the gripper, or it may be picked by selecting a tool (i.e., a
suction tool). The push action is used to break the edge
connectivity of the diagraph representing the on-top-of rela-
tion, Several tools may be used to implement this action. An
object may be pushed using the gripper, or it may be pushed
by selecting a push tool (such as a spatula or the suction
tool). Complete planning of the push actions is very compli-
cated [14]-[17] and requires knowledge of the friction coef-
ficients of all objects in the scene as well as knowledge of the
spatial relations of all objects in the scene to decide where
and how far to push.
In Section II, we described a nondeterministic finite-state
Turing machine as the control model for sensing and manipu-
lation for scene segmentation. This very general model is
sufficient to describe every strategy for the following reasons:
Let us recall that the ''read from tape'' are the sensing
actions, ''write to tape'' are the manipulatory and error
recovery actions, and the states are scene descriptions. Even
with the restriction that one can categorize every scene into
distinct classes (discrete states) we had to add the following
rules:
With the above rules and the theory described in Sections
II and IIII, we can compose several different strategies to
examine the validity and generality of our theory for scene
segmentation.
Strategy 1 is a noninteractive loop: (look, pick, look,
.. .). The control structure is shown in Fig. 7. The strategy
does not use local displacement (push). The general idea is to
look, pick the top-most object, and look again. If the scene is
ambiguous or unstable, it shakes the heap. If shaking fails, it
continues with the pick action. This strategy is simple and
very effective in dealing with scenes where all objects are
graspable with the set of acquisition tools. The strategy
eliminates ambiguities via the shake and pick actions. If the
shake action fails to remove the ambiguity, then nontopmost
objects are picked up. This causes objects to be lost during
acquisition. For the strategy to succeed, the sensor thresholds
must be raised to enable the system to tolerate higher torques
caused by picking objects off the center of gravity. When the
threshold is raised, the probability of tool losses increases as
well as the probability of damaging the sensors. Therefore,
the probability of entering the fatal error state is increased. If
the weight of the objects is low, the probability of damaging
the sensors (even if the system picks objects supporting other
objects) is low, and the strategy converges; see [23].
Strategy 2 is a noninteractive loop: (look, push until
dispersed, pick, look, . . . ). The control structure is shown in
Fig. 8. This strategy allows no interaction between the pick
and push actions. The only interaction allowed is when the
push action cannot reduce the number of edges in the associ-
ated graph any further. The strategy enforces a rather strong
partition on the manipulation actions. This shows up as a
serial plan where a single action is triggered from one state
and the automaton iterates until the ''look'' action brings the
automaton to another state. This strategy is very effective in
dealing with heaps of few, small-sized objects relative to the
workspace. As object size and number increases, so does the
number of objects pushed out of the scene and never picked
up. For a proof of convergence, see [23].
Strategy 3 is an interactive loop: (look, pick/push,
look, . . .). The control structure is shown in Fig. 9. The
central idea is to allow immediate interaction between the two
manipulation actions. Since pick is more effective than push,
priority is given to pick. Only if an object cannot be picked
up after several unsuccessful attempts is the next immediate
action to push that object, and to immediately return to pick
the next object (if one exists) or to the look action. This
strategy is most effective in dealing with heaps containing a
small number of top-most objects located far away from each
other. These types of heaps can be decomposed with the
minimum number of collisions.
Strategy 4 is an interactive loop: (look, push partially
visible, pick, push, look, . . ). The control structure is shown
.in Fig. 10. The central idea is to interleave the interaction
between the pick and push actions. In other words, the
strategy is to look, then execute a series of push actions and
displace partially visible objects out of the scene, then to pick
all topmost objects, then push all objects that the pick action
failed to remove after several attempts, and, finally, to look
again. This strategy segments the heap from both the top and
the sides. The partially visible objects are first pushed out of
the scene. This creates free space for future push actions and
minimizes the likelihood of collisions toward the borders of
the scene. By ordering the sequence of actions, we achieved
minimum interference between the manipulation actions and
we sequenced the execution of the look action to occur when
it is needed the most (after a series of local displacements). If
all the objects targeted for pick are graspable, they are
removed one by one, following the topological ordering of
the graph. This strategy is the most effective. It keeps action
interference to a minimum. It uses the most expensive action
(look) only when it is necessary (after a shake or a series of
push actions). This grouping and sequencing of actions has
performed very well for the majority of heaps and objects.
The strategy keeps the number of tool changes to a mini-
mum.
The system block diagram is shown in Fig. 11. It consists
of a range imaging system, a linear stage, a PUMA 560
robot, a LORD Corp. servoed instrumented gripper, a micro-
VACX-IDI computer, a support structure, several tools, tool
fixtures, and accessories such as a vacuum pump and a
solenoid valve.
The range imaging system communicates with the micro-
VACX via a video link. The magnitude of the video signal at
any point on the raster is proportional to the height of the
scene at that point. This signal is quantized and stored as an
array of 8-bit numbers (rangels).
The PUMA 560 robot communicates with the micro-VAX
via an RS-232 serial link and with the outside world via a
16-line input, 16-line output I/O module. This module is
used to control auxiliary devices such as the laser power, the
vacuum pump, and the solenoid valve. In addition, asyn-
chronous interrupts are supported by the robot language
VAL-II. The binary vacuum sensor is an example of such an
asynchronous interface.
The LORD Corp. gripper and sensors communicate with
the PUMA-560 via a 16-bit parallel I/O port, and with the
micro-VAX via an RS-232 serial link. The linear stage
communicates with the micro-VAX via an RS-232 serial link.
The range imaging system used in this research is de-
scribed in detail in [23]. None of the commercially available
range imaging systems were fast enough, easy to calibrate,
and inexpensive for our experiments. The range imaging
system is based on the invariance of the cross-ratio, a well
known ratio from projective geometry. In [23] we have
shown that by using this technique rather than the ''classic''
principle of triangulation it is possible to generate range
images in real-time without the need for further processing.
The range imaging system is comprised of a laser and
mirrors, a camera, and electronics for real-time generation of
range images.
1) The Laser Subsystem: The laser subsystem is com-
prised of a 5-mW helium neon (HeNe) laser light source
emitting at 632.8 hm. An oscillating mirror spreads the laser
beam into a straight line, thereby creating a plane of light. A
'positioning mirror, mounted orthogonal to the first, deflects
the plane of light to a desired position in the scene. Both
mirrors are mounted on galvanometers. See Fig. 13. These
galvanometers are under computer control. The parameters
under control are: gain of the oscillating galvanometer that
controls the length of the plane of light and the offset of the
oscillating galvanometer, the deflection angle of the position-
ing mirror was well as the offset of the positioning mirror.
2) The Camera Subsystem: The camera subsystem is
comprised of a Sony XC-39, B/W, CCD video camera with
384 horizontal by 480 vertical pixels, a 16-mm lens, and a
632.8-nm laser interference filter mounted in front of the
lens. See Fig. 14. The purpose of the interference filter is to
filter out ambient light and to allow only the laser stripe to be
seen by the camera. The camera is mounted so that the image
of the laser stripe incident onto the horizontal plane is seen
by the camera as a vertical straight line.
3)The Electronics Subsystem: The authors have designed
and built an electronic device capable of solving the cross-
ratio equation at TV field rates, therefore generating a line of
a range image in real time (15 360 range elements per
second). The details of the device will not be discussed here
because they are well beyond the scope of this paper. For
more details, see [23].
4) Range Image Resolution: In our prototype, the verti-
cal field of view of the camera is 320 mm. We have 240 lines
in every field. Therefore, the vertical size (A) of each range
element (rangel) is 320/240 = 1.34 mm. The horizontal field
of view of the camera is 345 mm. We use only 176 horizon-
tal pixels. Therefore, the ( Y) or horizontal rangel size is
345/176 1.96 mm. The maximum height is 384 mm,
quantized into 256 levels. This yields a height resolution of
384/256 = 1.5 mm.
5) Range Image Accuracy: In our prototype system the
measured range error, after quantization by the DT /2651
Data Translation image digitizer, was experimentally found
to be within (s 1/2 LSB), or (S 0.75 mm).
The linear stage is a subsystem that allows a scene to be
scanned under a stationary laser plane of light. This method
has two advantages over the method where the scene remains
stationary and a laser plane of light is scanning the scene.
The advantages are:
The linear stage, shown in Fig. 15, is comprised of the
following components: a linear slide, with a 25-in length of
travel, carrying a 20 x 20 in loading plate; a ball screw; a dc
servomotor; a dc tachometer; a servodrive; an incremental
position encoder; and a computer controller. The linear stage
can be programmed to move the loading plate at user-defined
speeds and accelerations. The positioning accuracy of the
linear stage is 0.004 in. The linear slide /loading plate were
mounted on a workbench.
The linear stage must be calibrated once and requires no
recalibration. The laser ranging system requires that the
linear stage transports the scene at constant velocity during
the range data generation cycle. Therefore, the loading plate
must reach constant velocity before its enters the scanning
volume. In addition, the velocity of the linear stage must be
set to match the range imager's speed to avoid shape distor-
tion, The correct velocity was found experimentally. This
was done iteratively by scanning a circular disk, computing
the moments of the resulting range image (an ellipse), and
modifying the velocity of the linear stage until the ellipse
became a circle.
The robot sensors used in this work are two force /torque
sensors mounted on the jaws of a LORD Corp. servoed
instrumented gripper (see Figs. 17 and 18), a gripper dis-
tance sensor mounted on the same gripper, and a vacuum
sensor mounted on the suction tool. Two tactile array sensors
are provided and constitute an integral part of the LORD
gripper. However, in the current implementation, no tactile
information was used.
[ZThe lorce/Torque Sensors: Each of the force/torque
sensors is a six-degree-of-freedom miniature sensor designed
to fit within each finger. The sensor transduction is based on
a-Maltese cross arrangement of strain gauge instrumented
beams, riding on elastomeric bearings. The sensor provides
force data at a resolution at 0.01 lb, and torque data at a
resolution of 0.01 in-lb, over a 0.0-20.0 Ib range.
In this work, force /torque information is used to control
the clamping force of objects and tools, to monitor the weight
of an object, to avoid collisions with objects or tools, for
off-axis gripping of objects and tools, as well as for slip
detection by monitoring shear forces in the plane of the
gripping surface.
2) The Gripper Position Sensors: The LORD gripper is
equipped with two position sensors. Sony Magnascale posi-
tion encoders are used. These linear encoders have a set of
pickup coils that ride on a rod containing minute magnetic
domains along its length. This transduction mechanism is
noise immune, and it is impervious to dirt and oil, The
position sensors are attached to the base of each finger and
provide a resolution of 1 um.
Position feedback is used to provide basic servoing infor-
mation, as well as to gauge something that the gripper is
holding. The minimum and maximum gripper openings are
16.0 and 50.0 mm, respectively. Therefore, objects larger
than the maximum gripper opening or smaller than the
minimum gripper opening must be handled by some other
tool,
3) The Vacuum Sensor: The first vacuum sensor used in
this research was a low-cost conductive-elastomer vacuum
sensor. A conductive membrane separated two air-tight
chambers; a pressure chamber and a vacuum chamber. The
conductive membrane was connected to an electrode. The
pressure chamber was left at atmospheric pressure, while the
vacuum chamber was connected to the vacuum hose. When
the vacuum exceeded a threshold of approximately 3 lbf/in',
the conductive membrane came into electrical contact with an
electrode inside the vacuum chamber and indicated the pre-
sense of vacuum. This sensor was found to be very sensitive
to environmental parameters such as temperature and humid-
ity, In addition, it suffered from hysteresis.
The second vacuum sensor used in our experiments is a
simple vacuum gauge, modified by the authors to operate as a
variable-threshold binary vacuum sensor. The threshold ad-
justability proved to be a useful feature in compensating for
small vacuum readings (Bernoulli effects) when the suction
tool was not attached to an object. In addition, it is possible
for a partial vacuum to be created when the suction tool
attempts to lift a porous object or an object partially within
the suction tool.
The main tool is a gripper (Fig. 17) attached to the PUMA
560 (Fig. 16). It is used to grasp objects that can fit within
the gripper such as tubes and small boxes. In addition, the
gripper is used to grasp additional tools such as a suction tool
and a pushing tool (spatula). The gripper is designed for use
on a PUMA 560, which has a payload capacity of 5 Ib. The
gripper's weight is 3 lb. Therefore, the maximum tool or
tool-plus-object weight must be kept under 2 lb. The gripper
has two independently controllable fingers to allow for off-axis
gripping. The maximum finger velocity is 6.6 in/s. The
gripper can generate a 20-lb maximum clamping force.
1) Suction Tool: The suction tool is used for lifting
objects out of the scene having large enough, approximately
planar, surface regions for the tool to be attached. The
suction tool is a 2.5-in-diameter collapsible vacuum cup
mounted on a hollow metal block. See Fig. 19. Attached to
the metal block is a vacuum hose. The vacuum hose is
connected to a vacuum sensor and the common port of a
two-way solenoid valve. One port of the solenoid valve is
connected to the vacuum port of a vacuum pump, while the
other port of the solenoid valve is connected to the pressure
port of the vacuum pump.
The solenoid valve is controlled by the robot controller.
When activated, it applies vacuum to the vacuum cup. If a
large enough nonporous object is attached to the vacuum cup,
the pump creates vacuum high enough to lift the objects used
in our experiments. When the solenoid valve is deactivated, it
applies pressure to quickly neutralize the vacuum and release
the object attached to the vacuum cup.
The suction tool weighs approximately 1 lb. Although the
tool is capable of lifting more than 5 lb, payload considera-
tions on the PUMA 560 restrict the maximum weight of
objects to be lifted using the suction tool to under 1 lb.
2) Push Action Tool: The second tool used in the experi-
ments is a spatula. This tool is used for pushing objects in the
scene. See Fig. 20. Both the suction tool and the spatula were
placed on specially designed tool holders, which guarantee
that the tools, when released, will always be placed at a
known robot location. See Fig. 21. The spatula weighs
approximately 4 oz.
The laser, mirrors, and camera were mounted on an
overhead structure and are elevated approximately 48 in
above the workbench. See Fig. 22. The structure allows for
adjustments in overall height above the workbench. In addi-
tion, the camera-to-laser distance, the laser's angle of inci-
dence, and the camera's angle are all user adjustable. Fi-
nally, the height of the camera above the workbench is
adjustable. The structure was made of aluminum tubes and
was stiffened to minimized vibrations during range image
generation.
The domain was mostly objects found in the mail stream
such as parcels, flats, tubes and rolls. A number of additional
experiments were conducted with objects containing holes,
cavities, and some porous objects. The weight of every
object was under 1 lb. Typical scenes contained 10 to 30
objects of different shapes and sizes thrown at random. The
objects formed heaps two to six layers deep. The average
time to segment and extract an object from the scene was
4,12 s. We performed each experiment approximately 100
times. The results of our experiments and observations about
the performance of each strategy are described in the follow-
ing subsections.
The purpose of this group of experiments was to evaluate
strategy 1. The strategy performed well on unstable, over-
lapped, and dispersed heaps. Difficulties were observed with
ambiguous configurations. The shake action was not very
effective in removing ambiguities. One reason is that the
action was implemented using the linear stage in a vibration
mode at maximum speed and acceleration. These speeds and
accelerations were not enough to produce a significant change
in the scene. Using the pick action to remove ambiguities
resulted in an increased number of tool and object losses. The
shake action failed to eliminate the ambiguities caused by
configurations of flats. This is because flats form stable
configurations. However, because flats are rather lightweight
and flexible, it was possible to use the pick action to break up
cyclic object configurations without many tool or object
losses.
In 100 experimental trials, strategy 1 failed to converge a
total of 20 times. Six of these failures were caused by
insufficient depth resolution in the range imaging system.
These six failures occurred when very thin flat objects formed
cyclic configurations. Nine failures occurred when the heap
contained large porous objects. The system entered into a
endless loop because in this strategy we use only the pick
action. Three failures occurred when the suction tool slipped
off the grippejaws, Finally, two failures occurred when the
force/torque sensors failed to respond.
In 100 experimental trials, the vision system identified an
average of 2.9 top-most surface segments per scene. The
theoretical average number of transitions through the state
diagram is (1 + 1/2.9) = 1.345. However, errors in com-
puting the true top surface centroid (caused by very dark
regions in the scene) resulted in frequent triggering of error
recovery actions. The average number of transitions per
object was found to be 1.67.
The purpose of this group of experiments was to evaluate
strategy 2. For the strategy to work efficiently, a very large
work space is required. This is because all overiaps must be
removed and the scene must become dispersed before any
pick action. In many unstable scenes containing a mix of
flats, boxes, and tubes, the shake action forced the tubes to
fall into the cavities of the heap. This stabilized the heap and
created more overlaps. The shake action was very effective in
dispersing heaps of cylindrical objects. In scenes containing
only tubes/rolls, the shake action removed all overlaps.
Strategy 2 performed well in heaps of few, small-sized
objects. As the object size and number increased, so did the
number of objects that were pushed out of the scene and
never picked up.
In 75 experimental trials, strategy 2 failed to converge 14
times. Six of the failures were caused by jams induced by the
push action and high friction coefficients between boxes and
the linear stage. Two failures occurred when two flat objects
were entangled and could not slide off the conveying mecha-
nism. Four failures occurred when the suction tool and the
push tool slipped of the gripper jaws. Finally, two failures
occurred when the force /torque sensors failed to respond.
Strategy 2 did not fail to converge when the heap contained
porous objects. These objects were eventually pushed out of
the work space.
If 75 experimental trials, the vision system identified an
average of 1.9 top-most surface segments per scene. The
theoretical average number of transitions through the state
diagram is (1 + 1 + 1/1.9) 2,53. However, errors in
computing the true top surface centroid (caused by very dark
regions in the scene) resulted in frequent triggering of error
recovery actions. In 17 trials we observed that the push
actions generated more complex scenes. The average number
of transitions per object was experimentally found to be 4.24.
The purpose of this group of experiments was to evaluate
strategy 3. This strategy has the tendency to produce addi-
tional overlaps when an object being pushed falls on other
objects in the next layer of the heap. This was not catas-
trophic because this overlap was detected during the next
look action. We have observed that if the objects cannot be
picked up by any tool, then the strategy degrades into a
sequence of unsuccessful pick actions followed by push ac-
tions. The system picked all objects with cavities and pushed
all large objects containing holes, as well as all porous
objects. The major drawback of this strategy is that if push
actions are interleaved with pick actions, then the push action
may displace other objects targeted for the pick action.
Therefore, the next pick action will most likely fail. Al-
though this is not catastrophic, it makes it necessary to
trigger another look action. Better planning of the push
actions may help to eliminate some of these problems. How-
ever, one has to keep in mind that even better planning will
not solve the problem, unless assumptions about the heap and
knowledge of the surface properties of all objects composing
the heap is introduced.
In 100 experimental trials, strategy 3 failed to converge 12
times. Eight of the failures were caused by jams induced by
the push action and high friction coefficients between objects
and the conveyor. Two failures occurred when the suction
tool and the push tool slipped off the gripper jaws. Finally,
two failures occurred when the force /torque sensors failed to
respond.
In 100 experimental trials, the vision system identified an
average of 2.5 top-most surface segments per scene. The
theoretical average number of transitions through the state
diagram, assuming that we pushed each object to the top
surface centroid (caused by very dark regions in the scene),
resulted in more frequent triggering of pushing and error
recovery actions. In 27 trials we observed that the push
actions generated more complex scenes. The average number
of transitions per object was experimentally found to be 3.1.
This strategy is the most effective and capable of handling
a variety of objects and heaps. One reason for its success is
the way it manipulates the heap: from the top and sides.
Another reason is that the actions are ordered and vision is
applied when needed the most, after global displacement
(shake) or a series of local displacements (a series of push
actions).
In 115 experimental trials, strategy 3 failed to converge six
times. Two of he failures were caused by jams induced by
the push action and high friction coefficients between objects
and the conveyor. One failure occurred when the suction tool
slipped of the gripper jaws. Finally, three failures occurred
when the force /torque sensors failed to respond.
In 115 experimental trials, the vision system identified an
average of 2.4 top most surface segments per scene. The
theoretical average number of transitions through the state
diagram (assuming that we push each object 50% of the time)
is (1 4 0.5 + 1/2.4) = 1.917. However, errors in comput-
ing the true top surface centroid (caused by very dark regions
in the scene) resulted in more frequent triggering of error
recovery actions. Only in three trials did we observe that the
push actions generated more complex scenes. The average
number of transitions per object was experimentally found to
be 2.3.
We introduced the paradigm of iterative interactive scene
segmentation and simplification via vision, manipulation,
force/torque, and other sensory input. The scene simplifica-
tion is based on graph decomposition operation of vertex and
edge removal. These operations are in turn defined isomor-
phic to the pick and push manipulation actions. We have
shown that the sensors can be used as the partial graph
generators and the manipulator as the decomposing mecha-
nism of this partial graph. The actions and strategies are
modeled as nondeterministic finite-state Turing machines that
decompose these graphs under sensor supervision. The
strategies converge (for theoretical proof, see [23]). If patho-
logical states are detected, then error recovery actions are
invoked.
We have integrated a vision system, a manipulator, and
force /torque and other sensory input into an experimental
robot work cell and conducted experiments to test conver-
gence, error recovery, and graceful degradation of four
different strategies. We have found that many of these strate-
gies can recover from pathological states, tolerate errors in
the sensory data, recover from unsuccessful actions, and
converge. What we have learned during this work is:
CELLULAR pyramid is an exponentially tapering stack of
arrays of processors (''cells''). Communication between cells
on successive levels of the stack allows global analysis of data
input to the base of the stack in log(base sie) steps. Cellular
pyramids support fast parallel algorithms for multiresolution im-
age analysis. See the books edited by Rosenfeld [31], Cantoni and
Levialdi [8], and Uhr [37]; for solving computational geometry
problems [25], with the image input to level 0, the base of the
pyramid.
Usually the cells on each level are connected to form a square
lattice but triangular or hexagonal grids have also been used
[2], [6], [14]. A cell on level 1 + 1 (the parent) is connected
to a K x K neighborhood of cells on level l (its children).
Neighborhoods associated with adjacent parents overlap by K-2
cells along both directions, yielding a fourfold reduction in
number of processors (twofold along each side of the square),
however, twofold reductions can also be achieved using modified
architectures [11], [18].
We restrict ourselves here to pyramids defined on a square grid
with fourfold reduction between successive levels. If an image
is input to the base of the pyramid, we can generate reduced-
resolution versions of the image at higher levels. Usually the
value of the parent is a weighted average of the values of its
children and the same set of weights is employed at every level.
Burt [7] defined rules for which the set of weights converges to
sampled Gaussians with increasing standard deviations. Optimal
weights have also been proposed [24].
Let the base of the pyramid be of size N + 2'' 2'*. Then the
lth level has size 2''' y 2*', so that the total number of cells
is less than }N%, The height of the pyramid, ie., the number of
levels, is n = log N. Many image analysis tasks which require
O(N%) operations on a single processor can be accomplished in
O(log N) on a cellular pyramid.
When a pyramid is used to reduce the resolution of an image,
features of the input image become smaller and move closer
together as one proceeds from the bottom level of the pyramid
to its apex. Thus at the appropriate level, local operations are
sufficient to detect and analyze global features (see [31] for
numerous examples). Reduced resolution representations are also
useful in image compression applications (e.g., [1]).
The case of K = 2, ie., nonoverlapping 2 x 2 neighborhoods,
is related to the quadtree description of an image [34]. The
reduced resolution representations can be severely distorted when
the input is shifted [36]. This problem is known in the quadtree
literature as the shift-dependence of the description [20]. In the
worst case a one pixel shift of the input image can lead to a
significantly modified quadtree structure [35].
The dependence of the low resolution representations on
the position of the sampling grid and the input image is also
important in image pyramid applications. The shift-dependence
phenomenon is not restricted to the case of nonoverlapping
neighborhoods. Bister [5] shows many examples of such artifacts.
The rigidity of the pyramid structure may give rise to ar-
tifacts when pyramids are used for tasks such as analysis of
line-drawings [19], object-background discrimination [10], or
compact object extraction [13], [16]. To compensate for these
artifacts, in many of these algorithms the parent-child links (or
link weights) are iteratively changed after the initial resolution
reduction stage. Recently Baronti et al. [4] proposed a modifica-
tion of this concept by increasing the size of the neighborhoods
associated with parents once an initial segmentation of the image
is obtained.
Another approach to compensating for the artifacts of pyramid
structure is to adapt this structure to the content of the input
image. In custom-made pyramids [28] weights are defined based
on a local ''busyness'' measure during the construction of the
reduced resolution representations. Rom and Peleg [29] and
Chassery and Montanvert [9] employed the Voronoi tessellation
defined by a set of randomly chosen lattice points to build the
coarsest representation of the image, which was then adaptively
refined. Note that the method computes the representations top-
to-bottom.
In this paper we also use irregular tessellations to generate
an adaptive multiresolution representation of the input image.
In our approach, however, the hierarchy of representations is
built bottom-up and is adapted to the content of the input image;
thus most of the properties of ''classical'' image pyramids are
preserved. We employ a local stochastic process to build the
lower resolution representations.
In Section II we introduce the graph formulation of irregular
tessellations and the concept of a stochastic image pyramid.
In Sections III and IV we give two applications of stochastic
pyramids: connected component analysis of labeled images and
segmentation of gray-scale images. Further issues are discussed
in Section V.
In image pyramids based on regular sampling, <.g., at points
on a square grid, artifacts caused by the rigidity of the sampling
structure are always present. On the other hand, an image
pyramid defined by an irregular sampling hierarchy can be
molded to the structure of the input image. Note, however, that in
such a pyramid the metrical relations among cells are no longer
carried implicitly by the sampling structure. A cell at level ! + 1
cannot know a priori where its neighbors on level I + 1 or its
children on level l are located relative to the original sampling
grid. To describe the structure of such an image pyramid it is
more appropriate to use the formalism of graphs.
The cells on level l of the pyramid are taken as the vertices of
an undirected graph G[l]. The edges of the graph describe the ad-
jacency relations between cells at level l. Thus G[l] = (V[l], E[l])
where V[l] is the set of vertices and E[l] is the set of edges. The
graph G[0] defined by the 8-connected square sampling grid on
level 0 is shown in Fig. 1(a). An example of a graph G[1] that
might represent level 1 is shown in Fig. 1(c).
We construct the pyramid by a sampling or decimation process.
Each level is constructed from the level below it by selecting a
subset of the vertices. Thus a vertex on any level can be regarded
as a vertex of G[0], the sampling grid of the original image. In
addition, when we decimate level I to construct level I + 1. we
associate each nonsurviving vertex with one of the surviving
vertices. Thus each vertex on level I + 1 is associated with a
set of vertices on level l (itself and the nonsurviving vertices
associated with it). Each of these vertices is in turn associated
with a set of vertices on level I - 1, and so on; thus a vertex o
any level is associated with a set of vertices, called its ''region,
in the original image. These regions define a tessellation of th
image.
If the pyramid is to be build recursively bottom-up we mus
define a procedure for deriving G[l + 1] from G[l]. Since th
number of vertices in G[l + 1] must be less than in G[l] we ar
dealing with a graph contraction problem. We must design rule
for:
In order to have any vertex (ie., cell) in the hierarch
correspond to a connected region of the image, the cell c[l -
1] g V[! 4 1] must represent a connected subset of cel'
(ca[l],ci[l],- ,c,[]} c V[I]. We shall use the conventio
cs[l] sz c[l + 1], ie., the surviving vertex of the subset is fir-
on the list. In pyramid terminology, [ca[l], ci[l], -,c,[]} a
the children of c[l + 1]. Note that the location of the parent o
the sampling grid of the original image always coincides wii
the location of one of its children.
In pyramid construction based on Voronoi tessellations [9
[29] the parents are initially chosen by a random process. TH
edges are given by the Delaunay diagram of the tessellation ar
the children are grid sites inside the tiles associated with tt
parents, The process can then be repeated for individual til
(by randomly choosing grid sites inside each tile) to obtain
finer description. Note that such a pyramid is built top-down ar
the definitions of parents and parent-children links are based c
nonlocal processes.
When we use graph contraction to construct a pyramid, tw
constraints must be satisfied if we want to employ only parall
local processes:
where c4, d are survivors on level I. Constraint (1) assures th
any nonsurvivor on cell at level I has at least one survivor in i
neighborhood and thus can be allocated to a parent by a loc.
decision. In the example shown in Fig. 1(b) this constraint
satisfied. Constraint (2) assures that two adjacent cells on lev.
I cannot both survive and thus the number of vertices mu
decrease rapidly from level to level. In Fig. 1(b) this constrai:
is not satisfied since the survivors da, es and co, 9o are adjacen
The construction of G[ + 1] can also be regarded as findir
a maximal collection of vertices of G[l] no two of which a:
adjacent. This is the maximal independent set problem for grapl
(eg., [21]); we will return to it in Section V.
A possible alternative method of constructing G[l + 1] fro
G[l] is to partition G[l] into connected subgraphs and the
select one cell in each subgraph as a survivor. However,
we do so, the first constraint no longer assures locality of th
processing. In Fig. 1(b) cell b, has survivor c adjacent to i
but must be allocated to survivor b two sites away. Choosin
the survivor independently for each region may also violate th
second constraint since two adjacent regions can both have the
survivors at the border [Fig. 1(b)]. Thus the set of children shou:
be defined in G[l] only after the vertices of G[l + 1] tthe
parents) have been chosen.
The last step in constructing G[ + 1] is to define the edges
E[1 + 1]. Let the connected subsets ca[l],ci[l],- ,c,[l]} C
V[l] and [d.[4],di[l],-- ,d,[]} c V[l] be he children of wo
different parents. Our condition for an edge between vertices
c[{ + 1] s cs[] and d[l + 1] sE d[] in G[l + 1] is
In other words, two vertices are joined by an edge in G[I + 1]
if there exists a path between them in G[l] of length at most
three edges. (Note that by (2), the path cannot be of length 1.)
G[l + 1] is now completely defined. Fig. 1(c) shows, the graph
corresponding to the partition in Fig. 1(b).
The irregular sampling hierarchy is thus built recursively from
G[0] (the original sampling grid). The apex of the hierarchy G[m]
has only one vertex. Constraint (2) assures that the apex is always
reached.
In the next section we describe a probabilistic parallel algo-
rithm that constructs graph contractions satisfying (1) and (2).
The algorithm is analyzed in more detail in [22], [23].
We have seen that the derivation of G[l + 1] from G[l] must
start by defining the vertices of the new graph. Since V[l + 1] C
V[l] we are dealing with a decimation process, ie., only a subset
of the vertices V[I] are retained. We want the decimation to be
performed in parallel on G[l].
We will define a decimation process that is dependent on the
image data. We assume that every cell c; (a vertex of G[I]) carries
a value g, characterizing its region of the image-for example,
the average gray level of the region. Without loss of generality
we can assume that g; is a scalar value; the treatment of feature
vectors is identical. From now on the explicit indication of the
level l will be dropped to simplify the notation.
Let cell cs on level have r neighbors on level l, ie., let its
degree as a vertex of G[l] be r. (Note that for the moment cg is not
necessarily a survivor and the set of its neighbors has no relation
with the connected subset allocated to a parent.) We examine
every neighbor c;, i = 1,--,r of cs and decide whether or not
it belongs to the same ''class'' as c. This decision can depend in
any desired way on the values g,,i = 0,---,r. We associate a
binary number A;, i = 0,--,r with each neighbor, where A, == 1
if c; belongs to the same class as cs, and A, = 0 otherwise; note
that A4 = 1,
The decimation algorithm employs three variables for every
cell: two binary state variables p and , and a random variable
uniformly distributed between [0, l] with outcomes x. The sur-
vivors are chosen by an iterative local process. Let k =< 0, 1,---
be the iteration index. Initially all p,(0) = 0. A cell survives if at
the end of the algorithm its pg(k) state variable has the value 1.
Every'iteration has two steps. First ;4(k) is updated based on
the states p;(k - 1) of neighboring cells in the same class:
In other words, g4(k) becomes 1 if and only if there is no survivor
among the cells belonging to the same class in the neighborhood
of cg, Note that the neighborhood includes the cell itself. The
initial conditions always yield ;4(1) = 1. Then pg(k) is computed
on the updated values of ;(k):
To become a survivor the outcome of the random variable
x drawn by the cell must be the local maximum among the
outcomes drawn by the neighbors in the same class. Note
that only those neighbors are taken into account which do not
already have a survivor adjacent to them (q;(k) = 1). This
condition extends the region of influence of a cell beyond its
immediate neighborhood and yields faster convergence of the
algorithm. The local maximum property assures that (2) is always
satisfied. The state of a survivor is not reversible. Once a cell
is labeled pa(k - 1) = 1, at subsequent iterations the product
g4(k)g(k) (5) is always 0 by the definition of ;4(k) (4). Thus
in (5) the second condition, preserving the current state is used.
It can be shown [22], [23] that after a finite number of iterations
(at most five, in the experiments reported there) the algorithm
reaches a final global configuration in which the survivors satisfy
(1) as well.
The algorithm is entirely local, every cell computing its states
based only on the states of its immediate neighbors. Except at the
highest levels of the hierarchy, where due to the small number
of vertices artifacts may occur, the decimation ratio between two
consecutive levels exceeds four. This lower bound results from
the fact that two adjacent cells never survive. On the random
graph structure of higher pyramid levels the average degree of
a vertex is around 6 [23]. To satisfy the nonadjacency condition
(2) the number of vertices must be reduced by about the same
order relative to the previous level.
By employing the algorithm an irregular sampling hierarchy
can be built in parallel in log(class size) steps. (The distinction
between class size and image size becomes clear in the next
section.) The stochastic decimation is performed independently
within classes. In the next two sections we describe two appli-
cations of the process, one to connected component analysis of
labeled images, the other to segmentation of gray level images.
In a labeled image the pixels are classified into a small
number of classes distinguished by different labels. A connected
component is a maximal set of connected pixels sharing the same
label. For simplicity we will restrict ourselves to the case where
there are only two labels, i.e., to the case of a binary image,
but images with multiple labels can be handled in essentially the
same fashion.
Sequential algorithms for analyzing the connected components
in a binary image usually employ a row-by-row scan [30]. An
alternative approach makes use of the quadtree representation of
the image [34]. In this section we apply the techniques described
in Section II to obtain in log(class size) steps a description of the
connected components in a binary image. The description takes
the form of a graph whose vertices represent the components
and whose edges represent the adjacency relations among the
components.
The fact that the pixels are labeled makes classification of the
neighbors of a cell immediate. Let the label of cell c; be gi-
In the binary case the label can have only two values. Thus in
the neighborhood of cell c, we have for i = 0,---,r the class
membership variables
Note that (6) is symmetrical; cs gets the same value of A in the
neighborhood of c; as c; gets in cs's neighborhood. Since the
definition of A, is symmetrical it can be regarded as the weight
of the edge (ca, c;). The case A = 0 is equivalent to removing
the edge from E[l]. Let E'[l] be the set of edges having M = 1,
and let G'[l] = (V[l], E'[1}). The connected components in the
labeled image are represented by connected components in the
graph G'[l], for all 1 2 0.
The subgraphs of G' are processed independently, each sub-
graph being recursively contracted into one vertex, the root of the
connected component. The contraction process is based on the
technique described in the previous section: first the survivor
vertices are designated and then the nonsurvivor vertices are
locally allocated to survivors. If a nonsurvivor has more than
one survivor neighbor it chooses the one carrying the largest
outcome of the random variable x from the last iteration of
the decimation process. Because the neighbors are neighbors
in G', the survivors can only have children belonging to their
own class. Thus from each connected component of the input
image a pyramidal hierarchy of irregular tessellations is built in
O(log(component size)) steps.
The different hierarchies may have different heights, but in
log[max(component size)] steps the entire image is reduced to
roots, This situation is detected at the level m when E'[m]
becomes empty. Evidently component size can differ from im-
Age size. For example, a connected linear pattern passing through
every second row of the image has length N(N + 1)/2 pixels.
Since the hierarchy is built over the pattern the number of levels
depends on its intrinsic diameter.
At each level, the graph G[l] includes edges between cells
that arise from different labels; it preserves the spatial relations
among the connected components. At the root level, G[m] is
the adjacency graph of the original labeled image; it has one
vertex for each connected component and its edges represent the
adjacencies between these components.
Fig. 2.(a) shows an example of a graph G[l] superposed
on the binary image from which it was derived. The induced
graph G'[l] is shown in Fig. 2(b). Note that in G'[l] each
connected component corresponds to a connected subgraph. The
cells surviving level l and the allocation of the nonsurvivors are
shown in Fig. 2(c). The graph G[l + 1] of the next level is shown
in Fig. 2(d) and the corresponding graph G'[! + 1] in Fig. 2(e)
Level 1 + 2 is the root level and its graph G[m] is shown in
Fig. 2(f). It correctly represents the adjacency relations among
the three connected components of the image: the background
and the two blobs.
In Fig. 3 a checkerboard image and the adjacency graph of it
root level are shown, The checkerboard is a ''worst-case'' image
the two connected components (both defined by the relation o
eight-connectedness) being distributed across the entire input
Cibulskis and Dyer [10] employed a regular pyramid structur
to segment this image. In their results the ''white'' componen
was allocated to one root at the apex, but the representation o
the black squares had to be spread over several levels. The siz.
of the image is 64 x 64 and the two roots were obtained a
the eight level of the hierarchy. Recall that the height of th
hierarchy depends on component size. Since random processe:
are involved in the construction of the irregular tessellations th
location of the roots depends on the outcomes of local processes
Nevertheless, the same root level adjacency graph is alway
obtained at the top of the hierarchy.
The famous connectedness puzzle of Minsky and Pa
pert [27, Fig. 5.1] can be solved by our technique ii
O(log(component size)) steps. The pattern in Fig. 4(a) contain
three black and two white bands, while in the pattern in Fig. 4(b
the two white bands are connected, leaving only two black bands
The adjacency graphs obtained at the root level clearly show th
different topologies.
The irregular tessellations that arise in the hierarchies define
by the connected components do not convey meaningful rep
resentations at intermediate levels. Let us define the receptiv
field of a cell on level I as the set of all the pixels at level (
(input) associated with it. This field is always a connected se
and the image is the disjoint union of the fields. Also, each field
is a subset of a connected component of the image. In Fig. 5 the
receptive fields of four levels of a hierarchy derived from a simple
image are shown. The different fields are randomly colored to
emphasize their shapes. At intermediate levels the shapes of the
fields are arbitrary, since they depend on the outcomes of random
variables. At the root level each field is a complete connected
component.
Our multiresolution representation consists of several inde-
pendent hierarchies, each built independently over a connected
component. The shape, size, position, and orientation of the
connected component have no influence on the final result.
The individual hierarchies can be used for the fast recovery of
geometrical properties such as area or perimeter [32]. It should
be mentioned that Miller and Stout [26] also proposed a data
structure in which a separate ''essential'' regular pyramid is built
over every object.
All the discussion in this section was restricted to binary
images. If more than two labels are used the discussion is
essentially identical. In particular, our method can be used to
label the connected components of constant gray level in an
arbitrary digital image. In the next section we study the less well-
defined problem of segmenting a gray level image into ''natural''
regions.
In gray level images the difference between the values of
two adjacent pixels is bounded below only by the size of the
quantization step. In our technique, to build the hierarchies
the pixels in a neighborhood must be assigned to classes. The
class membership induces the graph on which the stochastic
decimation takes place. For labeled images the classes correspond
to the labels and the hierarchy always converges to the same final
representation: the adjacency graph of the connected components
defined by the labels. For gray level images it is no longer
obvious how to define the classes (unless our goal is to segment
the image into connected components of constant gray level). In
the first part of this section we discuss this problem.
The simplest approach is to define class membership by
thresholding the gray level differences between the center cell
c and its neighbors c;, i = 1,-,r. The class membership
variables A, are thus defined by
As in the labeled case, (7) based on an absolute threshold T is
symmetrical. This symmetry, however, can create artifacts when
we attempt to segment gray level images, as we show in the next
example.
Fig. 6 shows an object having four gray levels on a white
background. The graph G[l,] of an intermediate level is shown
superposed on the image in Fig. 6(a). Let the differences between
the gray levels be less than T and let the two lighter gray
levels be within T of the background. The resulting graph G'[l,]
is shown in Fig. 6(b). Note the edges connecting regions that
have different colors. The stochastic decimation algorithm selects
survivor cells and the nonsurvivors are allocated to their most
similar surviving neighbors. The survivors (parents) compute new
gray level values based on their children. After a few more levels
of the hierarchy we might arrive at the graph G'[l,], > i,
shown in Fig. 6(c). The difference between the gray levels of
the two cells located in colored regions now exceeds T and in
G'[lg] these regions are no longer connected. If a different set
of outcomes of the random variables had been employed in the
stochastic decimation process, a different set of surviving vertices
might be obtained, and the new parents might have different gray
level values, yielding a new graph at level l4 [Fig. 6(d)]. We
conclude that using a symmetric class membership criterion for
gray level image segmentation strongly influences the structure of
the hierarchy and therefore the final representation of the image.
Our next example, a ramp image, shows the severity of
the resulting artifacts. In Fig. 7 (top-left) the image of a ramp
going from level 0 (black) to level 255 (white) is shown. The
difference between adjacent rows of pixels is either four or
five gray levels, depending on the quantization error. The pixel
values are the same along each row. The receptive fields of the
root level obtained for T = 33 are shown in Fig. 7 (top-right).
The color of a region is the gray level value computed by its
root, ie., its average gray level in the original image. These
receptive fields define a possible segmentation of the image,
but the segmentation is not esthetically satisfactory; the different
local configurations generated by the stochastic decimation yield
ragged boundaries between the region. When at each level the
children can be reallocated to adjacent parents having similar
gray levels, the shapes of the segmented regions improve (Fig. 7,
bottom-left). The procedure is essentially the same as the linking
processes employed in traditional pyramid structures (e.g., [13],
[16]). The receptive fields, however, may now be fragmented,
and connectivity may not preserved, because of inconsistent
exchanges of children.
To achieve satisfactory results, a nonsymmetric class member-
ship criterion must be used. We now describe such a criterion.
Let cell c, have r neighbors. In this neighborhood we will define
a local threshold S[c,] such that 0 5 S[cs] 5 T. Methods
for computing S[c4] will be discussed later in this section. The
definition of the class membership variables A, then becomes
for i = 0,1,--,r. If the number of neighbors having A, = 1
is s and the number of neighbors for which g, - gs] 5 T is t,
then 0 5 s 5 t 5 r. The threshold S[cs] used in (8) is specific
to the neighborhood of cell c4 and therefore the criterion is not
symmetric: in general,
does not imply
since the two thresholds are computed based on neighborhoods
that only partially overlap. As a consequence of the nonsymmetry
the graphs G'[l] become directed. An arc from c; to c;, meaning
that in the neighborhood of c; the two cells belong to the same
class, may not be reciprocated by an arc from c; to c;.
The receptive fields of the ramp image's root level for a hier-
archy built with nonsymmetric class memberships (as described
below) are shown in Fig. 7 (bottom-right). The same absolute
threshold T 33 was used. The boundaries between regions
are now correctly delineated along the rows of the image. The
neighborhood-dependent local threshold S[cs] ensures that every
cell connects first to its neighbors that have the most similar gray
level. Thus the individual rows in the image are reduced to single
cells before two cells belonging to adjacent rows can become
neighbors in the graph G'[l]. The effect of the random processes
used in building the hierarchy is reflected by the different widths
of the regions. However, this artifact can also be eliminated as
we shall see at the end of the section.
We now discuss how the value of the local threshold S[cg] is
computed. Note that the extreme case S[cg] = 0 corresponds to
extracting connected components of constant gray level. Several
approaches are available to determine the S[cg] value that best
dichotomizes the neighborhood into two classes. A wide class
of thresholding methods (e.g., [33]) can be considered. Another
approach is to use the neighbor dichotomization techniques
employed in nonlinear image smoothing techniques (e.g., [15]).
Since the neighborhoods are defined on the graphs, the spatial
relations among the neighbors will not be used; we will employ
only gray level information in computing S[cs].
Let 8;;, i = 0,1,---,r be the ordered sequence of absolute
gray level differences b, = [g, - g4], Thus
The simplest way of defining S[cg] is the most similar neighbor
method. In this case S[cs] = &;; and s = max arg(&, = &i;).
This is the method that we used for the ramp image (Fig. 7,
bottom-right). In gray level images of real scenes with less
regular structure this method could yield ''tall'' hierarchies,
because the neighborhood sizes in the G'[l] graphs are small,
and therefore the decimation ratio between consecutive levels is
too low. Generalizations of this method such as k most similar
neighbors [12] S[cg] = &A;, or fured fraction of good neighbors,
S[c] = 0,0j;, have the disadvantage that the constants k or o
must be defined arbitrarily.
We have obtained good results using the maximum averaged
contrast method in which S[cg] is set by detecting the most
significant step in the sequence of b;;;. For all the t neighbors
within the absolute threshold gray level difference we compute
Let u = min arg(max,(V, - U;)) and s = maxarg(&, = 4.).
The threshold S[c,] = 8,; is the first occurrence of the maximum
averaged contrast. For example, the sequence ;;; = 0;6; = g;
0, &i = 4 = &4 = 1 yields S[c, = 8 = 0. On the other hand,
the sequence 8;;; = 0,; = 1, 4; = 2,4; = 3. ; , = 5
yields S[c,] = &i = 3.
The gray level value of a parent gs[l + 1] is computed as the
weighted average of its children's gray levels g,[l]:
where to bias the segmentation toward larger regions, the recep-
tive field area A,[l] (the number of pixels in the region) is used
as the weight; z is the number of children.
To see how this method works on a real image, in Fig. 8 (top-
left) a 64 x 64 aerial image magnified to 128 x 128 is shown.
The receptive fields for the root level and the locations of the
roots are shown at top-right. Since the gray levels of the roots
are used to color the regions, adjacent fields may appear fused. A
random coloring is shown in Fig. 8 (bottom-left). The root level's
adjacency graph, describing the adjacencies among the regions,
is shown at bottom-right. In this example the root level is at
m = 8. We have found that hierarchies built for this image with
different random variable outcomes can have m as high as 10,
significantly larger than n = 6, the number of levels in a regular
pyramid structure for a 64 x 64 image. The use of stochastic
decimation adapted to the local structure of the image slows
down the convergence of the process. As mentioned before, the
most similar neighbor method yields the slowest convergence;
in one hierarchy built using this method we obtained m = 15.
On the other hand, using an absolute threshold (T = 33) usually
generates hierarchies with m = n = 6 because the neighborhood
sizes used in the decimation are much larger.
Different outcomes of the random variables cause changes
in the hierarchy structure. For labeled pictures the final result
of the analysis, i.e., the connected component description at
the root level, is always the same; but this is not the case
for segmentation of gray level images. By changing the set
of survivor cells the values attributed to these cells may also
change slightly, yielding changes in the graphs on subsequent
levels. Four different segmentation results for the aerial image
are shown in Fig. 9. In each example different random variable
outcomes were employed, and the absolute threshold was always
T = 33. All the figures but the top-right have m = 10, while the
top-right has m = 8. The two-level height difference is mainly
caused by the relative inefficiency of the stochastic decimation
process when the similarity graph has only a small number of
vertices. The number of roots is 11 for the top-left image, 12
for the bottom-left, and 14 for the two images on the right.
As expected, regions with sharp boundaries in the input image
(Fig. 8, top-left) achieve very similar representations. The effect
of the stochastic processes is more significant in the segmentation
of smoothly changing regions.
Our method provides the adjacency graph of the segmented
image and a separate hierarchy ('pyramid'') for every region.
These tools can be used by model driven vision modules to
analyze properties of the regions and correct the segmentation
in a logarithmic number of processing steps.
The value of the absolute threshold T influences the result
of the segmentation by defining the ''good'' neighbors of a cell
that are taken into account in defining class membership (8).
Modifying T thus changes the structure of the hierarchies. In
Fig. 10 a medical image and its segmentation for three different
values of T are shown. We chose this image since many of the
features are less well delineated than those in the aerial image.
The height of the root level and the number of roots in each case
are given in the legend of the figure. The larger T, the smaller
the number of roots since the fusion of adjacent classes is more
probable. The correlation between T and the height of the root
level m is less immediate. At higher levels the decimation process
may slow down because of the smallness of the graphs. The effect
depends on the structure of the hierarchy, and therefore also on T.
Note that the medical image is noisy (Fig. 10, top-left) but only
a few noisy pixels remain at the root level. These single pixel
roots are easy to identify and remove from the segmented image.
Another example is shown in Fig. 11. The house image (left)
was analyzed with T 26. The height of the root level in this
example is m = 12 and 100 roots were extracted (right). Note
the accurate delineation of shadows and of the small feature
over the garage door. Segmentation of such complex images
is sensitive to the value of T. When we used T 33 the sky
and the roof fused in some of the hierarchies. However, once
the image representation is obtained, individual analysis of the
Segmented objects can be performed to correct any undesired
fusion by lowering the threshold.
In gray level image analysis, the problem of root detection
also requires careful treatment. In labeled pictures a root (an
isolated vertex in the graph G'[I]) remains a root at higher levels,
while other (larger) connected components of the image are being
extracted. In gray level images this is not true. The value of an
already extracted root may become within threshold of some
neighbor's value at a higher level. The root cell then becomes
connected in the graph G'[l] of that level and must be taken into
account in the stochastic decimation process. Thus a root may
disappear at subsequent levels, its receptive field being fused
into a larger region. Nevertheless, we found this approach more
desirable than the following alternatives:
Additional control over the result of segmentation is gained if
changes in the local thresholds computed by a cell on successive
levels are taken into account. Let S;[cg] be the local threshold
of a cell surviving the decimation of level l. Assume that on
level 1 + 1 the cell computes the new threshold S;,;[cs]. A more
detailed segmentation is obtained if we declare the cell to be a
root when
where 2 is a constant. For example, by using a small 2 in
the ramp image (Fig. 7, bottom-right) we can stop the fusion of
adjacent rows. At the root level the adjacency graph becomes
linear, with every vertex now representing an entire row of the
image. We can now apply the stochastic decimation algorithm
to this one-dimensional structure to allocate to every vertex its
address in the corresponding linked list, This operation is also
achieved in O(list size) steps [22]. Thus the adjacency graph can
be contracted into a graph in which the vertices correspond to
collections of rows; the new graph defines the segmentation of
the ramp into constant width bands.
In this paper we have presented an image analysis technique in
which a stochastic decimation algorithm constructs a tessellation
hierarchy that reflects the structure of the image. For labeled
images the final tessellation is into connected components, and
is unique. For gray level images the tessellation is not unique,
but it constitutes a reasonable segmentation of the image.
If our methods can be implemented on suitable parallel hard-
ware, every root can recover information about its region in a
logarithmic number of processing steps, and the adjacency graph
can become the foundation for a relational model of the scene.
On appropriate hardware our technique should be useful in real-
time analysis of the visual environment. We intend to implement
our methods on the Connection Machine where the architecture
allows communication among any two processors in a few steps.
This is essential since on a graph any two processors can become
neighbors.
Any definition of the classes can be used when building the
hierarchy. For example, the definition of the classes can take
into account the properties of the corrupting noise if they are
available, or positional information derived from the previous
level. In the latter case, however, the data driven component of
the method is weakened. The decimation process can be modified
to be biased toward cells with high informational value. Jolion
and Montanvert [17] have proposed an adaptive pyramid in which
cells belonging to the most homogeneous regions have priority to
become survivors. Such an approach, however, is not useful for
labeled images in which many cells carry identical descriptions.
The evolution of the local connections within the hierarchy,
driven by both the image data and the stochastic processes, is
itself of interest, and might serve as a neural model for early
visual perception. Appropriate class membership criteria might
transform the hierarchy in a connectionist model for extraction
of perceptual invariants [3].
The graph contraction used to build the hierarchy satisfies
two constraints: 1) a removed vertex always has a retained
neighbor; 2) two adjacent vertices cannot both survive. In graph
theory, finding a surviving subset of vertices is known as the
maximal independent set problem (e.g., [21]). The stochastic
decimation process that we employed provides such a subset
and therefore solves the problem. Our algorithm is different
from other solutions proposed in the literature. In these methods
vertices are first chosen at random and then some of them are
discarded in order to have the constraints satisfied. Such trial-
and-error approaches are considerably slower than our algorithm,
which has the constraints embedded within the selection process.
Simulations have shown a fourfold speed-up for the stochastic
decimation procedure relative to other algorithms.
We would like to thank W. Kropatsch for challenging us with
the checkerboard example, S. Banerjee for calling our attention
to the similarity between the decimation process and the maximal
independent set problem, and D. Y. Kim for suggesting the local
threshold computation.
URRENT radiofrequency (RF) radiation safety
standards (e.g., ANSI C95.1-1982) are based, to a
significant extent, on presumed rates of human whole-body
RF absorption. To date, whole-body absorption rates in
actual human subjects have only been measured by our
group. The experiments were performed using a TEM cell
as the exposure system [1]. Initially, the effect of frequenc
and grounding on the E-polarization absorption rates wa-
studied [2]. In that study, only the ideal free-space and
grounded conditions were simulated. In the present work,
the effect of different spacings from the ground plane on
the E-polarization absorption rates is reported. The other
two possible body orientations with respect to the wave, K
and H, will be ignored since their absorption rates are
much smaller than for the E orientation [1].
All measurements were performed using the modified
version of the TEM cell [3] in which all the TE resonances
are suppressed. Tests showed that the modified cell could
only be used reliably at frequencies below 25 MHz or from
40 to 42 MHz. Within the latter range, the ISM frequency
of 40.68 MHz was selected as the measurement frequency.
Absorbed-power measurements were performed with the
RF system previously described and evaluated [l].
All volunteers were adult males in good health. Ex-
posures were limited to one hour per day at a power
density not exceeding 13 yW -cm'' and no subject ever
absorbed more than one W,
M All experiments were performed with the body in an E
orientation (electric field parallel to the body length) and
the subject's feet closest to, or touching, the ground plane.
Both of the two possible orientations with respect to the
nagnetic field were employed: the EKH orientation, in
which the magnetic field was perpendicular to the chest;
and the EHK orientation, in which the magnetic field was
in the shoulder-to-shoulder direction. In general, the results
differed little between the two possible E orientations.
8. Most of the RF absorption rates are displayed in a
(6omparative way, i.e., as a ratio to a reference absorption
rate for the same subject. The reference value is either the
5free-space or grounded rate. Table I gives the reference
5figures for each combination of subject and frequency that
was used. Only three subjects were employed in the study
because other volunteers were not available for reasonable
periods of time and the results for the three subjects were
always found to be quite similar.
The five frequencies used for the experiments are also
given in Table I. Frequencies of 13.56 and 40.68 MHz were
employed because they are both ISM frequencies, at which
human occupational exposures often occur. The highest
usable frequency below all the interacting-resonance fre-
quencies of the cell is 23.25 MHz. The use of 10 MHz
made a direct comparison possible between our measure-
ments and the two relevant published theories; the cylinder
theory of Iskander et al. [5] and the block model calcula-
tion of Hagmann and Gandhi [6], [7]. Finally,7 MHz is the
lowest frequency for which reasonably accurate results are
possible.
Since it was impractical to achieve a uniform-thickness
air gap between the subject's feet and the ground plane,
that ideal condition was simulated by using spacers of
low-dielectric-constant materials. Two materials were used
(both of dielectric constant 1.03): expanded polystyrene,
and a hydrocarbon resin foam (ECCOFOAM PP-2 from
Emerson and Cuming, Canton, MA).
In the first RF absorption study, 40.68 MHz was found
to be at or near the grounded resonance. All frequencies
below 25 MHz were clearly in a different, below-resonance,
region. This distinction is also supported by the present
results.
Before the feet were separated from the ground plane,
the effect of separating the two feet on the ground plane
was tested. The results, in Table II, are clearly independent
of frequency and of which E orientation (EKH or EHK )
is employed. A small gap between the feet has only a slight
effect. A larger separation significantly reduces the absorp-
tion rate compared to the rate with the feet together. This
effect may be partly due to the reduction in effective height
of the subject as the feet are spread far apart. In the case of
subject L, used for Table II, his effective height is reduced
by 10 cm from its normal value of 173 cm when his feet are
90 cm apart.
The effect of an air gap on the normalized specific
absorption rate (NSAR) is shown in Fig. 1 for three
subjects exposed at the same frequency. Fig. 2 shows the
same plot for one subject in both E orientations and at
three different frequencies. It can be seen that the air-gap
effect depends only slightly on the subject and choice of E
orientation. Results for the two below-resonance frequen-
cies are very similar to each other, but strikingly different
from the results for the near-resonance frequency, 40.68
MHz. It can be seen in the two figures that, for below-reso-
nance frequencies, the absorption rate is reduced to half
the grounded value by an air gap of only three to six mm.
At the near-resonance frequency, on the other hand, an air
gap of 50 to 80 mm (based on Fig. 2 and other data) is
necessary to produce the same effect.
The two relevant existing theories are the approximate
cylinder calculation of Iskander et al. [5] and the block
model calculation of Hagmann and Gandhi [6], [7]. Since
the latter theory was only calculated for the EHK orienta-
tion at 10 MHz, the comparison between theory and
experiment was done for those conditions. The results are
presented in Fig. 3. The theories are normalized to NSAR(0)
for each theory, and not to the measured NSAR(0). This is
important to note because the two measured NSAR values
are both approximately four times the two calculated val-
ues (see Table I). This discrepancy will be addressed in a
later paper on improved models. Fig. 3 can be used,
however, to compare the relative decrease in absorption as
a function of separation distance from the ground plane.
For separations up to three cm, the measurements agr :e
with the block model and disagree with the cylinder mod.
At a separation of five or six cm, the two theories bc.hh
agree reasonably well with the measurements.
At higher frequencies, quantitative predictions were only
published for the cylinder theory. Since that model obvi-
ously does not distinguish between the EKH and EHK
orientations, the comparison was made to the average of
the measurements for the two E orientations. The results
are presented in Fig. 4. The measured curve for 23.25 MHz
is well below the cylinder theory, just as was found for the
other below-resonance frequency (10 MHz). The measure-
ments at 40.68 MHz are compared with the cylinder theory
for the same frequency, and also for 47 MHz because the
latter frequency is calculated to be at the grounded pec'.
for the cylinder. The measurements are seen to be close(
the 47-MHz curve for all separations measured. This agrec-
ment supports our contention that 40.68 MHz is actually
very near the peak for grounded human subjects.
The block model only made a semiquantitative predic-
tion for the resonant frequency [6, p. 25], one that is
obviously wrong: ''Several calculations made for the
grounded resonant frequency of 47 MHz show a fall-off in
magnitude of grounding effects with increasing distance
from the ground plane, which is similar to the results at 10
MHz.''
As far as the air-gap effect on the whole-body absorp-
tion rate is concerned, we conclude that the cylinder theory
is more accurate for the near-resonant frequency, and the
block model calculation is more accurate for the below-res
onance frequencies.
Finally, the block model calculation made one more
prediction that could be tested. That theory predicts [6,
fig. 5] that the SAR in the heel is much larger than in the
ball of the foot. This implies that most of the body's RF
current is carried to ground through the heel and that
contact with the ground plane is much more important for
the heel than for the ball of the foot. The data of Table III
prove that the opposite is the case. Most of the RF current
through the foot goes through the toes and ball of the foot.
This may be due to the larger contact area of that part of
the foot, or due to the complex bone structure of the ankle
and foot.
For small separations from the ground plane, the soles
of the subject's feet may be considered to form a parallel-
plate capacitor with the ground plane, The equivalent
circuit representing the exposure situation then consists of
a resistive human body separated from its image in the
ground plane by a capacitive impedance. Iskander et al. [5]
used this approach in calculating the RF absorption rate of
a cylinder. To test the capacitance idea, the functional
dependence of the absorption rate on capacitance is not
needed. It is only necessary to utilize the fact that the
capacitance of a parallel-plate capacitor is proportional to
the ratio of the material dielectric constant to the separa-
tion between the plates. Thus, the capacitance idea is
confirmed if the absorption rate depends only on the ratio
of the two quantities.
This idea was tested using different numbers of plastic
sheets between the subject's feet and the ground plane. In
each test, the plastic sheets extended beyond the edge of
the feet by a distance at least as large as the total thickness
of the sheets. The two plastics that were used, and their
dielectric constants (measured in our lab), were: methyl
methacrylate (or lucite, K = 2.6) and cellulose acetate
(K = 40).
The results of these experiments are shown in Fig. 5. It
can be seen that most of the data points, after scaling for
the effect of the dielectric constant, lie on or close to the
curve for K = 1,0. The last point for cellulose acetate, at a
reduced separation of 1.6 cm, is likely off the curve because
the actual separation of.6.A cm is comparable to the width
of the feet; this violates the assumptions underlying the
parallel-plate capacitor model. Overall, the data of Fig. 5
adequately confirm the capacitance concept. That idea will
now be applied, in a qualitative way, to the practical case
of the effect of footwear.
The effect of different footwear on the nearly grounded
absorption rates was first tested at 23.25 MHz. The data
are presented in Table IV.
The results for socks alone are interesting for two rea-
sons. First, even thin nylon socks, only 0.65 mm thick,
reduce the absorption rate by a measurable amount, to 87
percent of the grounded rate. Secondly, the results for both
the nylon socks and the wool socks (1.7 mm thick) are the
same as the results for an air gap of exactly the same
thickness. This is a rather indirect way of proving the
well-known fact that the bulk of the volume of a sock
consists of air pockets, not material.
The use of shoes as well as socks further reduces the
absorption rate (Table IV). It is not possible to compare
the data with shoes and socks to the capacitor model
because: the heel is usually further from the ground plane
than the ball of the foot; the socks and shoes form a
dual-layer capacitor; and the dielectric constant of the
leather soles (which could not be measured easily) depends
strongly on its moisture content. For the experiments using
both shoes and socks;;the absorption rate compared to no
footwear was found tö'vary from 548 1 percent (for nylon
socks and dress shoes with leather soles) to 3281 percent
(for wool socks and rubber-soled athletic shoes).
The same two combinations of shoes and socks, which
represent the maximum rapge of the results for 23.25 MHz,
were used to study the effect of footwear as a function of
frequency. Those measufements are presented in Fig. 6.
Results are very similar at all the below-resonance frequen-
cies, At 40.68 MHz, however, footwear produces a much
smaller reduction in absorption; this is consistent with the
data of Fig. 2 for the effect of an air gap.
In occupational exposure situations, where the ground-
ing effect may occur, it is recommended that footwear
always be worn. This will provide some radiation protec-
tion at all frequencies: a rediuction in the RF absorption
rate of 15 to 35 percent neat resonance; and of 45 to 75
percent at below-resonance frequencies. For the commonly
used ISM frequency of 27.12 MHz, the reduction is esti-
mated from Fig. 6 to be between 40 and 60 percent. A
second radiation protection alternative is the employment
of a thick rug, rubber mat, plastic sheet, or any other
insulating material over the ground plane.
The rate of change of the absorption rate with separation
from the ground plane is seen in Fig. 1 to diminish
considerably for separations greater than about 2 cm. It is
obvious that the graphical analysis of Fig. 1 is poorly
suited to extending the measurements to the free space
situation, which is simulated in our TEM cell by a separa-
tion of 90 cm from the ground plane. A much more
suitable plotting scheme, illustrated in Fig. 7, was found.
Absorption rates are plotted as a function of the inverse
separation distance d''. Two advantages of this method
are that the extrapolation curves are linear (at below
resonance frequencies) and that the absorption rate for the
ideal free-space condition (d = oo) is simply the intercep'
of the regression line with the ordinate scale.
The data of Fig. 7, for subject L, overlap the data of
Fig. 1 for the same subject. The three points on the
right-hand side of Fig. 7(a) are the same as the three points
on the right-hand side of Fig. 1. It is seen in Fig. 7 that the
linear extrapolation to free space is valid for d''' g 0.5
cm'' or d a 2 cm. Graphs similar to Fig. 7 were also
plotted for the other two subjects at the same frequency.
All the lines fit reasonably well (R > 0.9 for all six lines).
The regression-line slopes and intercepts for the three
subjects in both E orientations are compared in Table V.
Slopes range from 2.80.5 to 7.430.3 cm. Additionally,
the slope is not consistently larger for either of the two E
orientations. The reason for these variations is not known.
The farthest a subject can be from the ground plane in
the TEM cell is 90 cm, when the subject is located halfway
between septum and wall. The subject's length of about
180 cm half fills the distance from septum to wall, The
question of whether or not these spacings adequately
simulate the ideal free-space situation can be answered by
comparing the intercepts of the regression lines (d = oo)
with the last data points (d = 90 cm). In every case, the
difference was less than 10 percent and not statistically
significant. The average difference between the two values
was 43 2 percent. This result proves that, for below-reso-
nance frequencics, a septum-to-wall separation of twice a
body length provides an exposure situation which very
closely simulates the ideal free-space situation.
The linear extrapolation process which was found to
work so well for below-resonance frequencies did not work
for 40.68 MHz. The measurements for subject L, shown in
Fig. 8, are clearly not on a straight line. Neither are similar
plots for subject I. This difference in behavior at near-reso-
nance frequencies is not surprising since it is also observed
for small separations from the ground plane.
For the near-resonance frequency, a separation of 90 cm
from the ground plane may not be considered equivalent to
the ideal free-space condition. Based on the four available
curves, it is estimated that NSAR([90 cm]') is 10335
percent greater than the extrapolated intercept NSAR(0),
and that this difference is real. Thus, all our simulated
free-space absorption measurements at 40.68 MHz should
be reduced by 10 5 percent. This has the effect of re-
ducing slightly the frequency esponent n (NSAR o: f') for
the free-space absorption curves from 18 to 41 MHz. The
corrected mean exponent is 2.7 0.2, in comparison to the
value of 2.90.2 originally reported by us [2, table 3].
A complete set of absorption measurements for one
subject in the EKH orientation is shown in Fig. 9. The data
for the grounded condition and the smallest separation (0.6
cm) are fit by a single (weighted) regression line on the
log-log scale. The data for the free-space condition are fit
with two regression lines, as was previously found neces-
sary [2]. Two lines were also found to give a better fit to the
data for a separation of 5 cm.
In terms of both the positions of the curves and the
number of required regression lines (one or two), it can be
seen that the data for a 0.6-cm separation are similar to the
grounded results, while the measurements for a separation
of 5 cm are more like the free-space results. This supports
our previous observation that a separation of about two cm
is the dividing line between the nearly grounded and nearly
free-space behaviors.
The results of this study fall conveniently into four
distinct categories, depending on whether the frequency is
near the grounded resonance ( f 4 40 MHz) or below it
(f s 25 MHz) and whether or not the subject's feet are
within two cm of the ground plane.
Near the ground plane, the decrease in NSAR with
increasing separation is very rapid. The absorption is re-
duced to half the grounded value by a separation of only 3
to 6 mm. The results agree very well with the predictions of
the block model for all separations out to 6 cm, while they
only agree with the cylinder model at a separation of 5 or 6
cm.
The idea that the soles of the feet and the ground plane
effectively form a parallel-plate capacitor was proved by
measuring RF absorption rates with different thickness of
three different materials between the two surfaces. Natu-
rally, the capacitor model only works for separations from
ground which are less than the width of the foot.
Ordinary footwear provides practical radiation protec-
tion by reducing the RF absorption rates, compared to
grounded, by 45 to 75 percent, depending on the choice of
footwear.
Finally, the absorption rates were found, when plotted
against inverse separation distance, to extrapolate in a
linear manner to the ideal free-space limit. The linear
relationship permitted the inference that a separation of 90
cm, the maximum possible in our TEM cell, is a very good
approximation to the free-space condition.
For near-resonance frequencies, the decrease in NSAR
with separation from the ground plane is an order of
magnitude slower than for the below-resonance frequen-
cies, The curve agrees fairly well with the predictions of the
cylinder model, but disagrees with the block model. Simi-
larly, footwear provide much less RF radiation protection;
the RF absorption rates compared to grounded are re-
duced by only 15 to 35 percent, depending on the choice of
footwear.
Finally, the absorption rates could not be extrapolated
to free space in a linear maner, and it appears that a
separation of more than 90 cm is needed to properly
simulate free space for frequencies near the grounded
resonance.
The author would like to thank J. A. Walsh for perform-
ing many of the measurements and S. J. Allen for review-
ing the manuscript. He is also grateful to the Division of
Biological Sciences, National Research Council, Ottawa.
for providing the facilities where the work was done.
GVery computer system is accompanied by at least one
''program assembly language.'' This language is used
both as a hardware description language and as a pro-
gramming language. When it is used as a programming
language, it requires a ''program assembler'' to translate
the assembly programs into machine language. As a con-
sequence, at least one program assembler must be devel-
oped for every computer system.
This need led researchers to look for ways to automate
the construction of program assemblers. The result of
their research was a new software tool which was named
the ''program meta-assembler.'' The objective of pro-
gram meta-assemblers is thus quite clear-they are tools
that facilitate the construction of program assemblers.
Likewise, the development of microprogrammed com-
puter systems led to ''microprogram assembly lan-
8uages,'' ''microprogram assemblers,'' and ''micropro-
gram meta-assemblers,''
Few program meta-assemblers were constructed with
large computers in mind. In practice, the few that were
developed were used for implementing program assem-
blers only for various models of the same machine. The
Meta program meta-assembler, for example, was used
for the CDC-3170, CDC-3300, and CDC-3500 models.
More program meta-assemblers were constructed (or ef-
forts were made for their construction) with minicom-
puters in mind. Still more were (and are being) con-
structed with monolithic (fixed instruction set)
microcomputers in mind.
Few microprogram meta-assemblers were constructed
before the advent of bit-sliced microcomputers (micro-
computers fabricated with LSI bit-slice microcircuits,
.g., ALU slices, sequencer slices, and interrupt con-
troller slices). These microcomputers are micropro-
grammed' and many microprogram meta-assemblers
have been constructed for them.% It must be noted,
however, that up to now only microprogram assembly
languages have been used to microprogram bit-sliced
rmcrocomputers.
It is mainly because of microcomputers that meta-
assemblers have established their presence among soft-
ware tools. Despite this, they have received little atten-
tion in the literature. Furthermore, what has been
published is not particularly explicit.' Hence, in this arti-
cle we will examine various meta-assemblers so that we
can judge the present status of their development and
use.
A program assembler is a tool which translates the
assembly programs of a ccmputer into the machine
language of that computer. Program assemblers that do
this only for a specific program assembly language are
called dedicated program assemblers. Program assem-
blers that do it for many program assembly languages are
called program meta-assemblers. Thus, program meta-
assemblers are program assemblers parameterized with
respect to the program assembly languages they trans-
late, while dedicated program assemblers are nW[;
Dedicated program assemblers are expressly construct4
to translate assembly programs for a particular type d(
computer; they cannot be used for any other type. Thf
first assemblers ever constructed were dedicated progra
assemblers.
With the advent of microprogrammed computers, mi-
croprogram assembly tools analogous to program assem-
bly tools started appearing, i.e,, ''dedicated micropro-
gram assemblers'' and ''microprogram meta-assem-
blers.'' At the beginning, these microprogram assembly
tools were physically and functionally distinct from the
program assembly tools. Later, meta-assemblers which
could act both as program meta-assemblers and as
microprogram meta-assemblers were constructed.
n this article, we will use the term meta-assembler to
mean both program meta-assembler and microprogram
meta-assembler. The term assembler will stand for both
program assembler and microprograrm assembler, and
the term assembly language will be used for both pro-
gram assembly language and microprogram assembly
language. The term machine code will mean either
machine instructions or microinstructions. Furthermore,
the terms program assembler and software assembler will
be considered equivalent, as will microprogram assem-
bler and firmware assembler.
Figure 1 shows how a dedicated assembler works. A
meta-assembler functions differently. As explained ear-
lier, it is designed to be a tool that can be used to imple-
ment assemblers automatically and efficiently. A meta-
assembler is furnished with a ''meta-language.'' The im-
plenventation of an assembler with the help of a meta-
assembler is a matter of describing the desired assembler
to the meta-assembler, using the meta-language. Here,
we will call this description the assembler definition.
With the help of the assembler definition, the meta-
assembler can either generate the desired assembler or
adapt itself to operate as the desired assembler. In the
first case, we will call the meta-assembler a generative
meta-assembler. The functioning of such a meta-
assembler is shown in Figure 2. In the second case, we
will call the meta-assembler an adaptive meta-assembler.
Its functioning is illustrated in Figure 3.
Features of a meta-assembler. ldealiy, a meta-asSem1-
bler should be
The first feature is self-explanatory. The utility of a
meta-assembler is highly constrained if it is not portable.
The second feature refers to how easily assemblers can
be implemented with the help of the meta-assembler. If
this task is not considerably easier than implementing the
same assembler from scratch using conventional means
then the meta-assembler is useless.
The third feature refers to the power of the assemblers
which can be implemented with the meta-assembler, that
is, the power of the assembly languages which the assem-
blers are capable of assembling. Though no universally
acceptable criteria exist for measuring the power of an
assembly language, one can safely characterize an assem-
bly language as powerful if it
All but ''expressive power'' are self-explanatory. Ex-
pressive power has to do with the notation for coding the
assembly instructions, that is, with syntax and semantics.
More discussion on this follows below.
The fourth feature refers to the syntax and semantics
of the assembly languages that can be assembled by the
assemblers that can be implemented with the meta-
assembler. There should be no restrictions on syntax and
semantics.
The fifth feature refers to the ability of the meta-
assembler to be used for implementing both program
assemblers and microprogram assemblers.
We will see that no existing meta-assembler possesses
all these features.
Assembly languages are machine-dependent. They
provide the programmer with two sets of symbolic state-
ments for coding his programs. The first set comprises
the so-called ''pseudostatements'' or ''directives.'' The
second set comprises the ''machine instructions'' if the
assembly language is a program assembly language, or
the ''microinstructions'' if the assembly language is a
microprogram assembly language. Assembly languages
are usually equipped with a macro facility which makes
them extensible. That is, the programmer can define and
then use new symbolic statements through macros. These
new statements are equivalent to strings of pseudostate-
ments or machine instructions (microinstructions). A
macro Iacility mmakes ab assembly lai)guagw uuuiow and
more powerful.
Pseudostatements. This set of statements is not con-
nected with the instruction (or microinstruction) reper-
toire of a computer. These statements, which are directed
to the assembler and not to the hardware processor, pro-
vide the programmer with facilities for
Machine instructions. In machine language form, a
machine instruction is a string of fields, and each field isa
string of bits. For a particular instruction, the number of
fields is fied, as is the bit-length of each field. In symbolic
form (program assembly language form), a machine in-
struction is a string of symbolic entities.
There is a unique relationship between these two
forms. This relationship should be portrayed in a simple,
understandable, and straightforward manner tnrough
the chosen symbolic notation. The degree to which thisis
achieved is a measure of how successful a program
assembly language is.
In program assembly languages, the following nota-
tion has prevailed:
clabel> action verb > c 0perands-list> ccomments>
where C operands-list > has the notation
centity 1> entity 2> .. . entity n>
sC action verb > designates the functional operation
the machine instruction, while operands-listi
designates the operands required by the operation ofi
instruction. We will call this notation ''action verb no
tion,''
Each entity in operands-list> can be related to 4i
or more fields of the corresponding machine instructii
If the entity is related to more than one field, then tj
number of entities in a particular instruction will be ki
than the number of fields for that instruction. This reli
tion to more than one field is achieved through an si
coding scheme. In entities designating addresses, for si
ample, special characters which stand for the various@
dressing modes (direct, indirect, indexed, etc.) are u@j
The symbolic entity sVECT1(B2) might designatedi
direct addressing, indexing with register B2, and the[
bolic address VECT1. Thus, this entity is implii
related to three fields. We will call this type of ei
notation ''encoded entity notation,''
If each entity is related to only one field, hen 4i
assembly a value will be calculated directly from eachi
tyand no decoding will be required. We will call thisti
of entity notation ''non-encoded entity notation.''Ai
this notation, the three-field example above would re-
quire three separate entities.
In both these notations the ordering of the entities
within operands-list > is significant and cannot be
violated by the programmer. In practice, the number of
entities contained in most machine instructions is seldom
too large, so in both encoded and non-encoded entity no-
tation remembering the fixed ordering of the entities is
not a serious problem.
Microinstructions. Microinstructions are similar in
structure to machine instructions. However, they can
have more fields than machine instructions and may
designate more than one operation to be performed in
parallel. These two characteristics create two problems
concerning the symbolic form (microprogram assembly
language form) of microinstructions.
The first is that action verb notation is not very mean-
ingful for microinstructions, since this notation desig-
nates only one operation. For this reason the following
notation is used for microinstructions:
cbbee > centity 1> c entity 2> .. , c entity n> COrmments >
Each entity is related, in microcode form, to one or more
f 'dsof the corresponding microinstruction, We will call
th.s notation ''list notation,''
The second problem is to find a meaningful notation
for the entities of a symbolic microinstruction. This
problem is more difficult than the problem of finding a
notation for machine instructions, since microinstruc-
tions can have more fields. This problem has been tack-
led in various ways. The objective is to always keep the
number of entities as small as possible while at the same
time keeping the notation as meaningful as possible.
To keep the number of entities small, almost all micro-
program assembly languages use default values for those
fields to which no values have been assigned either direct-
ly or indirectly by the symbolic entities. This default
capability is used in three entity notations which we will
c: 1 ''positional notation,'' ''nonpositional notation,''
a:d ''free notation.''
Positional notation. In positional notation, each entity
is implicitly related to one field. The ordering of the en-
tities is significant, since it is the position of an entity
within the list of entities which implicitly designates the
field to which the entity is related. The value of the entity
Ws assigned to the implied field.
Nonpositional notation. In nonpositional notation,
the ordering of entities within the list of entities does not
matter. They can be written in any order.This notation is
further divided into three different notations which we
will call ''function reference notation,'' ''value mne-
monic notation,'' and ''keyword notation,''
In function reference notation, an entity has
the format
tname(Pii. , .. ;tfn).-
where fname is the mnemonic name of an implied func-
tion and P1, . . .,Pn is a list of parameters. This nota-
tion provides a useful mechanism for parametrically
assigning values to one or more fields; that is, it has a
dynamic multifield assignment capability. This notation
is appropriate for expressing the various operations
designated in horizontal microinstructions, since it codes
them as function references,' The mnemonic name of the
function in this case is the mnemonic name of an opera-
tion, and the parameters of the function are the operands
on which this operation acts.
In value mnemonic notation, an entity can only be a
mnemonic name which is implicitly associated with a
value and a specific field. This value is assigned to the
field. This notation permits the use of a mnemonic name
for assigning a predefined value to a predefined field.
In keyword notation, the entity has the format
fieldname = value
where fieldname is a preassigned mnemonic name of a
specific field and value is a value to be assigned to the
field that is associated with the mnemonic name field-
name. This notation provides a straightforward way to
assign a value to a field.
Free notation. In free notation, there are no con-
straints, in principle at least, on the notation the pro-
grammer uses for assigning values to the fields of a
microinstruction. He designs a notation which suits him
and then uses it to code the microinstructions.
Assembly-language-independent statements, All pseu-
dostatements are computer-independent, except those
used for data generation. Data types supported by con-
temporary hardware are the integer numbers and the
floating-point numbers. There are only a few hardware
representations of these data types, as well as only a few
representations of the character set. One for each data
type is used in a particular computer. So, data generation
pseudostatements can be parameterized with respect to
the representation of the data they generate. Hence, all
pseudostatements are assembly-language-independent.
Machine instructions (or microinstructions) are
assembly-language-dependent. This observation, namely
that the set of pseudostatements is assembly-language-
independent while the set of machine instructions (or mi-
croinstructions)is assembly-language-dependent, greatly
influenced the development of meta-assemblers.
Every meta-assembler is furnished with a meta-
assembly language. The task of implementing an assem-
bler with a meta-assembler is restricted to preparing the
assembler definition (see again Figures 2 and 3) by means
of the meta-assembly language. Assembler definition by
means of a meta-assembly language is conceptually equi-
valent to assembly language definition by means of a
meta-assembly languag.
As has already been noted, the set of statements pro-
vided by any assembly language can be divided into two
subsets. The first comprises those statements that are
assembly-language-independent and the second those
tha atc assembly-language-dependent, Thus, it is possi
ble to think of a family of assembly languages, all mem-
bers of which have the same syntax, the same semantics,
and the same set of assembly-language-independent
statements. It is obvious that the definition of an
assembly language in this family will be restricted to the
definition of the assembly-language-dependent state-
ments only. This definition is done via the meta-assembly
language.
All meta-assemblers constructed so far are based on
the above principle. With them, one can implement
assemblers which are able to assemble assembly lan-
guages that belong to an assembly language family that is
meta-assembler-dependent. The task of implementing an
assembler with a meta-assembler will be confined to
defining the assembly-language-dependent statements of
the assembly language that the assembler under im-
plementation will assemble.
The definition of assembly-language-dependent state-
ments is done by means of the meta-assembly language
of the meta-assembler. To make this task easier, meta-
assembly languages should provide facilities to
What meta-assembly language is the most suitable is
still unknown. The ones utilized so far can be divided in-
1o two caeguries, Inose in the first caucguy are miacro
languages. They provide some directives for specifying
the information required by (1) and (2) above, and they
provide macros for specifying the information required
by items (3) to (6) above. Meta-assembly languages in
this category were the first ever used. The meta-assem-
blers furnished with them are very powerful, though they
have some drawbacks. Among them is their inability to
use encoded entity notation and free entity notation, As
a consequence, these meta-assemblers cannot be used to
directly implement assemblers as specified under the pro-
posed IEEE Microprocessor Assembly lLanguage Draft
Standard.'
Those in the second category do not use macros. In
principle, they allow encoded entity notation and free en-
tity notation. In practice, however, they greatly restrict
their use, since these notations are completely meta-
assembler-dependent.
In an earlier part of this article, we divided meta-
assemblers into two categories according to the way
they function. We called those in the first category
generative meta-assemblers (see again Figure 2) and
those in the second category adaptive meta-assemblers
(see again Figure 3).
The adaptive meta-assemblers can be divided into two
subcategories according to the nature of their meta-
assembly languages. We will place in the first sub-
category those which have a meta-assembly languag<
which is a macro language. We will call them ''Ferguson-
type meta-assemblers.'' We will place in the second sub-
category those which have a meta-assembly language
which is not a macro language. We will call these ''non-
Ferguson meta-assemblers,'' The first meta-assemblers
were of the Ferguson type. Indeed, the term ''meta-
assembler'' originally embraced only this category.?-
Thus, assemblers can be classified according to the
scheme shown in Figure 4.
Meta-assemblers evolved as a generalization of co-
ventional assemblers. As a consequence, the same tech-
niques were used for the construction of the first mete;
assemblers as were used for the construction of conven-
tional assemblers. This was the case with the adaptive
Ferguson meta-assemblers.
For all other types of meta-assemblers, a mixturt-
of techniques, derived from ones used in the construc:
tion of conventional assemblers, compilers, and metu
compilers, has been used.
Another method for constructing a meta-assemblgj
the conversion of an existing, conventional assembleij
to a meta-assembler. This can be done by emploYi
special preprocessors and postprocessors and by usr2
the macro language of the conventional assembler a6
meta-assembly language.'-'A The meta-assemblers th'
have been implemented in this way have also been adap-
tive Ferguson meta-assemblers.
Meta-assemblers are a special case of meta-compilers.
Theoretically, a meta-compiler can be used as a meta-
assembler. However, it seems that there have been some
problems when existing meta-compilers have been used
this way. Besides these problems, meta-compilers are not
widely available. Nonetheless, attempts in this direction
have been reported,.1 We should note that meta-
assemblers can be used to complement compilers and
meta-compilers. In this case, the high-level programming
language is compiled into assembly language,'' which is
then translated into machine code. The second step can
be performed by a meta-assembler.
For various reasons, the translation of assembly pro-
grams into machine code is done in three separate phases.
In the first phase (assembly phase), an ''assembler''
transforms the assembly program into a form of code
called ''assembled code.'' In the second phase (linking
phase), a ''linker'' transforms the assembled code into
another form of code called ''linked code.'' In the third
phase (relocating phase), a ''relocating loader'' trans-
forms the linked code into still another form of code
called ''loadable code.'' The loadable code contains the
machine code in absolute form; it can be loaded into the
memory of the target computer with the help of another
tool called the ''absolute loader,'' Note that the linker
and relocating loader can be combined into one tool, the
relocating linking loader.
Splitting the translation process into the subprocesses
described above is desirable because it allows program
modules from different language processors (e.g., as-
semblers, Fortran compilers) to be combined, and it
greatly facilitates the development of modular programs.
Another translation scheme has the assembler directly
produce loadable code, in which case no linkers and
relocating loaders are required. With this scheme,
however, we lose the advantages cited above.
If the assembler-linker-loader scheme is adopted, then
a meta-assembler should be accompanied by a ''meta-
linker,'' a ''relocating meta-loader,'' and an ''absolute
meta-loader'' (or by a ''combined meta-linker relocating
meta-loader'' and an ''absolute meta-loader''). These
tools facilitate the development of linkers, relocating
loaders, and absolute loaders in the same way meta-
assemblers facilitate the development of assemblers.
Special meta-languages are required for this purpose,
namely a ''meta-linking language'' for the meta-linker
and a ''meta-loading language'' for the relocating meta-
loader. The first is used for describing linkers to the
meta-linker and the second for describing loaders to the
relocating meta-loader, in the same way that a meta-
assembly language is used for describing assemblers to
the meta-assembler.
It has been demonstrated that the linking process is
target-computer-independent,'. 12-14 Hence, meta-
linkers can be constructed as simple linkers and no meta-
linking language is required.
A list of existing meta-assemblers is presented in Table
1. Although I do not claim that the list is complete, 1
believe that it is a good approximation to a complete list.
The table presents general characteristics of meta-
assemblers, such as category, intended usage, and port-
ability. Also given are details about the construction of
the meta-assemblers, such as who constructed them, on
which host computer they were developed, and in which
language they are coded. More important, the table is set
up to provide answers to questions about the assemblers
that can be implemented by each meta-assembler: What
sort of assembly languages do they assemble? Can they
be readily used or does the user have to develop com-
plementary tools such as linkers and loaders?
The table is divided into five sections headed general,
construction, assembly langtlage, meta-assembly lan-
4ge, and complementary tools.
The general section is divided into five columns. The
first gives the name of the meta-assembler. The second
gives the class of the meta-assembler according to the
classification scheme shown in Figure 4. The third col-
umn indicates the meta-assembler's usage as intended by
its constructor-that is, the kind of assemblers that it can
implement: software assemblers, firmware assemblers,
or both. The fourth column gives a ''yes/no'' answer to
the question of the portability of the meta-assembler.
The fifth column cites the pertinent literature, to the best
knowledge of the writer of this article.
The construction section of the table is divided into
three columns. The first gives the name of the construc-
tor of the meta-assembler, usually a computer manufac-
turer, software house, or research institute. In the first
and second cases, the names are given. In the third case,
the symbol ''R'' indicates that the constructor is a
research institute, the name of which, and possibly the
names of the particular scientists involved, can be found
in the cited literature. The second column gives the name
of the computer system used for the development of the
meta-assembler. The third column gives the name of the
programming language in which the meta-assembler is
coded.
The assembly language section of the table gives infor-
mation about the assembly languages that can be assem-
bled by the assemblers that can be implemented by each
meta-assembler. This section is divided into seven sub-
sections. The first subsection gives a ''yes/no'' answer to
the question of whether action verb notation (column
one) or list notation (column two) can be used. The sec-
ond subsection concerns entitv notation and indicates
''yes,''''no,'' or ''not applicable'' for the various entity
notations. The positional and non-encoded notations
have been grouped. The same has been done for the free
and encoded notations. The third, fourth, and fifth
subsections give a ''yes/no'' answer to the question of
whether the corresponding assembly languages have,
respectively, an expression evaluation capability, a
macro facility, and conditional assembly statements. The
sith subsection indicates whether the addresses are
calculated at assembly time relative to the start of the
pwgram module, or are calculated as absolute addresses.
In tUhe first case, the indication is ''yes'' and the object
code produced by the assembler needs to be processed by
a relocating loader. The seventh subsection gives a
''yes/no'' answer to the question of whether the assem-
bly language permits the splitting of assembly programs
into modules that can be separately assembled.
The general and assembly language sections of Table 1
indirectly give information about the meta-assembly lan-
guage of each meta-assembler. Some additional informa-
tion is given in the meta-assembly language section. The
fitst column gives the maximum length in bits of the ad-
dressing unit. The second column gives a ''yes/no''
answer to the question of whether the meta-assembly
language permits the programmer to declare the data
ty e representation-for example, for negative integers
o1s-complement or two's-complement representation.
The third column gives a ''yes/no'' answer to the ques-
tion of whether the programmer can create his own error
diagnostics. The fourth column gives a ''yes/no'' answer
to the question of whether the programmer can perform
checks on the values of the various fields during the code
generation process. This is useful in assembling horizon-
tal microinstructions, a process in which the values of
some fields are related. The ability to confirm that these
relations hold is a contribution to microprogram correct-
ness at the assembly level.
The complementary tools section gives an answer to
the question of whether each meta-assembler is comple-
mented with a meta-linker and a relocating meta-loader,
or : ith a combined meta-linker/relocating meta-loader.
The answer ''not applicable'' is given when such tools are
not needed. (This means that the implementable assem-
blers generate absolute object code.)
Undoubtedly demand exists for both software and
firmware assemblers. This is due mainly to the exploding
use of microcomputers, both monolithic and bit-sliced.
Meta-assemblers can be the answer to the problem of im-
plementing microcomputer assemblers easily and quick-
ly, However, very powerful meta-assemblers are required
because of the great number of incompatible microcom-
Duter assembly languages now in use.
Constructing meta-assemblers is a difficult task. So is
using them. Existing meta-assemblers are of limited
usefulness because they are not powerful enough. And
sonstructing the relocating meta-loaders that are needed
t0 complement meta-assemblers is not a trivial task
either. Standardization seems to be the key to an easy
solution to these problems.
The areas requiring such standardization are program
assembly language, microprogram assembly language,
meta-assembly ianguage, relocatable object code lan-
guage, meta-loading language, and absolute object code
language.
A draft standard already exists for program assembly
languages. This standard could be expanded to include
language facilities not covered by the present draft, such
as macros, conditional assembly directives, and program
modularization directives. This standard, by suitable ex-
pansion, could also embrace microprogram assembly
language. For relocatable object code language, the one
developed and in use today at CERN P could be used as
is, or suitably adapted. No standard exists for absolute
object code language, but devising one should be simple.
The question of finding standards for meta-assembly
language and meta-loading language is still open-and
difficult. However, the development of such standards
will be facilitated if it is based on already-adopted stan-
dards for assembly language and relocatable object code
language.
I would like to thank the referees for their constructive
comments.
Computer vision is a subset of the
machine intelligence systems area. The
goal of a computer vision system is to
interpret the given ''visual'' data and
to use the interpretation to complete a
task. Typical tasks include (1) the navi-
gation of autonomous vehicles on the
land, in the air, or under the sea, (2)
the assembly or inspection of manufac-
tured parts, and (3) the analysis of
microscopic images and medical x-rays.
In a number of applications, the goal of
the vision system is to identify and locate
a specified object in the scene. In such
APPENDD A. DIFFERENTIAIL GEOMETRY OF
SURFACES
APPENDID B. SURFACE PROPERTY
MEASUREMENTS
ACKNOWLEDGMENTS
REFERENCES
cases, a vision system must have full
knowledge of the shape of the desired
object. Such a priori knowledge of
the object is provided through a model
of the object, and in most cases it con-
tains information regarding the geome-
try of the objects; some models may
contain additional information such as
thermal and stress properties of the
objects. A vision system which makes use
of an object model is referred to as a
model-based vision system, and the gen-
eral problem of identifying the desired
object is referred to as object recognition.
While there is no single definition of the
object recognition problem, the objective
is to identify a desired object in the scene
and to determine its exact location and
orientation.
An ideal model-based vision system
should be able to locate objects in a scene
assuming that any of the following are
true: (1) objects may have arbitrary and
complicated shapes or forms; (2) objects
may be viewed from any direction; and
(3) objects may be partially occluded by
other objects. Specifically, such a system
may be used in determining the location
of grasp sites for a robot arm to mani-
pulate an object, in the navigation of a
robot or an autonomous vehicle, and in
the assembly and inspection of parts
in a manufacturing environment. For
instance, robots with such vision capabil-
ities may carry out instructions regard-
ing the handling of objects with fewer
specifications than are currently required
and with more tolerance for minor
disturbances.
To design such systems, system
designers must resolve the following
issues: (1) the type of sensor for data
collection, (2) the methods of construct-
ing the necessary object model, (3) the
means of describing the collected data
and the model, and (4) the methods of
matching the object descriptions obtained
from the input data to that of the model.
The sensor determines the resolution (the
total number and frequency of sample
points) and precision (the accuracy of
each sampled point). More importantly,
it determines whether the data provides
2D or 3D information of the scene. Mod-
els provide the a priori knowledge of the
vision system. Representations are used
to describe the collected data and the
object model, a key issue in the field of
computer vision. The representations dic-
tate the matching strategy, its robust-
ness, and the system's efficiency. Also,
the descriptions are used in the calcula-
tions of various properties of objects in
the scene needed during the matching
stage. Matching strategies are performed
at run-time and must resolve many
ambiguities that exist between the data
and the model descriptions. Once the cor-
rect match has been determined, the ori-
entation and translation of the located
object, with respect to the model, can be
calculated, completing the task of object
recognition.
This paper surveys recently published
papers' addressing the problem of the
model-based recognition of objects in 3D
dense-range images.' Section l discusses
in detail the issues outlined above to
introduce the specific problems of model-
based vision. Next, Section 2 reviews var-
ious sensing modalities, beginning from
the data collection step, giving emphasis
to sensors providing 3D information.
Section 3 reviews various low-level pro-
cessing procedures necessary as a prestep
to the description and recognition of
objects. Section 4 reviews various repre-
sentations used to describe the objects.
Section 5 discusses modeling schemes,
giving emphasis to the computer-aided
design (CAD) systems used in the exist-
ing model-based vision systems. Section
6 presents a study of the matching
strategies. Finally, Section 7 presents the
summary and concluding remarks. A
brief overview of differential geometry of
surfaces is presented in Appendix A, and
in Appendix B various computational
methods to calculate surface properties
are reviewed.
The introduction outlined the issues
involved in the design of a model-based
vision system. This section analyzes fur-
ther these issues: data collection, repre-
sentation, model construction, and the
matching strategies (see Figure 1).
The first issue addressed in a com-
puter vision system is data collection,
which may be performed using one or
more of the many existing modalities.
The intensity camera is perhaps the most
commonly used sensing module, measur-
ing visible light. The output of the cam-
era from a scene is digitized to provide a
2D array of numbers; each number corre-
sponds to the average intensity sensed in
a sampled, typically square area. Exam-
ples of other sensory modules include
thermal cameras, which measure the
emitted thermal radiation rather than
the emitted visible light, and laser range
scanners and sonar sensors, which are
used to calculate the distance to the
objects in the scene. These sensors each
provide information on a different aspect
of their environment; the choice of the
sensor is largely application dependent.
For the task of 3D object recognition,
various object dimensions and surface
shape information are essential. Two
general approaches exist to collect the
necessary data. In the first approach,
sensing modalities, such as intensity
cameras, are used, and many depth cues
are analyzed to recover the necessary 3D
information. In the second approach,
external energy sources, such as lasers,
are projected onto the scene. While the
first approach is preferred (since no
external sources are required), the recov-
ered data lacks the necessary resolution
and precision for many common tasks;
therefore, the second approach is used
most frequently in 3D object recognition.
Section 2 studies the schemes for collect-
ing 3D information from the scene.
The second issue in a computer vision
system is to represent the collected data
and the modeled object. The 2D array of
numbers provided by the sensor is not
of much use in its ''raw'' form. A suitable
representation scheme must, therefore, be
used to describe the data and the model.
A representation is desirable if it is (1)
unambiguous (no two objects have the
same representation), (2) unique (there is
a single description for each object using
the representation scheme), (3) not sensi-
tive (with respect to missing data points,
such as in the cases of partial occlusion),
and (4) convenient to use, in the match-
ing stage, and to store. Representation
is a key issue in computer vision. Vari-
ous schemes, such as surface-based and
volumetric-based representations, will
be discussed, with emphasis given to
recently published representation
schemes.
The construction of object models is
the third issue which must be addressed
in a model-based computer vision sys-
tem. There are two main approaches for
model construction. In the first approach,
the actual objects are used to generate a
model; i.e., data points obtained from
several viewpoints of the object are inte-
grated in a coherent fashion to provide
information from all the viewing angles.
In the second approach, a CAD system is
used, and a set of predefined primitives
allows the user to construct interactively
the model of an object. Much of the ear-
lier work in object recognition used the
first approach; however, most recent
research efforts use a CAD or similar
system. Both approaches are reviewed,
and the advantages and the disadvan-
tages of each are discussed.
Once the appropriate descriptions are
derived from the data and the models,
the vision system is able to match the
two descriptions. This is performed
in two steps. In the first step, a corre-
spondence is established between the two
sets of descriptions. Since in most cases
data is collected from a single view and
there may be partial occlusions present,
the matching strategy must establish
correspondence between the partial
description of the object and its full model
description. The correct match of the
collected data to the representation of
the given model ''establishes an interpre-
tation'' of the input data [Ballard and
Brown 1982]. The exact choice of the
matching strategy is dependent on
the representation scheme, the applica-
tion, and the system designer's expertise.
In the second step, using the established
correspondences, a geometrical transfor-
mation (usually a rotation matrix and a
translation vector) is derived such that
the model may be transformed to the
orientation of the object in the scene.
An important aspect of any computer
vision system is its data acquisition mod-
ule, This section reviews the schemes in
which 3D information is acquired from
the scene. The task is performed in one
of two approaches: passive or active. In
the passive approach, 3D information is
inferred from the scene using existing
energy in the environment, such as
reflected light. In the active approach,
the 3D information is derived by project-
ing external energy waves, such as sonar
waves and laser light. As mentioned in
the introduction, 3D data recovered from
current passive approaches lack the nec-
essary precision and resolution for 3D
object recognition; in this section, the
active methods are reviewed [Besl 1989;
Nitzan 1988; Freeman 1988; Fu et al.
1987; Kanade 1987; and Ballard and
Brown 1982].
Active-range sensing can be divided into
two main classes. In the first class, the
principle of triangulation is used. Each
point in a scene is highlighted, using a
sheet of light, and observed by the sen-
sor. Then, using the known geometry of
the imaging system, the distance of each
highlighted point to the sensor is calcu-
lated. In the second class of active-range
sensors, known as time-of-flight sensors,
a signal is emitted, and its return time is
measured and used in calculating the
distance.
This method uses a laser source which
projects a sheet of light onto the scene,
casting a line on the objects'' (see Figure
2). A camera is positioned so that the
laser line is visible. Using the known
imaging geometry, i.e., the distance
between the laser source and the camera
(referred to as the baseline), their angles
a, and a,) with the z-axis (the axis
along which depth is measured) and by
applying the principles of triangulation,
the distance of the illuminated points,
along the cast laser line, to the baseline
is calculated (see Figure 2). Sweeping the
sheet of light across a scene results in a
range map. The sweeping of the sheet is
performed in one of two ways: a rotating
reflector can be used to project the sheet
of light, or the objects can be placed on a
stage which slides by or rotates in front
of the sheet of light. In all cases, the
laser may not illuminate some areas, or
the sensor may not be able to detect the
illumination (i.e., the object concavities
may occlude some areas of the object);
and there are no data points at those
locations of the scene. These areas of
missing points are referred to as shad-
ows.' An advantage of using the stage is
that more uniform spatial sampling
can be achieved, satisfying a common
assumption for surface property mea-
surement approaches (see Figure 3 and
Appendices B and A). The disadvantage
of using the stage is that it limits the size
and number of objects in the scene, and
it may not always be feasible to place the
objects on the stage. Also, the accuracy of
the motion control mechanism of the
stage or the reflector must also be taken
into consideration in determining the
approach used.
In most of today's available triangula-
tion systems, the time required to scan a
typical 256 x 256 scene with 8 bits of
accuracy is on the order of minutes.
However, some triangulation-based sys-
tems exist which are capable of scanning
scenes in seconds. For example, Rioux
[1984], and Rioux et al. [1989] developed
a system able to scan a 256 x 256 pixel
scene with a precision of 0.5 mm in less
than one second. Kanade et al. [1989]
and Gruss et al. [1990] introduced a small
prototype of a VLSI-based system, which
performs the triangulation on a 4 x 4
array of specialized sensors on a single
CMOS chip. Once fully developed, this
system could significantly reduce the
acquisition time in triangulation-based
range sensors.
To reduce the data acquisition time,
several sheets of light in parallel may be
projected onto the scene (first introduced
by Will and Pennington [1972[). The dis-
advantage of this approach is that,
depending on the scene's geometry, a
stripe position may be shifted more than
the existing spacing between the stripes
[Mundy and Porter 1987], causing ambi-
guities. Therefore, it is necessary to
determine which sheet of light is pro-
jected at each pixel in the scene. There
are several ways to resolve this ambigu-
ity. Mundy and Porter [1987] and
Inokuchi et al. [1984] use a set of gray-
coded stripes, assigning to each stripe a
unique code value. Boyer and Kak [1987]
and Vuylsteke and Oosterlinck [1990] use
only one set of simultaneous projections,
instead of a series over time, to obtain
the range information from the scene.
The advantages of using a single set of
patterns are that less time is spent pro-
jecting light patterns, and more impor-
tantly, nonstatic scenes may be scanned,
allowing a broader application area such
as robot navigation, motion analysis, and
moving-object recognition. Boyer and Kak
use a few colors to project single-colored
stripes. Vuylsteke and Oosterlinck use
a binary pattern to illuminate the scene.
Tajim a and Iwakawa (of NEC)
[1990] have reported on a triangulation-
based sensor using collimated white light
diffracted by a grating to form a rainbow
pattern on the scene capable of producing
30 3D frames per second.
One of the disadvantages of a triangula-
tion-based laser scanner is the shadow
effect, where a region of the scene is not
visible to either the laser or the sensor.
In time-of-flight range finders, a laser
beam is emitted and received along the
same path, eliminating the shadow prob-
lem. However, since the system depends
on the return laser light to measure the
distance, high-energy laser sources, pos-
sibly harmful to the human eye, are
required. Also, most time-of-flight range
finders require complex electronics, rais-
ing the cost of such sensors. Two classes
of laser sources are used in time-of-flight
scanners: pulsed and continuous-beam
lasers.
In this class of range finders the laser
light is reflected, and its return time is
measured [Lewis and Johnston 1977];
since the speed of the laser light is known,
the distance can easily be determined.
Such devices have ranges of from l to 4
meters with a precision of 5 0.25 inches;
however, such precision requires sensi-
tive electronic instrumentation capable
of resolving 30-50 psec time intervals
Jarvis 1983a; 1983b].
The second class of time-of-flight range
finders uses a continuous-beam laser
rather than a pulsed one [Jarvis 1983b;
Fu et al. 1987]. In this method, the delay
6 in the returned signal (i.e., the 0 to 2m
phase shift with respect to the transmit-
ting signal) is used to measure the dis-
tance D to the object:
where A is the wavelength of the signal.
This measurement is performed one point
at a time, and the laser beam is scanned
horizontally and vertically across the
scene. An inherent problem in this
approach is that all distances corre-
sponding to 2m multiples of the phase
shifts will be measured as the same dis-
tance by the system (referred to as the
ambiguity interval); one may, however,
assume that 6 is less than 2m. Addition-
ally, since A is usually on the order of
nm, D has a very small range which
is not feasible for most object recogni-
tion tasks. The amplitude of the laser
light may be modulated, however, using
long-wavelength waveforms, effectively
increasing A.' However, the ''ambiguity
interval'' of the scanner still exists and
may be resolved by transmitting at sev-
eral frequencies and checking all fre-
quencies at the ambiguity intervals. To
decrease the effects of photon shot noise,
the distance at each point is measured
several times, and the average is used.
A problem common to all laser range
scanners is the specularity problem,
causing erroneous measurements in both
the triangular-based and the time-of-
flight methods. In laboratory setup it is
possible to decrease this problem by
painting the surfaces appropriately; how-
ever, in outdoor scenes this problem per-
sists. In addition, when laser lights are
used, other sources of error, such as
''speckle,' exist [Technical Arts 1987].
The speckle phenomenon is a conse-
quence of the laser light coherency, caus-
ing patterns of darkness and brightness
on object surfaces. This phenomenon is
particularly unfavorable since the result-
ing errors may not be averaged out over
time during data acquisition; rather, the
error must be reduced optically.
Various other approaches exist in recov-
ering 3D information from the scene.
Additional methods include: tactile sen-
sors, force and torque sensors, proximity
sensors (which can be capacitive, induc-
tive, or ultrasonic), Holographic interfer-
ometry (which requires the surfaces to be
flat and smooth), and Moire techniques
(which require no depth discontinuities
present in the scene). These sensors have
very limited use in computer vision
although they are used in other appli-
cations. A brief overview of these sys-
tems may be found in Fu et al. [1987]
and in Besl [1989].
Once the necessary measurements from
the scene have been made, the data must
be represented using symbolic descrip-
tions to enable the system to carry out
the specified high-level processes. The
first step in obtaining the symbolic repre-
sentation is the partitioning of the input
data based on the desired description. In
general, this step depends on the nature
of the input data and the higher-level
processes, such as matching. Range scan-
ners sample points on the surface; there-
fore, the vision system must rely on the
information derived from the surfaces.
Further, as outlined earlier, the ideal
vision system must be able to recognize
arbitrarily shaped objects from any view-
ing direction. Also, the system must be
able to cope with possible partial occlu-
sion from other objects in the scene and
with cases of self-occlusion (for example,
the inside of a cup is self-occluded when
the cup is viewed from one side).
The key to performing such tasks is
the means by which shapes are described
by the system, In a large number of cases,
local-surface properties, such as surface
cturvatture and sturface normal, are used
to describe the shapes. Briefly, surface
curvature is the rate at which the sur-
face deviates from its tangent plane, Cur-
vature is an important measure since
it is invariant to viewing directions
and does not change with occlusion.
More importantly, however, curvature of
smooth surfaces may be approximated by
using only a small neighborhood (i.e.,
a local neighborhood) on the surface.
Appendices A and B cover, in detail,
curvature, surface normal, and many
other surface properties used in shape
description.
Prior to the calculation of most surface
properties, such as curvature and sur-
face normal, the collected data must be
smoothed. The reason is twofold. First,
the measurements made by the sensors
are often inaccurate due to detector noise,
quantization, calibration errors, and as
mentioned earlier, other sources of error,
such as ''speckle.'' Second, the calculation
of curvature involves second-order par-
tial derivatives, magnifying the effects of
any noise present.
In the past, most noise processes
have been modeled as additive Gaussian
stochastic processes independent of the
scene [Pratt 1978]. While many studies
have been done on noise reduction for
gray-scale images [Rosenfeld and Kak
1982; Mastin 1985], very few such stud-
ies have been reported for range maps.
Rather, most researchers have adapted
noise reduction methods developed from
gray-scale images. Further, most studies
assume the error to exist only in the z
coordinate (the depth value) of the mea-
surements; i.e., the x and y coordinates
are assumed to be error free. With few
exceptions, making the assumption is not
true for cases where all coordinates have
been considered (see Jain and Dubes
[1988l and Jolliffe [1986]).
Brady et al. [1985] and Yang and Kak
[1986] have used Gaussian smoothing
[Marr and Hildreth 1980]. In Gaussian
smoothing, the data is convolved with
the rotationally invariant Gaussian filter
where (u, v) is the (intrinsic) coordinate
of each point in the data. Ideally, the
variance, o', is chosen such that the noise
in the data is suppressed while the ''sig-
nificant'' features of the data are pre-
served; however, larger o' s necessary
for effective noise reduction result in too
much blurring of the image and in the
loss of the image details. Alternatively,
the image may be convolved with a small
operator, such as
many times. Using the central-limit the-
orem, this operation approximates the
Gaussian window with a standard devia-
tion of yn , where n is the number of
iterations [Burt 1983]. In addition to
reducing the noise by Gaussian smooth-
ing, Besl and Jain [1988] approximate
the standard deviation of the noise and
use the measure to determine certain
thresholds used in the subsequent steps.
Ponce and Brady [1987] report on some
orientation-dependent errors observed
in smoothing with the Gaussian win-
dow. These errors are magnified dur-
ing the calculation of curvature values
partly because such calculations include
second derivatives). They suggest that
smoothing be performed in the intrinsic-
coordinate system (see Appendix A) to
minimize such effects. This may be done
by first approximating the surface nor-
mal at every point; then each point is
''moved along the direction of its normal,
a distance that depends upon the pro-
jected distances of the point's neighbors
from the tangent plane'' [Ponce and
Brady 1987, p. 227]. Simple iterative
averaging is equivalent to intrinsic-
coordinate smoothing when the surface
normal is along the viewing direction.
Citing expensive computational costs,
Ponce and Brady did not use this method
in their paper; however, an example was
given (see Figure 4).
Hoffman and Jain [1987] investi-
gated several smoothing techniques
such as the median filter, nearest-
neighborhood smoothing, and maximum-
likelihood smoothing [Hurt and Rosenfeld
1984]. In maximum-likelihood smooth-
ing, a square neighborhood of each pixel
is considered, and each pixel is replaced
by the average of a subset of the neigh-
borhood pixels. For example, in a 3 x 3
window, four 2 x 2 subsets are consid-
ered, and the central pixel is then
replaced by the average value of the pix-
els in the subset which is the most repre-
sentative neighborhood. Using such an
approach, Hoffman and Jain report that
on synthetic images, the standard devia-
tion of the added noise was dropped by a
half using a 3 x 3 window.
Saint-Marc and Medioni [1988] presen-
ted an adaptive smoothing filter, which
is applicable to intensity or range images.
The image is smoothed iteratively with
a monotonically decreasing function
(monotonically decreasing as a func-
tion of increasing discontinuity proba-
bility) [Perona and Malik 1987]. To
preserve tangent orientation discontinu-
ities as well as step discontinuities in
depth, important measures in 3D vision,
the authors (Saint-Marc and Medioni)
suggest applying the filter to the deriva-
tives of the signal rather than to the
original signal. On the other hand, the
smoothed derivatives may only be used
to calculate the curvature and the sur-
face normal; lower-order properties, such
as the surface area of a patch, are not
computable directly. The integration of
the smoothed derivatives is therefore
necessary, a process which may introduce
further inaccuracies.
Once noise reduction processes are
completed, many of the surface proper-
ties such as curvature and surface normal
see Appendix A), may be approximated
using one of two general approaches. In
the first method, a mathematical func-
tion, such as a spline, is fitted to the
data, and then the necessary partial
derivatives are solved at each desired
Data structures used in machine vision are often
classified as either ''iconic'' or ''symbolic''. An iconic
data structure is one whose principal organization is
that of a two-dimensional array. The elements of the
array may be bits, integers, real or floating-point
numbers, or more complicated structures. Each cell
of an iconic data structure is implicitly associated
with a location in two-dimensional space, by virtue
of its being indexed by two numbers. On the other
hand, a symbolic data structure, although it may
represent pictorial information, takes the form of a
scalar, list, graph, string, or table. Unlike an iconic
structure, spatial information, if it is to be included
in a symbolic data structure, must be explicitly repre-
sented, for example, by including coordinate pairs
in the structure. It is usual for iconic data structures
to be used for image data (including ''intrinsic
images'') and symbolic ones to be used for more
abstract information such as scene descriptions in
terms of regions and relationships, highly codified
shape descriptions, and semantic models or inter-
pretations.[')
Corresponding to the distinction between iconic
and symbolic data structures in machine vision, there
isa separation of ''retinotopic'' and ''nonretinotopic''
descriptions of neural areas in the neurophysiology
of the mammalian visual system. A retinotopic area
is one in which the neural activity is generally in
spatial correspondence (i.e. mapped in continuous
fashion) with the activity in the retina. A non-
retinotopic area is one in which the activity shows
no general spatial dependence upon the distribution
of activity in the retina.
Several factors motivate the study of special iconic/
symbolic architectures. Most importantly, parallel
image-processing systems such as the CLIP4 and
the MPP,) while very effective for point-neigh-
borhood image transformations, lose much of their
speed advantages when they must communicate with
single-processor hosts in computations of such non-
iconic transforms as chain encodings, polygonal rep-
resentations, Hough transforms, region-adjacency
graphs, syntactic descriptions of shape, or schema
instantiations.
A second factor is the awkwardness of computing
''hypothesis maps'' for topdown image analysis on
existing architectures; such operations seem to call
for special symbolic-to-iconic hardware that can per-
form certain kinds of ''plotting'' very rapidly.
A third motivating factor is that a study of the
architectural problems of iconic/symbolic trans-
formations may suggest new computational models
for related information-processing activity in natural
(human and animal) visual systems, and conse-
quently, to improve our understanding of natural
vision.
We note that, in general, multiprocessor systems
exist which can support both iconic and symbolic
processing, but these are expensive, and are non-
optimal for iconic-to-symbolic processing. Examples
of these include the following commercial and
research prototype systems: the BBN Butterfly,
Columbia University's Non-Von, the NYU Ultra-
computer, the CalTech Cosmic Cube, and others. A
survey of some of these parallel systems may be
found in reference (4).
In the following sections we present a discussion
of the relative strengths and weakneses of special
architectures that support rapid iconic-to-symbolic
and symbolic-to-iconic transformations.
One can classify the proposed architectures for
iconic/symbolic processing into two types: (1) hard-
wired transformation devices and (2) interfaces. The
hardwired devices provide specialized computing
capability for particular iconic-to-symbolic trans-
formations. The interfaces, on the other hand, make
it easy for iconic processors and symbolic processors
to communicate efficiently and leave the actual pro-
cessing to these more conventional components.
Four particular approaches are reviewed in this
article. Two of these are hardwired transformation
devices and two are interfaces. Each of the four
approaches is presented here in a somewhat abstract
form. That is, each method is described with a com-
putational model which is somewhat simpler than
what one might actually implement. Nonessential
features are omitted from the models to facilitate
comparison and analysis of the basic approaches.
Each of our four models is based upon a proposed
device or system that has appeared in the literature.
Our ''Image-Function Inverter'' is modeled after the
''ISMAP'' (Iconic/Symbolic MAPper) proposed by
Kent at the National Bureau of Standards and sub-
sequently developed by Aspex, Inc. of New York.')
The ''Chain-Run Encoder'' described here is based
upon a device designed by Pfeiffer at the University
of Washington.''' (Pfeiffer is now at New Mexico
State University.) The ''Bi-Modal Memory'' system
has been proposed by Tanimoto, and the ''Tile-
Based Interface'' is a model inspired by the
''CAAPP/SNAP'' (a joint effort between the Uni-
versity of Massachusetts-'-' and the University of
Southern California'? which was an important step
in the development of the DARPA Image Under-
standing Architecture). The first two of these pro-
posals fall into the hardwired-device category. The
other two are essentially interface schemes.
At the time of this writing, one of these devices
has been built and tested. The ISMAP is the least
complicated and is now available as an option for
the PIPE from Aspex, Incorporated of New York.
Two others have had chips designed and fabricated
(the BMM and the chain-run encoder), but the chips
have not yet been integrated into an overall system.
These four approaches give one perspective on the
range of problems and possible solutions that arise
in studying the issue of iconic/symbolic architecture.
Figure 1 is a chart which offers an analysis of the
appropriateness of each of these four architectures
to the iconic-to-symbolic computations listed. It is
assumed that each iconic/symbolic architecture
would be used in the context in which it is proposed.
lLater sections of this article describe some of the
algorithms involved in this analysis.
The transformations between iconic and symbolic
representations of pictorial information constitute
what may be called ''intermediate-level'' computer
vision. This general problem of intermediate-level
vision has been explored in a collection of papers,'')
with a working definition given in the first of them.'!l!
The three levels of processing for computer vision
are therefore the following:
Although this report is concerned primarily with
the intermediate level, the other levels must be men-
tioned at least to the extent to which they constrain
or help to solve the intermediate-level problems.
One can organize a treatment of intermediate-
level computer vision by focussing on several selected
problems. Let us consider six:
terms of its runs. Such an encoding may consist of
the starting coordinates for a contour, and a list of
runs, each described by a symbol (direction) and a
repetition count. Alternatively, each run may be
described with a pair of endpoints (since the run
represents a straight line segment). Several vari-
ations are possible, making use of absolute or relative
coordinates, explicit or implicit coordinates, runs in
order or out of order, etc. A chain-run encoding may
be a preliminary step towards obtaining the chain
code, or it may be regarded as an alternate form of
the chain code which, for some images, may be more
compact a representation than the chain code.
4, Hough transform. In order to detect the pres-
ence of lines or curves in an image, the Hough
transform or a generalized form of the Hough trans-
form may be used. For lines, the Hough transform
works by quantizing the space of possible values of
the parameters for the line equation. Each ''bin'' of
the quantized space is regarded as an accumulator
which accumulates evidence for the presence of a
line with a particular pair of parameter values. For
each pixel of the source image, a set of bins is
identified and a weight is added to each bin; the
weight is proportional to the extent to which the
source pixel seems to be part of a line (e.g. how
bright the pixel is).
5. Partial Hough transform. Rather than compute
the evidence for each of all the possible lines in the
image,it is often sufficient to determine the evidence
for the most prominent (one or a few) lines. When
this is sufficient, much of the computation can be
eliminated.
6. Region-adjacency graph. Some kinds of
machine vision require that a symbolic description
of a scene be computed before the objects are ident-
ified in the scene. One basis for a symbolic descrip-
tion is a graph structure called a ''region-adjacency
graph'', The problem of computing a region-
adjacency graph (RAG) is to take a segmentation
map (a two-dimensional array in which each element
contains the unique region-number for the region to
which it belongs; each region is a four-connected set
of pixels), and to produce a graph having a single
node for each region, and having an edge between
two nodes if and only if the corresponding regions
are adjacent (share a common boundary). A region-
adjacency graph is typically represented using adjac-
ency lists.
These six problems form a representative sample
of iconic-to-symbolic transformations. We use them
here to compare the four model architectures for
iconic/symbolic computing.
Other sets of intermediate-level operations have
been suggested as a basis for comparing architectures
such as the set consisting of area, perimeter, con-
nected components, convex hull, closest points, and
diameter.'? This set emphasizes problems of com-
putational geometry, which are, no doubt, of wide
interest. The set presented in this paper, however,
focusses on pixel-value/coordinate relation inver-
sion, chain encoding, line detection and sum-
marization of region adjacency. Such a set covers a
variety of image processing styles that are more
typical of picture-processing applications, and this
set is particularly useful in bringing out major dif-
ferences in the architectures under discussion.
Several strategies can be used to come up with
machines that can efficiently handle iconic-to-sym-
bolic transformations. One approach is to begin with
an iconic image processor and then augment it gradu-
ally, adding capability for more and more symbolic
or abstract computations.This approach has received
the attention of theoretical computer scientists and
has supported some interesting mathematical
results,)
Another approach is to attempt to tightly integrate
radically different kinds of processors, each of which
is optimized for a particular style of processing. This
approach can produce powerful systems that are
practical and easier to program than totally new
architectures. However, they may be harder to char-
acterize theoretically in an elegant way. The com-
bination of a pipelined image processor with the IFI
is a good example of such an architecture. The BMM
together with an iconic subsystem and a symbolic
processor network is another.
A third approach is to try to make general MIMD
systems cheaper and more parallel than they are
now. If the cost of the general-purpose systems could
be brought down far enough, and the parallelism
increased enough, they would be more effective for
iconic-to-symbolic transformations than they are cur-
rently.
Next is a presentation of several particular archi-
tectures, and descriptions of how they handle iconic-
to-symbolic data transformations.
A device that we shall call the IFI (Image-Function
Inverter) is modeled after hardware proposed by
Kent in conjunction with Aspex, Inc of New York
to lend extra capability to the PIPE (''Pipelined
Image Processing Engine'') system. That device is
known as the 'ISMAP'' (Iconic/Symbolic MAPper).
The ISMAP is consistent with the PIPE convention
that computations are organized according to the
video field rate (i.e. one field is processed in each
one-sixtieth of a second). Consequently, the ISMAP
(and our abstraction, the IFI) scans an image at the
frame rate, in its normal operating mode.
Since the PIPE is a system which transforms
images into images, it is an iconic processing system,
and it does well at low-level computer vision tasks
such as filtering an image. On the other hand, the
IFI is designed to compute more global features of
an image such as the counts one obtains in a histo-
gram of pixel values. The IFI is illustrated schem-
atically in Fig. 2.
The normal operation of the IFI consists of three
steps. In the first step, the source image is scanned,
and a histogram of pixel values is produced by the
IFI in a special memory buffer. The second step
produces a cumulative histogram from the normal
histogram. The cumulative histogram contains, in
each bin, the number of pixels of the source image
that have a value less than or equal to the bin index.
In the third step, the image is rescanned and for each
pixel value, the IFI makes a list of the coordinate
pairs at which that pixel value occurs in the image.
The cumulative histogram gives the starting address
for each such list.
The IFI is an ideal device for inverting the pixel-
value/coordinates relation. In a sense, it transforms
the image representation from a coordinate-oriented
one into a feature-oriented one. The principal motiv-
ation for this operation is to permit the host to search
only the list of potentially relevant coordinates when
seeking global geometric relations among selected
types of features. The IFI could be extended to
permit its mapping to be inverted', however, we
shall not consider these possibilities here.
In spite of its simplicity, the IFI appears to be a
rather useful device. Two novel algorithms which
use it are described later.
A device has been designed?' which would accel-
erate the conversion of binary images into chain
codes, in conjunction with a ''systolic'' cellular array
computer. Pfeiffer's device, consisting of a VLSI
chip that receives inputs from the systolic array,
treats one column of a binary image at a time. It
rapidly scans the column, identifying the pixels in
which the value 1 is found. If, in the sequence of
columns processed, the value in a row changes from
one column to the next, then the chip outputs an
indication that a horizontal segment either began or
ended in that row (a status bit keeps the state of each
run). It is intended that two to four of these chips
be used for one iconic processor: one processing
columns (as described), another processing rows,
and optically two more of them processing diagonal
lines. The outputs of the chips would be collected by
one or several Von-Neumann-style processors and
sorted into run-length-compressed chain encodings.
For purposes of comparison with other
approaches, we model this device so as to omit the
details of how columns are scanned. Our model,
referred to as the CRE (for Chain-Run Encoder)
simply takes N binary inputs in each of N steps and
outputs a list of events where each event is of the
form (i,e)where itells which of the N inputs changed
and e tells whether the change was from a 0 to a l
or from a 1 to a 0.
Let us consider how two or four CRE units would
be used with an iconic processor and a symbolic
processor to determine chain encodings of edges in
an image. In order that each of the four encoding
devices receive only ones for the pixels that form
parts of edge segments in the appropriate direction,
the iconic processor must produce four separate
binary images, one for each direction. Figure 3 shows
how two chain-run encoders could be integrated
with a cellular-array processor and a collection of
microcomputers. It is possible to use a single chain-
run encoder for runs in all four directions (sequen-
tially) by providing additional switches and inter-
connections to permit either row data or column data
to be fed into the device.
In order to provide a high-bandwidth interface
between a parallel image processor (such an MPP or
CLIP4-like system, or a pyramid machine'''') and a
collection of microprocessors, a ''bimodal'' memory
system has been proposed''! that would support
image-wide, parallel transfers with the parallel pro-
cessor, and ordinary byte-at-a-time transfers with
the microprocessors. By suitably partitioning the
memory, many microprocessors could access por-
tions of the memory simultaneously, and the parallel
processor could access a portion while most of the
microprocessors access portions. A clean implemen-
tation of the bimodal memory requires a memory
chip not commercially available at present. An MPP
or Pyramid, plus BMM, would provide a system
that is powerful, yet conceptually simpler than some
proposed extentions to pyramids and arrays.'' At
the same time, it is less expensive than purely MIMD
systems such as the Ultracomputer,!''V The BMM is
shown between an iconic cellular array and a col-
lection of microcomputers in Fig. 4.
What the BMM permits is a flexible scheme for
sharing work between a parallel image processor and
a collection of microprocessors. One can imagine
that the images are stored in shared memory that is
accessible to the parallel image processor at the same
rate that the image processor's ordinary memory is
accessible. Similarly, the shared memory is accessible
to a microprocessor as fast as, and as simply as if it
were ordinary memory on the bus.
In order for such a system to compute a histogram
of an image several methods are possible. One
method uses the parallel array to make several copies
of the original image and store them in different
portions of the bimodal memory; then each micro-
processor scans a portion of its copy of the image,
forming partial histograms. Finally the micro-
processors combine their results through an
additional communication network (or bus), sum-
ming the component histograms to obtain a global
one. A variation of this method is to have each
microprocessor produce certain bins of the histogram
(which nonetheless requires each micro to scan the
entire image) and then for the sequences of bins
to be concatenated into a full histogram. Another
algorithm gives some of the responsibility for
accumulation to the parallel array. If the number of
bins is small, it may be better to have the parallel
array compute the bin values over neighborhoods of
size, say, 16 x 16. The partial histograms could be
stored in the same 16 x 16 subarrays, one bin per
cell. Then copies of this ''histogram image'' would
be placed into different portions of the BMM, where
each microprocessor would combine the sums for
one or several of the bins of the global histogram.
This requires each micro to look at only one cell (or
a small number of cells) in each 16 x 16 block of the
image.
Other iconic-to-symbolic transformations can be
performed by dividing up the image (or making
copies of it) and letting each micro transform a part
of the image.
The 'Tile-Based Interface'' that we describe here
was inspired by ''Interface Array Processor'' pro-
posed in a joint study by researchers at the University
of Massachusetts at Amherst and at the University
of Southern California.
The ''Content-Addressable Array Parallel Pro-
cessor'' is an MPP-like system that was developed at
the Universitv of Massachusetts at Amherst. The
VISIONS group at UMass has proposed to interface
the CAAPP to a device that was designed at the
Universitv of Southern California called the ''Sem-
antic Network Array Processor'' (SNAP). The SNAP
consists of an array of modules each consisting of
a microprocessor and an associative memory. The
proposed IAP interface gives each SNAP module
access to a rectangular window of the 128 x 128 or
larger array handled by the CAAPP. The IAP itself
includes processing elements, memories and a pro-
grammable crossbar switch. The IAP is a more com-
plex scheme than the TBI one that we describe here.
The TBI is a model which substantially simplifies
the IAP. The TBI is more readily compared and
contrasted with the other three models in this paper.
The TBI omits the crossbar switch and the memorv
and processing elements specific to the IAP. What
remains is a coupling of svmbolic processors to zones
of the iconic array. These zones are a partition of
the image array into squares. The TBI scheme is
illustrated in Fig. 5.
The TBI team could extract a line drawing from
an image by having the CAAPP perform edge detec-
tion, spurious edge removal and gap filling. The
SNAP modules would then scan their windows for
edge segments, track them, and chain code them.
Higher-level processes would then take the local
chain-coded segments and merge them into a more
global description and approximate the chain-coded
segments with line or curve segments.
One can propose to build special hardware for any
of the infinite number of possible transforms that
map information from the space domain into a trans-
form domain, and it is expected that future work
will treat more of this kind of hardware. However,
because of its importance to computer vision, an idea
should be mentioned for a device to rapidly compute
the Hough transform of an image. Ballard and
Brown'! have suggested that a device with many
processing elements and an even larger number of
interconnections could allow the evidence-gathering
(or ''voting'') steps of the Hough transform com-
putation to be done in parallel, greatly reducing the
amount of time required to obtain the result. The
device would be difficult to build efficiently with
existing VLSI technology. However, the scheme is
given as a plausible model for line detection by
biological vision svstems.
Each scheme for iconic/symbolic processing
described above is intended to be part of a larger
environment, generally including an iconic sub-
system and one or more Von-Neumann-style
machines. However, the proposed interconnection
schemes are all different.
1. The IFI takes a pixel stream from the PIPR)
and writes to a VAX-like host through a DMA port.
2. The chain-run encoder sits at the side of a
systolic-array image processor'-'! and outputs to one
or more Von-Neumann-stvle machines.
3. The BMM system is directly coupled by a
highly-parallel ''vertical'' bus to a cellular parallel
image processor, such as the proposed pyramid com-
puter, and also coupled by ''horizontal'' busses to
several conventional microprocessors.
4. The TBI architecture is a particular inter-
connection of the Content-Adressable Array Parallel
Processor and the Semantic Network Array Pro-
cessor (the proposed interconnection is organized
according to a windowing partition of the image
space).
In comparing the four proposals, an important
observation is that the IFI and the chain-run encoder
were designed as devices to speed up relatively
specific transformations, whereas the BMM and the
TBI architectures were designed as more general but
more costlv interfaces that leave most of the actual
iconic/symbolic computation to conventional micro-
processors. However, the specific transforms of
feature/coordinate inversion and chain-run encoding
are operations that find a variety of uses and are not
as limiting as they might seem at first.
Other problems for iconic-to-symbolic processing
include connected-component determination, tex-
ture description and shape description. The con-
nected component problem is one of particular
interest because of its utility as a preliminary step to
shape description, blob counting, region-adjacency-
graph creation and other operations.
The sketches of algorithms in the previous section
give a general basis for comparing the architectures.
In this section, two algorithms are presented in
greater depth for three purposes:
1, to illustrate more clearly the nature of algor-
ithms for iconic/symbolic architectures by showing
the interplay of the iconic, iconic/symbolic. and sym-
bolic components of these algorithms;
2, to show how a problem can be re-expressed
to yield an efficient solution on an iconic/symbolic
architecture: and
3, because the algorithms are novel and may find
applications to, or lend insight into solving problems
that arise in the future.
We now give an algorithm for partial Hough trans-
formation which employs the PIPE and the IFI in
combination with a Von-Neumann host machine. It
would run on other pipelined systems to the extent
that they can support both the neighborhood opera-
tions (which are effectively handled by the PIPE)
and the global summarization operations (here
handled by the IFI). The algorithm makes the fol-
lowing assumption about the lines to be detected:
the local strengths and slopes (directions) of the lines
can be detected using local-neighborhood opera-
tions. The algorithm works as follows: an edge or
line operator is applied by the PIPE to the image to
obtain a line-strength image. This is thresholded and
used to select areas for slope determination. The
slopes are determined using local-neighborhood
techniques. The IFI is then used to make a histogram
of the slopes. The class of slopes with the most entries
is determined (by a host processor) and a new image
is produced in which pixels where this slope occurs
have value 1 and all others have 0. This image is
used as a ''region-of-interest'' selector, and masks
the next step (intercept computation) so that only
pixels in the region of interest are processed. The
intercept at each pixel of interest is computed using
two ''coordinate images.'' The r coordinate image is
an 8-bit deep image in which each pixel has its x
coordinate represented to 8 bits of accuracy. The y
coordinate image is similarly defined. Using the line
equation y = ax + b, the intercept is given by b =
y - ax. The PIPE computes b by multiplying a by x
(using the 12-bits-in lookup table) and then sub-
tracting the product from y, using a 16-bit ALU.
Finally the IFI makes a histogram of the intercepts,
and the host determines the most frequently occur-
ring values. This gives the intercepts which go with
the slope already chosen, and together they give the
parameters for a prominent class of parallel lines in
the image. Successive prominent lines may be found
repeating the process for other slope classes. One
may find parallel lines by selecting the lesser peaks
in the histogram of intercepts, or one may go back
to the slope histogram and work with lines at a
different slope. This algorithm requires only a fixed
number of field times to determine a prominent line
or class of lines in the image. This scheme may
also be used with the polar line representation p =
x cos 6 + y sin 6 to avoid the problem with vertical
lines having infinite slope.
It is interesting to note that the host could easily
scan one of the lists of coordinates of pixels found
to lie on a particular prominent line, and determine
the extreme values, thus finding the two endpoints
of the shortest line segment that includes all of the
pixels in the class. With an additional step, consisting
of sorting the pixels byx coordinate (or alternatively,
by y coordinate), the component segments could all
be determined.
Let us now describe one of the possible ways in
which the Bimodal Memory may be used in com-
puting iconic/symbolic transformations.
In the following sketch of an algorithm, a classical
Hough transform is computed and its principal peaks
identified.
Step 1. In the iconic subsystem, a local edge detec-
tor is applied resulting in an edge image in which
each pixel has a magnitude corresponding with the
strength of the strongest edge passing through that
pixel in any direction.
Step 2. In the Bimodal Memory, m copies of the
edge image are produced. (Assuming that the mag-
nitudes are represented as 8-bit bytes, this requires
8 machine cycles, regardless of the value of m.)
Step 3. In the symbolic processing subsystem, each
processor reads its own (1/m)th of the edge image,
and for each of these pixels, it plots and accumulates
votes in a parameter-space array which is another
layer of the Bimodal Memory accessible to this pro-
cessor. There are O(N) votes for each of the N/p
pixels, and this step thus requires O( N /m) units
of time to complete. Since each processor works
independently and accesses its own layers of the
Bimodal Memorv, there is no contention or inter-
action among the m processes.
Step 4. The Bimodal Memory does nothing now,
but it holds m different''versions'' of the parameter-
space array. The iconic subsystem accesses each of
these versions in turn, adding and accumulating the
votes at each position in a single parameter-space
array. This requires m vertical-mode accesses from
the Bimodal Memory and m - 1 image additions.
Step 5. In the Bimodal Memory, the final par-
ameter-space array is replicated m times, with one
copy for each symbolic processor.
Step 6. In the symbolic subsystem, each processor
searches its own (1/m)th of the parameter-space
array for local minima. This requires O( N)/m time,
assuming that the number of local minima desired is
a constant.
Step 7. Finally, the symbolic processors merge
their lists of peaks to get an overall list of peaks.
This requires O(log m) time, assuming again that the
number of peaks desired is a constant, and that
the symbolic processors are connected together in a
network of O(log m) diameter, such as a binarv tree
or a hypercube.
The bottleneck in this algorithm is most likely to
be in Step 3, since, for reasons of cost, m is likely
to be small in comparison with N?. A reasonable
assumption is that m would be on the order of N,
so that the computational complexity of Step 3 is
O(NA).
However, there are many practical ways to further
reduce the computational effort required in Step 3.
It is usually the case that the edge pixels in an image
make up only a small fraction of the image. By
restricting the voting to only edge pixels whose mag-
nitudes exceed a threshold, approximately another
factor of N may be shaved off the expression, leaving
us with an O(N) effort. This effort may be further
reduced by adding in Step l the production of edge-
directional information-the approximate angle at
which the apparent edge passes through the pixel.
In Step 2, the angle image is replicated as well as the
edge-magnitude image. In Step 3, the edge angle
information for a pixel is used to limit the number
of votes cast by an edge pixel. Only lines at approxi-
mately the angle given may receive votes. If we limit
the number of votes per edge pixel to O(log N)votes,
then the amount of time required by Step 3 is reduced
to O(log N).
There is an interesting algorithm for computing
the region-adjacency graph that uses the PIPE, the
IFI and a host processor. The algorithm begins with
a segmentation map in one of the P[PE frame
buffers. It is assumed here that the number of regions
is small enough that unique region numbers can be
assigned to each region. and still fit in the 8-bit deep
PIPE frame (clearly no more than 256 regions can
be handled). Actually, the algorithm reserves region
number 0 for special use, and so a maximum of
255 regions are allowed. This algorithm builds an
adjacency list for each region in the image, and it
requires a fixed number of PIPE field times for each
adjacency list. In order to compute the adjacent list
for region k, the segmentation map is used to produce
an image of adjacency information for region k. This
image is the result of taking the segmentation map
and setting to 0 the value of any pixel which does
not have a neighbor (i.e. a pixel to the north, south,
east or west) in region k. The IFI then makes a
histogram for this adjacency image. Bin j has a
nonzero value if and only if region j is adjacent to
region k. From this histogram, the IFI then makes a
cumulative histogram. The ordinary histogram and
the cumulative histogram form a structure that the
host can scan in an amount of time proportional to
the length of the adjacency list for region k, rather
than the maximum number of regions one might
have. As the host examines this structure, it builds
the adjacency list for region k. It has been con-
jectured by Shneier and by Kent that PIPE and
IFI can process more than one adjacency image
simultaneously, further reducing the time required
to obtain the region-adjacency graph. If the con-
jecture is true, the maximum permitted degree of
the RAG may be reduced, and the maximum number
of regions permitted in the input image may also be
reduced; however, additional parallelism would be
gained.
The four iconic/symbolic architectures reviewed
here fall into two categories: hardwired devices (IFI
and the chain-run encoder) and interfaces (BMM
and the TBI). Notably missing are architectures that
are at once active computing components and pro-
grammable. Such a system might allow, for example,
the rapid computation of a variety of symbolic (or at
least non-iconic) transforms of images under pro-
gram control. Presumably, such an architecture
would contain several processing elements and an
interconnection scheme especially appropriate to the
class of transform it is to compute. Some possible
transform classes are these: generalized Hough trans-
forms, global image statistics, and matching geo-
metric models to image data.
As mentioned previously, there are several ways
toward the objective of high-speed iconic-to-sym-
bolic computation. One may begin with an iconic
structure and generalize it to make it more efficient
for symbolic work. One may take a collection of
symbolic processors and attempt to integrate them
in a way that aids iconic processing, or one may take
both kinds of processors and develop an efficient
interface between them.
The two most important parameters in the design
of this kind of system are performance and cost.
Performance may be evaluated in terms of speed and
generality, while cost may be evaluated by assessing
hardware complexity, hardware size, component
prices, and programming difficulty.
In order to increase speed, one can increase par-
allelism, elaborate the interconnection network,
and/or use higher-performance processing elements
and memory, generally all at increased cost. To
increase the generality of the system (i.e. widen the
class of transforms that can be efficiently computed),
the interconnection scheme may be enlarged, the
processing elements may be given more autonomy
(e.g. by changing from an SIMD system to an MIMD
one), and/or the speed of the system components
increased.
There need not be great complexity in an efficient
interface. The two interface schemes that have been
presented (the BMM and the TBI interface) may
be viewed as relatively conservative proposals for
iconic/symbolic systems, because they employ well-
understood processing components and fairly simple
interconnection schemes. They are substantially sim-
pler than, say, the MPP staging memory.'l
The size of an iconic-to-symbolic subsystem
impacts its cost strongly. The IFI has a constant size
for increasing image sizes, except that it may need
to address pixels or list items in a larger memory.
On the other hand, the size of a BMM increases as
the product of image size and number of layers
desired. The TBI interface size is directly pro-
portional to image size if the ratio of pixel processors
to semantic network processors is kept constant.
The chain-run encoder needs an amount of circuitry
proportional to the length of a row of the image, and
this is inexpensive for large arrays in comparison with
the interface schemes. To summarize, the greater the
parallelism across the iconic/symbolic gap, and the
greater the flexibility in programming those trans-
fers, the greater is the cost of the system.
Designing an algorithm for a system that includes
separate components for iconic, symbolic, and poss-
ibly iconic/symbolic processing presents a different
kind of challenge than does designing for a con-
ventional machine. Because an iconic processor
handles images very well but may be very slow for
symbolic operations, and because a symbolic pro-
cessor has just the opposite characteristics, it is very
important to factor a problem into its iconic and
symbolic components. These problem components
must be separated and distributed on the system, but
coupled in a way that allows adequate cooperation
between the iconic and the symbolic activities.
If it is not obvious where to draw the line between
iconic and symbolic parts of a problem, there are
several criteria that can be called upon to help.
Operations that work on local, rather than global,
neighborhoods are apt to be iconic. Operations that
admit high degrees of parallelism, rather than requir-
ing long sequences of steps, are more likely to run
efficiently on an iconic processor than a symbolic
one, particularly if the SIMD rather than the MIMD
variety of parallelism can be applied. Perhaps most
obviously, transformations that require images as
most of their working representations are more likely
to run well on iconic processors than are trans-
formations that are primarily list-based.
Some of the researchers who work with MIMD
systems may feel that since an MIM1D system may
incorporate a large number of processors and a
reasonably general interconnection network, there
is little need to develop special systems for iconic/
symbolic processing. We feel that the specialized
architectures will probably not only cost less than
the MIMD systems that might be offered to fill
similar needs, but outperform the MIMD systems,
as well. This is because a specialized device can have
a precisely-tailored interconnection structure that
makes parts of an algorithm implicit whereas an
MIMD (having a ''general'' interconnection scheme)
would have to emulate the special interconnections
incurring a time penalty.
The problem of providing a way for a computer
vision system to rapidly transform an iconic rep-
resentation into a symbolic one is a challenging one
that is clearly of importance for the development of
machine-vision technology. An iconic-to-symbolic
transformation is one in which the input is in the
form of a two-dimensional array (where position
information is implicit) and in which the output is in
the form of a list, vector, graph, table, or array of
parameters in which the array indices of a parameter
are no longer an indication of location in the input
image. Typical transformations map images into lists
of line segments or region descriptions in terms of
sizes and adjacencies (or other features), or they
produce lists of locations at which particular local
features are found in the images.
Specialized computing hardware for iconic/sym-
bolic computations has thus far exhibited two forms:
hardwired devices for particular transformations and
high-bandwidth interfaces between separate iconic
and symbolic processor subsystems. Four proposals
for specialized hardware have been discussed and
compared with respect to a set of image analysis
problems.
The designers of algorithms and of hardware for
iconic/symbolic computing face many challenges.
However, the possibilities for interesting new
methods and devices appear to be wide open and
exciting.
Compared to structured environments, navigation in
an unstructured environment such as cross-country terrain
presents a significantly different set of problems for a
perceptual system. While recognition of man-made objects
performs adequately by utilizing rectilinear surface features,
color characteristics, or other well constrained surface
properties, there are no comparable structural expectations
available in natural terrain. It is reasonable for indoor
mobile robots to project a route using a mobility map that
includes obstacles identified as occupying vertical space
and assumes that the floor is flat. However, meaningful
objects in natural terrain are more diverse and difficult to
characterize. Specific features such as local slope, three
dimensional edges, or color of specific regions may or
may not represent obstacles. In general, the features are
difficult to extract and difficult to combine to form reliable
descriptions of the terrain. For example, trees can have
widely varying shapes, colors, and sizes making tree
recognition in itself a formidable problem. The
recognition problem is compounded by seasonal and
weathering effects, and the need to interpret the
environment in a timely manner.
The goal of he Knowledge Based Vision Techniques
(KBVT) research at the Hughes AI Center is to provide the
necessary perceptual descriptions of the environment
which allow an autonomous vehicle to successfully
navigate amongst obstacles. It is also our goal to transfer
technology and perform experiments on-board a vehicle.
The latter goal has constrained our research in the past
year to approaches and algorithms which could ''perceive''
obstacles in a timely manner; that is, the perception
processing has to allow ample time for the vehicle to
safely respond to obstacles. An important feature of the
perception system is its evolution in conjunction with the
planning system also developed by Hughes AI Center.
Our approach defines a system architecture which
supports a decomposition of the problem directed by
immediacy and assimilation considerations [4,5]. A brief
overview of the architecture is included as an introduction
to the technology development and experimentation
results.
In this hierarchical control structure as shown in
Figure 2, the vertical structure generally reflects the level
of information fusion. Higher levels function on the basis
of highly assimilated data that are generally symbolic and
quire longer interpretation time and larger spatial area;
the lower levels exploit more immediate data. A failure at
one of the lower levels is signalled to the next higher
level, which then re-assesses the situation and adjusts
accordingly.
For the ALV experiments a subset of the
hierarchical control system was used. A simple mission
was defined by start and goal locations with the path
constrained to maintain a direct line-of-sight with the
communications tower. At the route level of hie hierarchy
A map-based planner was used to generate all experimental
routes. A route consisted of a set of subgoal points. Our
mapor emphasis in the past year was to develop and
rAperiment with the lowest level of the hierarchy. At this
level, virtual sensors and reflexzive behaviors (or
behaviors) are used as the real-time operating primitives
for the rest of the system. Knowledge assimilation is
mumimized in order to provide the fastest possible vehicle
=Sponse. Virtual sensors are sensing and processing units
%4 detect specific environmental features and relay
information about features to the reflexive behaviors. A
virtual sensor is ''contracted'' by a behavior to provide
information at a required processing rate and accuracy. The
reflexive behaviors are highly procedural units that operate
on virtual sensor data to provide real-time control. Virtual
sensors and reflexive behaviors are grouped into activities
so that multiple behaviors can operate concurrently to
produce control decisions. These activities are scheduled
by the local planner to achieve current goals.
No single reflexive behavior and virtual sensor
combination is ever expected to be able to handle vehicle
navigation problems in general; but rather, several are used
in conjunction, each designed to handle a specific sub-
problem within the overall range of navigation tasks. It
is the responsibility of the local perception and planning
modules to guarantee that the selected activities are
Appropriate for the current environment. Our recent
experimentation on-board the ALV has shown the concept
of virtual sensors and reflexive behaviors provide a viable
approach to local vehicle control.
In the context of cross-country navigation, we have
defined an obstacle as any region a particular vehicle
cannot traverse. This definition allows obstacle detection
to depend upon the mobility characteristics of the vehicle.
Therefore, a natural approach to perception for cross-
country navigation models the vehicle's interaction with a
three dimensional representation of the sensed terrain to
determine traversability. The ALV is equipped with a laser
range scanner which measures the distance along the line
of sight to the nearest object. This sensor inherently
supplies information of surface geometries; however, the
interpretation of this information is difficult. Several
methods to interpret surface geometry in terms of an ALV
mobility model were developed that employ a down-
looking Cartesian Elevation Map (CEM). In addition,
methods were developed to weight obstacles such that
potential paths are penalized in areas ''rich'' with
obstacles. These methods are briefly described in the
following discussion. We also mention briefly parallel
considerations and implementations. A separate paper is
recommended for a more complete description of these
techniqwes [1].
The Cartesian Elevation Map (CEM) is a
representation for range information in which data from
the viewer centered coordinate system of the sensor is
transformed into a Cartesian z(x,y) coordinate system.
This results in a down-looking map view representation of
terrain which is useful for autonomous navigation. This
same representation may be obtained from other depth
sensors, such as stereo or sonar.
The development of the CEM required us to deal
with a variety of range processing issues. As one would
expect, the elevation data represented in the CEM are
highly oversampled in the immediate foreground and
undersampled at greater distance from the sensor. In
addition, there are some regions in the CEM tUhat fall
outside the field of view of the scanner and other regions
that fall behind (in the shadows of) tall features in the
terrain. In the CEM, areas with sufficiently dense
sampling of points are fitted with a continuous surface and
undersampled arcas are explicitly labeled ''unknown''.
Due to the limited vertical field of the laser range
scanner (30 degrees). terrain immediately in front of the
sensor is not visible. The closest scanned ground in our
experiments is approximated 13 feet in front of the
vehicle. We have investigated the fusion of data from
previous CEM's o fill in this unknown area. One method
uses the orientation sensors on board the ALV (heading,
pitch, roll, x, and y) together with an estimate of change
in the z-position to determine vehicle motion in all six
degrees of freedom. We are currently investigating another
method that recovers the motion directly from sequences of
range images.
We have developed a relatively sophisticated three
dimensional model of the ALV. This model together wiUh
the CEM yields a formal definition of obstacles, thus
avoiding ad hoc and incomplete definitions. The model is
currently represented by minimizing the energy of the
suspensions springs associated with each wheel as the
vehicle is applied at a position and orientation in the
CEM. Three types of obstacles are detected with the
vehicle model: suspension, slope, and clearance as shown
in Figure 3. This definition of obstacles has performed
fairly reliably in our experiments on the ALV. The model
will be extended to include constraints such as vehicle
weight distribution, weather conditions, risk factors, and
vehicle speed and dynamics.
The vehicle model allows us to produce a three
dimensional traversability map by applying the model at
each possible position and heading. In most cases, a
complete traversability map of the entire sensed area is not
needed. Because of constraints on perception processing
time, we have developed techniques for applying the model
only at those points necessary to fulfill requests issued
from the planner. This method, called tUhe Vehicle Model
Trajectory (VMT) virtual sensor, simply calculates the
projected heading of the vehicle at each point along a
linear trajectory and applied the model at that heading and
location until it either reaches the end of the trajectory or
assumes an unstable configuration. The virtual sensor is
contracted with the behaviors to return the distance
travelled (or ''safe distance'') and the reason for
termination. The VMT virtual sensor can provide different
levels of accuracy and processing speed by varying three
parameters independently: CEM size and resolution,
vehicle model accuracy, and sampling frequency of the
range image.
In the recent ALV experiments, the VMT virtual
sensor was used along seven linear trajectories. The total
processing time from image acquisition to trajectory output
was approximately six seconds on a Sun 3 with a floating
point coprocessor.
The current implementation of the VMTs uses only
linear trajectories although the Hughes planner controls
the vehicle through speed and turn rate commands which
result in curved trajectories. We plan to generalize the
VMT virtual sensor for any given trajectory; Figure 4
shows curved trajectories. Other enhancements are
discussed in [1].
The current VMT strategy simply tells the planner
how far the vehicle may go along a given trajectory. The
planner has no idea how close the trajectory passes by an
obstacle. In simulation and in real life, the vehicle tends
to pass too close to obstacles since no part of the current
system deals with side clearance.
Like hhe CEM, he Cartesian Weight Map (CWM) is
a down-looking map in the local coordinate system of the
vehicle. A pixel value in the CWM represents the weight
identified with the cost of raversing the corresponding
pixel in the CEM. Nontraversable obstacles found using 4
Gradient of Gaussian (GoG) technique and verified by the
vehicle model are given an infinite weight in the CWM so
that no path will ever travel through an obstacle. To
solve the problem of the vehicle passing too close to
obstacles, all other CWM pixels are given weights which
eponentially decay with distance from the nearest
obstacle.
The CWM could be extended to penalize areas which
are ''bumpy'' and reward regions that are smooth. This
concept of multiple virtual sensors concurrently updatin
the CWM is consistent with our hierarchical system
design. We have tested he CWM in simulation and found
that the technique safely guides the vehicle equidistant
between obstacles and avoids small cul-de-sacs.
A great advantage of he CEM and CWM methwW 8
that they are extremely parallel. In cooperation wiu MIT
in March, 1987, we implemented (during a two wess
programming spree) the CEM construction algorithms and
the GoG detection technique on the Connection Machine.
Creating a 128x128 CEM from a range image took less
than 0.5 seconds with unoptimized Lisp code and intensive
floating point calculations; the GoG required an additional
88 milliseconds of compute time. We expect these times
will be significantly reduced with he CM-2.
In addition, we have explored implementing the
CEM on a WARP. Since the pixel in the range image that
contributes to a given location in the CEM is data-
dependent, the entire range image is distributed to each of
the 10 cells in the WARP array. The CEM is Uhen divided
into 10 column swaths with several columns overlap
between each swath. Each WARP cell processes every
pixel in the scan, but saves only those points that fall
within its assigned column swath. Progress on the WARP
implementation has been delayed while we await the new
release of WPE 2.6.
An implementation design similar to the WARP
implementation has been designed for the Hughes
Hierarchical Bus Architecture (HBA) [6]. The HBA is
available for use in the lab simulation environment to
improve simulation throughput.
The planning system for obstacle avoidance is
designed to provide real-time vehicle control while
maintaining the flexibility needed for operation in realistic
environments, Because the primary objective of the cross-
country navigation experiments was to test critical real-
time perception and planning interfaces, the map-based and
reflexive behavior modules were the primary focus for
development. More detail of the planning system are
included in these proceedings [2].
At the route level of the hierarchy resides the map-
based planner providing route information obtained from
digital terrain maps. The route planning data includes
maps of landcover, elevation, hydrology, roads, and
landforms, and also data, such as visibility from the
vehicle to the communications shell, that was derived from
these maps. The map-based planner generates all
experimental routes; a rouie consists of a set of subgoal
point locations.
The reflexive planning module is tasked with
achieving the subgoals received from the map-based
planner. The execution of the path requires the planner to
react to perceived information, but do so in a consistently
reasonable manner (i.e., intelligently). Reflexive
planning controls the vehicle within its immediate
environment with minimal data assimilation.
The behaviors used in the ALV experiments respond
to VMT virtual sensor, The interface between the
perception, planning, and vehicle control is critical. For
instance, planning to avoid an ''unknown'' area changes
dynamically as more information is perceived or as known
obstacles are detected. In addition, as obstacles pass
below the range scanner field of view a method is
necessary to determine when the vehicle has traveled
completely beyond the obstacle bounds. Other timing
related issues must also be addressed; such as, when the
vehicle reaches the end of a VMT it must slow down and
stop if necessary to wait for new VMT data,
For the first set of experiments, the behaviors were
grouped into two activities: the first activity was designed
to travel toward a goal when the vehicle was in a clear
area, and the second activity was designed to control the
vehicle when obstacles were present. For the recent
experiments, the behaviors were incorporated into a single
activity that used a technique which weighted the
importance of the goal according to the difficulty of the
terrain. This is desirable because as the vehicle's
movement becomes more restricted it becomes more
important to get clear of the rough area than to make
progress directly toward the goal. In the case when the
vehicle is in an area free of obstacles, the goal weight
becomes predominate and the vehicle tends to head straight
toward the goal.
Simulation plays a critical role in the development
and analysis of our perception and planning systems by
allowing us to discover and resolve many discrepancies
before attempting experiments with the ALV in the field.
In addition, it provides the essential link between vehicle
control commands and new perceptual inputs. The ultimate
objective of these simulations is to close the loop between
sensing and acting; that is, between the perception,
planning, and vehicle navigation systems. By simulating
the terrain, the sensor, and the vehicle, we are able to test
the efficiency, correctness, and usefulness of the virtual
sensors and behaviors as they are developed.
To provide an accurate model of the AILV terrain, we
extract a portion of the five meter resolution map
elevation data from ETL and interpolate a smooth surface.
Also we allow specific objects such as contours, ramps,
plateaus, walls, cliffs, and ravines to be inserted in terms
of elevations. The resulting surface defines the basic
structure of the underlying ground. We place cultural
features such as bushes, ditches, rocks, and grass over the
ground surface data. Finally, additive noise is applied to
''randomize'' the terrain. IInterpolation schemes are used to
smooth areas or produce gently sloping terrain. The
resulting terrain provides both a source of data for the
synthetic scanner and the surface on which the simulated
vehicle moves.
To simulate the laser range scan, we apply a ray-
tracing algorithm in the synthetic terrain to produce a
synthetic range scan from any vehicle position and
orientation. The synthetic scanner reproduces many of the
image artifacts observed in actual scans, including those
due to vehicle motion during image acquisition. Using the
simulation, we are able to analyze the effect of the sensor
depression angle upon obstacle detection.
The vehicle simulation includes parameters for
vehicle dimension,the vehicle sensors for location and
orientation, and vehicle dynamics such as acceleration and
braking. We can simulate collision by applying the
vehicle model. IIn addition, we have simulated the actual
MMAC/ALV control algorihm. This was necessary to test
the conversion of the control algorithms based on speed
and turn-rate control used by Hughes into the vector
control algorithms used at MMAC. WiUh this simulation,
we can evaluate the effects of time delays between
command and vehicle action.
The interface simulation also includes the ability to
mimic the software and hardware configuration at MMAC.
The full simulation requires the use of several Symbolics
lLisp machines and Sun workstations joined via pronet
interfaces. This environment identifies and resolves many
inconsistencies and shortcomings in the system interfaces
and load distribution without consuming valuable time at
the test site, Most importantly, such preparation allows
us to perform meaningful and successful cross-country
experiments within a relatively short experimentation
period at MMAC.
The experiments run on the ALV were designed to
determine the feasibility of cross-country terrain
navigation and to demonstrate the Hughes perception and
planning software. The experiments accomplished both
objectives [7].
The cross-country experiments required the
integration of multiple computers distributed on-board the
vehicle and in the ALV lab as shown in Figure 5. The
perception system resided on two Sun workstations on-
board the vehicle; one Sun was used for the virtual sensors,
the other was used to archive data and experiment records.
The planning system resided on two Lisp machines in the
ALV lab: one Symbolics ran the reflexive behaviors, the
other was employed for map-based planning. In addition,
vehicle control was interfaced (by MMAC) through an Intel
computer.
For the cross-country experiments, we chose a site
rich with obstacles and potential paths for obstacle
avoidance. The area is a hillside consisting of steep
slopes (some over 15 degrees), rock outcrops, large scrub
oaks, very small junipers (15 inches high). as well as a
narrow sign post (post approximately 2 inches wide and
ign approximately 4 inches by 6 inches). IIn addition,
the area includes a complex set of gullies and ravines
caused by rain and snow runoff. These gullies span a range
of widths from 6 to 25 inches and depths from 4 to 30
inches. A narrow area with a very constrained approach
angle is available to cross the area of gullies. The soil is
loose and sandy in the vicinity of the gullies resulting in
mud and vehicle slippage during the December
experiments, We also set up an obstacle course on a flat
field at the far end of our experimentation area. The
difficulties associated with this test area are sufficient to
require caution by a human driver.
In general, the perception system adequately
perceived the environment in that the obstacles were
detected. The VMT virtual sensor was used with seven
trajectories applied on every other pixel in a CEM with 1
foot per pixel resolution. We experienced the greatest
difficulty with gullies and rocks ''hidden'' in the grass. The
planning system utilized the VMT information to
successfully navigate around obstacles; the average speed
for the recent experiments was 3 km/hour. Specific
experiments and paths are described in greater detail in
another paper in these proceedings [2].
Autonomous cross-country navigation with obstacle
avoidance was successfully demonstrated. It is the first
such demonstration which integrated map-based and sensor-
based vehicle control. It also demonstrated the feasibility
of an experimental system operating with conventional
computing hardware located both on-board the ALV and
remotely in the ALV laboratory. The concepts of
behaviors and virtual sensors have been shown to provide
a viable approach to local vehicle control, responding
reliably to the dynamic conditions of the real world.
lLastly, it is significant that Hughes was able to
accomplish so much with relatively little vehicle time
(with only 2 weeks for the first experiment and 1 week for
the second). Much of this success must be attributed to the
preparation at Hughes through extensive simulation.
These experiments represent significant technological
progress for autonomous vehicles.
The interpretation of large high resolution images
with highly variable backgrounds can be facilitated by
examining features extracted from multiple resolutions of
the image. The objects of interest are modeled at each
resolution in terms of features that can be used to provide
evidence for the objects. By examining lower resolutions
during the initial stages of image interpretation, object
hypotheses can be made based on large, prominent
features. Given these initial hypotheses, higher
resolutions are examined only in those areas in which
objects of interest are expected.
An object is modeled according to its expected
characteristics in the image using two kinds of features:
salient features that create initial object hypotheses, and
supporting features that provide evidence for the
hypothesized object. At each resolution, hypotheses are
generated in two ways: first, a hypothesis may result from
a feature that is due to an instance of the object, and
second, a hypothesis may result from objects that already
exists at either the same or a lower resolution. In either
case, the object creates hypotheses in accordance with the
model that specifies the confirming evidence.
The image interpretation system does not follow an
algorithmic approach, but instead chooses procedures based
on the current state. The approach is both data-driven and
model-driven, utilizes hypothesis generation and
verification, and employs evidential reasoning to evaluate
tUhe hypotheses. The system adheres to the principle of
least commitment in two ways: 1) object hypotheses
occur only if there exists supporting intrinsic feature
properties, and 2) final interpretations are not determined
until all hypotheses have been made.
The system has been exercised on two aerial
images: one consisting of a single submarine and the
other consisting of three airplanes. The submarine
scenario had sufficient resolution to analyze using three
resolutions. Features at each resolution that were used as
evidence for a submarine are shown in Figure 6. The
airplane image was analyzed using only two resolutions.
Features for detection of an airplane are shown in Figure 7.
In both of these examples, the original object
hypotheses are made at the higher levels, These
hypotheses are then projected to the lower levels. In the
submarine example, the lowest level model contains
additional detail, namely, the tail. In the airplane image,
only those areas near the original hypotheses are
considered at the lower level, thereby making the
interpretation process more efficient. The features used to
extract the airplanes are independent of the features used
for the submarines. By appropriately choosing features
the interpretation of a poor quality image was possible. A
more complete system description and discussion of results
is found elsewhere in these proceedings [3].
Ackno wledgements, The progress presented in
this overview represents the combined efforts of the
technical staff members of the Computer Vision and
Autonomous Planning Sections at the Hughes AI Centers.
IIn addition we acknowledge the many valuable discussions
wih Dr. R. Nevatia. We also thank D.Y. Tseng for his
continued support of our technical pursuits,
Road tracking in remotely sensed imagery has often been
equated with inear feature extraction. The rational was that finding
linear features in imagery either by region extraction or line finding
was equavalent to finding roads. Further, t also worked for other
types of inear features such as drainage, bridges and railroads.
This view was appropriate considering the very low resolution
imagery, ANDSAT 1,I that was available at the time. However, t is
no longer appropriate given that the research community now has
access to large scale, high resolution aerial mapping imagery
allowing for the structural analysis of the road surface. In fact, for
practical mappitng applications such detailed analysis must be
performed in order for a road extraction system to be a useful
component in a digtal mapping environment,
A second development is the emergence of computer vision
systems that pertorm integration of image processing results
notably edge-based and region-based techniques, to generate more
robust intermediate representations [5, 6, 2]. These systems can be
contrasted with traditional image processing systems that exhibited
a single ine of sequential processing without much effort in the
constuction of intermediate intepretation structures or high-level
analysis. However, even these innovative approaches exhibit single
thread control structures even though they integrate muliple lines of
sequential image processing.
ARF (A Road Folkower), is the first system to utilize mutiple
independent control structures resulting in mutiple lines of analysis
and the generation of atemative intermediate representations. It is
also unique in that t explicitely uses a cooperative method to allow
for single point failures to be overcome by using an alternative
(successful) tracking method. In this paper we report on:
In Section 2 we present a brief discussion of previous work in the
automated extraction of roads from remotely sensed imagery.
Section 3 describes the two kow-level tracking methods. Section 4
focuses on the nature of cooperation in ARF. Sections 6 and 7
discuss some tracking examples and a set of experiments
pertormed to establish performance of ARF run using individual
trackers, and both trackers in cooperative mode. Finally, section 8
briefly discusses some future work.
In this section we survey five research systems over the last ten
years that were devekoped specifically for road tracking, or were
developed as linear feature detectors and were demonstrated within
the context of road delineation. For each system we outline several
of the major assumptions based upon reading the published
literature and give some subjective indication of performance based
upon published examples. The types of 'road knowledge' used by
each method is described when appropriate.
Bajcsy and Tavakoli [1] used kow resolution imagery from
LANDSAT-1 where each pixel had a ground resolution of
approximately 57 meters by 79 meters. Due to this kow resolution,
only very major roads of three or more lanes coukd be found. This
system utilized the knowledge that roads are made of concrete or
asphalt to directly determine the approximate intensity range to
expect in MSS band 2 of the uNDSAT-1 images. The paper does
not say whether concrete and asphat surfaces are processed
separately or whether the resufts are merged at some point in the
processing; possibly both materials kook the same in ANDSAT-1
MSS band 2. It first performs a threshold operation, setting all
points within some range of the expected road intensity to i and ali
other points to 0. Then it finds ikely road points by scanning the
entire image looking for points with the right intensity profile -- that
is, points which match one of a number of templates indicating that
there is a ine point there. The templates are constrnucted so as to
accept points that kocally kook like roads of width 1, 2 or 3 points, a
total of 52 templates are used. Vertical, horizontal and diagonal
roads and points where the road width changes by one point are
accepted. It then grows the road by linking connected road points,
using constraints on curvature and distance between road points. It
then thins the road points so that the road is only one point wide.
then eliminates short segments as noise sitnce short roads are not
expected. It also labels intersections although the reason for doing
this is not clear.
The program seems to find sorme of the roads in the imagery
used athough t also finds some spurious roads. The qualty of the
roads found is difficut to judge since photographs are not inchided
in the paper. One obvious limitation of this work is that roads are
expected to exhibt very specfic spectral signatures. This
assumption may work iif the 'ri;htt muti spectral scanner (MSS)
bands from ANDSAT-1 images are used but it will not work for
images from many sources since different materials and lighting
conditions can cause roads to be of any grey vale. Also, t does
not use much road-specific knowledge. The road knowledge used
is knowledge about expected width for the given type of roads and
imagery and knowledge about curvature and length. H is claimed
that the program will find rivers, bkood vessels and bubble chamber
tracks as well as roads. This may be an indication of too ttle
specialization for the task of finding roads since t is clear that rivers,
bood vessels and bubble chamber tracks are similar only at the
coarses level of description, ie., linear 2D shape. However, many of
the features that distinguish roads from other inear objects are not
visibie at the resolution used so a general purpose iine finder could
be sufficient for the data.
The sR low resolution road tracker [3, 4] used kow resolution
imagery (ground resoktion is not reported). R kooks for line features
using some weak high level constraints on the shape of the road
path. First an area of the image to search is designated,
presumably by a user or a higher level program. I computes scores
for several operators over the area designated, under the
assumption that a combination of operators can do better than a
single one. A distinction is made between object detection
operators and object analysis operators. Object detection operators
are binary operators which detect whether the object is definitely
present or not, These operators are designed to detect only points
which are very reliable, at the possible expense of a large number of
omission errors. Object analysis operators are designed to gre
some quanttative measure of the qualty of the object point once t
is found. The object detection operators are first used to find all
reliable road points. Then a cost array is generated for each object
analysis operator. The cost arrays contain 0 for reliable road points
and a function of the object analysis score for all other points. The
function used can be designed to favor paths of a particular shape.
A path optimization is then applied to each cost array and the path
with the kowest score is chosen as the final road path.
Road knowledge used by the program is very weak. Some
limitations in this approach are that both the road start and end
points must be known at least approximately before tracking. In
addition the results of the object analysis operators must be used in
areas where roads have not been clearly detected in order to find a
path between the points found by the object detection operators.
However, the object analysis operators were designed to give
reliable resuts only when the object is already known to exist. If it is
really the case that the object analysis operators are only valid at
points where the object is known to exist, t seems that they are
completely superfluous.
At 58. Guam [12 used hgh resoluton imagery, with ground
resolution of approximately 1 to 3 meters. Ouam's road tracker was
part of the sR road expert, the HAWKEYE system. It uses an
agorithm based on correlation of the road surface pattem or
intensity profile in the forward direction along the road. It must be
grven an initial starting point plus direction and width. This is
expected to be supplied either by a user or a road finding program.
It keeps a model of the road to use in finding the road points. The
road model contains a surface model and a path model. The path
model is a ist of recent road points used for parabolic extrapolation
of the path, the surface model is an array of intensity values
sampled from the image in the direction perpendicular to the road.
The surface model is allowed to change gradually as the road is
followed.
The program first uses the path model to predict the position of
the next road point one step ahead. It then extracts a cross-section
fom the image at the predicted point and performs cross-correlation
with the road surface model to compute an error in the predicted
position. The actual road postion is computed from the correlation
offset. If the correlation peak is poor, the prograrm uses the path
rmodel to guess ahead another step and try to re-acquire the
surface. t will continue to guess ahead until t finds a good match
or until has gone further than the length of the kongest expected
anomaly. there are a large number of anomaly points, it then
hypothesizes a surface change and extracts a new surface model.
lf t can follow the road successfully with the new surface model t
continues, otherwise t quiits.
The road knowledge used is the assumption that roads have a
consistent surface wear patten but with possible anomaly points, an
assumption of constant width and the possibility of a change in
surface material. However, it does not use any high-level
knowledge about roads. For example, road width changes can
cause problems due to gradually changing surface patterns that
often occur at places where a new lane is added or one is deleted.
Gradually changing surface pattens can also occur at intersections.
This is a problem that does not occur with some of the techniques
using lower resolution imagery. This makes t clear that although
more information is available in higher resolution imagery, much
more processing is required to take full advantage of t.
Nevatia and Babu [11, 10] used medium resolution imagery
(ground resolution is not reported). Their program first rnuns an edge
operator over the entire image to compute an edge magnitude and
angle for each point. It then selects edge points based on three
criteria:
It then links the edge points together and fts piecewise linear
approximations to the connected edges. Finally, t groups the line
segments together into antiparallel pairs -- that is, pairs with
opposite directions -- under the assumption that the road is of
approximately uniform intensity and the background material is the
same on both sides of the road.
The road information used by this method is the assumption of
uniform road intensty and uniform background material, and
assumptions of minimum length and straightness of roads. One
problem is that t is difficuft to group antiparallel Iines properly,
especially in the case where there are more than two lines. Another
problem is that it is not ahways the case that the background
material is the same on both sides of the road, for example, in the
case where the road is one lane of a mulilane highway and there is
a median strip of another material separating t from the other lane.
Finally, from a pragrmatic standpoint, the edge test may be too strict
to find many roads.
Kestner [7] used medium resolution imagery where roads were
on the average 3 or 4 pixels wide. It uses two methods to track
roads - a correlation follower and a region based follower. The
program is started by a human or an automatic road finder with the
road position and direction. The corelation follower kooks in the
direction of the expected road path for points with the expected
intensity profile of the road against the background, that is, light on
dark or dark on light. Where the correlation follower fails to find an
acceptable path, a region based method is used to re-acquire the
road. The region based method extracts a two-dimensional area
from the image and searches the entire area for points with the
expected intensity profile, marking each point with a score which
indicates how well it matches the expected profile. It then
eliminates all but the best points and links them together using
constraints on road curvature to find the most likely road path.
According to Kestner, the tao correlation and region based trackers
complement each other perfectly although t is not clear exacty how
they are used together. A third method, called the binarizing
method, is also described. This method first binarizes the section of
the image which contains the road by setting all points within some
range of the expected road intensity to 1 and all other points to 0. I
then eliminates regions that are too wide to be part of the road and
links the remaining thin regions together. tt is not cear whether the
binarizing method was implemented or how t was used in the
system.
The primary road knowledge utilized concerns road shape and
the assumption that roads will be of approximately constant
intensity. An implicit assumption of this method is that the
background is of approximately constant intensty. This assumption
of constant intensity is a disadvantage since t cannot accomodate
rmajor surface changes or large changes in background intensity.
One can categorize road findertfollowers into one of three major
types; correlation trackers, egion based followers, and edge linkers.
AIl of these methods use a single local tracking strategy to find
roads. Therefor a major problem with previous work is that f the
method fails at some point t is very difficut to recover. Further, t is
often difficut to automatically reoognize when the tracking method
has failed since the kocal nature of these methods assumes that the
kocal maximum, no matter how poor, is ts best guess for the
postion of the road. As a side note, t is dfficuft to compare the
performance of the different trackers against each other since they
all use different resolution imagery frorm different sources and there
is no consistent method for reporting results.
We believe that the main way to improve road tracking is to
develop multiple methods to extract more information about the road
and to use oooperative techniques to allow a failed method to be
detected and restarted using an alternative technique. In the
following section we describe teo tracking methods in sufficient
detail that others should be able to implement similar road trackers.
In section 4 we discuss the cooperative architecture within which
these trackers are imbedded, and the use of feature specfic
detectors for road events.
ln this section we describe the two tracking methods currently
used by ARF, a suriace correlation tracker and a road edge tracker.
Each tracker makes assumptions and uses road specific knowledge
in ts image analysis. The surface tracker makes the following
assumptions:
The edge tracker makes the following assumptions:
Figure 3-1 gres an pictoral description of each of the tracking
methods. These methods will be described in detail in the following
sections.
This section describes a correlation-based road tracker for
medium to high resolution images. I uses techniques first
described in [12] with several improvements. All correlation-based
techniques are based on the assumption that there is a discernable
intensity pattern or texture on the road surface. For example, there
is usually a light cokored line down the middle of muftiple-lane roads
and there are usually darker wear patterns in the lanes themselves.
We do not ook for any such specific pattern but rather look for a
pattem that is known to be on the road at some starting point.
Provisions for skowly changitng road surfaces and for sudden
changes in road material (from asphalt to concrete, for example) are
made.
The correlation tracker takes as input parameters the starting
coordinates of the road, its initial direction and ts width. In normal
operation these parameters will be supplied interactively by a
human or provided by a higher level program, perhhaps by using
information in a database to determine where a road might exist or
by using some sort of road finding operator. A road is followed by
extracting a cross-section perpendicular to the road from the image
at points predicted by a road trajectory model and using cross-
correlation with a cross-section model to determine the actual
position of the road. H the correspondence is poor, or f the offset
(the shift between cross-sections) is greater than expected, we
continue guessing the road position until we find a good
correspondence or we have gone so far that t is unlikely we will
re-acquire the road. In the latter case, we assume the road surface
has changed and attempt to find a new surface model.
The cross-section model is an array of intensity values taken from
the image. The trajectory model is the coefficients of a parabola
ftted to the most recent points on the road.
We predict the position of the road one step ahead by ftting a
parabola to the most recent road points. We store a list of about
twenty road coordinates to use for curve ftting. In addition to the
points themselves we also store a correlation score, which is a
relative measure of the quality of the cross-cormelation, for each
point. When we ft a curve to the points, we ignore those with the
worst scores to get a correspondence with higher confidence.
ln determining the number of points to use for correlation, several
things must be taken into consideration. In order to get reasonable
accuracy, we need to use enough points so that the ft is not too
dependent on any one point. However, we cannot use too many
Rs because tne oad may not be weli suled for modeling with a
parabola over kong distances. In addition, the maximum nuinber of
points we can use for curve-fitting is dependent on the size of the
steps we take since the number of steps that can be modeled well
P 4 Prabow w be smawer tor aoer stes. Tius, iei yohiä
94OY i tajectoy predicton we need io take smaller steps.
However, with very shhort steps the acceptable correlakon gjj
between suocessive cross-secfons (corresponaing o a mairmuni
aGoeptabie change in oad direction over a small distance) becomes
very small and nearly impossible to achieve. The selection ot the
best number of points to use is thereore a iradeoii tewee
WGuracy of road predicton and confidence in the road positions
found.
To take a single step along the road, we use the following eight
steps, which are identical in form to those described in [12]. The
initial trajectory model is generated by extrapolating backwards
along the initial angle given and storing the necessary number of
points. In this way, we start out looking straight ahead without
otherwise treating the first points as a special case. The initial
cross-section model is created by taking the average of several
cross-sections extrapolated forward from the initial point.
[ONE]: Extrapolate the position of the road one step iorward.
We represent the road path parametrically as two separate
functions x(t) and y(t) where t is the total length in steps that we
have ttaversod akong the road's path. We use multiple regression
[13] wiith t and f as the independent variables to ft parabolas to
x(t)and y(t), getting approximate functons:
It can be shown, by eliminating t from these equations, that the
ftted curve is a parabola in the x-y plane as welll as in the x-t and
y-t planes, Further, the parabola in the x-y plane can have
arbtrary orientation, having an equation of the form:
We do not actually use this equation since the parametric form is
more convenient for our purposes.
The position of the road is predicted by computing x'(t+1) and
y'(tw1). Since this does not guarantee equal step sizes, due to
nonlinearty of the fitted curves, we must adjust the predicted
position so that the length of the step taken is equal to the step
sbze we intended. The problem is that we are using t to predict
postion and t has only an implict, and not necessarily constant,
relation to the length traversed along the road. The error for any
point is normally fairly small, the main concern is that these errors
do not accumulate. We first find the tangent to the fitted parabola
near the current point by taking the slope of the ine connecting
the fitted points for t and ts1. This gives an average tangent
between the two points. We then project the previous oad point
onto the tangent ine by finding the intersection of the tangent ine
and the line which is perpendicular to the tangent and goes
through the previous point. The predicted road point is then
found by taking one step fornward akong the tangent iine from this
intersection.
[TWO]: Extract road cross section.
We take a cross section perpendicular to the road at the point
predicted in step [ONE], using a linear interpolation to get muhiple
sample points per pixel. To allow for cross-correlation, we take a
cross-section wider than the road tsef. We take a straight cross-
section as opposed to a circular one as suggested for kow
resolution images in [7] both for simplicity and because t is not
clear what radius of curvature to use or how to match circular
cross-sections when we are ooking for a pattern on the road
itsef.
The linear interpolation used is a weighted average of the four
pixels around the floating point image coordinates desired. We
imagine the pixehs as being squares with their integer coordinates
representing the point in the center of the square. To determine
the weight used for each neighboring pixel, we draw a square of
area one pixel around the point requested. The weight used for
each pixel is the area of overlap between the square and the
pixel. The advantage of this interpolation method over non-linear
methods, such as that described in [14], is speed. We need a kot
of points to get a good correlation and we can perform several
linear interpolations in the time t takes to do a single non-linear
interpolation.
[THREE]: Cross-correlation.
We do a simple valley finding correlation to determine the best
match between the cross-section model and the new cross-
section. We look for the offset that gives the smallest value for
the sum of the squares of the differences between corresponding
elements. We compare the cross-section from the model with a
window on the new cross-section -- shfting the window to get the
score for different offsets. The correlation is done only over the
width of the road, To reduce the number of sums we need to
compute, we stop as soon as we have a valley. ln doing this, we
are assuming that the point we are kooking for will be close to the
center - using the first valley we find reduces the chances of
finding a good correspondence that is not what we are looking for
(for example, a different lane of the road that happens to yiekd a
better correspondence).
[FOUR]: Generate a mask indicating the positions of noise
elements.
Using the correlation computed in step [THREE], we create a
mask of the elements which dffer from their corresponding points
in the surface model by a significant amount, These are noise
elements which will be ignored in a second, more accurate
correlation.
(FIVE]; Re-correlate over the unmasked elements.
To get a better match we re-correlate using only the unmasked
elements. The basic correlation used is the same as in step
TTHREE]. To get a sub-element match we fit a parabola to the
three points ckosest to the valley and take ts minimum. We
check several things to see if the correspondence is good
enough. We check the magnitude of the offset of the
correspondence, the sum of the squares of the differences
(correlation score) and the number of points marked as noise
points. By using three different measures, we need not be so
strict on any single one. any one of these is too large, we will
not accept the correspondence. the correspondence is not
good, we guess the position of the road on successive steps until
we find a good correlation and then backtrack to output the road
points.
[SD]: Update the cross-section model.
ff the correspondence was good, we create a new surface
cross-section model from a weighted average of the okd surface
model and the matched cross-section. This essentially results in
an exponentially decaying model. A large weight is used for the
model and a small weight for the new cross-section to prevent the
model from changing too rapidyy.
[SEVEN]: Adjust the position of the road center.
UUsing the offset from the correlation, we generate the road
point. We store this in the output and also add t to the list of
points used to predict the road position, and delete the okdest
point from the ist.
[EIGHT]: Anomaly detection.
Anomalles are lound in the same way the mask elements were
found in step [FOUR] except a greater difference is tolerated. We
are more restrictive for detecting anomalies than we are for
gnoring noise and random fluctuations, ln the current version,
the step size is greater than one pixel so we must do mutple
lines of anomaly detection for each step.
At each step along the road, we check f the correspondence is
acceptable (in step [FIVE]). If t is, we update the surface and
trajectory models and try to advance another step. HB
correspondence is poor we must figure out why and try to find
where the road really is. Poor correspondence can be caused by
two things:
We treat these two cases differently. In the first case, we skip over
the occkided section until we can re-acquire the road. We use the
backtracking as suggested in [12] to avoid outputting false
anomalies. ln the second case, we constn)ct a new cross section
model and continue following.
Believing that occhIsions are more common than surlace
changes, we check for occlusions firt. We look ahead along the
extrapolated road position and try to find a cross-section with a high
correlation with the current model. When we are guessing the
position of the road, we alkow a greater absolute deviation from the
expected postion t we have been guessing for a while (that is, the
acceptable postion of the road spreads out as we guess for a
greater distance). A inear function of the number of guessed steps
is added to the normal acceptable offset to get the oftset we will
actually tolerate. we find a good correlation along the guessed
ti98OY. 8e ws p maMe sure we have eat buno ine roaa b9
requiring that more than one consecutive cross-section give a
reasonable match. If we have been guessing the oad for onty a
few steps, we only require that one point give good correspondence.
If we find a good correspondence, we backtrack and do road track
and anomary output tor the points we skipped over, starting from the
last suocessful point. We then continue following the road from th
newly found position.
When we are guessing, t we find a point with good
correspondence we add t to a termporary road model (for both thg
road surface and the trajectory). In subsequent extrapolations we
use the temporary models under the assurmption that we are
actually folkowing the road. This allows us to guess the road
postion based on all the information we have and makes
backtracking easier f we decide we have found the road. H we
eventually decide the points added to the temporary road model are
really on the road, we backtrack, adding the intermediate points to
the permanent model,
we decide the points are not good, we throw out the temporary
rmodel and rmake a new temporary model out of the okd permanert
model. When we create the new temporary model, we do not go all
the way back to where we started guessing, we just get rid of the
new points we added and continue from where we are.
If we have not found a reasonable correspondence with the
current model by extrapolating forward we presume the road
surface has changed. We then try to start a new model by retrieving
a new cross-section at the last successful poinnt to serve as the
model. H we find a good correspondence with the new model, we
backtrack over any points we skpped, then begin following again. H
we do not find a good correspondence, we use the cross-section
where the new comespondence failed as the model and try again.
We continue doing this until we find a good cormespondence or we
have gone so far that t is unlikely that the road will be re-acquired.
The edge tracker is based on the' road finding method of Nevatia
and Babu. R tracks the edges of the road by inking points with high
gradient and orientation in the direction expected for the road. The
gradient and orientation are computed by a 5 X 5 Sobel gradient
operator. We use the Sobel gradient rather than the Nevatia-Babu
operator because we found that the 12 vale grain of the Nevatia-
Babu operator was too course ior accurate angle codmparisons.
During tracking, the position of the next road point is predicted by
parabolic extrapolation from the recent path the same way as is
done in the surface tracker. The Sobel is then computed for the
points along the line perpendicular to the road direction at the
predicted point. A score based on the weighted sum of several
component scores is computed from the Sobel vahues for each
point. Each ot the components is a near tunction between 0.0 and
1.0 in the range between some minimum and maximum, 0.D outside
the range at one end and 1.0 outside the range at the other end.
The component scores are edge strength, orientation, difierence in
magnitude from each of the neighboritng points and difference in
angle from each of the neighboring points. The edge strength score
is high for edges of greater magnitude, the orientafion score is high
for angles of the expected orientation. The difierence in edge
magnitude score is high for larger differences, selecting for points
whose neighbor points have relatively small edge strengths. The
difference in angle grves high score to points whose neighbors have
similar angles, selecting ior points in areas of consistently linear
texture.
The final score for each point is taken as the sum of all of the
components. The point with the highest score is then chosen as the
edge point. If no points have a score higher than some minimum
threshokd, no point is selected as the edge point. If edges are iound
on both sides, the center ine point is marked as the point in the
middle. only one edge is found, t is assumed that the width has
remained constant and the center line point is marked as haf of the
width from the one edge found. If no edges are found, guessing
ahead is done the same way as for the surface tracker. As with the
surface tracker, f guessing ahead goes too far without finding a
good point, the tracker quits.
To decrease the possibilty of finding an edge point that has a
ngnh score ocay but e not bant ot te ad eoo. 40= 9 4
eihe between points where poss4xwe. We use the same method as
Nevatia and Babu, ooking ahead n the direction ot the edge and
accepting tne next point f t is a maximum in edge magnitude and
has the correct direction.
In this section we discuss the organization of ARF, an Automatic
Road Follower. ARF is organized into three processing and analysis
leveis, The kow-level is composed of two independent kow-level
road trackers that generate an estimatkon of the center Iine of the
i6i aid proauce vänous tacker dependent ieatures s as =.
surtace matenial change, and anomaly detecikon. These tackers
have been describbed' in aetail n Sectons 3.1 and 32. The
iremediate-ievei s composed of several modules that detect and
repon oaa ieatures such as overmasses, ad wdh chanos 4
vehicle detection. The high-level provides overall control and user
interface. It detects situations where one kow-level tracker must be
restarted from the other and allows users to interact to start or
restann the system. Figure 4-1 gives a description of the overall
organization of the cooperative road tracker system.
The basic control cycle in ARF is to invoke each of the road
tackers naependentiy and aow them wo tack asynohonousy unti!
eieinas gerieratea a siep lorwara. This sten moht eauire sevre!
advance, lal, guess, and predkction steps wihin each of the road
tackers as described in Section 3. Once both trackers have
cormpleted a step foraard, feature detection is performed by
invoking each of the intermediate level detecters with access to the
intenal path model of both trackers. These detector modules and
the type of state information that they require is descrbed in Section
5. Finallly the control queries whether a path divergence has
occured. Such a divergence is currently a dfference of 7.5 meters.
If no divergence is detected the basic control cycle continues.
Upon detection of a divergence, the high-level invokes each
tracker to advance 5 additional units. The path history and
confidence values are used to evaluate the relatie goodness of
each tracker, both in terms of ts history, and in terms of the 5
additional units. The high-level can then decide to terminate the
overall rnun, f the confidence in both trackers is sufficiently kow, or
can decide to restart one tracker from the path model of the other.
The process of restarting is made straightforward since the
internal data structure representation of the path history for each
tracker is identical. Thus an edge tracker path history can be
koaded directly from the surface tracker, and edge vahues and
confidence can be computed without search. Similarily the
correlation tracker can be forced to assume a new path model, and
rebuikd ts weighted decay cross section.
There are six types of road features which ARF attempts to detect
and delimit - intersections, width changes, overpasses. surface
material changes, vehicles, and occhisions. Any of these condtions
can cause simple tracking schemes to fail. Multiple conditions, such
as occlusions on a sharp curve, width changes followed by a
surface material change require mutiple sources of road estimates
in order to be robust. Each of the intermediate level feature
detectors has access to each of the trackers internal road history
data strnucture. This data structure contains an entry for each path
point tracked on the road including center position, match
confidence, edge confidence, width estimate, left/right edge track,
and model history. The results of each feature detector is combined
with the original tracker information to form a composite path model.
For each point in the composite path model we store
in the following sections we summarize how each feature
detector determines that a feature event has occured, how t is
verified, and what actions (if any) shoukd be taken by the high level
control. The major problem for most feature detectors is in the
generation of hypotheses about continuous features that occur over
several path points only using pointwise information as above.
There role is to survey and combine the noisy pointwise into the
occurance of a discrete event. A second problem is in the
determinization of the actual location of the feature. For example, in
the case of width changes, the actual feature occurs, usually slowly
over many road points. This ocalization is important since we rely
on reasonably accurate positions for overpasses and intersections
in order to skip over them.
To detect intersections, we detect cases where there are several
consecutire road points with bad edges and possibly with poor
correlation scores. We also expect no significant change in average
pixel value over the cross-section. On roads in residential ahd
urban areas, we might expect intersections to appear at fairty
regular intervals. The history then must contain a measure of the
quality of the edges and a measure of the quality of the correlation
scores. Verification can be accomplished by adding the intersection
ocation to an agenda of possible new road starting points and
invoking the tracker using the intersection width and direction'. To
@; 98G W9 4nead past tne predkcted wkdth ot tne intersectng
road. tt s generaly not necessary to fush portions of the surnacä
model.
The deteton of road'wkdth changes requires the integration of
s48=w> uy awan onanoewe ex-ecio tnaooäaesges,
g P, 9 8%w =mwen,'ane mie srous b+ i&äi
g%89g P= we ot me aa. we we me a niiryi&
PWtee tne numter ot anomay ponts or each ioaa poirnt ana
their kocaton on the cross-secton. in addlon, tnere ihouia te
some disagreement between edge center ana correiation c6+äs+ ä
t9e 9e 9o0 soores. 'soneani wemi coanoes esi tte
989 4w5 a o enouon'nnory ot me eso+ >6iieW
789 %= wwn oanoäs onen +ccur on idiwäys isa
ovem4ses, an overmass has aready been detectedwe herease
[e P y =8h @woe Yes+ 5>es 6ioieissää
for s tohth ooubled path hisiory accessföie to a ieature aeiectäi.
r s 4se. =nteanwonn s noi necessay siws ihäWiäi äiG'&
detected directly.
Tn wtant implcaton of a oad wkoth change is tne
eventual breakoown ot the sutace tracking moaei beäause e
correlaton cross secton no onger modeis ie actuai oad eross
section. This is exactly what causes the generation of anomaly
points along one edge, usually the edge that is expanding or
contracting. As a resut of a road width change the model width is
updated and t is necessary to re-acquire a surface model using the
path model of the edge tracker.
Overpasses exhibt properties similar to intersections at the kow-
level path history. The major distinguishing feature is that we
should expect some changes in the average surface intensiity, more
so than in an intersection. Generally we detect lack of consistent
road edges conincident with a series of bad surface correlation
scores. Overpasses are lkely to have shadows that precede or
follow the previously mentioned features, and these shadows are
usually detected as anomalies. Verification can be perfromed by
finding leading and trailing edges of the overpass, usually also
detected with anomalies..
Surface material changes are relatively easy to detect, since the
pavement changes causing a large, abrnupt change in average
surface intensity akong with poor surlace correlation but possibly
good edges. Often surface material changes coincide with bridge
decks, and two cknsely spaced changes can be detected. Shont
changes due to patching are detected as anomalies. ln this case
the path history is re-acquired using several consecutive matching
cross-sections.
Vehicles can be detected by looking for patches of anomaly
points of approximately uniform intensity. Since the path history
information stores the numbber of anomaly points, their average
intensity, and their kocation in the cross-section, the vehicles ieature
detector performs simple grouping.. Since vehicles can appear at
any point on a road but there should be some rules about where
they can be with respect to each other and rules about which lanes
they are ikely to be in.
Occlusions are detected initially by looking ior irregular patches of
anomalies along the side of the road. Such irregular patches can be
caused by buildings and trees. They are too kong and irregular to be
vehicles, tend to not cause bad correlations but may coincide with
intermiittent edges on one side of the road.,
Photographs 6-1, 6-2, and 6-3 are three typical examples of the
ARF cooperative road tracker. Photographs 6-1 and 6-2 are
continuations of the same tracker run on a limited access highway
with a ground sample of about 1 meter per pixel. Figure 6-3 shows
a similar road at more about 3.5 meters per pixel.-
ARF generates a real-time display with iour display windows. The
'edge tracker' window shows the independent progress of the edge
tracker including centerline, edge points and predictions and
guesses. The same information is displayed in the 'surface trackerr
window, except that only the anomalies replace the edge points.
The 'cooperative output' window displays the resut of the high-level
integration of centerline paths and the superposition of road feature
events. The text window gNes a running interpretation of the
feature symbols superimposed on the cooperatie output. The
three image windows scroll independently as edge and surtace
tracking proceed. The cooperative output usually lags the output of
the trackers since it often must wait for delayed events from the
feature detectors.
Photographs 6-1 and 6-2 show several examples of road width
changes, surface material changes and overpass detection.
Anomaly detection is particularily visible in photograph 6-2 in the
area of the overpass. While initially this area is marked as an
anomaly, the cooperative output correctly identifies it as an
overpass (12).
Photograph 6-3 shows an exarmple of nearly complete failure by
the edge tracker in a kow resoltion road at the point where several
roads merge and the road direction changes. However, the surface
tracker correctly tracks the direction change and the cooperative
output reflects the use of each trackers confidence scores to
correctly weight the final output.
Figure 6-4 illustrates the symbolic descriptions that are produced
by ARF during a tracking session. All imagery used by ARF is
maintained in the MAPS database [8, 9]. The MAPS database
provides a digital elevation model and a camera model that allows
for image-to-map and map-to-image correspondence. ARF uses
these image-to-map correspondence equations to transform sub-
pixel road centeriine positions into geographic coordinates, and then
uses the geographic coordinate to index into a digital elevation
databae to calculate an interpolated terrain elevation. This
capability allows for reasoning about image features in metrio
distances as opposed to pixel units and is used to represent
distances in each of the intermediate-level feature detectors..
Our input data consists pnimarily of aerial photographs ol tN
washingfon D.C. area fom nwo dtferent soyoes. We selected 3
roads idr tuning and iniial debugging and 35 for testing. attemptisd
to select faity äitiicutt data so as io allow for tuture improvements to
be reflected in tests on the same input. We adjusted the
parameters of the program to obtain the best overall performanoe
(measured subbjectvey) when nun on the tuning roads and then to
it on the test roads also. We intend to run future versions of ARF on
this input and use the numbbers given above as benonmo%4-
Pernomumnce data for tuning and test oads are gren in Fgures 7-1.
7ä, and75 er ine tainirigt data and Fsgures 7-2, 7-4, and 7-6 for
the biind ttest' data. For each table road Width and Length are in
pieis meiers1 Tme gren are system ad uger e. 893%
i seconas. Unaer Subjectve Reason or Stoping. A indcates
auomae stopping, M dicates tmat the program was stopped
manually by the operator.
When ARF does not folkow a road to the edge of the image, t
usually fails for one of the following reasons:
Additionally, in some cases the road path generated is not very
smooth or does not stay on the center of the road. This usually
happens in fuzzy images or on roads with little texture.
Speed is usually between 7 and 14 pixels of road length per
second of CPU time on a VAX 11-785. Timings depend mostly on
the road width, the number of anomalies and the amount of
guessing and backtracking necessary. About 35 percent of program
time is spent in piel access and interpolation.
Figure 7-7 gives the relative performance of the each tracker vs.
the other method and vs. the combined tracker. These results
indicate that the surface tracker is of significantly higher quality than
the edge tracker. The surface tracker produces a konger track ior
nearly all of the rnuns.
It is important to note that the combined tracker is better than
ether tracker alone in a significant number of cases. Although the
individual trackers are better than the combined tracker in some
cases, improvements to the mechanism which measures the quality
of the path could increase the overall quality of the combined
tracker.
In this paper we have presented a complete system for road
t4ing and feature detecton n hgh resoluton aerial imagery. Th
%8 s eer wesed on a wroe number ana vaney of cömpiex
4 9W- We beleve thai s petommmance woua n nany
cases justfy ts use as an interactive assistant for road network
delineation. The qualty and reliabilty derives irom the use ot No
WMenendent methods ior oad ttacking. organized nto a cooperairve
system architecture that tolerates failures and allows for automatic
restart.
There are many avenues for future work. First and foremost we
are investigating the use of an automatic road starting point finder
so that we can relax the requirement that ARF be given an initial
starting point, road width, and direction. Second, we believe that
other tracking methods could be integrated into the ARF system,
such as the use of mutitemporal imagery, or stereo pairs. What
remains to be seen is whether additional information from other
SOUrces actually improves overall system performance.
Finally, ARF should be viewed in a larger context of road network
extraction. That is, at many points where ARF fails a road finder
ould be invoked to attempt to locate new plausible starting points
which would allow for continuation. ARF generates areas such as
Overpasses or surface material changes which should also become
the focus of road finding. Road network extractionn then becomes
the integration of various search strategies ior invocation of road
finding and road ttacking.
In recent years there has been a renewed interest in
tackling problems related to character and document
recognition. For handwritten numeral recognition, ex-
cellent systems were developed ([6] [7] [8] [18]). But,
despite advances, human performance has not been
matched.
More sophisticated systems are needed, incorpo-
rating more knowledge in all stages of the recogni-
tion process-preprocessing, feature extraction, clas-
sification. However, in our view, the most important
problem to address is feature selection and ertraction.
In the literature, little is said about how and hou
twell features are located and extracted by the var-
ious methods proposed. Furthermore, such aspects
are rarely addressed when a system's recognition per-
formance is discussed.
In this paper, we examine the problem of extract-
ing curvature features from the contours of objects.
While the focus of our work is handwritten numerals,
we believe our remarks and experimental results have
broader applicability.
Our goal is to detect significant concavities and con-
vexities of contours. To be more precise, what is im-
portant here is the identification of regions of 'consec-
utive' pirels along the contours which correspond as
closely as possible to the global shape features identi-
fied by humans. For the '3' of Figure 1, these regions
can be roughly indicated by the sequences of circles,
delimited by square boxes. For now, we are not con-
cerned with the labelling of these regions as endpoints,
smooth or sharp concavities etc.
Furthermore, we are seeking methods which gen-
erate their feature regions automatically, without hu-
man interaction; the location and relative size of these
regions should be roughly invariant to rotation and
scaling; the bottom of concavities and the tip of con-
vexities, especially endpoints, should be located with
some accuracy (see filled circles of Figure 1).
In the remainder of this article, we shall use the
following definitions. Let p1 .P2.. -pN be the sequence
of N 8-connected contour points labelled in a counter-
clockwise fashion. Furthermore, let c; be the (Free-
man) direction code from p,-1 t0 P-
The interest in corners (or dominant points) stems
from the knowledge that much information about the
shape of 2-D objects is concentrated at points of high
curvature along their boundaries.
In O'Gorman [13], methods which estimate the con-
tour curvature at a point based on the difference be-
tween the slopes of 2 line segments which fit the data
before and after that point are called DOS methods.
In these methods, the angle %;, used for curvature es-
timation at point p;, is the angle between vectors V,
and Vf, where V7 joins p.-w12-. % P-a5 and V,
stretches from P.-4+475 % P-4e124, Figure 2 illustrates
the situation for s 5 and m s 2.
In Rosenfeld & Johnston [16], m = 0 and the cur-
vature estimate is the cosine of the supplement of our
9. To improve this method, Rosenfeld & Weszka [17],
performed local averaging of the cosine values.
In Freeman & Davis [5], m = 2- s. The 'cornerity
measure' of each point is based on a local average of
the 8-values and on the lengths of the straight regions
before and after the potential corner. Beus & Tiu [2!
average these 'cornerity' measures at each point, for
several values of s, and impose an upper bound on the
lengths of the straight regions.
In O'Gorman [14], small positive values are used
for m. Regions of the 6-plot which extend beyond
the 'zero-range', defined as the curvature due to noise,
correspond to corners and curves aiong the contour.
When the features of interest exhibit different levels
of detail, Teh & Chin [19] argue that the determination
of a proper region of support for dominant points is
more critical than the measure of curvature chosen.
There are several methods based on arc-chord dis-
tances, which are often used to find polygonal approx-
imations of planar curves. Another group of methods
is based on Gaussian smoothing. Here parametric rep-
resentations of contours are convolved with Gaussian
filters, for different scale values (o). Knowing the fil-
tered responses of basic curvature changes, it is pos-
sible to locate them by examining the movement of
peaks and zero crossings across several scales.
Due to limited space, we do not elaborate on these
types of methods any further.
Since corners (or dominant points) are not rigor-
ously defined, the value of different algorithms is best
judged by direct examination of their results against
the specific requirements of particular applications.
For example, methods which have no input param-
eters and detect all corners at the finest level of detail
[19] may not be appropriate for certain applications
because they can pick up too many noisy details;
For DOS methods, O'Gorman [13] has shown an-
alytically, using certain assumptions, that the signal-
to-noise ratio is better for small positive values of m
(DOS). For equal values of the signal-to-noise ratio,
the same study shows that this DOS' method yields
narrower peaks than the Gaussian smoothing method,
thus having better signal detectability.
In recent years, other comparative studies have
been published ([19] [15] [11] [1]). Based on their re-
sults, it appears that Rosenfeld-Johnston, Rosenfeld-
Weszka, Beus-Tiu and Teh-Chin achieved some of the
best results. However, while their conclusions are in-
teresting, it should be kept in mind that the number
of images used in these studies is generally very small
(from 2 to 8 images).
D'Amato et al. [4], Commike & Hull [3] and Hull
et al. [7] use 8 types of features based on the amount
of curvature present at any point. Their computation
of a curvature estimate at each point is analogous to
the DOS methods; the angle of curvature at point p,
is given by
For handprinted characters, the 'angle accumula-
tion algorithm' of Lee et al. [9] uses differential chain
code values as a measure of local change in curva-
ture. A concavity is defined as ''the longest sequence
of perimeter points whose angular changes between
consecutive elements are all non-negative and their ac-
cumulated sum is greater than 1''.
lLegault & Suen [10] also use an angle accumula-
tion scheme to partition the boundaries of 2-D objects,
for piecewise approximation. This algorithm was used
successfully to extract feature regions for one numeral
recognition method described in Nadal et al. [12].
Based on our appreciation of the suitability of the
above-mentioned methods to achieve our stated goals,
we have chosen 5 corner and curvature detection algo-
rithms: Rosenfeld-Johnston, Rosenfeld-Weszka, Beus-
Tiu, Teh-Chin, O'Gorman; as well as 3 algorithms
from the recent OCR. literature: D'Amato et al (also
used by Hull et al and Commike & Hull,) Lee et al.,
lLegault & Suen. The methods are implemented based
on the information given in their papers and additional
information obtained in private communications.
For the Rosenfeld-Johnston and Rosenfeld-Weszka
methods, the user-supplied parameter was set to
mar{N/10, 4]. The Teh-Chin algorithm was imple-
mented with 3 different ''measures of significance''.
The results being almost identical, we retained only
one version for comparison purposes. For the ap-
proach of Lee et al, we did not eliminate diagonal-
facing concavities, since these do convey relevant infor-
mation for handwritten numerals. To detect convexi-
ties, all differential chain code values were negated.
Several experiments were conducted to select the
Beus-Tiu input parameters. More importantly, some
criteria had to be established for the automatic selec-
tion of corner points, since their number is an input
parameter in the original algorithm.
The methods were tested on 100 binary images of
handwritten digits, selected from a subset of a 20 000-
sample CEN PA RMI database. Ten samples were cho-
sen for eachh numeral class, offering a variety of styles
and sizes. They are shown in Figure 3. Our observa-
tions concerning the specific strengths and deficiencies
of each method are summarized in the next section.
We have also tried to provide an overall quantifi-
cation for the performance of each method by com-
paring the feature regions that it detects to the fea-
ture regions perceived by humans. For each sample,
regions of the external contour corresponding to the
global shape features to be detected were determined
manually by the authors. The feature regions of Fig-
ure 1 illustrate the result: the selected regions are the
sequences of circles, delimited by square boxes; the
keypoint of every region is identified by a filled circle.
A 'measure of goodness' (M G) was computed for
each method. For sample i, let R; be the set of the
indices of all contour points belonging to the n; regions
selected by humans. Furthermore, let S7 be the set of
the indices of all contour points belonging to the mf
regions selected for sample i by method k. We define
Qf = R, n Sf and f = n,/mf or mf /n;, whichever is
less than (or equal to) 1.
The 'measure of goodness' of method k in deter-
mining the global feature regions of sample i is then
defined as
where u; is the weight associated with each point
whose index belongs to I,. For simplicity, we have
assigned a weight of 5 to points in 'endpoint regions'
and a weight of 2 to other points in R,. Results ob-
tained so far are presented in Table 1.
Among the methods tested, the Beus-Tiu (BT),
Legault-Suen (LS), and Rosenfeld-Weszka (RW) algo-
rithms give best results in terms of their M G-values.
The LS method was found to be the most reliable
in the sense that it detects some points within each
and every feature region. Only once is a feature re-
gion missed altogether. On the other hand, its major
weakness is in the extraction of long smooth curves,
which are not detected as single global regions; in-
stead, one or more smaller subsections of those curves
will be detected.
On average, within all the feature regions that they
do extract, RW and BT tend to capture a larger num-
ber of points than LS. However, this advantage is offset
by the fact that they altogether miss certain regions
completely. This generally occurs when we have pairs
of nearby features, but it is not limited to that situa-
tion. Another problem, common to these methods, is
their poor localization of certain endpoint regions.
RJ and LE (Lee et al) suffer from the same defects
that were just presented for RWV and BT, only to a
greater extent. The definition of a concavity (convex-
ity) in LE includes the straight portions of contour
which precede and follow the actual curved portion.
In some cases, these extensions may even cover the
early part of following regions of opposite curvature.
The poor performance of TC is linked to its detec-
tion of a large number of tiny consecutive regions, thus
preventing the method from extracting features at a
higher, more global. level.
The problem of detecting significant concavities
and convexities of external contours, corresponding to
how human beings describe these contours, was ex-
amined. WVhile none of the methods compared were
specificaliy designed for this purpose, the Legault-
Suen method probably represents the best compro-
mise, so far.
T'he problem of detecting feature regions related to
human perception is inhherently difficult and escapes
straightforward definition. Nonetheless, we believe
that more careful feature extraction can play a key
role in improving recognition performance for hand-
written numerals.
Finally, we note the scarcity of information avail-
able when one tries to establish what methods are suit-
able for a specific purpose. Authors rarely discuss the
uueaknesses of their approaches. Also the comparative
studies involving different methods are often limited
to a very small number of images, making it difficult
to draw conclusions concerning their relative merits.
We would like to acknowledge the kind assistance of
A. Commike and D.-S. Lee. Their prompt and precise
replies to our inquiries were very useful. We particu-
larly thank Dr. L. O'Gorman for providing us with the
full code for his method and Dr. L. Lam for sharing
her code (and experience with) the Beus-Tiu method.
This work was supported by research grants
awarded by the Natural Sciences and Engineering Re-
search Council of Canada and an FCAR team research
grant awarded by the Ministry of Education of Oue-
bec, as well as the National Networks of Centres of
Excellence research program of Canada.
In the literature several methods for the estimation
of curvature from digital image data are known. They
are based on the three different formulations of cur-
vature which are equivalent in the continuous space,
but not in the digital space. As a consequence of the
digitization we have that when a continuous object is
repeatedly placed on a discrete grid with random po-
sition and orientation the results will not be the same.
A method should be judged in terms of its accuracy
and precision under repeated measurement of such a
randomly placed object. Accuracy and precision of
a method are quantified respectively by the resulting
bias B and standard deviation S. The appropriate ref-
erence object for curvature measurement is a circular
disc.
The accuracy and precision are not only depending
on the curvature of the disc, but also on scale. To that
end, we use a scale parameter in all methods. Further,
due to the anisotropy of the grid, curvature estimation
is also depending on the local orientation of the disc
with respect to the discrete grid. Therefore, the esti-
mated curvature is studied as function of radius, scale,
orientation and resolution of the grid.
As methods for curvature estimation are based on
three different formulations of curvature, they fall in
three different catagories.
Orientation based: In [2], [S] and [10] curvature,
in effect, is estimated through low pass differential fil-
tering of estimated tangent (or gradient) direction. In
these methods (named I) only a limited number of dif-
ferent orientations are recognized (for this paper the
Freemancodes are used).
In [4] (method II) the effect of the anisotropy of
the grid is reduced by resampling the main and diago-
nal elements of the 8-connected contour in equidistant
samples along the digital contour, and then proceed as
before. We make one addition to the method, using
the fact that the average distance between two dis-
crete points (prior to resampling) is computed to be
1.107 [3]. On the basis of this result, we divide esti-
mated curvature by this factor. For the differentiation
step involved in all methods, we have chosen to use a
differentiating Gaussian kernel, as used in [2]. The fre-
quency response of this kernel is similar to the other
kernels used in the references. For all methods, scale
is now regulated by the standard deviation o of the
Gaussian kernel. In practice we have to truncate the
kernel to a finite size M. A common choice for M i.
to take M s 6o (see for example [5]):
In [1] a line fit approach (method III) is used on
the digitized contour to estimate tangent direction.
The curvature estimation step is, in effect, done by
linear filtering of estimated orientation with the ker-
nel [-41,-1}. This differentiation step is followed by
division of the results by the distance between two
succeeding points. We introduced a Gaussian weigh-
ing function into the fitting procedure, to allow for
comparison with respect to scale.
Path based: In [5] and [7] a linear filtering approach
on the x- and y-coordinate sequences is applied using
differentiating Giaussian kernels (method IV). Rather
than using linear filtering. one can also fit a spline
through the coordinate sequences and compute deriva-
tives analytically [6.. However, this latter procedure
also boils down to linear filtering of the x- and y-
coordinate sequences. Frequency characteristics are
similar to a Gaussian kernel and therefore results of
this method are not shown explicitly.
Osculating circle based: Although not used for
that purpose in the reference, one can take the curva-
ture of the circular disc best fitting the contour points
[9] (method V). Similar to the case of line fitting, a
Gaussian weighing function is introduced.
Results were obtained using discs of radii 10 to
40 and for the following range of scales: o s
3.0. 4.0. 5.0, 6.0.8.0.12.0. 16.0. For the disc of radius
25 the bias is shown explicitly in figures 1-5.
Orientation based: The Freemanbased method I
shows an orientation depending bias (see figure 1).
where the difference found for 6 s 0 and 6 s /4
decreases with increasing c. For o z 16.0 it reaches a
bias independent of orientation of approximately 10%.
For a s 3.0 the difference in bias between 6 s< 0 and
6 s x/4 is equal to 95%. This is far more than the
standard deviation which, at its maximum, is equal to
30%.
Resampiing (method Il) shows a significant reduc-
tion of this orientation depending error with approx-
imately equal standard deviation (see figure 2). In
method II. for o s 16.0 the bias is onlv -19. where
the standard deviation is even lower than the absolute
bias. In fact the overall performance of method II is
superior to all the other methods considered.
Method III based on linefitting shows a lower de-
pendency on orientation as compared to the Freeman-
based method (see figure 3). however the standard de-
viation is high. even for m = 16.0 (7%.14%).
Path based: For all scales, the pathbased method
(IV) shows very poor performance (see figure 4). For
o s 3.0, bias ranges between 749 and 113%, where the
standard deviation reaches even higher values (151%
- 236%). This improves for higher scales, but for o =
16.0 bias is still 26% to 339. Standard deviation for
this value of o is approximately 16%
Osculating circle based: For small o the errors
found for the method V based on arcfitting are large
(see figure 5 ). For o = 3.0 the bias is ranging from
41% to 108% with a standard deviation between 10%
and 69%. For o = 16.0 the bias is a high 31%, where
standard deviation is less than 1%, An interesting
phenomenon occurring in the arcfit based method is
the fact that for small o large errors are found for an
approximate orientation of E m/16.
Orientation based: The method I, based on the
Freemancodes, shows a large orientation depending
bias. This is an immediate consequence of the fact
that the average distance between samples is depend-
ing on orientation. leading to an orientation depending
frequency response. We computed the error for a con-
tinuous disc. For orientations in the direction of the
main grid lines, the error is 0, for orientations in the di-
agonal directions of the grid, the error varies, depend-
ing on the curvature of the disc between 44%(r z 40)
and 55%(r z 10). This computed error is independent
of the scale. The error of 10% found for large o is due
to the fact that the result is not normalized for the
average distance between two points.
Although similar to method I the resampling based
method II shows far better performance. Equidistant
sampling of the contour turns out to be a crucial factor
when estimating curvature by linear filtering of esti-
mated orientation. The normalization step is also im-
portant. Application of method II without normaliza-
tion would, for large c, lead to a bias of approximately
10% similar to method I. The remaining orientation
depending bias for small o is a consequence of the
fact that the resampled points are equidistant samples
on the polygon through the original discrete points,
rather than an equidistant sampling of the original
continuous curve.
The results for the linefit based method III show an
orientation depending bias and poor precision. This
poor precision was to be expected, as linear filtering
is done using the filter [-41,-1) which has undesirable
frequency characteristics. Although orientation esti-
mation is very precise, the small errors are amplified
by this filter. We further note that using large win-
dows in orientation estimation is only valid for curves
which have local symmetry, which is not the case for
general curves.
Path based: The path based method (IV) shows
large bias as well as a high standard deviation. To an-
alyze this method, we took a close look at the parame-
terization of the curve, following from the digitization
method. It was found that the parameterization has
a discontinuous second derivative. In the continuous
case, this is ruled out by the fact that it is multiplied
by a factor which is zero, but this is not necessarily
the case for the discrete curves considered. Further,
and more important, is the fact that the claim that
taking a windowsize of 6o is sufficient [5] does not
hold. Results drastically improve when 8 or 10o is
used instead. Bias for c s 3.0 with window size 10o
is maximally 4% where for the window size of 6o it
was 113%.
In practice this large window size poses a lot of
problems when considering non-closed curves.
Another factor influencing the results is the fact
that the size of the disc decreases after Gaussian
smoothing. Therefore the curvature of the smoothed
disc is larger than the curvature of the original disc. In
[5] a method for correcting this error is given. It is not
applied in this comparison as calculations show that
these errors are only significant for very large scales.
Osculating circle based: The method V, based on
circular arc fitting, shows good performance, but only
when large o are considered. This good performance
was to be expected, because the object under consid-
eration, in principle, perfectly fits the model on which
the fitting is based. However, the model assumes a
complete circle. For small o, a circular arc of limited
extent would be the appropriate model. We found
that for small o it is difficult to distinguish the circu-
lar arc from a straight line. In [2], criteria are given to
make such a distinction. We could assign curvature 0
in the linear case, but this would yield a discontinuous
curvature function, which is undesirable. We further
emphasize that for objects different from a disc the
quality of the measurement degrades as the model of
constant curvature will not longer be valid. Taking a
fixed scale in that case is not the proper way of cur-
vature estimation.
A practical issue in the evaluation of the measure-
ment errors, is the question what resolution to use
to obtain a certain predefined accuracy or precision.
First we have to parameterize the notion resolution in
the context of curvature measurement.
Consider the curvature measurement of an arc of
size 2f (given in radials). As the average distance
between points is 1.107 [3], the number of points N
on an arc of radius r is approximately equal to N z
ra8/1.107.
This will serve as our parameter of resolution. As
we have chosen to take fo as number of points for the
Gaussian kernel, the scale parameter o of the kernel
is related to r and 20 by: o = r26/6 + 1.107.
To find a relation between resolution on the one
hand and accuracy and precision on the other, the
bias and deviation for discs with radii between 10 and
140 were computed. The o to use for a particular
radius was calculated using the relation derived above
for 220= /4 and 2@ e m/2.
For the resampling method (II) errors as a func-
tion of resolution are shown in figure 6 ( = >/4
only). Note that in the figure the scale of the y-axis
has changed with respect to previous figures.
From the figure and further analysis of the exper-
iments we found that bias is a function of the local
orientation of the disc and not of resolution. In fact
for the smaller arc, depending on the local orientation,
bias will range between -7% and -45%. For the larger
arc the range is between -3% and -15. A bias correc-
tion is recommended as a function of local orientation
only.
The deviation of the method decreases with increas-
ing number of samples. However, for the smaller arc
( 6 z >/4 ), far more samples are needed to reach
a similar precision as for the larger arc (26 = x/2).
So, resolution should be based on the smallest arc of
interest given figure 6. From there one computes the
appropriate scale parameter o.
Methods from literature to estimate curvature from
digital image data are based on the three different for-
mulations of curvature. These three formulations are
equivalent in the continuous case, but not in the digital
case. In fact, the accuracy and precision of a method
are clearly depending on the choice of the formulation.
Results show that for some methods the bias can
be very large (> 200%). For all methods, the bias is
depending on the local orientation of the object with
respect to the grid, as well as scale.
The method with the best overall performance is
clearly the method based on resampling (V). For this
method the relation between resolution and measure-
ment errors was studied.
This results in a practical guide for tackling the cur-
vature estimation problem. First one has to find the
approximate size of the circular arcs of interest. From
figure 6 one can find the required resolution to ob-
tain a certain predefined accuracy and precision. This
immediately gives the scale of the kernel to use in fil-
tering. From there, the resampling method (V) should
be applied, with the normalization by 1.107.
Echocardiography is a popular clinical method for the
identification and assessment of an entire spectrum
of heart abnormalities [1]. In recent years visualiza-
tion and quantitative analysis of the heart from two
dimensional echocardiograms has received increased
attention. It is possible to detect, quantize and visu-
alize a large spectrum of heart abnormalities through
segmentation of various regions in a two dimensional
echocardiogram [2]. HBowever, automatic segmenta-
tion systems developed for quantitative analysis of
echocardiograms have not been successful in a clini-
cal environment. Furthermore, owing to the complex-
ity and importance of the task, human supervision of
computer generated segmentation is often essential.
Two dimensional echocardiograms (2-D) are im-
ages of cross sections of the heart, obtained by me-
thodic registering of the echoes generated by sound
beam scans. The intensity of sound echoes are de-
pendent on the type of the tissue, and change in the
tissue type, at the point of backscatter. Echocar-
diograms record high intensity echoes at the points
that correspond to the heart walls, valves and other
tissue changes that lie within the scanning range.
Therefore, it is possible to separate the wall regions
from the blood volume by simple binary segmenta-
tion. Several researchers have used binary segmen-
tation to extract the wall regions [2]. However, be-
cause of the nature of imaging and complexity of the
data, automatic methods may misclassify some re-
gions, To alleviate this drawback, researchers have
proposed semiautomatic schemes that enable a user
to interactively correct the misclassification [3, 4]. In
this paper, we study this problem of interactive re-
finement from the point of view of both image pro-
cessing and human computer interaction, and develop
a collaborative method for accurate segmentation.
A schematic overview of the developed supervised im-
ae segmentation scheme is shown in Figure 1. The
overall system consists of two major components, viz.
the Graphics user Interface (GUI) and the supervised
image segmentation algorithms. The user initiates
the binary segmentation process by simple mouse
based selection, and observes the computer generated
segmentation results along with the original images
on a graphics display. If the segmentation is satisfac-
tory, he uses it directly for further processing such as
quantitative analysis and visualization. On the other
hand, if he finds errors in segmentation, he indicates
corrections through simple mouse gestures, and the
refinement algorithms refine the segmentation.
A snapshot of the several stages of processing done
on two different frames of two-dimensional ultrasound
heart images is shown in Figure 2. Figure 2-(1) shows
a pair of two dimensional long-axis echocardiograms.
To accurately segment the echocardiogram a local
threshold is computed at the interactively suggested
wall section, and it is adaptively propagated to the
rest of the image, and the rest of the frames of the
sequence, if it is a dynamic scene [5]. This thresh-
old gives a good estimate of the walls for the most
part as shown in the pair of corresponding frames in
Figure 2-(2).
The upper image in the Figure 2-(2) pair shows
a clear contiguous top-right chamber (left ventric-
ular chamber), while showing some fragmented re-
gions inside it. These fragments are quite often ex-
traneous reflections caused by the overlapping pap-
illary muscles, and the endocardium. The expert
points at those fragments using mouse-clicks as shown
in Figure 2-(3), and a rcgion growing algorithm [6]
deletes those regions and displays the refined output
as shown in the upper frame of Figure 2-(4).
On the other hand, if the initial seginentation
yields poor results, as shown in the lower frame of
Figure 2-(2), the expert may want to delineate the
entire chamber. If so, he circles the chamber and
indicates that a contiguous internal contour be ob-
tained. A Constrained Contouring algorithm uses
that circled input as the approximation of the precise
contour, and generates a refined contour that closely
approximates the actual contour [7]. The lower frame
of Figure 2-(3) shows the approximate input contour,
and Figure 2-(4), shows the corresponding final con-
tour.
These image analysis operations are performed
through a user friendly graphics interface that ac-
cepts interactions through natural gestures such as
pointing and circling [8, 9]. Furthermore we incor-
porate a user's visual attention model [10, 11], that
accounts for the spatial extents of the gestured cor-
rections and visual expectations. A snap-shot of the
GUI display is shown in Figure 3.
Posner et al. [12] and Hoffman et al. [13] experimen-
tally demonstrated a discrete bounded region around
the point of attention, called the spot-light, with an
uniform reaction time. The angle subtended by the
spot-light is about 1' at the viewer's eyes [14]. Later
Eriksen [15] and LaBerge [16] showed that the spot-
light has fiexible boundaries and its size is task de-
pendent.
Hughes et al. [17] examined the spatial extent of
the visual attention by inducing subjects to expect
the target at a location and introducing occasional
probe flashes at other locations throughout the visual
field. The main conclusion of their study was that the
directed visual attention was not a spatially limited
phenomenon, but it extended throughout the visual
hemifield. It was independent of both the expected
target locations and the validity of precue. They fur-
ther supported the spot-light theory by showing that
the field of attention had a uniform reaction time.
A psychophysical analysis of the visual attention
span is reported by Mangun et al. [10], who studied
changes in the event related brain potentials (ERPs)
with respect to visual stimuli as the locus of attention
was shifted across the visual fields. They observed a
decline in attention indicated by decline in the ampli-
tude of the sensory evoked components of the ERPs,
at increasing distances away from the attended loca-
tion. Their study supported both the spot-light and
the gradient models of the visual attention maps. Ad-
ditional evidence to support these models is provided
by Shulman et l. [18] and Bashinski [19]. Tsal [20]
showed that the decrease in attention was linear with
respect to the angle subtended at the viewer's eyes.
We have combined the spot-light and gradient
models of the visual attention maps proposed in psy-
chological and psychophysical studies in our super-
vised image segmentation algorithms. In Figure 4,
a schematic plot of the attention response curve
that combines the spot-light and gradient models, is
shown. The curve shows small reaction time at the
point of focus, and is constant for points that are
with a small but discrete distance. This constant fo-
cus area is called the spot-light. For all the points
away from the spot-light, the reaction time increases
linearly with respect to the angle subtended at the
viewing point. A linear approximation of this re-
sponse curve is used as a constraint in the supervised
segmentation refinement algorithms.
From the spot-light region of the visual attention map
we know that there is small region around the pointed
location that characterizes the precise region to be
reclassified. To trace the boundaries of the pointed
region, we collect all the connected points with the
same characteristics. Our region growing algorithm
uses the visual attention map as its distance mea-
sure used for gathering the points in the region. This
distance measures account for image features such as
intensity and edges. Figure 5 shows a segmented two
dimensional echocardiogram on the left side, and the
corresponding refined output on the right side.
If the initial segmentation is erroneous, or if there are
too many isolated misclassified regions, the user can
refine the region by circling. This gesture roughly
approximates the boundaries of the indicated region.
Our goal now is to modify this input so that it traces
the precise boundaries of the intended region. This
problem closely relates to the problem of active con-
fouring proposed by Kass et al.[21].
We determine the precise contour indicated by a
circling gesture by a graph search alggorithm. A Lo-
cally Optimal Contour Locations (LOCL) are com-
puted in all the radial directions according to the
constraints of context(shape), distance from the cir-
cled approximate contour, and the relevant image fea-
tures. The detected LOCLs are represented as the
nodes of a graph of all possible contours. The links of
the graph are weighted so that the shortest path be-
tween a pair of nodes is a smooth contour that passes
through the maximum number of intermediate LO-
CLs, and the shortest cycle in the graph gives the
optimal contour.
Figure 6 outlines the endocardium in a short-axis
two-dimensional echocardiogram. The left image
shows the circled input, and the right image shows
the refined boundary of the intended region.
The multimodal human computer interface module
translates commands and corrections from the super-
visor into operations that are to be applied to the
image. The interface also presents the results from
these operations to the user in a concise and useful
manner. Our current work is intended to allow the
user to verbally specify cardiac structures. The image
analysis system will combine the verbal and gesture
information identify and segment cardiac structures.
Efective human-machine communication, like
communication between two humans, is built upon a
shared domain model [22, 23]. In general, the domain
model defines a set of objects, attributes and relations
that serve to organize domain knowledge [24, 25].
For example, in the domain of kinematics, knowl-
edge is organized around the concepts of force and
mass [26]. These concepts are associated with math-
ematical equations that form the basis of procedures
for calculating the values of unknown quantities. For
our purposes, we will claim that the domain model is
meaningful to the extent that it associates procedures
and strategies with the included objects, attributes
and relations [27].
As illustrated in Figure 7 the interface mediates be-
tween the user and the system, and therefore should
reflect the cognitive processes of the user and the
capabilities of the image analysis system. In other
words, each party involved in the communication is
able to invoke a set of procedures based on exchanged
information phrased in terms of the shared domain
model.
To enhance ease of use by the human supervisor, the
model should allow the supervisor to maintain the
following features of his own domain model:
We have provided a brief description of the proper-
ties of human reasoning with visual stimuli, and sug-
gested examples of these in the domain of echocardio-
graphy. The advice that a human supervisor provides
most naturally to an image analysis system is likely
to reflect these properties. To promote an effective
exchange between the human and machine, we pro-
pose these properties as the basis of a shared domain
model, Thus, the model should define a preferred
level of analysis on echocardiograms, and identify the
cardiac structures required by clinical goals.
Figure 8 contains an example of the model we en-
vision for mediating human-machine communication
in this domain. The information to the right of this
row defines categories of parts. Parts are connected
to category names through ISA links. The primary
purpose of the categorization scheme is to describe
expected changes in the visual appearance of parts
with heart movement. The expected changes with
movement are indicated by the changes-shape link as-
sociated with the category names. A heart wall, for
example changes from thin to thick proportions with
heart beats. These changes-shape relations are innher-
ited by the exemplars within this category, so that
the several heart walls change in similar ways with
heart beats.
Information to the left of the center vertical row
of objects defines their positions and shapes. This
part of the domain model links the qualitative and
symbolic information about the heart to quantitative
properties that can be identified on an image. To
identify the position of objects, we have made use of
polar coordinates, an arbitrary center point and max-
imal radii of 1 and -1 defined by the heart bound-
aries. We have illustrated the position information
for selected cardiac structures in the figure. The left
atrium has a reference point at zq,V2, and occupies
a location from 0 to 360? in both directions at a ra-
dius from 0 to rg.. Shape information defines this
area more precisely, future work will precisely specify
these shape descriptions.
Figure 10 illustrates another set of information as-
sociated with the cardiac structures. It is represented
separately from the information in Figure 8 here only
for the purpose of having a readable figure. The infor-
mation in Figure 10 identifies prototypical patterns
that are generated from standard views. These view-
specific cardiac structures are linked to the general
three dimensional object from which they emerge.
These prototypes constitute the user's ordinary ex-
pectations for echocardiograms. Their appearance is
pre-stored, and they are easily recognized by the pres-
ence of one or two prominent features.
The psychological phenomenon of prototypes is re-
lated to the topological notion of a characteristic
view [31, 32, 33, 34, 35]. The concept of characteristic
views is often used when recognizing a 3-dimensional
object from a single image. While a 3-D object can
project into many different 2-D geometries there are
only a small number of topologies that it can assume.
Each of these topologies corresponds to one or several
convex regions of space from which the mathematics
of projection ggenerates the topology. Each of these
regions of space is a characteristic viewpoint and the
topology of the object corresponds to a characteristic
view. As far as we know, this concept has not been
applied to density images. But we feel that in fact
the small set of 'views'' popularized in the cardiology
textbooks may correspond well to stable characteris-
tic views of the heart.
We have begun research on characteristic views by
studying characteristic views of polytopes. Figure 9
shows charts indicating characteristic views of sev-
eral simple polytopes. We will extend our results on
polytopes to complexes of polytopes and eventually
general three dimensional structures.
We have illustrated three of these prototypical
views: the five chamber view, the short axis view
near the apex, and a long axis view. Each view is
composed of its several 2-D cardiac structures.
In summary, we have defined a candidate domain
model intended to reflect some aspects of a cardi-
ologists interpretation of an echocardiograms, focus-
ing on cardiac structures and their types. We de-
scribed the positions of cardiac structures within a
3-D model. We linked prototypical views of echocar-
diograms with the concept of characteristic views in
topology, as means of singling out expected 2-D views
of the 3-D structure. This model will allow the inter-
face and image analysis system to map user feedback
and advice onto specific parts of the image, and prop-
agate this advice through its interpretation.
We suggest here some functions that can be provided
by image analysis systems using this model as the
basis of human system communication. Analogs of
these functions are already available in the prototype
implementation of our system.
Given an approximate contour of a cardiac struc-
ture in the model and an approximate position and
range of orientations for the contour the image analy-
sis system can find, using a minimization process, an
exact contour, whose shape and orientation is as close
as possible to the contour supplied by the interface.
This function allows the interface to determine exact
outlines for cardiac structures such as valves or walls.
The system also will track the structure through a se-
quence of frames in the echocardiogram, and report
apparent errors in such tracings.
If the user indicates the exact contour determined
by the system is incorrect then the system can ad-
just the contour. The user may correct small por-
tions of the contour or make general comments such
as, ''the contour is too thick,' The interface will
translate these comments into approximate geomet-
ric commands. The system will then try to adjust
the contour, taking into account the modifications
that are suggested by the user and translated by the
interface into geometric descriptions.
To fully take advantage of the psychological model
discussed in the previous section we intend to en-
hance the modalities available to the system for in-
put and output. The enhanced system will accept
input using these devices: speech input device, key-
board,graphics tablet, touch screen, and mouse. The
system will produce output via two output devices:
high-resolution color-graphics display and speech out-
put device.
Figure ll provides an overview of the proposed
user interface design. The primary path that the in-
put data follows is indicated by the solid arrows in
the figure: (1) Input Coordinator, (2) Multi-Media
Parser/Interpreter, (3) Executor/Communicator to
Target System, (4,5) Multi-Media Output Plan-
ner/Coordinator/Generator. The Input Coordina-
tor module will accept input from the several in-
put devices and fuse the input streams into a com-
pound stream, maintaining the temporal order of
data in the original streams. The Multi-Media
Parser/Interpreter will accept the compound stream
produced by the Input Coordinator and produce an
interpretation of this compound stream. Appropriate
action is then taken by the Executor module. The Ex-
ecutor module will be implemented by extensions of
the current system for interpreting echocardiograms.
This action may be a command/request to the image
analysis system or an action that requires participa-
tion of the interface system only. Visualization of
the interpretation and explanation of the visualiza-
tion will be generated by the Multi-Media Output
Planner/Coordinator/Generator.
Our computational methods and knowledge rep-
resentation paradigms will enable a computer pro-
gram to function as an intelligent multimodal user
interface[36, 37, 38, 39, 40, 41, 42, 43, 44] We expect
improvements on the state of the art in image under-
standing can be achieved by accepting a wider range
of inputs including speech, sophisticated gestures and
text and by generating a wider range of outputs in-
cluding speech, a variety of tabular and graphical out-
puts and a variety of imagery[45, 8, 46, 47, 48].
An example of a graphical output we will generate
is shown in figure 12. This output uses color and nu-
merical labels to indicate velocity information about
the heart wall. Using color to indicate velocity or
change in wall thickness can provide a cardiologist
with more information for diagnosis.
We will enable the user interface system to accept
and understand language that refers to either: (1)
parts of the image, (2) the object(s) portrayed by the
image (i.e., the semantics of the image), or (3) spatial
relations between objects. Previous interfaces that
we have developed understand input that referred to
presentation objects such as the icons or windows on
a computer screen as well as the objects represented
by the icons and windows (represented in the sys-
tem's knowledge tase). However, such a capability
for an image analysis system is more complex since
the image is not a collection of predefined icons and
windows.
Little research has been conducted to determine
which media/modalities are preferred or superior to
others or the basis for the preferences and almost no
research has been performed to determine when to
use combined modalities for human-computer com-
munication; The task may indeed be the most im-
portant factor [49, 50, 51]. Other studies emphasize
the compatibility between the stimulus, central pro-
cessingg, and response media [52, 53, 54, 55, 56, 57]
Coordination of media is critical for a multi-
media interface system to effectively support human-
computer communication. Our research on the
CUBRICON project solved the coordination prob-
lem for discrete speech and discrete pointing gestures
[39, 38, 40, 58, 41, 42]. The problem of coordinat-
ing continuous speech input and continuous gestu-
ral/drawing input remains to be addressed.
We will extend and refine the CUBRICON design
for accepting and fusing the input from different de-
vices into a compound stream, maintaining the infor-
mation as to when the different tokens (e.g., words,
graphic gestures) that it receives from the different
devices were actually input by the user. A possi-
ble solution to the problem of coordinating continu-
ous forms of input (e.g., speech, drawing gestures) is
to attach time tags, representing time of actual oc-
currence, to the signals comprising each of the input
streams from the different media devices.
This work is supported by Grant No. 91-050G to the
second author from the New York state affiliate of
the American Heart Association. We are grateful to
Steve Rosenthal, M.D. for serving as a domain expert
for the development of the expert model.
Matching is a ubiquitous problem in computer vi-
sion. Correspondence matching can be broken into
two general areas: model-to-image matching where
correspondences are determined between known 3D
model features and their 2D counterparts in an itm-
4ge, and image-to-image matching where correspond-
ing features in two images of the same scene must be
identified. Fast and reliable matching techniques exist
when good initial guesses of pose or camera motion are
available [Bevei90] or when the distance between views
is small [Anan87]. What is lacking are good meth-
ods for finding matches in monocular images, formed
by perspective projection, and taken from arbitrary
viewpoints.
This paper examines the problem of matching copla-
nar structures consisting of line segments. A simple
method is presented that, when applicable, allows fast
and accurate matching of coplanar structures across
multiple images, and of locating structures that cor-
respond to a model consisting of significant planar
patches, The main point to this paper is that the full
perspective matching problem for coplanar structures
can often be reduced to a simpler four parameter affine
matching problem when the horison line of the planar
structure can be determined. Given the horison line,
it is possible to transform the image to show what it
would have looked like if the camera's line of sight had
been perpendicular to the object plane. This process
is called rectification in aerial photogrammetry.
Essentially all matching problems involve solving for
both a discrete correspondence between two sets of fea-
tures (model-image or image-image) and an associated
transformation that maps one set of features into reg-
istration with the other. These two solutions together
constitute matching: a match being a correspondence
plus a transformation. For planar structures under a
perspective camera model, the relevant set of transfor-
mations is the eight parameter projective transforma-
tion group [Faug88.
More restrictive transformations are worth special at-
tention. Often these transformations are more easily
computed, thus making matching easier. One such
special case occurs for frontal planes, planar structures
viewed 'head-on' with the viewing direction of the
camera held perpendicular to the object plane. When
the intrinsic camera parameters are known, perspec-
tive mapping of a frontal plane to its appearance in
the image can be described with just four affine pa-
rameters: an image rotation angle, a 2D translation
vector, and an image scale [Sawh92].
Under the standard pinhole camera model, the image
projection of a 3D world point (X,Y, 2) is the image
point (K/2,Y/2). In this case, the appearance of any
3D object is governed only by the relative position and
orientation of the camera with respect to the object,
i.e., the camera pose. There are 6 degrees of freedom
for camera pose: three rotation angles of roll, pan and
tilt, and three translations T., T, and T,. If the cam-
era is constrained to point directly perpendicular to
an object plane, yielding a frontal vieu of the plane,
its two pan and tilt angles must stay fixed. This leaves
one free camera rotation about the normal of the ob-
ject plane, and the three unconstrained translations,
four parameters in all The effect of camera roll on the
image plane is an in-plane rotation about the origin.
Translation parallel to the planar surface shows up as
as a 2D translation of the image. Finally, translation
directly towards or away from the object plane mani-
fests itself as a uniform change in scale of the projected
image. These are precisely the effects of the four pa-
rameter affine similarity mapping. The pinhole camera
projection of a frontal plane is therefore described by
four affine parameters that are directly related to the
physical pose of the camera with respect to the plane.
A more realistic camera model must take into account
the intrinsic parameters of the camera lens. To a first
approximation, lens effects are often modeled by a set
of inear parameters including focal length, lens aspect
ratio, optical center, and optical axis skew, whose com-
bined effects can be described by a six dimensional
affine mapping of the pinhole image onto the actual
raster image [Horn86]. A more realistic model of the
projection of a frontal plane is thus a four parameter
affine mapping of the plane onto an idealised pinhole
plane, followed by a six parameter affine mapping onto
the actual measured image.
In summary, the perspective projection of a frontal
plane is described in general by a six parameter affine
transformation. When a calibrated camera is used its
intrinsic lens effects are known, and can be inverted
to recover the ideal pinhole projection image. After
correction for lens effects, the frontal view of a plane
can be described by a four parameter affine similarity
mapping.
For planes viewed at arbitrary orientations, the full
six degrees of pose freedom may manifest themselves
in the image. The two camera rotation angles, pan and
tilt, not used for frontal images, are directly related to
the tilt of the object plane with respect to the camera's
line of sight. Under perspective projection, lines that
are parallel on a tilted object plane appear to converge
in the image plane, intersecting at a vanishing point.
The locus of vanishing points of coplanar parallel lines
of all orientations forms a line in the image called the
vanishing line or korizon line of the plane.
For frontal planes, all parallel lines on the object plane
remain parallel in the image. By convention a set of
parallel llines in the image is said to intersect in a point
''at infinity.' When all vanishing points appear at in-
finity, the vanishing line passing through them is also
said to be at infinity. Since a transformation is affine
if and only if all parallel llines of arbitrary orientation
remain parallel, it follows that the defining feature of
a frontal view of a coplanar structure is that the van-
ishing line of that structure appears at infinity.
Conversely, by applying a projective mapping taking
the vanishing line of an image of a coplanar structure
to the line at infinity, the vanishing points of all lines
in the plane will now be at infinity, hence all parallel
lines in the planar structure must now be parallel in
the image. This implies that the new image is a frontal
view of the original set of coplanar lines.
We have seen that the vanishing line of a frontal plane
appears at infinity in the image plane, and further-
more, that it is possible to recover a frontal view from
the image of a tilted object plane by applying a pro-
jective transformation that maps the object's vanish-
ing line to infinity. There is, however, a whole si-
dimensonal space of projective transformations that
all map a given line in the image off to infinity. Bow
to choose a 'best' one is described in this section.
For a pinhole camera image, the position and orienta-
tion of the vanishing line of an object plane determines
the true 3D orientation of the plane with respect to the
camera's line of sight. When the equation of the van-
ishing line is as + by + c = 0, the normal to the object
plane, in camera coordinates, is
For a frontal plane, the normal of the plane must be
parallel to the Z camera axis, since the plane is per-
pendicular to the line of sight. If the camera could
move, the image of a frontal plane could be recovered
from the image of a tilted plane by merely rotating
the camera to point directly towards the plane. The
camera can no longer be moved physically, of course,
but the image can be transformed artifically to achieve
the desired 3D rotation.
Assume the unit orientation of the object plane has
been determined to be n, as in equation l, oriented
into the image (c 2 0). To bring this vector into coin-
cidence with the positive 2Z axis requires a rotation of
angle CosPH(n - (0,0,1)) about the axis n x (0,0,1).
The effects of this camera rotation on the image can
be simulated by an invertible projective transforma-
tion in the image plane [Kana88]. In homogeneous
coordinates,
where
The image is transformed to appear as it would have
if the camera had been pointing directly towards the
object plane. The result therefore is a frontal view of
the object plane, as seen by a pinhole camera, ie. a
rectified four parameter affine view.
This mapping cn be used to map a vanishing line to
infinity even when the intrinsic calibration parameters
are not known. However, when the original image is
not a pure pinhole image, the position of the vanish-
ing line in the image can no longer be interpreted ge-
ometrically in terms of 3D plane orientation, and the
resulting unwarped image will be in general some six
parameter affine view of the object plane.
Plane rectification forms the essence of our approach
to matching perspective images of coplanar structures,
The idea is to find the vanishing line of an object plane
in the image by any means possible, then apply a pro-
jective transformation that maps it to the line at in-
finity, thereby producing an affine frontal view of the
original object plane. The effect is to reduce a per-
spective matching problem to a simpler affine match-
ing problem.
To search for the optimal affine map and correspon-
dence between two sets of line segments, an efficient
and robust local search algorithm is used [Beve90].
The local search matching algorithm searches the
discrete space of correspondence mappings between
model and image features for one that minimises a
match error function. The match error depends upon
the relative placement implied by the correspondence.
More particularly, to compute the match error the
model is placed in the scene so that the appearance
of model features is most similar to the appearance of
corresponding image features. The more similar the
appearance the lower the match error.
To find the optimal match, probabilistic local search
relies upon a combination of iterative improvement
and random sampling. Iterative improvement refers
to a repeated generate-and-test procedure by which
the algorithm moves from an initial match to one that
is locally optimal. This is done by repeatedly testing
a local neighborhood of matches defined with respect
to the current match. Each neighbor is a distinct cor-
respondence mapping between model and image fea-
tures. Tractable neighborhood sises, for instance n
neighbors in a space of 2? possible matches, tend to
yield tractable algorithms. Bowever, there is an art
to designing small neighborhoods that do not induce
a profusion of local optima. New neighborhoods defi-
nitions have been developed that are particularly well
suited to shape-matching [Beve90].
Despite clever neighborhood definitions, local search
can become stuck on local optima. Random sampling
offers a probabilistic solution to this problem. The
probability of finding the globally optimal match start-
ing from a randomly chosen initial match is analogous
to the probability of getting heads when ffipping an
unfair coin. Even with an unfair coin it is a good bet
that heads will appear at least once in a large number
of throws. For instance, using a coin that only comes
up heads in 1 out of 10 throws, the odds of getting
heads 1 or more times in 50 throws are 99 out of 100.
Similarly for local search matching, even if the proba-
bility of seeing the optimal match on a single trial is
low, the probability of seeing the optimal match in a
large number of trials is high.
The combination of iterative refinement and random
sampling has proven to be very effective. This ba-
sic form of algorithm reliably finds excellent, and
usually globally optimal, matches under difficult cir-
cumstances. The algorithm performs well even when
scenes are highly cluttered and significant portions of
a model instance are fragmented or missing entirely.
Although other methods are available (see discussion
in Section 5), the results in this paper rely exclusively
on vanishing point analysis for finding vanishing lines
in the image. This simple approach works surpris-
ingly well for many man-made scenes, both indoor,
outdoor, and aerial. Vanishing points are found using
a standard Hough transform approach [Barn83]. Each
line in the image is entered into a two dimensional
Hough array representing the surface of a unit hemi-
sphere. Each image line 'votes' in a great (semi)circle
of accumulators, and potential vanishing points are de-
tected as peaks in the array where several great circles
intersect. For most man-made scenes, either two or
three cluusters will dominate the Eough array; clusters
corresponding to the vanishing points of the two or
three dominant line directions in the scene. Each pair
of vanishing points defines a vanishing line for planes
consistent with those line orientations.
At present, only a four parameter affine version of
the local search matching system is implemented. We
therefore needed to know the calibration parameters of
the camera for each experiment. It should be stressed
that only rough knowledge of the calibration param-
eters is generally needed to find acceptable matches,
The most important parameters to determine are fo-
cal length and aspect ratio. We assumed for all our
exrperiments that the image center was at the center
of the image, and the optical KT and axes were per-
pendicular (no skew) and aligned with the row and
column axes of the raster image. Aspect ratio was
determined from the camera manufacturer's specifica-
tions, when available, otherwise it was assumed to be
one-to-one (square). The focal length for each experi-
ment was computed as a byproduct of vanishing point
analysis and apriori knowledge that the actual angle
made by the two dominant line directions is 90 degrees
[Capr90]. This focal length was chosen by finding the
distance of the focal point from the image that re-
sulted in perpendicularity of the two vectors from the
(variable) focal point towards the two (fixed) vanishing
points in the image.
Figures la) and b) show a set of straight line segments
extracted from an image of a wall poster using the
Burns algorithm [Burn86], and a set of model lines to
be matched to the image. The first stage in match-
ing is to detect two clusters of lines converging to the
two main vanishing points in the image, and from the
resulting vanishing line rectify the image to present a
frontal view of the poster (Figure lc).
The four parameter affine match found by the lo-
cal search matching algorithm yielded a set of cor-
respondences between model lines and image lines.
These correspondences were used to estimate an eight
parameter planar projective transformation to bring
the model lines into registration with the image data
lines, using the least-squares estimation procedure of
[Faug88]. Figure ld shows the transformed model
overlaid on the input image lines.
Because it does not rely on computing 3D object pose,
The current formalism extends easily to image to im-
age correspondence matching. In this case, both im-
ages are rectified using the techniques of the last sec-
tion, and one is treated as the model while the other
becomes the data to be matched. The goal is to dis-
cover a transformation that maps one set of rectified
image lines into another.
When both cameras are calibrated, both images can be
rectified into frontal views of the object plane. Since
the mapping from one image to another can be written
by inverting one transformation and composing it with
the other, and since the four parameter affine group is
closed under inversion and composition, the resulting
image to image transformation can be described by
a four parameter affine similarity map. As may be
expected, when either camera is uncalibrated, the re-
sulting transformation between unwarped views is a
general six parameter affine mapping.
Figure 2shows an example ofimage to image matching
in the context of aerial image registration. Figures 2a)
and b) show sets of extracted straight line segments
from two aerial photographs. In the first image, the
camera is nearly perpendicular to the ground plane,
a fact verified by vanishing point analysis, which finds
two orthogonal sets of nearly parallel lines. The second
image is clearly not a frontal view.? Figure 2c shows
the image after rectification based on vanishing point
analysis.
To apply the local search matching algorithm, image l
was assumed to be the model and the unwarped lines
from image 2 the data. Both line sets were filtered
to only inclde lines greater than 100 pixels long, re-
ducing the matching problem to 55 long lines in one
image and 68 lines in the other. The best match found
is displayed in Figure 2d.
The domains we anticipate are scenes depicting either
indoor or urban outdoor environments with much pla-
nar and parallel linear structure. Such scenes often
contain lines and planes in two or three dominant
directions. The approach to matching taken in this
paper requires each plane to be matched separately,
therefore there needs to be some way to partition lines
in the image into sets belonging to planes in the world.
This would be nearly impossible in monocular images,
were it not for the rich structure of man-made envi-
ronments, uggesting domain-specific heuristics based
on corners and perpendicularity. In particular, L-
junctions made of two lines from different vanishing
point clusters are good candidates for coplanar cor-
ners. We are currently exploring heuristic geometric
methods, as well as more formal approaches based on
projective invariance, for partitioning image lines into
coplanar groups.
We are also exploring other methods besides vanish-
ing point analysis for detecting the horison line of an
object plane's image projection. Some possibilities are
analysis of texture gradients [Gard9l], and exploita-
tion of the perspective properties of convex planar
curves [Arns89].
When structures are present in the scene that devi-
ate significantly from coplanarity with respect to the
viewing distance, then their correspondences may not
be adequately found using the above techniques. How-
ever, to the extent that aome scene features are copla-
nar and are found, this initial set of planar corre-
spondences provide strong constraints on the remain-
ing features, particularly when the cameras are cali-
brated, in which case the relative rotation and direc-
tion of translation between the two camera positions
can be computed from the planar perspective trans-
formation [Faug88]. This reduces the problem to that
of induced stereo, where point correspondences must
lie along known epipolar lines. Even for uncalibrated
camera systems, knowledge of the perspective trans-
formation relating the image features of a single plane
in the scene constrains the positions of point features
in one image to lie along epipolar ines in the other
image.
In its current form, the local search affine matcher de-
scribed in this paper is used for image to image feature
matching simply by declaring the features in one image
to be a model This is not ideal, since the the current
treatment of model and image lines is not symmet-
ric. Future work on the affine matcher may include
developing a more symmetric error metric for image
to image matching, and extending the range of the
match transformation space to handle si parameter
affine matching so that images from uncalibrated cam-
era systems can be used.
According to sociologist Julien Freund, conflicts
among groups of people arise because they become
hostile over maintaining, reaffirming or restoring
their rights [3]. From the point of view of one of the
groups, it is necessary to break the resistance of the
members of the other groups. The Latin word hostis
and the Greek word polemios describe this type of
rival or enemy, rather than inimicus (Latin) or ejzrdz
(Greek), which characterize a private opponent [8].
Social conflicts may arise because different groups
hold opposed economic, political, national or reli-
gious views. For example, an economic conflict occurs
when the organized workers of a certain sector ask
for salary increases the organized managers consider
excessive. An example of a political/international
conflict may arise because two Governments claim
sovereignity over a certain geographical region for
their respective countries. In both cases, there may
be mediators (e.g., the Ministry of Labor in the for-
mer and the United Nations in the latter).
A conflict of this kind may be settled by force. Al-
ternatively, there may be negotiation, that is, each
party agrees to talk in order to accept part of the
others' point of view if its own claims are partly ac-
cepted as well.
A typical negotiation setup is composed of two or
more parties (eventually one of them may be a medi-
ator), each represented by a group head or negotiator
and several assistants or advisers, The speaker of
each group is the negotiator and the assistants pro-
vide him with the necessary specialization in various
fields. Thus, for an international peace talk, for in-
stance, assistants may be experts in subjects such
as international law, geography, military affairs, na-
tional politics, etc. Typically, assistants are seated
behind their negotiator.
During the negotiation,an individual assistant may
approach his negotiator and deliver a short verbal
comment or quickly pass him a written note. In many
cases, that is all the information the negotiator gets
from his group during the talk. A negotiator may
request a break or a new meeting in order to have
enough time to discuss new proposals with his advis-
ers. Therefore, the negotiator-assistants interaction
during a meeting is poor, and the negotiator may get
worried of saying too much because he does not have
enough input from the experts of his side. As a conse-
quence, interruptions slow down the negotiation. On
the other hand, if a negotiator does indeed give away
what it is more than convenient to the other party
due to lack of counsel, later on he may be unwilling
to give any other concession and this may result in
stalling progress in other aspects of the negotiation.
An improvement over the previous situation is to
furnish a voice communication system linking nego-
tiators and their assistants. This system would com-
plement the traditional voice communication system
designed to listen the current speaker, in more than
one language if translators are provided. Bowever,
the interaction can be much richer if a multimedia
system is available: the negotiator can continue lis-
tening to the current speaker while asking counsel
with a few hand movements over a computer mouse,
or quickly see the first opinions of his experts on a
recent proposal presented by the opposing party.
An additional shortcoming of the traditional ap-
proach to negotiation is that the advisers do not en-
joy privacy or isolation to exchange views during the
meeting. As a result, the input they can provide to
the negotiator is only based on personal opinions.
These considerations led to the design of SHINE
(Shared Interactive Negotiation Environment), a
collaborative software system currently under con-
struction at the University of Chile.
Collaborative systems have been applied to brain-
storming [10], document writing [2, 7], drawing [l],
spreadsheet use [4], folder circulation [5] and software
development [5]. Among their advantages over stand-
alone systems, include freer participation, increased
productivity, and in the case of redmote systems, no
burden of personal transportation to a meeting site.
Our proposal calls for a redesign of the physical ne-
gotiating environment (Fig. 1). A separate room is
provided for each group of assistants, This room has a
window to the negotiating room and a video monitor
to receive the audio and close-ups from the speaking
negotiators, Each assistant is provided with a work-
station and a video camera focusing on him.
The negotiators remain on the main floor, each
with a workstation in front. Workstation screens
are only visible by their corresponding negotiators.
There is a large electronic board, controlled from a
terminal by a technician.
A negotiator may speak as usual, but he may also
send short text messages to designated negotiators
by typing on his workstation and send text, pictures
or tables to the technician to be displayed on the
electronic board. Ee may also have a contents-rich
communication with his assistants.
Up to seven advisers engage in a permanent face-
to-face meeting in their room and they communi-
cate through their workstations among themselves
and with their negotiator. They can have private
or shared views over text documents and send tables
or pictures to each other. Inspiration for the idea of
the assistants' reduced participation in the meeting
was taken from an early experience in group dialog
reported by Sheridan [9].
The advisers may send the negotiator written pro-
posals, short telegrams of high priority and evalua-
tions of various kinds previously asked by the nego-
tiator. They may also send a video message consisting
of a recording of one of the advisers speaking. The
latter type of message has the advantage of letting
the negotiator watch the facial emphasis the assis-
tant wants to provide. Eowever, it has the disadvan-
tage the negotiator has to switch audio channels and
perhaps miss another negotiator's speech.
The human-computer interface of SEINE was care-
fully designed considering that first, the users are
supposed to be computer novices, and second, the ne-
gotiator can not afford much time to interact with his
workstation. Thus, the user interface is icon-based.
The negotiator communicates with two basic
paradigms displayed on his screen: a push-button
panel and a bulletin board. The panel is very simple
to manipulate: the user chooses an icon, presses a
mouse button and the action occurs, For example,
if an icon signals that mail messages have arrived,
choosing it causes a window to be opened showing
the contents of the messages. The bulletin board is
of simple operation as well and it is explained in the
next section.
Display of results is also visual. For example, eval-
uation results are shown using faces, with obvious
meanings: smiling, annoyed, etc.
Figure 2 shows the negotiator's workstation screen.
The control panel is a private window shown on the
left hand side. In its upper part, there are icons
depicting the assistants; the first one represents the
whole group of advisers. These icons are useful to
selectively ask the services explained below. An arbi-
trary number of advisers can be chosen for a service
by pointing with the cursor over each icon in turn
and depressing the first mouse button (selection is
indicated by an icon shade change). A second button
depression deselects the corresponding icon. People
icons are also useful to show their current state and
keep the negotiator informed. In Fig. 2, Adriana is
out of the room while Alejandra is in the assistants'
room but she is busy with another task.
The bulletin board is a non-strict WYSIWIS win-
dow [10] shared with the advisers. There are pinned
doum items on this board which can be opened by
the negotiator or any adviser. If one person opens
an item by selecting its pin, another window pops
up only on his screen showing the contents of the
item; selecting that pin again closes the window and
a 'closed'' item icon appears on the board. Figure 2
shows three closed items: an evaluation, a reguest and
an agreement, and one open request. The closed re-
quest is ready while the advisers are still working on
the closed evaluation.
The open request of Fig. 2 illustrates the type of
services the negotiator can ask from the advisers.
When the negotiator asked for it, he pressed the re-
quest button on the control panel. A dialog box then
opened (shown on Figure 3) and he wrote the note
'How about increases..,''; then he chose two advisers
(Edgardo and Eduardo) and selected the Send button
on the dialog box. Thereafter, the box disappeared.
Meanwhile, a shared window appeared at the chosen
advisers' workstations. Perhaps they talked about
the subject and each wrote a note; when they were
finished, one of them pressed a send button. Since
it was a shared service, the system asked for confir-
mation, and once received, the shared window was
gone and a 'ready'' signal appeared in the respective
closed item of all bulletin boards of the negotiating
group.
An evaluation is a service the negotiator may ask
the advisers to do. The assistants are asked to vote
on one or more texts. The vote format is very sim-
ple: only five choices are available and range from
ezcellent to unacceptable . An adviser simply selects
a 'face'' representing his choice and then selects the
urn at the side of the text being evaluated. The eval-
uation can be formatted by the negotiator hitmself
or he may ask one of the advisers to prepare every-
thing. The negotiator decides this after he chooses
the evaluation button from the control panel: the di-
alog box that pops up has buttons in its upper part
to send the prepared poll to the selected advisers or
send an unprepared poll to an adviser for its com-
pletion. The dialog box in this case has a vertical
line separating two columns: the left one to contain
text and the right one to place urns. Urns are placed
in front of the text to be evaluated, simply by de-
pressing the mouse button when the cursor is in the
right place: the system automatically inserts an urn;
in fact, in order to further help the user, the cursor
changes shape to a shaded urn when the cursor is in
the right column.
lImportant features of communication in the context
of negotiation are the specification of task urgency
by the sender and the perception of this specification
by the recipient of the corresponding message. In re-
quests and evaluations, the negotiator specifies one
out of three urgency levels when completing the dia-
log box. The three levels of urgency have pre-assigned
suggested completion times, and when the task re-
lated window appears at the advisers' workstations,
a sand clock in the upper part of the window depicts
the remaining time to complete the task. Therefore,
it is easy for the negotiator to choose an icon illus-
trating the very urgent concept to ask the assistants
maximum priority to get a task done quickly, as it is
the case of the open request in Fig. 2 which was a
very urgent task with a expected reply time of three
minutes.
Private communication is done with mail and tele-
gram messages. The difference between them is
again, the urgency the sender assigns to them. These
messages can be sent from the negotiator to the ad-
visers and viceversa and among negotiators. In the
case of negotiators, separate icon buttons are pro-
vided for mail and telegram messages from/to ne-
gotiators and advisers, in order to avoid confusions
(Figure 2). These four buttons are reduced to two -
team mail and telegram - in the case of advisers, since
they are not permitted to communicate with other ne-
gotiating teams, The operation of mail and telegram
messages is similar; they will be referred as messages
in what follows. The receiver selection and message
sending has a similar procedure to the one needed for
sending a request.
Ifa negotiator chooses to send a message to another
negotiator, SBINE automatically opens another win-
dow with the pictures of the negotiators and selection
is enabled. This window is also automatically closed
when the operation is finished.
Reception of a message is signaled by a mark at
the corresponding icon in the control panel. Alterna-
tively, each workstation can be configured using the
Definitions button of the telegram or mail window
to display messages as soon as they arrive. Imme-
diate display of telegram messages is recommended
in the case of the advisers' screens in order not to
miss any urgent message from the negotiator. Defini-
tions buttons are provided in all other types of SEINE
windows to accommodate various preferences or sit-
uations.
To read or write messages, a user simply chooses
the mail or telegram icon button on the control panel.
A window is open displaying the first unread message.
Net and Previous buttons on the window provide
the controls to read other messages. Another button
Iabelled Cancel lets the user close the window. Three
other buttons- WYite, Edit and Send - require further
explanation.
The Write option of the message window clears the
text part of the window and enables the user to type
a message. Edit provides several services to easily
prepare the message. Edit has a menu with options
to Paste a text from a buffer, which has previously
been Copyed from another text, Include the contents
of a file and Read fom Clipboard. The Clipboard is a
private window, and the text transferred to the mes-
sage window can be all the contents of the clipboard
or only a part of it, as chosen with the Definitions
button of the Clipboard window.
Other options of the Edit menu enable the user
to Delete a part of the text been written. Undo, of
course, reverses the last action. Other Edit options
are useful also when reading messages: WYite to clip-
board, Save to file, Copy to bufer. The Edit envi-
ronment is also provided in the Evaluation, Request,
Agreement and Share windows (the last two are de-
scribed below). Edit services are also provided with
the function keys on the left keypad of the keyboard
in the OpenWindows implementation for compatibil-
ity with other applications running on this environ-
ment.
Once a message is prepared, it can be sent using
the Send button. Again, the activity can be cus-
tomized with the Definitions button. For instance,
it can be specified that the system asks confirmation
before sending a message.
An adviser message may also be a video recording.
His window has an additional button called video;
when chosen, a familiar VCR control panel is drawn
on the window: record, pl4y, patuse, reuuind and ad-
ance forwvard have the obvious meanings. The ne-
gotiator gets the control panel and a time duration
indication when reading the message (the record but-
ton is disabled); selecting the play button creates a
window where the video is shown.
SHINE also provides a tool to support face-to-face
discussion. The Share button in the control panel
opens a shared window for people in the same room:
negotiators or advisers. After choosing the Share but-
ton, the user chooses the people with whom the shar-
ingis to occur (itmay be all). The shared information
is saved when the person who initiated the window
selects the Unshare button in the window. Previous
Share windows can be viewed again, saved in private
files, and deleted using the Read button of the win-
dow.
Finally, selecting the Electronic Board button of
the control panel opens a menu offering the Magni-
fier and Marher choices. The marker activates a tele-
pointer on the electronic board, with the negotiator's
name; its movement is controlled by the negotiator's
mouse and it is cancelled off by pressing the mouse
select button. The magnifier option opens a window
on the negotiator's screen showing an outline of the
electronic board; the cursor shape is now a magni-
fier. The mouse select button, in this case, opens an-
other window showing a part of the electronic board
pointed by the magnifier cursor. This latter window
has normal font size letters and vertical and horizon-
tal scrolling bars; it has edit and cancel buttons with
the usual meaning.
Figure 2 shows another type of item pinned down on
the bulletin board: an agreement. Thia is a presenta-
tion made by the assistants to the negotiator. It may
be a suggestion or a general proposal made by all
the assistants. To write it, one assistant chooses the
Agreement button from his control panel and writes
it similarly to a message.
The negotiator also has a mechanism to make a
general remark to the advisers: it is the Note which
gets also pinned down on the bulletin board. To write
it, the negotiator must select the typewriter icon on
the top of the bulletin board and gets a window re-
sembling a message window.
Comments can be placed by the advisers on any
item of the bulletin board. They simply select the
item on the bulletin board and then select the Com-
ment button. At this point, they are allowed to write
a comment which is afterwards concatenated with
other previous comments. These comments can be
viewed by choosing the Comment icon in the respec-
tive bulletin board item window.
Face votes can also be placed over requests, notes
and agreements. An adviser selects one of the faces
shown on the lower part of the control panel and se-
lects the item window. The system will place the
person's name under the face on the control zone of
the window.
Any comment or vote appearing on bulletin board
items are signaled in their closed (pinned down) for-
mat. For example, in Fig. 2 the closed Request has
votes and comments.
As it was mentioned, Share sessions can be reviewed
afterwards, Read messages are not automatically
deleted by the system and thus they can be reviewed
as well,
The log feature in the control panel displays - when
selected - a llisting of all SEINE interactions of the
negotiating team including the time of occurrence.
A prototype of the system is under development. The
interface has been written in Guide 3.0 [11] using Sun
Sparc workstations running X Window X11R5 and
OpenWindows 3.0. Implementation is being done
using a client-server architecture: there is just one
server and all workstations are clients; therefore, par-
tition of the workstations into negotiating parties is
internally managed by the server.
The objective of this prototype is to show the capa-
bilities of the system and let some people involved in
real negotiations try it. This prototype has no built-
in tight security measures which a system applicable
to real negotiations should have. Also the approach
taken to issues such as performance, reliability and
data management is simple and not appropriate for
a final software product.
The design of a collaborative multimedia system to
assist large-scale negotiation has been described. Its
implementation is under way and therefore has not
been tested yet.
Simplicity of use is one of the main design objec-
tives and perhaps many negotiators may get used to
the system. In the worst case, at the beginning, one
assistant may sit beside the negotiator and handle
the workstation, showing him all interesting input
the other assistants may provide him. In the long
run, even the most technology-opposed negotiators
may feel attracted to ask for services from his collab-
orators through the workstation.
It is not clear the usefulness of the video-voice mes-
sages. It may become valuable in some cases and not
in others, Other tools may be ignored by a negotiat-
ing team: they may not use agreements and notes at
all, and decide to use telegrams only for very impor-
tant messages, for example. In this same ffexibility
of tools vein, the Definitions feature makes possible
adaption to different preferences and cases.
Alejandra G&lvez helped in the production of draw-
ings used in this paper.
The focus of the UMass mobile robot navigation
project is robust landmark-based navigation, with
a focus on automated model acquisition and model
extension. Thus, for navigation in unmodelled or
sparsely modelled environments, our general
scenario would involve the initial acquisition of
prominent visual features that can serve as
landmarks. This initial phase of partial model
acquisition is necessary because there are few
situations where a model of a complex outdoor
scene will be available a priori, Once a sparse
model is available, then the vehicle position and
orientation (i.e. pose) can be recovered by
recognizing landmarks. The model extension
phase involves tracking new unmodelled features
(points andfor lines), and using the landmarks and
partial model to determine the camera pose for
triangulation of the new features and incorporation
into the 3D model.
Most of the algorithms have been described in
previous IUW proceedings and the general vision
literature [Beveridge 92, Kumar 92, Sawhney 92,
93]. These algorithms have been shown to be very
accurate in many indoor experiments using a
camera mounted on a mobile robot and on a
moving robot arm. One new experiment that
integrated several components involved the
detection of shallow structures - an aggregatation
of line features that can be approximated in an
image sequence as a frontal planar surface. The
3D position of these features served as the
acquired model, with a depth error of less than 4%.
As motion of the camera continues, the model is
extended with depth information on other tracked
points to accuracies of less than 2% error in depth.
The UMass Mobile Pcrception Laboratory (MPL)
is based on a significantly modified HMMWV.
The design of the overall system includes actuators
and encoders for the throttle, steering column and
brakes that closely match those being used by
CMU, controlled by 68020s in a 6u VME cage.
The low-level control software for controlling
speed and steering angle will also be the same as
that of CMU. The modifications and component
installation is being performed by RedZone, Inc., a
Pittsburgh-based firm specializing in custom
robotics, and was completed at thc beginning of
February 1993.
Electrical power is supplied by a 10kW diesel
generator, whose output is split into two 5kW
circuits. The first circuit is conditioned and
backed by a 5kW uninterruptible power supply
(UPS) system, and is used to supply power to all
sensitive electronic equipment. The sccond circuit
The system also contains a video image frame
buffer and STAGET control subsystem. This
system maintains image and pose temporal
histories (time-stamped images and corresponding
pose estimates) in a fixed-length first-in last-out
queue. This information is available to the
remainder of the system. The STAGET control
interface permits the STAGET to be repositioned
relative to the vehicle and maintains information
about the various STAGET parameters and
conditions, including information about the current
lens aperture and focal length.
All the landmark matching and pose refinement
algorithms have been tested extensively, although
to a great extent only in indoor domains. A large
portion of the original LISP has been ported to C.
The plan for the coming year of research is to
develop the following behaviors: road following,
obstacle avoidance, landmark detection, landmark
tracking, and model extension.
Initially, two types of landmark processing
behavior will be specified. The first behavior for
landmark tracking assumes that a landmark (or set
of landmarks) are currently being tracked via the
STAGET and all that is necessary is that the
vehicle pose be recomputed from the tracked
landmarks. However, there are computational
tradeoffs as a function of the speed of the vehicle,
and the distance and number of landmarks. Thus,
not all landmarks may be tracked frame by frame.
The second landmark navigation behavior assumes
that no landmarks are currently being tracked and
therefore a new landmark must be acquired. This
will involve access to a stored 3D model of the
campus environment (which initially has been
constrnucted a priori) in order to control the Staget
and window on subimages via the Staget.
However, the availability and density of landmarks
will vary significantly in different areas of the test
environment, and therefore model extension will
be a necessary goal. Ultimately we seek to
demonstrate that an accurate 3D model of the
environment can be acquired via exploration in a
purely bottom-up manner, while carrying out
independent goal-oriented navigation tasks.
If tthe world changes or the robot fails to recognize
a landmark, the robot's perception of the world
will not correspond to its current map of the world.
However, there is ambiguity in whether the errors
are in its perception or its map, and if the latter, it
must update its map.
Pinette [Pinette 91] has been developing a
principled approach to automatic map construction
and maintenance. In place of the usual
construction of a geometric map, snapshots of the
world at selected target locations along the route
are stored as the robot's knowledge of that path.
By noting places where a set of memorized routes
intersect, a topological ''road map''of routes and
junctions are represented. To retrace a stored
route, a qualitative homing algorithm based on
purely local visual servoing is employed to home
between successive target locations along the
route. This homing algorithm uses no geometric
model or positional information; rather, it servos
directly on the stored image for a target location,
choosing headings that reduce the difference
between features of the current bearings and those
in the target snapshot. A ''consistency-filtering''
algorithm has been developed for handling
incorrectly matched landmark features [Pinette
92]. It is shown that this algorithm guarantees
reliable homing as long as more than two-thirds of
the landmarks are correctly identified.
A very robust implementation of a robot
navigation system has becn developed using
image-based homing with a spherical mirror for
encoding a 360 degree view at each target
location. This navigation system has been
implemented as part of an indoor manufacturing
automation application domain. It is not yet clear
whether these techniques are dircctly applicable to
unconstrained outdoor domains and large-scale
space.
In robot navigation a modcl of the environment
needs to be reconstructed for various applications,
including path planning, obstacle avoidance and
determining where the robot is located.
Traditionally, the model was acquired using two
images (two-frame Structure from Motion) but the
acquired models were unreliable and inaccurate.
Generally, research has shifted to using several
frames (multi-frame Structure from Motion)
instead of just two frames. However, almost none
of the reported multi-frame algorithms have
produced accurate and stable reconstructions for
general robot motion. The main reason seems to
be that the primary source of error in the
reconstruction - the error in the underlying motion
- has been mostly ignored. Intuitively, if a
reconstruction of the scene is made up of points,
this motion error affccts each reconstructed point
in a systematic way. For example, if the
translation of the robot is erroneous in a certain
direction, all the reconstructed points would be
shifted along the same direction.
Recently, Thomas [Thomas 93a,b] has
mathematically isolated the effect of the motion
error (as correlations in the structure error) and has
shown theoretically that including these
correlations in the computation can dramatically
improve existing multi-frame Structure from
Motion techniques. In several experiments on our
indoor robot, the environmental depths of points
from 15 to 50 feet away from the camera (and for
which ground truth data was available) were
reconstructed with errors in the 1-3% range. In
one further experiment, the multi-frame full-
correlation algorithm was first used to create a
model (a set of points) of an indoor hallway from
several initial frames of image data. 'This model
was then used to compute the pose of the robot
over subsequent frames using Kumar's pose
recovery algorithm. The estimated robot pose and
actual robot position in the hallway differed by a
maximum of three to four inches over a 12.8 foot
path.
Deformations due to relative motion between an
observer and an object may be used to infer 3-D
structure. Up to first order these deformations can
be written in terms of an affine transform. The
recovery of an affine approximation to image
deformation has recently been the focus of a large
amount of research, and has found application in
such disparate areas of computer vision as image
stabilization, optical flow computation and
segmentation, structure from motion, stereo, and
texture, and obstacle avoidance.
Manmatha [Manmatha 93] has developed a
technique for measuring the affine transform
locally between two image patches using weighted
moments of brightness. Unlike previous methods,
this technique correctly handles the problem of
finding the correspondence between deformed
image patches, as is necessary for a correct
computation of the affine transform. It is capable
of determining affine transforms of arbbitrary size,
whereas most previous approaches are limited to
small transforms. It is first shown that the
moments of image patches are related through
functions of affine transforms. Finding the
weighted moments is equivalent (for the purposes
of measuring the affine transform) to filtering the
images with gaussians and derivatives of
gaussians. In the special case where the affine
transform can be written as a scale change and an
in-plane rotation, the zeroth and first moment
equations are solved for the scale. In experiments
on synthetic and real images for this case, the scalc
was recovered robustly and shown to give reliable
depth estimates. Work is continuing on extending
the basic techniques to the general case.
Grupen and Weiss [Grupen 93] have continued
their work on a multi-sensor approach to dextrous
manipulation. The goal of this project is the
integration of sensing and control for the task of
finding a stable grasp configuration for an
unknown object. A subgoal is the integration of
visual and haptic (proprioceptive) sensory data to
incrementally build a model of the object. This
approach uses knowledge of the task and the
accuracy and completeness of the model to control
the sensing actions.
The system consists of a camera mounted on one
robot and the Utah/MIT hand mounted on another.
The system calibration or identification problem
involves computing the transformation from the
coordinate system defined by the manipulator
robot to the coordinate system defined by the
camera robot. The pose determination algorithm
of Kumar and Hanson [Kumar 92] has been
adapted for this purpose. As the manipulator robot
moves, known feature points are tracked. Given
the kinematics of this robot, the pose of the camera
with respect to the coordinate frame of the
manipulator robot are computed and incrementally
refined using iterative, extended Kalman filtering.
Experiments were performed to demonstrate that
the accuracy of the filtering algorithm was
comparable to that of smoothing using a least
squares fit with all of the data, yet the computation
time was much less. An additional feature of the
method is that the kinematics of the camera robot
can be computed at the same time.
Grupen and Huber [Huber 92] have obtained 3D
surface points from the Utah/MIT hand without
the use of tactile sensors. 'The measurements used
are posture, velocities, and torques. This will be
integrated with the measurements obtained from
the camera sensor.
Recovering the shape of an object from two views
(e.g, stereo) fails at occluding contours of smooth
objects because the extremal contours are view
dependent. For three or more views, shape
recovery is possible, and several algorithms have
recently been developed for this purpose. Szeliski
and Weiss [Szeliski 93] have developed a new
approach to the multiframe shape recovery
problem which does not depend on differential
measurements in the image, which may be noise
sensitive. Instead, a linear smoother is used to
optimally combine all of the measurements
available at the contours (and other edges) that are
tracked through the set of images. This allows the
extraction of a robust and dense estimate of
surface shape and the integration of shape
information from both surface markings and
occluding contours. The results provide an
extremely promising path for recovery of 3D
shape models in an industrial setting where the
motion is known.
MOst knowledge-directed vision systems are
tailored to recognize a fixed set of objects within a
known context. Generally, the programmer or
knowledge engineer who constructs them begins
with an intuitive notion of how each object might
be recognized, a notion which is refined by trial-
and-error. Unfortunately, human engineering is
not cost-effective for many real-world
applications. Moreover, there is no way to ensure
the validity of hand-crafted systems. Worst of all,
when the domain is changed, the systems often
have to be rebuilt from scratch.
The Schema Learming System (SLS) [Draper 92,
93b] automates the construction of knowledge-
directed recognition strategies. Starting from a
knowledge base of visual procedures and object
models, SLS learns robust strategies for locating
landmarks in images and recovering their positions
and orientations, if necessary. Each strategy is
specialized to a landmark, taking advantage of its
most distinctive characteristics, whether in terms
of color, shape, or contextual relations, to quickly
focus its attention on the landmark and recover its
pose. Furthermore, because S1LS leans from
experience by a strict generalization algorithm, it
is possible to predict both the expected costs and
the expected error rates (due to a lemma by
Valiant) of the strategies it develops.
Figural completion is the preattentive ability of the
human visual system to build complete and
topologically valid representations of
environmental surfaces from the fragmentary
evidence available in cluttered scenes. A
description of a grouping system developed by
Williams, employing a two-stage process of
completion hypothesis and combinatorial
optimization, appeared in a previous workshop
proceedings [Williams 90]. Preliminary
experimental results were also reported. Since that
time there has been significant progress in two
major areas. First, the mathematical basis for the
grouping constraints employed in the optimization
stage has been clearly elucidated. This has
allowed a proof of the necessity and sufficiency of
the grouping constraints for scenes composed of
flat embeddings of orientable surfaces with
boundary. Second, a more advanced grouping
system which uses cubic Bezier splines of least
energy to model the shape of perceptual
completions has been implemented. 'The new
system is demonstrated on a number of figures
from the visual psychology literature which are
beyond the capability of the old system.
During the past year, Dolan has continued his
work on curvilinear grouping [Dolan 92]. A
SIMD implementation of the curvilinear grouping
system has been developed, along with a
simplified, distributed representation of curves for
use in the CAAPP. The integration of multiple
grouping processes--in particular, curvilinear and
area grouping -- is currently being examined.
Many of these ideas are being incorporated in a
general grouping module for KBVision, which
will facilitate research and experimentation with
many diverse forms of grouping.
The use of projective invariants for object
recognition and scene reconstruction has been the
subject of intense interest in the image
understanding community over the past few years.
Although classic projective geometry was
developed with mathematically precise objects in
mind, practical applications must deal with
errorful measurements extracted from real image
sensors. A more robust form of projective
geometry is needed, one that allows for possible
imprecision in its geometric primitives. In his
Ph.D. thesis [Collins 93], Collins represents and
manipulates uncertain geometric objects using
probability distributions in projective space,
allowing valid geometric constructions to be
carried out via statistical inference. The result is a
methodology for scene reconstruction based on the
principles of projective geometry, yet also dealing
with uncertainty at a basic level. The effectiveness
of this framework has been demonstrated on
several geometric problems, including the
derivation of 3D line and plane orientations from a
single image using vanishing point analysis, the
extraction of a planar patch scene model using
stereo line correspondences, and the reconstruction
of planar surface structure using multiple images
taken from unknown vicwpoints by uncalibrated
cameras.
More specifically, Collins shows that projective N-
space can be visualized as the surface of a unit
sphere in (N+ 1)-dimensional Euclidean space.
Each point in projective space is represented as a
pair of opposing or antipodal points on the sphere.
By the identification of projective space with the
unit sphere, antipodally symmetric probability
distributions on the sphere may be interpreted as
probability distributions over the points of
projective space, and standard constructions of
projective geometry can then be augmented by
statistical inferences on the sphere. Probability
densities defined in this way can also be used for
representing uncertainty in unit vectors,
orientations, and the space of 3D rotations (via
unit quaternions).
Oliensis' previous work on shape from shading
[Oliensis 92] has been extended in a number of
ways, First, while our earlier work usually
assumed that the illumination was from the
direction of the camera, the shape reconstruction
algorithms and convergence proofs have been
extended more recently to the case of illumination
from any direction [Oliensis 93a]. As before,
these algorithms are provably and monotonically
convergent, and (in many cases) can be shown to
converge to the correct surface. Moreover, it has
been shown that a whole family of algorithms
could be developed, and that all would give
equivalent surface reconstructions. This is
convenient since some of the algorithms are better
for theoretical analysis while others are more
efficient in practice. The uniqueness proofs for the
surface given the shaded image, and the corollary
that regularization is not necessary for shape from
shading, have also been extended.
Experimentation with these algorithms on
synthetic and real images show that they are fast
and robust, taking less than 10 seconds on a
DECstation 5000 for a 200 x 200 real image.
These algorithms still require that a small amount
of information on the surface be provided, namely:
1) a list of those singular points (the brightest
image points) corresponding to local minima of
the surface height (as opposed to the other
possibilities of a local maximum or a saddle
point): and 2) the heights of these singular points.
However, in a second extension of previous work
[Oliensis 93b], Oliensis has developed a new
algorithm that is capable of determining this
information automatically, and thus can
reconstruct a general surface from shading with no
a priori information on the surface. In
experimental tests on complex synthetic images,
this algorithm has produced good surface
reconstructions over most of the image. For 128
x128 images, the reconstruction takes less than 30
seconds on a DECstation 5000. Moreover, the
algorithm appears noise resistant, giving good
reconstructions even in thc extreme case of an
added pixel noise of 10%. It appears that it will
also be possible to prove the convergence of this
algorithm to the correct surface in the limit of
perfect resolution.
All algorithms thus far have assumed that the
imaged surface was matte. Even with this
restriction, the algorithms are potentially useful in
controlled industrial or research applications. At
UMass these algorithms will be ported to the
robotics laboratory environment, and used in
combination with other means of shape sensing
and recovery to aid in research in grasping
partially or unmodeled objects. Further extensions
include adapting the current algorithms to the
realistic case of a partially specular surface. With
this extension, shape from shading could become
practical for a variety of applications.
Work on the IUA [Weems, 1993] has advanced in
three areas in the preceding year: compilers and
system software, hardware and architecture, and
applications and algorithms. The IUA is a tightly
coupled, heterogeneous parallel processor being
developed by UMass, Hughes Research Labs, and
Amerinex Artificial Intelligence (AAI) under
DARPA funding. It is intended to support real-
time knowledge-based vision applications and
research by providing three distinct parallel
processors in a single architecture: a fine-grained
SIMD/Multi-associative array for low-level vision,
a medium-grained SPMD array for intermediate-
level symbolic vision, and a coarse-grained
multiprocessor for high-level, knowledgc-based
processing. A proof of concept prototype of the
IUA was constructed under a previous effort and
the current work is dirccted at developing a second
generation of the system with enhanced
performance and the ability to be fielded in the
DARPA Unmanned Ground Vehicles (UGV)
program.
AAI has completed development of the C++ class
library for the low level of the IUA. The class
library defines a set of image plane types upon
which parallcl operations may be performed.
Work at AAI includes the incorporation of
additional optimization code into the Gnu C++
compiler so that image planes are treated more like
first-class objects in C++. An automated test
system has also been developed for the machine's
microcode library, to facilitate regression testing
of new releases. For the intermediate-level
processor, basic operating system support,
multitasking, and messaging have been
implemented on a TMS320C30 Single Board
Computer (SBC), and recently these were
transported to another SBC with two TMS320C40
processors that are configured to simulate the
intermediate level of the IUA. A debugger has
also been implemented for the intermediate level.
Work is now under way to transport the
KBVisionM system to the IUA.
UMass has implemented a version of the Apply
language for the low-level processor of the second
generation IUA. The compiler generates code
compatible with the C++ class library. It permits
us to easily import image processing operations
written for the CMU Warp or Intel iWarp
machines.
The prototype IUAhas been running at Hughes for
most of the last year. Under the prototype
development contract, only a very simple
controller was built to demonstrate the basic
functionality of the processor arrays. It was never
intended that the prototype controller be fully
programmable. However, Hughes and Amerinex
AI invested additional effort to develop software
that allows C++ code for the second generation to
execute on the prototype hardware. Because of the
nature of the controller, instructions can only be
issued at VME bus rates to the array, which is
significantly slower than the array can accept
them. However, it does permit demonstration of
the functionality of the array hardware on real
applications. Hughes has since implemented the
low-level portion of the DARPA IU Benchmark,
an SDI application, and an ATR application on the
prototype.
The custom chips used in the IUA have been
fabricated and are undergoing testing. Each chip
contains 256 bit-serial processors with on-chip
cache, and has roughly 600,000 transistors. The
system's array control unit, backplane, and chassis
have been built and tested. Processor and memory
boards are currently under construction. The I/O
subsystem for the machine has also been designed,
and will support the equivalent of 20 simultaneous
sensor inputs at 512 X 512 X 8-bit resolution with
automatic mapping onto the processor
virtualization schcme used for the low level, with
almost no latency. The I/O subsystem will also
support the selection of multiple regions of interest
from an image. Hughes has indicated that tihe first
machine should be assembled by the end of
February, 1993.
Work has already begun on the analysis and
design for the third generation IUA. UMass has
developed a system for capturing traces of
programs written in the C++ class library as they
execute on an abstract parallcl machine. The
traces are then fed to a simulation system that
models hardware architectures with diffcrent
features and parameters. The system allows us to
gather real performance data for different
architectural configurations, and to analyze the
data statistically. The performance data will then
be contrasted with cost estimates for the different
configurations to produce a specification for the
third generation IUA.
The low-level processor of the IUA is a square
mesh of processing elements, augmented with a
second (reconfigurable) mesh, called the Coterie
Network , This network allows the mesh to be
partitioned, for example, into areas corresponding
to regions in an image. One particularly useful
operation is the ability to enumerate elements
within a partition or to summarize (reduce) the
information in a partition to a single value. The
parallel prefix operation is the general form of this
type of operation. It is especially desirable to be
able to carry out parallcl prefix in all partitions at
once, i.e. to perform a multi-prefix operation
[Herbordt, 1992]. An algorithm has been
developed for multi-prefix that is significantly
faster than alternatives using general purpose
routing in the mesh.
As recommended by the DARPA IU Benchmark
Workshop participants, much of the benchmark
[Weems, 1988, 1990] has been recoded as a set of
library routines which are called by the core of the
benchmark. We have also begun developing the
second level benchmark, which will incorporate
tracking of moving objccts over a sequence of
images. The goal of the new benchmark is to test
system performance over a longer period of time
so that, for example, caches and page tables will
be filled. The benchmark will also explore UO
and real-time capabilities of the systems under
test, and involve more high-level processing.
UMass has developed a parallel algorithm for the
IUA that computes a dense depth map for a scene
from a pair of images taken by a moving sensor
[Dutta 93]. The algorithm has an average error of
about 8 percent in depth, as computed from
randomly sampling points corresponding to
objccts in the scene with known distances from 21
to 76 feet from the camera. The experiments were
done with fairly large displacements (four feet of
forward motion bctween the images) so that a
large (41 X 4 pixcl) search window was required
to establish correspondences, resulting in 1681
image-to-image corrclations being performed. In
simulations of the second generation IUA, it was
determincd that the execution time will be about
0.54 seconds, of which 0.53 seconds is taken up
solely by the correlations. We are thus looking
into approaches in which an estimate of the motion
is available or in which a series of frames with
smaller displacements can be used (allowing the
search window to be constrained).
UMass has also developed a parallel algorithm for
extracting straight lines from an image. Using the
second generation IUA simulator, the algorithm
executes in as little as 31 milliseconds for images
that map to the array with a 1:1 virtualization ratio.
We are currently evaluating the quality of the
results, but a preliminary examination indicates
that the algorithm gives very consistent lines over
sequences of images, which is an important
attribute in the support of algorithms that use line
tracking.
UMass is developing mechanisms for site model
acquisition, extension and refinement [Collins
93a] based on technology that has already proven
effective in the mobile robotics domain..
Automatically acquiring the initial 3D site models
from scratch is a challenging problem that will be
the focus of future research. Our current work
assumes that a partial model of the site is provided
apriori by the image analyst. Our model-based
refinement and extension algorithms are then
applied to automatically correct inaccuracies in the
initial site models, and extend them to include
previously unmodeled cultural features (buildings,
roads, etc.) based on information from new
images.
Rather than building a tum-key system, UMass
will be delivering a set of modules for performing
specific tasks of direct benefit to the image
analyst. The following is a list of the early
deliverable modules that are currently being
evaluated on the model board test imagery
supplied to the research community.
In addition to developing new techniques for
automatically acquiring initial site models, new
research will investigate statistical techniques for
applying projective invariants to the modeling
process to accurately derive structure without
explicit camera models or knowledge of
viewpoint. Initial experiments in this direction
have yielded promising results. Oher encouraging
results have becn obtained regarding the difficult
problem of image to image registration. A
technique based on vanishing point analysis
[Collins93b] allows an oblique acrial view to be
rectified (unwarped) to present a simulated vertical
view, allowing full pcrspective acrial images to be
registered with a computationally tractable affine
matching approach.
Image t'nderstanding research at the MIT AI Lab has
continued along a range of fronts, from low level process-
ing, such as stereo, motion, color and texture analysis,
through intermediate stages of integration of visual infor-
rmmation, to higher level tasks such as object recognition
and navigation. This report summarizes our main recent
accomplishments in these areas. As is usual in these re-
ports, we refer interested readers to other publications
for more details.
Because it has been one of our central focal points, we
begin with our recent work in object recognition. ln ap-
proaching the problem of recognizing ob jects from noisy
images of cluttered scenes, we have found it convenient
to separate out several different aspects of the problem:
We will describe our recent work in each of these areas.
We have argued for some time that robnst and efficient
solutions to the selection (or grotuptfng) problem are es-
sential to practical recognition systems. Earlier work,
using both formal analysis and experimental studies [22;
13; 27], has shown that the complexity of many ap-
proaches to recognition are dramatically reduced if de-
cent seiection is provided, and that the false pos1-
tive/false negative rates for such methods are also sig-
nificantly improved with good selection.
One advantage of focusing on the issue of selection for
recognition is that it provides constraints on the reanire-
ments of early processing stages. For example. cues suchh
as color or texture are often considered from the yiew-
point of extracting object surface measurements, which
requires that one account for illumination and other
scene effects in inverting the image measurements to ol-
tain ob ject parammeters. If one simply wants to use these
cues to separate regions of an image likely to have cotne
from a single ob ject, much less stringent requirenments
are placed on the task, leading to simpler and hopefully
more robust algorithms.
Towards this end, Tanveer Syeda-Mahmood has re-
cently completed a Ph.D. thesis [46] that explores the
role of cues such as color and texture in selection for
recognition. She does this by developing and implement-
ing a computational model of visual attention, which
serves as a general purpose selection mechanism it aa
recognition system.
The approach supports two modes of attentional le-
havior, namely attracted attention and pay-attenton
modes. The attracted attention mode of behavior -
spontaneous and is commonly exhibited by an unbiased
observer (i,e., with no a priori intentions) when sotu
ob ject or some aspect of the scene attracts his / her at-
tention, while the latter is a more deliberate behavior
exhibited by an observer looking at a scene with a yrtwrr
goals (such as the task of recognizing an ob ject, say) and
hence paying attention to only those obb jects/aspects d
a scene that are relevant to the goal.
Briefly, the model suggests that the scene represented
bby the image be processed by a set of interacting featnrc
detectors that generate a hierarchy of maps, representing
features such as brightness, color, texture, depth, group-
ing of edges, and others such as shape, size, symmetry,
etc. The feature maps are then processed by filters in-
corporating strategies for selecting distinctive regions in
these maps. The choice of these strategies is guided by a
central control mechanism that combines top-down task
level and a priori information with the bottom-up infor-
mation derived from the features, to demonstrate either
mode of attentional behavior as desired. Finally, an ar-
biter module housing another set of strategies selects the
most significant features across the feature maps, which
can then be used in an object recognition system.
A system implementing the computational model de-
scribed here was built using three features: color, tex-
ture, and parallel-line-groups. The respective feature
maps were built, and the selection filters for finding dis-
tinctive regions in these maps have been developed. In
addition, a version of the arbiter module to combine the
saliency information from the various features has been
built.
Because we are interested mainly in separating regions
likely to have come from a single object, we do not need
to exactly recover ob ject parameters such as body color.
Rather, we can focus on methods that describe the color
image as consisting of perceptually different colored re-
gions. This can be done by focusing on the components
of a color signal that are most relevant to human cate-
gorization of colors (e.g. saturation and brightness, but
not hue). Syeda has developed such a method of per-
ceptual categorization of a color-space, which supports
fast color region segmentation . A color saliency map was
then built which used a color saliency measure that em-
phasized attributes that are also salient in human color
perception. The key point is that such a saliency mea-
sure serves to highlight regions of interest for a recogni-
tion system.
The texture feature map was generated by regarding
the image as being generated by a space-limited station-
ary stochastic process. Here, the segmentation of the
textured image was obtained by a comparison of the lin-
ear prediction spectra of adjacent windowed regions of
the image. Properties such as the relative distribution
of dark and bright blobs were then made use of to judge
the distinctiveness of a region. This was used to generate
the texture saliency map.
Lastly, the parallel-line-groups feature map high-
lighted groups of closely-spaced parallel lines in an edge
image. It has been found that some texture information
can be modeled this way. For example, printed letters on
a surface (such as a bottle) appear as a bunch of closely
spaced parallel lines when passed through an edge de-
tector. Similarly, some types of wooden tables show this
type of texture in an image.
These feature maps can then be combined to isolate
regions of an image for analysis by a recognition system
(in our implementation this was a combination of a tree
search algorithm and a linear combinations approach)
The feature maps and their associated saliency maps can
be driven in a pay-attention mode, in which the color
and texture information in the model (extracted using
the feature maps described earlier) is used to build a de-
scription of the object-model. This description was then
used to design strategies for the selection filters. This
involved developing new algorithms for finding instances
of regions in the image satisfying object-model color and
texture descriptions. Such regions are then passed to
the recognition system for analysis. Experimental results
show that the methods drastically reduce the complex-
ity of the recognition process by rejecting clutter from
consideration. The system can also be driven in attract
attention mode, in which the most salient portions of the
scene are analyzed first, again reducing the complexity of
the recognition stage. An example is shown in Figure l.
The key results here are a framework for combining
sensory information to support recognition, the use of
an attention mechanism to select targets for recognition,
and novel methods for handling texture and color infor-
mation.
Even if we can isolate key regions of an image, we still
need to know what cbjects may be present. In some
tasks, we are looking only for a specific target, or a small
number of targets, in which case model-driven selection
can be used to isolate regions of interest. In other cases.
we need to consider large libraries of ob jects, in which
case we need some means of using selected image features
to identify subsets of the library as candidate models.
CCues such as color and texture, discussed above, can help
with this object indexing. In general, however, other
information is also needed.
David Jacobs, as part of his recently completed Ph.D.
thesis j2T], has shown that simple aspects of an ob ject's
shape can often be used to efficiently index ob jects in
a library. Jacobs' method views the indexing problem
as stating: can one compactly represent the space of all
images that an object model can create, given the type
of projection model used'? [f one can, then to handle
the indexing problem, we can precompute each model's
manifold of possible images in an image space. At recog-
nition time, we can compute the associated representa-
tion of the current image, use this to access image space,
and retrieve all models that could have caused it. The
key question is whether one can actually represent all
possible images of a model in an efficient way. Jacobs
has shown, perhaps surprisingly, that in several nontriv-
ial cases, one can.
The results are summarized as follows. We assume
that a 3D object is transformed by an arbitrary affine
transformation, followed by a scaled orthographic pro-
jection. For the case of 3D points, this is equivalent to
applying an arbitrary 3 x 3 matrix to the points, then
translating them, then projecting them. To describe the
images that a model can produce under this class of
transformations, we first define image space, then deter-
mine the shapes of the model manifolds. Jacobs argues
for using affine coordinates to represent image space. In
particular, if one selects three ordered point features to
establish a basis, then all other points can be written in
terms of coordinates with respect to this basis: that is if
4) .4;,-- 4; denote the image points, and if
are the affine basis, then all other points can be repre-
sented by coordinates (o,, D;) by
These coordinates are invariant to any affine transfor-
mation, and hence an image is uniquely identified by
It turns out that the first three vectors do not provide
any information about whether a scene could produce
this image, so we use only the (4, ) parameters to rep-
resent an image. Thus, an image with n ordered points
maps to a point in a 2(n - 3) dimensional space, which
can be divided into two orthogonal n - 3 dimensional
spaces, one for the o coordinates and one for the d co-
ordinafes. The advantage of doing this, as Jacobs has
shown, is that the set of all images that a model of n
ordered points could produce is simply a pair of parallel
lines, one in each space. In this case, indexing simply
says, given an image basis, compute the affine coordi-
nates of the points, then find all model lines that pass
through the associated point in o - d space. One must
allow for uncertainty in the measurements, but this can
be shown to be easily handled and simply expands the
image points to small regions in the o and d spaces [21].
An example of the indexing system correctly retrieving
candidate models is shown in Figure 2.
Extensions of this approach to deal with other types
of features are discussed in an article by Jacobs in these
proceedings. We note as an aside that considering this
model of linear transformations has proved fruitful in
other problems, such as the linear combinations ap-
proach to recognition [48] and in dealing with affine
structure from motion [28: 41].
The key result here is a very efficient method for han-
dling indexing for some classes of ob jects, as well as a
novel framework for investigating the interactions be-
tween ob ject structure and image pro jections.
A central part of recognition, once we have found sub-
sets of image features of interest and sets of models
of interest, is to determine whether the image features
are in fact consistent interpretations of the model fea-
tures. Over the past several years, we have developed
a variety of different approaches to this problem, in-
cluded Interpretation Tree Search [22], Alignment [24:
25] and Linear Combinations [48[. Here we report on
some new alternatives to these approaches, as well as
improvements on these approaches.
The alignment approach to recognition [24; 25; 6] pro-
ceeds by matching a small set (typically 3) of image fea-
tures to model features, using this match to determine
the associated transformation of the ob ject (modeled as
a weak perspective transformation ), and pro jecting the
remaining model features into the image for verification.
In the original system, uncertainty in the image mea-
surements was dealt with in a somewhat ad hoc manner.
Recently [21] we have shown how to analyze the effects
of that uncertainty on the set of possible transforma-
tions. Alter [l] has extended that work to supplement
alignment approaches with a verification stage that is
guaranteed to be correct. In particular, he shows that
using a bounded error model on the image features, one
can compute the range of image positions for all other
model features, both for planar and solid ob jects, and
for point and line features. This allows one to exactly
specify the range of image positions over which to search
for matching features, so that one will not miss any sup-
porting evidence, while at the same time keeping the
chances of false matches minimal. One can further ex-
tend this approach by adding a Bayesian analysis of the
actual matching regions, so that one can determine the
likelihood of each verified match actually being correct.
This allows one to determine the best regions in which
to search for features, by determining those most likely
to contribute to a correct interpretation. An example of
the image search regions is shown in Figure 3. Exten-
sions of the method to line features has also been done.
and results show, as expected, that lines are considerably
more powerful as verification features than points.
A related result concerns the models of sensor uncer-
tainty used both in the analysis of recognition methods
and in the derivation of verification and likelihood tech-
niques. Most of our earlier work has been based on a
i)ounded error of sensor uncertainty. haren Sarachik has
been working on the problem of estimating the effects of
sensor noise on the problem of model based ob ject recog-
nition, for other classes of uncertainty. For the analysis
it is assumed that the location of a point feature in an
image is corrupted by noise, which is modeled as a two
dimensional GGaussian distribution, and the presence of
the model in the image is posed as a binary decision
problem. The positional uncertainties of the point fea-
tures are traced through the recognition algorithm, re-
sulting in analytic expressions for the confidence level of
the algorithm's decision. Until now the analysis has been
completed only for one algorithm, geometric hashing as
introduced by Wolfson, Lamdan, Schwartz and Hummel.
Using this technique it is possible to explicitly compare
the expected performance of different recognition algo-
rithms and noise models, a useful tool for the domain of
rnulti-sensor fusion.
In earlier proceedings, we have reported on our work
in developing recognition algorithms that work directly
in the space of possible poses of an object, rather than in
the space of feature correspondences. In the ideal case of
perfect sensor data, one can simply search over all pos-
sible pairings of model and image features, compute the
associated transformation and vote for that transforma-
tion in pose space, a la Hough transforms. When un-
certainty is allowed in the measurements, however, one
must be more careful about voting for the entire volume
of transformations consistent with the pairing of a noisy
sensor measurement and a model feature, and this in-
creases the demand on searching fine tesselations of the
pose space.
Todd Cass has recently complete a Ph.D. thesis that
presents an elegant way around this problem, by ex-
ploiting the geometry of pose space directly. Cass [10]
has provided a formulation of the problem in which one
can develop a polynomial-time algorithm that guaran-
tees finding all feasible interpretations of the data, mod-
ulo uncertainty, in terms of the model. The approach is
based on representing the model and the sensory data
in terms of local geometric features such as vertices and
line segments. He assumes bounds on the uncertainty
in the position or orientation of the data features due
to sensor error. He then shows that there are only a
polynomial number of quantitatively different transfor-
mations that align the model and the data modulo error.
Object localization is accomplished using a polynomial-
time search through the set of all model transformations
to find those that align large subsets of model and data
features within the uncertainty bounds.
Intuitively, this approach can be considered as follows.
For each pairing of a data and model feature, there is
a set of transformations that will align the model fea-
tures within the uncertainty region about the data fea-
ture, This set of transformations carves out a volume
in pose space. If we consider all pairings of data and
model features, we get a set of such volumes, and we
are interested in finding points in the pose space con-
tained within the intersection of a large number of such
volumes. One could find such points by simply sampling
ponts in pose space at some fine spacing, a method used
earlier by Cass in implementing a very fast recognition
scheme on the CConnection Machine. It turns out, how-
ever, that one can efficiently find such volumes by decou-
pling the search over the full pose space into a coupled
search over the translational components and a second
search over the rotational components. Moreover, one
can use the structure of these geometric arrangements to
find very efficient, polynomial-time algorithms for find-
ing the boundaries of these pose-space volumes. ass
has extended his earlier work to allow for unknown scale
factors, unknown uncertainty values, and has used re-
sults from computational geometry to provide efficient
algorithms for exploring pose space.
An alternative to Cass' approach to analyzing Pose
Spaces is the RAST algorithm (Recognition by Adaptive
Subdivision of Transformation Space: [8]). The RAST
algorithm solves bounded error recognition problems ef-
ficiently.
Bounded error recognition is one of the most com-
monly used formulations of the visual ob ject recogni-
tion problem and has proven its utility in a number of
practical systems (for fturther references and related re-
sults, see, for example, [4], [22]). The simplest form of
the bounded error recognition problem is the following:
given a set of model features (points in R 6r Rf) and a
set of image features (points in R%), find maximal sub-
sets of image and model features that can be brought
into correspondence under given error bounds using rigid
body transformations.
The recognition algorithm is based on the idea of car-
rying out matching with variable sized error bounds: if.
for a given set of transformations, a good match can-
not be found for large error bounds, then matches with
smaller error bounds need not be examined. The RAAST
algorithm uses particularly convenient representations
for sets of transformations that make it simple to im-
plement, efficient, and flexible in the kinds of features
and similarity measures that can be used with it.
So far, we have applied the RAST algorithm to 2D
recognition problems that involve a very large number
(thousands) of very simple image features (short line
segments). In such applications, the RAST algorithhm
is found to be faster than alternative methods (recog-
nition by alignment, Hough transform, search, or corre-
lation). Actual applications using the RAST algorithm
include the prototype of a view-based 3D object recogni-
tion system and a system for handwritten optical charac-
ter recognition (OCR). Examples are shown in Figure 4.
For curved objects, both the visibility and the location
of object parts/features in the image varies in a compli-
cated way with the viewpoint. This has made the de-
velopment of efficient 3D ob ject recognition algorithms
diffcult.
In order to avoid these difficulties, many 3D ol ject
recognition systems have heuristically used vie uu-basedd
representations, 1.e., representations that encode obb ject
properties and shape from a large number of different
viewpoints,
However, using view-based representations only solves
the 3D object recognition problem approximately. In
order to understand the nature and signihcance of this
approximation, we have formalized the notion of view-
based representations and established error and com-
plexity bounds on the performance of recognition sys-
tems that are based on view-bbased representations[9].
The theoretical results suggest that, for the pur-
poses of object recognition from 2D images, view-based
representations are good approximations to true 3[D
shape representation. Furthermore, we have estab-
lished model-independent upper bounds on the number
of views needed in order to represent a model in a view-
based system. Finally, a complexity analysis suggests
that view-based recognition can be carried out more ef-
ficiently than 3D shape-based recognition.
These theoretical results are supported by a number
of simulations and experiments on real images.
An alternative approach to recognition is to treat it
as a problem of optimal estimation. Sandy Wells has
recently completed a Ph.D. thesis [53] that develops and
tests a framework for statistical object recognition.
To formalize this, let the image that is to be analyzed
be represented by a set of v-dimensional point features
The model to be matched is also described by a set of
point features, these are represented by real matrices:
To solve the recognition problem we need to find a
legal match between image features and model features.
Here, legal means that there is some physically meaning-
ful way of positioning the model in the scene so that it
would produce image features similar to those actually
observed. We can treat this as an optimization problem,
wherein we seek to estimate two sets of parameters: the
correspondences between image and model features, and
the pose of the model instance in the image. The corre-
spondences are described by an interpretation vector
Here T; e M; means that image feature i corresponds
to model feature j, and f; sll means that image feature
i is due to the background.
The pose of the model instance in the image, d, is
a real vector. An associated pro jection function P is
defined:
F maps model features into the v-dimensional image ac-
cording to the model's pose.
Our goal is to olbtain estimates of the correspondences
and pose by maximizing the posterior density with re-
spect to T' and dd, as follows
In other words, we want to find the assignment ot
model features to image features, and the related pose
(position and orientation) of the ob ject that optimally
accounts for the observed data, We can treat this as an
estimation problem, and use Baye s rule to calculate the
a-posteriori probability density of f and ,/:
where ' is a normalizing constant independent of f and
,$. Then we seek estimates for T and d that optimize
this objective function.
Note that we couple the effects of the objects pose di-
rectly into the matching problem. There are, of course.
some sensor features that are not directly pose related,
such as the fractal dimension of a region. These features
can also be incorporated into the matching process, ei-
ther as priors on the correspondence, or as filters that
remove some correspondences directly.
To solve this optimization problem, we need several
things. First, we need to model 6. This can be done
by a careful physical modelling of the sensor. by taking
into considerating the effects of noise on the transduc-
tion process, and providing careful models of the distri-
bution of uncertainty about the measured values. Such
rmodels can be derived for widely varying sensors, other
than visual, and in we are in the process of applying this
approach to LADA R and SA R sensors.
As an example, one simple model is to assume that
the probability density function on features is uniformm
for features arising from the background, and is normally
distributed about their predicted locations in the image
for matched features. Of course, this is a simple model.
For some types of features, we have more explicit mod-
els of the distribution of the feature, which will simply
replace the variance of the normal distribution.
Second, we need to model the probability of an inter-
pretation (or matching of features) and the probabbility
of a pose. One simple method is the following. The
probability that a feature belongs to the background is
H; the remaining probability is uniformly distributed for
correspondences to the m model features.
Our simple model also assumes that prior informma-
tion on the pose is a normal density. With this choice
for the form of the pose prior, the system is closed in the
sense that the resulting pose estimate will also be nor-
mal. This is convenient for coarse-to-fine techniques (or
multi-resolution methods), in which we use coarse data
to get an initial estimate, then refine this by focusing on
subportions of finer resolution data.
If little is known about the pose a-priori, the prior
tmay be made quite broad. This is expected to often
be the case, Note, however, that better models would
incorporate additional information about the scene. For
example, if we know the parameters of the ground plane.
and the target is known to be in a stable position on that
plane, we should be able to incorporate this knowledge
into better priors on the pose parameters. For example
if range information is also known, then only two of the
six parameters of pose are completely unknown. The
others can be constrained from such additional infortua-
tion, thereby reducing the complexity of the search for a
match.
If we assume independence of the correspondences and
the pose (before the image is seen), the composite prior
is a straightforward product of the prior distribution on
pose and the probability distributions on matched and
uunmatched features. Thus, in our simple example, we
can describe the probability of a pose and a correspon-
dence in terms of measurable quantities in the data.
Giiven this, we need efficient methods for finding op-
timal estimates for the parameters of interest. We can
choose those estimates that maximize the a-posteriori
probability (MAP), by maximizing the posterior den-
sity with respect to the matched features and the pose.
But to find such estimates, we need efficient methods for
searching the objective function.
To handle this search process, Wells has considered
several approaches including beam search through a tree
of interpretations [51], and posterior marginal pose es-
timation [52]. The latter is motivated by the obser-
vation that in tree searches of the ob jective function
of MAP model matching, hypotheses having ''poor''
matches scored poorly in the obbjective function. The
implication was that summing posterior probability over
all matches (at a specific pose) might provide a good
pose evaluator. This has proven to be the case in the
experiment described in [51]
The essence of posterior marginal pose estimation is to
choose the pose that maximizes the posterior probability
density of the pose, given an image:
The posterior probability density of the pose is computed
from the joint posterior probability on pose and match,
by taking the marginal over the possible matches:
UUsing Bayes' rule, the posterior marginal may isolated
as a function of the priors described above, and this
leads to a convenient ob jective function for optimization.
One can utilize the EM algorithm to provide an efficient
method for optimizing this objective function, thereby
leading to solutions to the pose problem. A more com-
plete description is given the paper by Wells in these
proceedings.
Wells has applied the method to several cases, include
2D point features, 2D oriented range features and linear
3d pro jection models. An example of recognition from
visible image features is shwon in Figure 5.
A second example shows the method applied to a very
different type of imagery. This work was done in con-
junction with Group 53 of Lincoln Labs, directed by
A. Gschwendtner. In this example, the features were
oriented-range features, consisting of fragments of image
edge contours, augmented with a vector pointing in the
direction normal to a range discontinuity, with length re-
flecting the inverse range at the discontinuity. Two sets
of features were prepared, the ''model features'', and the
''image features''.
The object model features were derived from a syn-
thetic range image of an M35 truck that was created
using the ray tracing program associated with the BRI.
C'AD Package [16]. The ray tracer was modified to pro-
duce range images instead of shaded images. The syn-
thetic range image appears in the upper left of Figure
In order to simulate a laser radar, the synthetic range
image described above was corrupted with simulated
laser radar sensor noise, using a sensor noise mode]
that is describbed by Shapiro. Reinhold, and Park [40].
In this noise model, measured ranges are either valid
or anomalous. Valid measurements are normally dis-
tributed, and anomalous measurements are uniformly
distributed. The corrupted range iimage appears in Fig-
ure 6 on the right. To simulate post sensor process-
ing, the corrupted image was ''restored'' via a statisti-
cal restoration method of Menon and Wells [31]. The
restored range image appears in the lower position of
Figure 6.
(Oriented range features were extracted from the syn-
thetic range image, for use as model features - and fron
the restored range image, these are called the noisy fea-
tures. T]e features were extracted from the range immages
in the following manner. Range discontinuities were lo-
cated by thresholding neighboring pixels, yielding range
ddiscontinuity curves. These curves were then segmented
into approximately 20-pixel-long segments via a process
of line segment approximation. The line segments (each
representing a fragment of a range discontinuity curve )
were then converted into oriented range features in the
following manner. The . and Y coordinates of the
feature were obbtained from the mean of the pixel co-
ordinates. The normal vector to the pixels was gotten
via least-squares line fitting. The range to the feature
was estimated by taking the mean of the pixel ratugr
on the near side of the discontinuity. This information
was packaged into an oriented-range feature. The model
features are shown in the fourth image of Figure 6. Each
line segtment represents one oriented-range feature, the
ticks on the segments indicate the near side of the range
discontinuity. There are 113 such. object features.
The noisy features, derived from the restored rangr
image, appear in the fifth image of Figure 6. There are 62
noisy features. Sopme features have been lost due to the
corruption and restoration of the range image. The setf
image features was prepared from the noisy features by
randomly deleting half of the features, transforming thhe
survivors according to a test pose, and adding sufficient
randomly generated features so that ; of the features
are due to the object. The 248 image features appear i
the sixth image of Figure 6.
UUsing this data, the EM algorithm was run in a nmulti-
resolution manner, and Figure T shows the convergenoc
of the algorithm to the correct pose.
In classic projective geometry of 3D space, projectivc
structure is typically defined by three cross-ratios usuiu.
five reference points (tetrahedron of reference and a umt
point) [39: 32] or, equivalently, by a tetrad of homogr-
nous coordinates. With such projective structure one
can reconstruct the scene tup to an unknown pro ject iv
transformation in 3D projective space, or equivalently.
the camera coordinate frame may undergo an affine trnn-
formation in space followed by an arbitrary projective
transformation of the image plane (taking a ''picture''
of the image). A projective framework does not make a
distinction between orthographic and perspective views
and does not require camera calibration (internal cam-
era parameters are folded into the affine transformation
of the camera coordinate frame).
Our approach has several characteristics: first, our
ddefinition of pro jective structure relies on four scene ref-
erence points and the camera's center (instead of five
scene reference points), and is defined using a single
cross-ratio (rather than three) - which means that it
is obtained by some invariant function of projective co-
ordinates. Second, the proposed projective invariant al-
lows us to reconstruct the scene up to an unknown 3D
projective transformation and, in addition, is particu-
larly useful for recognition, that is, reconstruction of
any third view becomes very simple in this framework.
Thirdly, we make use of the epipoles to compute the
projective invariant using only linear image-based com-
putations ([18; 32] propose other similar techniques for
using the epipoles in a linear reconstruction of homoge-
neous or non-homogeneous coordinates). Finally, we do
not recover the camera transformation in the course of
reconstruction, but instead recover the projective trans-
formations of two faces on the tetrahedron of reference.
Taken together, projective structure can be computed
from two images, perspective or orthographic, using an
uncalibrated camera. The computation requires the cor-
respondences arising from four reference points and the
epipoles -- the latter can be recovered by a number of
methods using six to eight corresponding points [29: 19;
Key results here are that the structure invariant is
useful for 3D reconstruction up to an unknown 3ID pro-
jective transformation of the object, and for purposes
of recognition via alignment by creating an equivalence
class for different views of the same object. In other
words, we can ''re-pro ject'' an ob ject onto any novel view
given two model views in full correspondence and a smzll
number of corresp onding points between the novel view
and the model views.
The geometric relation between different views of a .|)
object can be used to recover the change in viewing
geometry across views, and to recover the ob ject's 1)
structure. The work on pro jective structure demon-
strates that in a pro jective framework one can defimc
and recover a structure invariant that is sufficient for
uniquely reconstructing the ob ject with respect to a
frame of reference consisting of four scene points and th
camera's center. The location of the reference framee m
space is generally unknown, and therefore the ol ject -
structure can be reconstructed up to an unknow n .,1)
pro jective transformation in space. This allows ns t
work in a framework that does not make a distinction
lbetween orthographic and perspective views and does
not require internal camera calibration.
The geometric relation between ob jects and their
views can also be used for purposes of recognition. In
this case one is generally not interested in recovering ob-
ject structure from multiple views but instead in being
abble to predict the appearance of a novel view from a
small number of example views (''model'' views) given a
small number of corresponding points between the novel
view and the model views. The pro jective structure in-
variant can also be used for this purpose (see Figure ??)
but it is more desirable to achieve the same result di-
rectly without going through the computation of struc-
ture (metric or non-metric) and without the reconstruc-
tion of camera geometry (transformation parameters or
epipolar geometry).
In what is still an ongoing research we derive alge-
bbraic relations between image coordinates across three
views (two model views and a novel view). We show
here three results: first, the general result is that im-
age coordinates across three views (perspective or or-
thographic) are related by a small number of third-
order algebraic functions each having ll parameters
that can be recovered by linear methods. Second, if
the two model views are known to be orthographic,
then the algebraic functions reduce to second-order ones
with only 7 parameters. Thirdly, if all three views
are known to be orthographic, then the functions re-
duce further to first-order ones with only 4 parame-
ters. The latter is identical to the result derived by [48;
233] known as ''the linear combination of views'', and thus
the first two results can be viewed as an extension of the
linear combination of views to perspective.
In a projective framework, five reference points (a
tetrahedron and a unit point) are used for construct-
ing a coordinate system of 3D space ([49], for example).
A projection of a point P onto a plane with respect to
an arbitrarily positioned center of projection ((COP) and
arbitrarily positioned image plane can be achieved by
first mapping the reference frame such that one of the
tetrahedron 's vertices is aligned with the desired loca-
tion of the (C'OPP, and three other vertices are coplanar
with the desired image plane (in pro jective space five
points in general position can be uniquely mapped onto
any other configuration of five points in general posi-
tion). The point P is then projected onto the face of
the tetrahedron opposite to the (COP (in homogeneous
coordinate representation of space, this is achieved by
an orthographic projection, see Figure 9). If we assume
that there exists at least one configuration of the ref-
erence frame in which three of the reference points do
not intersect any of the scene points, then it is not dif-
ficult to show (but not shown here) that the space of
images we can get out of this framework are no more
than perspective and orthographic images of the scene,
and images of images of the scene, produced by a pin-
hole camera in which the camera's coordinate frame is
allowed to undergo arbitrary affine transformations in
space (rather than similarity transforms used in metric
structure-from-motion models).
Let /s. V6. be the affine coordinates of a point P
with respect to four of the reference points. If the fift
reference point (taken to be the COP) is not at infinity.
then the observed image coordinates (z, y) can be de-
scribed by an atfine change of coordinates followed by a
2D pro jective transformation:
for some fixed matrix A; and vector r and some scale
factor z (in a metric framework : would correspond to
''depth''). In the case of an orthographic projectionn
((COP at infinity and only 2D affine transformations of
the image are allowed), we have:
for some 2D affine transformation H (third row is
(0, 0, 1)) and an ideal vector s (third coordinate of s
is zero) [28; 41]. We can use these two equations to
ddescribe the transformation between image coordinates
iIn two views across Iour cases: two perspective views,
two orthographic views, a perspective to orthographic
case, and an orthographic to perspective case. This is
described below:
where p = z,p z z', and A,v are general for the
perspective-to-perspective case; p s p' : 8, A is a 2D
affine transformation and v3 = 0 for the orthographic-
to-orthographic case; p = z, p' = 1, third row of A is
(0, 0, 0) and v3 = 1 for the perspective-to-orthographic
case; p = ?5,p = 58, and A,v are general for the
orthographic-to-perspective case. Similarly, the itnage
coordinates (r'', y'') of a third view satisfy the following
relation to the first view:
Note that p remains fixed regardless whether the third
view is perspective or orthographic. The algebraic func-
tions of image coordinates across three views can be de-
rived by first eliminating p', p'' and then isolatingg v,
where a ;, 05, 01 are the row vectors of A. and p
(<,y, 1), p' = (', y', 1). Similarly, from equation 2 we
obtain
where b; , by,by are the row vectors of B, and p''
(r'', y'', 1). These two equations lead to nine alge-
braic functions of image coordinates across three views
For example, the first two terms lead to the function
F(e'', r', z,y) = 0 of the form:
Note that the bracketed terms are first-order polynomi-
als of r and y with fixed coefficients (depending only on
parameters of camera transformations). In other words,
equation 3 is a third-order algebraic function of the form:
where the coefficients o , ) s 1, ..., 12, are fixed con-
stants. Note that the constants can be recovered by
linear methods by observing ll corresponding points
across the three views (more than l1 points can be
used for a least-squares solution ). Then, for any ad-
ditional point (z,y) whose correspondence in the second
image is known (r', y'), we can recover the correspond-
ing x-omponent '' in the third view by substitution
in equation 4. In a similar fashion we can recover the
y-cormponent y'' by using one of the other functions, for
example:
The solution for r'', y'' is unique provided that v;, v3 do
not vanish simultaneously, or tu1, t43 dO not vanish simul-
taneously. These singular cases apply only to the two
functions above, and one can show that from the nine
functions we can always find two that are not singu-
lar under any viewing transformations that takes place
between the three views. The process of generating a
novel view can be easily accomplished without the need
to explicitly recover structure, camera transformation or
epipolar geometry - with the price of using more than
the minimal eight points that are required by less direct
methods.
The algebraic functions derived so far are general
in the sense that the scene is allowed to undergo gen-
eral 3D projective transformation in space. Reduced
lower order functions can be derived under more re-
stricted situations. For example, the third order com-
ponent of these functions vanishes when v ti4 0
(see Equation 3). This situation arises, for example,
when the views are taken by a camera moving along
a base line perpendicular to the optical axis. One ob-
serves, as a result, that this situation is intrinsically
more stable (errors in correspondence multiply to a
second-order rather than to a third-order) than the gen-
eral case - an observation experimentally made by [7;
Other results can be obtained by assuming that some
of the views are orthographic. This is especially impor-
tant in the context of achieving recognition via align-
ment: since the two model views are taken onlv once
(and offline), we may as well use a tele-lense for produc-
ing orthographic views. In this case we substitute v = 0
and aa p 1 in Equation 3 and obtain a second-order
function with only 7 free parameters which has the form:
for some fixed constants o;, ) = 1, ...,S. As a result. we
can generate novel views (perspective and orthographic)
by observing only i corresponding points across the three
views. This result is both direct (avoiding structure and
motion) and requires less than eight points (the minimal
under general conditions using linear methods). For in-
stance, with other tools we do not have an easy way for
making use of the fact that the two model views are or-
thographic - because the determination of the epipoles
and epipolar geometry between a perspective and an or-
thographic view still requires eight points in general.
Finally, it is easy to see what happens when all three
views are orthograp hic. In this case we have also uy = 0
and by p = 1, and thus Equation 3 reduces to a first-
order function, with only 4 free parameters, of the form:
for some fixed constants o; , J = 1, ...,5. This is iden-
tical to the ''linear combination of views'' result [48;
'33], stating that under the orthographic assurnption an
arbitrary view can be generated by applying certain lin-
ear combinations to the image coordinates of two moddel
vjews.
To summarize, we have shown that it is possible to
represent views as a function of image coordinates of two
other views. In the general projective case, the image co-
ordinates of three views are connected via third-order al-
gebraic functions with l1 free parameters. More restric-
tive cases (but applicable in the context of visual recog-
nition) reduce these functions to second-order with 7 free
parameters and to first-order with 4 free-parameters de-
pending on whether two or all the views are assumed
to be orthographic. The immediate application of these
results are in the context of visual recognition via align-
rment (especially the 7-point result), but other applica-
tions may also be possible. For example, the general re-
sult (Equation 4) may be useful in the context of model-
based image compression. In this case the number of
corresponding points required for reconstructing novel
views is not of critical importance whereas robustness
and simplicity are more of a concern. The 22 parame-
ters required for reconstructing a novel view can be re-
covered by many points in a least-squares fashion, but
the receiver eventually requires only the parameters and
not the corresponding points.
According to the 1.5 views theorem [33; 6] recogni-
tion of a specific 3D object (defined in terms of pointwise
features) from a novel 2D view can be achieved from at
least two 2DD model views (in the data basis, for each
object, for orthographic projection). Poggio and Vetter
studied how recognition can be achieved from a single
2D model view. The basic idea is to exploit transforma-
tions that are specific for the ob ject class corresponding
to the object - and that may be known a priori or mmay
be learned from views of other ''prototypical'' ob jects of
the same class - to generate new model views fromm; the
only one available. Their work divides in two distinct
parts. In the first part, they discuss how to exploit prior
knowledge of an object's symmetry. They prove that for
any bilaterally symmetric 3D ob ject one non-accidental
2D model view is sufficient for recognition. They also
prove that for bilaterally symmetric ob jects the corre-
spondence of four points between two views determines
the correspondence of all other points. Symmetries of
higher order allow the recovery of structure from one
2D view. In the second part of their work, Poggio and
Vetter study a very simple type of ob ject classes called
linear object classes. Linear transformations can be
learned exactly from a small set of examples in the case
of linear object classes and used to produce new views of
an object from a single view. More recently Vetter, Pog-
gio and Buelthoff have provided psychophysical support
for the hypothesis that the human visual system exploits
symmetry of 3D ob jects for better generalization from a
few views.
(OOver the last twenty years several different techniques
have been proposed for computer recognition of human
faces. Poggio in collaboration with R. Brunelli at IRST
compared two simple but general strategies on a common
database (frontal images of faces of 4T people, 26 males
and 21 females, four images per person). They have de-
veloped and implemented two new algorithms, the first
one based on the computation of a set of geometrical
features, such as nose width and length, mouth position
and chin shape, and the second one based on almost-
grey-level template matching. The results obtained on
the testing sets, about 90% correct recognition using geo-
metrical features and perfect recognition using template
matching, favour their implementation of the template
matching approach. Present work aims to extend the
system to deal with arbitrary poses and expressions of
the face.
(Complementary to our work on object recognition, we
have also investigated issues and methods in navigation.
One such method has built directly on our earlier work
in recognition by Linear (Combinations, and is reported
in a separate article by Basri in these proceedings.
A second approach to navigation has been part of a
broader research project, executed by lan Horswill, The
problem under consideration is the develop ment of sim-
ple real-time vision algorithms suitable for low-cost com-
puter systems such as personal computers. The specific
goals are to develop (a) simple vision algorithms useful
for problems such as robot navigation and interacting
with people, (b) a theoretical framework for analyzing
such systems, and (c) a concrete implementation demon-
strating the algorithms in a robot which gives primitive
''tours'' of the seventh floor of the laboratory.
This follows from the motivation of making vision
cheap as a necessary part of making it useful. For vi-
sion technology to be used routinely in construction and
manufacturing equipment, consumer electronics prod-
ucts, automobiles, etc., both design costs and unit costs
must be brought down to levels comparable with the
product into which they are to be incorporated. Mi].
lion dollar supercomputers, or even twenty thousand
dollar workstations are simply inappropriate for mmass-
produced products intended to cost less than the com-
puter.
Horswill's approach is one of situated agents. whereby
vision systems can be made much simpler and cheaper
by specializing them to a specific task and environment.
A task-specific system need only extract the specific in-
formation needed to perform the task. As well, a task
provides performance constraints that can simplify the
design process by allowing the use of approximate so-
lutions which might not be appropriate for all tasks. A
system specialized to its environment can take advantage
of domain knowledge which can simplify computational
problems. For example, a complete stereo systerm can
sometimes be replaced by a system which uses height in
the image plane to determine rough distance from the
agent.
A critical problem in developing these systems is the
reusability of components. It is important that we bbbe
able to apply experience gained in designing one system
to the design of other systems. For this reason, tasks
and environments must be analyzed at a theoretical level
so as to make explicit the ways in which they simplify
computational problems.
Low cost task-oriented vision systems could signifi-
cantly improve the performance and flexibility of au-
tonomous systems and reduce their cost. Such systems
have applications in transportation, surveillance, con-
struction, manufacturing, space applications, and haz-
ardous operations. Low cost vision technology could also
be extremely valuable in consumer electronics. Track-
ing systems could be incorporated into camcorders or
surveillance systems. Face recognition, person detection.
stereo and motion algorithms could be incorporated into
intelligent nightscopes and binoculars. Low unit costs
would be particularly important in this area.
Too date, we have developed systems for tracking and
following moving objects, detecting obstacles, proximity
detection using stereo (see Figures 10 and 11), following
corridors, and recognizing nods and shakes of the head.
All systems run in real time on inexpensive commputers
such as Macintoshes. All systems use very low resolution
processing (54 x 48 or less). A number of optimizations
are shared between two or more systems, suggesting that
some amount of recycling of design time is possibble.
The particular prototype system is an indoor navign-
tion system for a mobile robot that runs at 15 frames
per second on stock hardware. A computer equivalent
to the one on board the robot can be purchased for S.4-
4K. At present, the robot is capable of following corri-
dors, avoiding obstacles, recognizing corridor junctions.
navigating from point to point, and detecting the pres-
ence of people. The corridor follower is extremely well
tested, having seen hundreds of hours of service. Th,-
other capabilites are newer and have not yet been fnlly
evaluated. An article by Horswill in these proceedings
further describes the approach.
Although a primary focus has been on recognition and
navigation, we continue to develop methods for early
processing of visual information.
Ancona (CSATA, Italy) and Poggio have shown that a
new technique exploiting 1D correlation of 2D or even
[D patches between successive frames may be sufficient
to compute a satisfactory estimation of the optical flow
field. The algorithm is well-suited to VLSI implemen-
tations and a patent application is being filed by M]T
and (CSATA. The sparse measurements provided by the
technique can be used to compute qualitative properties
oof the flow for a number of different visual tasks. In
particular, they also showed shows how to combine the
1D correlation technique with a scheme for detecting ex-
pansion or rotation [37] in a simple algorithm which also
suggests interesting biological implications. The algo-
rithm provides a rough estimate of time-to-crash. It was
tested with good results on real image sequences.
CClay Thompson has recently completed a Ph.D. thesis
that considers the problem of fusing two computer vision
methods, using variational methods. The example algo-
rithms solve the photo-topography problem; that is, the
algorithms seek to determine planet topography given
two images taken from two different locations with two
different lighting conditions. The algorithms each em-
ploy a single cost function that combines the computer
vision methods of shape-from-shading and stereo in dif-
ferent ways. The algorithms are closely coupled and take
into account all the constraints of the photo-topography
problem. One such algorithm, the z-only algorithm, can
accurately and robustly estimate the height of a surface
from two given images.
Feature-based methods for recovering the motion of a
camera from a sequence of images have suffered from the
inability of the early vision processes to provide dense,
robust features. Typically, the features, such as Canny
edges, are very sparse in each image. Furthermore, most
features are unreliable in the sense that they often dis-
appear from one frame to the next. Similarly, sporadic
features often appear that are not associated with any
object in the scene. To address these problems, Ron
Chaney has developed a framework for early vision pro-
cessing that leads to a dense, robust set of features. The
early vision framework is based on the interpretation of
the Laplacian of (Gaussian (Lo(4) filter as a matched filter
for features of a particular size as well as an edge locator.
An object in the image that has roughly the optimum
width associated with a particular LoG filter will typi-
cally be nearly surrounded by the zero-crossings of the
Lo(4 filter. Hence, a naive approach would be to take
the regions bounded by the zero-crossings of the Lo4
filter as the set of features. Of course, the regions asso-
ciated with nearby objects in the image tend to merge
or iblend together. To alleviate this problem, we intro-
duce a stable, robust decomposition of such regions into
their salient subparts. These subregions, called snple
regon features, serve as the feature set for higher level
processing. The decomposition is based on the medial
axis skeleton of the region. Each subregion corresponds
to a portion of a branch of the skeleton; each branch is
divided at positions where the distance from the skeleton
to the bounding contour is minimized. To facilitate the
computation of the decomposition. a novel scale-space
is introduced for contours and the medial axis skeleton.
The scale-space is parameteric with the complexity of
the contour or the skeleton. The complexity measure of
the skeleton is the number of branches. A related cotm-
plexity measure of a contour is the number of extrema
of curvature of the contour. This leads to a complexity
scale-space for the region decomposition, The result of
the early vision process is the set of simple region fea-
tures for each frame. These features are dense, stable,
and robust. To demonstrate the utility of the early vi-
sion process, we present a relatively simple motioti and
structure-from-motion algorithm based on tracking sim-
ple region features at multiple resolutions of the Loi
filter.
C'omputing relative orientation is an important problemm
for calculating depth from binocular stereo and for de-
termining general camera motion. Lisa Dron has been
exploring methods for establishing the complete design
of a small, autonomous system with specialized VLSI
hardware for computing relative orientation in real-time
[14]. Such a system would be suitable for mounting on
mobile or remote platforms that cannot be tethered to
a computer and for which the size, weight and power
consumption of the components are critical factors.
There are two parts to this work. The first is theo-
retical and involves developing and adapting algorithu-
for finding point correspondences and solving the motion
equations which are robust as well as simple enough to
be easily implemented in hardware. The second part is
engineering and involves the design, fabrication and test
of prototype chips for the specialized processors which
will be used to find the point correspondences. Two sep-
arate processors are needed: one which computes a b-
nary edge map from the input image data, and the other
which determines translational offsets between patches
of the edge maps from two different images. Fabrication
of these circuits is done through MOSIS.
In support of such work, Dron has already developed
the edge detection algorithm known as the multi-scale
veto (MSV) method [15]. During the past year. she
has completed the design of a two-dimensional proces-
sor which combines (CCCD and (CMOS technology to im-
plement the MSV algorithm. A test chip containing a
4x4 two-dimensional array has been fabricated and -
currently being tested. Algorithms have been developed
loth to perform matching with the binary edge signals
produced by the MSV chip, and to solve the mot io
equations with a least-squares method suitable for itnpl:-
mentation on a programmable digital microprocessor. l
addition, Dron has developed a least-squares algorithm
to determine the internal camera calibration parameters.
which are required in order to compute motion from a set
of point matches, using a sequence of images for which
the translational motion is known. A preliminary de-
sign, comprising both analog and digital components.
hhas been completed for the second processor which will
compute point correspondences from the edge maps, and
have sent out for fabrication a set of test structures which
will form the basis of the matching circuit.
In related work, Gideon Stein has developed a simple
method for internal camera calibration for computer vi-
sion systems. It is intended for use with medium to wide
angle camera lenses. With modification it can be used
for longer focal lengths. This method is based on track-
ing image features through a sequence of images while
the camera undergoes pure rotation. This method does
not require a special calibration object. The location of
the features relative to the camera or to each other need
not be known. It is only required that the features can be
located accurately in the image. This method can there-
fore be used both for laboratory calibration and for self
calibration in autonomous robots working in unstruc-
tured environments. The method works when features
can be located to single pixel accuracy but subpixel ac-
curacy should be used if available.
In the basic method the camera is mounted on a ro-
tary stage so that the angle of rotation can be measured
accurately and the axis of rotation is constant. A set of
image pairs is used with various angular displacements.
If the internal camera parameters and axis of rotation
were known one could predict where the feature points
from one image will appear in the second image of the
pair. If there is an error in the internal camera param-
eters the features in the second image will not coincid.
with the feature locations computed using the first im-
age. One can then perform a nonlinear search for camera
parameters that minimize the sum of distances between
the feature points in second image in each pair and those
computed from the first image in each pair, summed over
all the pairs.
The need to accurately measure the angular displace-
ments can be eliminated by rotating the camera through
a complete circle while taking an overlapping sequence
of images and using the constraint that the sum of the
angles ust equal 360 degrees.
The closer the feature ob jects are located to the cam-
era the more important it is that the camera does not
undergo any translation during the rotation . A method
is described which enables one to ensure that the axis
of rotation passes sufficiently close to the center of pro-
jection (or front nodal point in a thick lens) to obtain
accurate results.
Stein shows that by constraining the possible motions
of the camera in a simple manner it is possible to devise a
robust calibration technique that works in practice wth
real images. Experimental results show that focal length.
aspect ratio and lens distortion parameters can be found
to within a fraction of a percent. The location of the
principal point and the location of the center of radial
distortion can each be found to within a few pixels.
In addition to the first method a second method of
calibration is presented. This method uses simple geo-
metric ob jects such as sphheres and straight lines to find,
first the aspect ratio, then the lens distortion paratneters
and finally the principal point and focal length. Cali-
bration is performed using both methods and the results
compared.
T wo other recently complete theses, reported in detail
in earlier reports are Steve White's work in highly accu-
rate representations for early vision, especially edges and
stereo disparities [54], and Subirana's work on recogni-
tion and representation of flexibble objects [45].
Median window filtering is a simple non-linear tech-
nique for reducing image noise while preserving sharp
discontinuities. It works by replacing the current value
of each image pixel with the median value of the pixel's
local neighborhood. Although the technique has been
extensively used for smoothing scalar image data like
grey-level intensities, little work is known about median
filtering in multi-dimensional data domains like image
color, image texture and motion fields. Perhaps this is
because the sample median is an ill-defined concept for
multi-dimensional quantities. Recently, Sung has pro-
posed a novel interpretation of the median concept fr
multi-dimensional metric spaces, The interpretation
follows from a mathematical property of the scalar me-
dian, and the basic idea is to similarly define the multi-
dimensional median as the sample member that mini-
mizes a mean absolute error term. Sung implemented
a multi-dimensional median filtering algorithm for color
images and showed that the operation indeed preserves
edges while reducing noise. He has also mathematically
derived that in the best case, the smoothing performance
of multi-dimensional median filtering is comparable to
that of local averaging. More recently, he has also de-
veloped algorithms for approximating multi-dimensional
medians that run in linear time with respect to data di-
mension and sample size.
Over the past twenty to thirty years, a number of djffor.
ent techniques have been proposed for segmenting iun-
ages into piecewise uniform regions. Like many other
computer vision tasks, most of these techniques contain
at least a few thresholds and operating parameters whose
values are crucial for producing reasonable results. (Oft+n
however, these values are determined either empirically
or by guess. Kah Kay Sung has explored an alternative
approach to the threshold selection problem for a simple
bsut fairly general class of uniformity based region find-
ing paradigms. He proposed a statistical formulat ion
for uniformity based region finding as a series of ''coonfi-
dence'' and ''significance'' tests, where each test roughly
eorresponds to a decision procedure in the original region
Rnding paradigm. The main advantage of his approach
that it replaces typical region finding thresholds and
parameters with a new set of confidence and significance
thresholds and parameters that gives greater insight to
the system's pertinent characteristics. A color region al-
gorithm, based on his formulation, was implemented on
the parallel Connection Machine.
Tnder separate contract, Tomaso Poggio and colleagues
have been developing techniques for the application of
learning methods to vision problems. In particular,
building on extensive earlier work by Poggio and collabo-
rators on the use of (4eneralized Radial Basis Functions,
they have been developing learning methods for use in
object recognition and computer graphics.
Under separate contract Berthold Horn and colleagues
have continued to developed VLSI implementations of
low level visual algorithms.
The problem that this chip solves is that of computing
the direction towards which a camera is moving, based
on the time-varying image it receives. There is no re-
striction on the shapes of the surfaces in the environ-
ment; only an assumption that the imaged surfaces have
some texture, that is, spatial variations in reflectance.
It is also assumed that the camera is stabilized so that
there is no rotational motion.
Once the translational motion has been determined,
it is possible to estimate distances tc points in the scene
being imaged. While there is an ambiguity in scale, since
multiplying both distances and speed by some constant
factor does not change the time-varying image, it is pos-
sible to estimate the ratio of distance to speed, This
allows one to estimate the time-to-collision between the
camera and ob jects in the scene.
Applications for such a device include systems warn-
ing of imminent collision, obstacle avoidance in mobile
robotics, and aids for the blind.
The projection of the translational motion vector into
the image is called the focus of expansion (FOE). It is the
image of the point towards which the camera is moving,
and the point from which other image points appear to
be receding.
The method used is based on least squares analysis -
that is, find the point in the image that best fits the
observed time variations in brightness. The quantity
minimized is the sum of squares of the differences at
every picture cell between the observed time variation of
brightness and that predicted, given the assumed posi-
tion of the FOE and the observed spatial variations of
brightness.
The minimization is not straightforward, because the
relationship between the brightness derivatives depends
on distance to the surface being imaged and that dis-
tance is not only unknown, but varies from picture cell
to picture cell. It turns out that so-called stationary
points, where brightness is constant (instantaneously).
play a critical role. If there were no measurement errors.
quantization effects or noise, then the F(OE would be
at the intersection of the tangents to the iso-brightness
contours at these stationary points.
In practice, image brightness derivatives are hard to
estimate accurately given that the image itself is quite
noisy. Hence the intersections of tangents from differ-
ent stationary points may be quite scattered, Reliable
results can nevertheless be obtained if the itmage con-
tains many stationary points and the point is found that
has the least weighted sum of squares of perpendicular
distances from the tangents at the stationary points.
This method was chosen from amongst a group of
competing approaches by considering both simulation
results of these methods and constraints of what can
reasonably be built in analog VLSI.
The amount of computation for every picture cell (in-
cluding a number of multiplications) is such that it is not
feasible today to perform the task in a totally unclocked
manner with the processing done at each picture cell.
Instead a row parallel scheme has been decided upon
where each row of the image has a single processor.
The first chip has been made by MOSIS and tested.
Minor revisions are being made.
A problem in motion vision that is somewhat more dif-
ficult than that of recovering the focus of expansion is
that of recovering both translational and rotational com-
ponents of motion of a camera from the time-varying
image. Presently there is no simple robust method for
solving this problem in general, but methods are known
in the special case that the surface being viewed is pla-
Applications for such a system include landing a ve-
hicle on a planar surface and station keeping of a sub-
mersible vehicle above the ocean floor. Also, such a sys-
tem could be used to recover the motion of a person by
aiming a camera at the flat ground in front of the per-
son. The motion estimates obtained from such a down-
ward looking camera could then be used to interpret the
time-varying image from a second camera aimed directly
forward. The resulting system could be an aid for the
blind that warns them of obstacles - even those that do
not have a support directly below - such as signs hanging
from beams supported off on the side.
Here also the proposed method involves a least squares
approach, although it is now considerably more complex
than in the case of simple translational motion. It is
known, for example, that there is an ambiguity in that
two quite different motions, and corresponding different
suurface orientations, can yield the same time-varying im-
age.
Detailed design will have to await the results of ex-
tensive simulations.
John Harris and Prof. Poggio are studying analog imple-
mentations of vision and learning algorithms. They are
interested in analog models because these models pro-
vide a novel mechanism for understanding and develop-
ing algorithms. Experimentation with these continuous-
time nonlinear circuits facilitates algorithm intuition and
leads to fundamental insights. Powerful analog algo-
rithms thus developed will prove useful even if a re-
searcher is limited to simulating the analog hardware on
a digital computer. In addition, biology has motivated
some of the circuits and, conversely, some of the VLSI
modules may help develop a better intuition for solutions
that biology has found for the same class of early vision
problems.
A real-world vision system must be adaptive in order
to operate in a unconstrained environment. The system
must be smart enough to deal with such nonidealities
as changing light conditions or slight variations between
components. For example, the thresholds for detection
of edges in edge detectors should dynamically change
with the brightness of objects in the scene. Or the ap-
propriate space constant of resistive network could by
dynamically determined by an estimate of the noise in
the input signal. Analog hardware allows for adaptation
in many instances by relying on basic physics to perform
the necessary computations. One specific pro ject un-
der implementation is the time-to-contact motion sensor
proposed by Poggio (1991) and Poggio & Ancona (1992).
This sensor combines the outputs of 1D motion correla-
tion sensors to produce an estimate of the amount of time
until crash (assuming constant velocity). The small, low-
power implementation will be useful as a crash-warning
sensor for robots or automobiles.
In this paper we introduce a new approach to build-
ing models for main roads in aerial images and a
new computation approach to finding them. The
goal is a completely automated sysyermä. The idea is
that this appr9ach could then be extended to finding
other types of objects in areal images of the ground.
Ip recent years a number of papers have appeared in
the published literature dealing with semi-dutomatic
extraction of roads from aerial photos. In general a
human operator gives the road starting points and
the road'directioiis at the startig poiifs. This is
a kugg help to the roal finding alfoiithm. This in-
teraction has been necessary becuse road images
can be very cogplicated. Examination of just ihe
two images in this paper, Figs 6 and 8, reveals that
94ge intensity accross a road can yary noticeably.
Theie may bg a barrier running along the center
of the roaß. Road- width can vaiy appreciably, es-
pecially when a barrier is present. Ifnage intensity
edges along s road boundry may disapear, espe-
cially when there is a building entrance'with a very
small parking area alongside'the road. The image
intensity structgre at roäd intersections can be veiy
complicated. There may be gars or markings on
roade, @;sbadows cast by buildings or tesT'ete,,
etc.. Three major types of road finder were used:
gdge linkers, correlation trackers, region based fol-
lowers.
Edge linkers, based on an edge operator output for
m4gnjtude and angle for each point in the' image
and'hen linking üi eiges scoofdlig w- sois'äiG.
ria, were used first by [6] and later by [8] and by [2].
Correlation trackers based on the assumption that
there exists a pattern or texture on the road surface
was used first by [7], and later by [2] in combina-
tion with edge linkers. A region based method as-
suming constant intensity in the region and in the
background was used first by [4] with a correlation
follower.
In [l] and [3], a Bayesian approach to low-level
boundary estimation and object recognition was in-
troduced. The problem considered ii this paper is
the automatic extraction of main roads when road
curvature, width, image intensity and edge strength
can vary considerably and when a barrier along the
road center may or may not be present. The ap-
proach is geperal, and we feel that it can be extended
tg handle' the full range of road image variability.
The approach is to build geometric-stöchastic mod-
els for representing road ipmages, and then use maxi-
mum aposteriori probability estimation for estimat-
ing the road boundaries (and other important fea-
tures) in an image. The modeling approach forces
the designer to model all significant phenomena, and
the model is generative so that its representation
power can be assessed. The map estimation provides
for the most agcurate road finding. Global map es-
timation is realised in a computationally reasonable
way by using dynamic prograpmming to search small
windows to obtain initial road candidates, and then
dynamic programming again with small windows in
order to obtain global esfimates.
We build a geometric-stochastic road model based
on the following assumptions:
A stochastic process model is built exhibiting the
preceding behavior. Specifically, autoregressive pro-
cesses are designed to model road center line, road
width, grey level within the road, edge strength at
the road boundary,and regions gutsidethe roads and
adjacent to the boundraies. We refer to these re-
gions as background. Note that the road geometry
processes are hidden, i.e, they are not observed di-
rectly in the data. For this purpose, consideration is
restricted to an N x N pixel window. The stochas-
tic processess are function of a discrete parameter
i which can be thought of as time or distance, in
pixels, along a horrizontal or a vertical axis. As an
example, gonsider Fig 1. The i axis here is horri-
zontal. The road center line at i is z; which takes
values in the vertical direction. This variable is not
quantized, it takes arbitrary real values. The [z;)
process is given by (la), where ., is a zero mean,
white, Gaussian driving noise. This process is de-
signed to generate a straight line if the driving noise
iszero.
Equation (2) describes road width, where d; is the
perpendicular road width through the unquantized
center point [z;,i]. The perpendicular direction is
measured as perpendicular to the line segment from
the point [s--a.i - 2] to the point [s.,i - 1].
The stochastic processes <e, and <a, are independent
Gaussian white noise sequences with zero means and
variances d.; and da,, respectively. The road ob-
tained for the unforced solution (sz; = 0,ed; = 0
) will be two parallel line as seen in Fig 1. Road
boundary location are uniquely detemined by the z,
and di, The road boundary location on the grid lo-
cations are determined as in Figure 1; where if;, 1
denotes the upper unquantized boundary and i;.5
denotes the lower unquantized boundary.
We use a second order Markov Process to model
the mean intensity of the image data in sequential
vertical strips of the road to be consistent with fea-
ture 3 of our road model.
The variable ur is the mean intensity in column i
of the window, and s,, is a Gaussian white noise
sequences with zero mean and variance d4,, and is
independent of the processes s., and s2,. The inten-
sity varies across the vertical direction of the road
too. We model this by adding an additive white
noise.Therefore, the observed pieture function at the
(j,i)th pixel (jth row, ith column ) in the road is y;,
given in (4), where n;; is Gaussian white noise with
zero mean and variance o'.
Assume for now that the we deal with a step edge (
we also deal with the blur edge but this is more in-
volved). The observation image intensities Ve,s+1.-
and Ve,,-1, immediately outside the lower and up-
per road boundary respectivly are determined by :
where r; and r; are random variables, taking values
S:1 with equal probabilities, ?, and ,, are i white
Gaussian iid sequence having some nonzero mean.
The purpose of r; and r; are to model whether each
of the background grey level is lighter or darker than
that in the road.
Assume that the image intensities in the background
regions above and below the road boundaries are
modeled by different Markov processes. These are
causal Gaussian AR ( autoregressive) models. The
lower background AR model is:
where s) are the model parameters, t4;; is the mean
value function, and y,, ls a Gaussian white noise
driving sequence with zero mean and variance 4y,,:
Note that this model geperates image data jp raster
scan mode - left to right top to bdttom. The con-
ditional distribution of the pocess at pixel j,i given
the previously generated process depends only on
the three pixels' immediately above and to the left
of j,i, A similar model generates the background
above the road, but here' the proces generation is
left to right, bottom to top, The reason for using
causal AR models is that they are computatioally
well suited to the dynamic programming road esti-
mation algorithm used. With the specified road edge
and background models, it is now possible to specify
the probabilty of the background data. We are ex-
pecting to detect roads having lengths between l---o
to --as with uniform distributioi. That feature is
very usefully in the high level processing, whereas
the other features are tmore useful in the low level
processing, In fig 2 are displayed various synthetic
images which wee generated by the road model de-
scribed in this section.
Our general framework for viewing road finding is
to estimate the geometry of the road by formulating
the problem as map estimation. We look for thaf
which P(hypothesized road modeljimage data)
is a maximum . Since this posterior likelihood of a
hypothesized road model given the image data can
be written as
and the denominator is not a function of the hy-
pothesized road model , the road model estimate
can be found as that which maximizes the numera-
tor, ie, the joint likelihood of a hypothesized road
model and the image data. The map estimation is
handled by partitioping the image into windows, re-
alizing the estimation parameters in each window
through the use of dynamic programing (detailes are
given in [5]). Our approach also includes of desion
rules for starting points of a road in a window, and
stopping points of a road in a window.
In the low leyel processing we are searching for seeds
of roads. The image is divided into square win-
dows, N x N pixels in each window, and the sys-
tem searches for road candidates that fit the road
model with high probability by using the dynamic
programmingstructure. We run the road search four
times, with s separate search starting at each of the
four sides of the window, to pass over all the possi-
bilities of road ggeometries. In Fig 3a we represent
those examples. There may be more than one can-
didate road within a window. This is handled by
searching a window for the best road, and then re-
peating a window search for the next best road. We
do not want the second-best road to be a variation
of the best road. This is handled by not permitting
boundary points for the best road also tobe bound-
ary points for roads ig subsequent searches. In this
way, the system will find a pair of roads such as in
figure 1b. The procedure is' repeated until all road
candidates are found. Fig 3b illustrate another ex-
ample, road junction where a main road forks into
twp roads. Ip a first seargh starting on, 3e pight
gide gf the window roadl between points H, K,'E,
F will be found, then road2 between points L, M,
F, G will be found, and road3 between points A, B,
C, D will be found, by a search startinig at the lefi
side of the wipdgw. The splitting arearepresented
by points C, D, E, F will iot be found at this level
.'The hidden structure for every road candidate is
found and observed to the high- level processing.
A high level processing stage is now used to extend
each road candidate produeed by the low level win-
gge search ip order to obtain global road gstimates,
This is done by using shifting windows, as illustrated
in Fig 4, where a new window is introduced at an end
of a ioad candidate centered on one of the sides of
the window. The best extension, of the road candi-
date, through the window is estimated by using the
dynami( programming algorithm. This process is
repeated until the stoppingg criterion stops the road
seargh or until the eitimäted road hits the image
border. Ip the process of extending a road through
a new shifted wipdow, the dynamic programming äl-
gorithm begins by using the last estimated state of
fhe road and the associited road image data. Upon
termination of the road search, if the' length of the
estimated road from the initial road caiididate is
greater than a threshold, the estimated road is ac-
cepted. If the length is less than the threshold, the
estimated road is iejected, unless there is other sup-
porting evidence. Supporting evidence we have used
in the experiments is that if three long roads enter
an intersection and the short road is adjacent to the
intersection and appears to be an extension of one
of them, i.e., has roughly the direction and width of
one of them, then accept the short road.
This section describes the results of road finding in
the synthetic images (Fig2), and two different real
images called, Rad1 (Fig6) and Rad2 (Fig8) that
were obtain from the Radius Program. The goal is
to find the roads in tbe real and' synthetic images
using our approach. The image field is partitionned
into an array of square windows (32x32). The road
finder runs simultaneously within the windows to
find initial road seeds in the images (section 3). It
then combines all the local results to obtain the fi-
nal maig roads in the images by using the high-level
approach. Using the low level approach only'for the
synthetic images, the results in Figure 5 were ob-
tained. The fecognized road boundary points are
indicated by black dots. The recognized' roads for
Radl are indicated in (Fig7), (detailes are given in
[5]). The recognized barriers and roads for Rad2 are
indicated in (Fig9), The system starts first with rec-
pgnising the road barriers, using knowledge that the
bärrjgr intensity is ]ighter than fhe road särrounding
it, The initial oad barrier seeds ip the image were
obtain by low-level processing within the windows,
and with the high-level following stage it combines
the local results to obtain the final barrier (detailes
are given in [5]). Each side of the barrier is bordered
by a road; therefore, by knowing the boundaries of
the barriers, the system also knows the correspond-
igg boundaries along one side of each main road.
TEe other boundaries along the second side of each
road are estimated by using the high-level approach
for road finding.
The automated compilation of man-made and
natural terrain in urban or built-up areas has been
the focus of our research for a number of years.
Built-up areas provide some of the most difficult
and time consuming tasks for the cartographic
community, and provide a rich environment of
varied structures and natural terrain features to test
the robustness of new approaches to computer
vision. The theme of our research is to understand
the computational aspects of automated recovery
of three-dimensional scene information using a
variety of image domain cues. These cues include
the analysis of cast shadows, stereo matching,
geometric models, and structural descriptions
based upon analysis and combination of low-level
image-based features. We look for opportunities to
augment traditional computational vision
techniques with domain knowledge since it is
clearly used by both cartographers and imagery
analysts in a variety of tasks ranging from mapping
to environmental land use analysis to natural
resource inventory.
In this paper we provide a status report in a variety
of research activities. Section 2 describes recent
work in the application of rigorous
photogrammetric methods to image orientation and
building extraction within the context of the
DARPA RADIUS program. We also describe recent
results using our stereo matching systems
[McKeown and Hsieh 92, Hsieh et al. 92] on
model board imagery. A companion paper in this
proceedings describes the incorporation of
vanishing point geometry into the BABE building
extraction system [McGlone and Shufelt 93]. Such
geometric analysis is a key requirement for
cartographic feature analysis for oblique imagery,
particularly in the fusion of results from multiple
views,
Section 3 describes new research in developing
symbolic and geometric descriptions for buildings
using multiple cues such as stereo, shadow
analysis, and monocular building detection. The
goal is to detect those regions in the disparity map
created by stereo matching that correspond to
buildings. Given that structures may appear on
rolling terrain, and that stereo analysis rarely
constructs an error-free model of the scene, simple
techniques based upon region analysis must be
augmented with other sources of information.
Section 4 details an experiment in measuring the
effectiveness of human-computer interaction to aid
in building detection. While most of our research
has focused upon automated end-to-end analysis,
there is a role for user interaction at various stages
of the cartographic process. Some preliminary
results are presented in interactive building
selection using a simple pointing method.
Section 5 describes a continuation of previously
reported research toward the development of
improved techniques for a terrain representation
called a triangular irregular network (TIN) [Polis
and McKeown 92]. A new point selection
algorithm is described and results for the
generation of a 2500 kilometer square area of the
National Training Center (NTC), Fort Irvin,
California, are shown.
Research on knowledge acquisition and refinement
for rule-based systems used for the interpretation
of aerial imagery is reported on in Section 6.
Using a large production system architecture,
SPAM, we report on ongoing research in the
evaluation of the utility of various sources of
knowledge for airport scene analysis.
Finally, in Section 7 we briefly describe current
research in multispectral analysis to determine
surface material properties as a knowledge source
of built-up area segmentation and cartographic
feature extraction. A detailed description of
performance evaluation for two classification
techniques can be found in a companion paper
[Ford et al. 93] in this volume.
Our goal is to apply rigorous photogrammetric
methods in several areas of research, particularly to
improve the extraction of geometric cues, and to
relate partial object descriptions across multiple
images. One of our first applications has been the
incorporation of vanishing point geometry into the
BABE building extraction system, as discussed in
[McGlone and Shufelt 93] in this volume.
In this section we discuss our research using the
RADIUS modelboard imagery. Under the RADIUS
program a set of images of a synthetic industrial
site, represented by a scale model, were created
and distributed to the RADIUS community. These
images differ from more typical mapping
photography used in cartographic research in that
they are taken from oblique angles rather than
near-nadir (down looking) mapping cameras.
Along with the imagery a set of ground control
points with known modelboard coordinates were
distributed.
Using this imagery we have addressed two major
areas. The first is the implementation of a rigorous
central projection camera model and the solution
for the camera parameters for the modelboard
imagery. The second is the evaluation of our
current stereo matching techniques developed for
traditional mapping imagery using the modelboard
imagery.
Our work to date has been focused on an initial set
of eight images, J1 through J8, of the modelboard
industrial site. Figures l and 2 are two overlapping
areas, from images J5 and J4 respectively, and
illustrate typical scene content. In order to obtain
valid position and orientation parameters for the
images we implemented a standard
photogrammetric resection procedure.
A relatively large number of modelboard control
points (70-80) were measured in each of five
images, J3 through J7. We scaled the RADIUS
modelboard control point coordinates into world
units using the modelboard scale information,
given as 1:500. In order to better integrate with
our existing landmark database software
[McKeown 87] we transformed the modelboard
coordinates into pseudo geodetic (latitude-
longitude) coordinates. The coordinate origin of
the modelboard imagery was taken to be
somewhere in central Kansas. For each of the
images we performed an individual image
resection to establish an error measure based upon
RMS image displacement of the measured points.
In addition we performed a simultaneous block
adjustment of the modelboard images using all of
the measured points. Simultaneous resection of the
images in the same adjustment allows better error
detection, due to the higher redundancy in the
solution, and gives orientation parameters that are
more consistent between images.
Results of the individual and simultaneous
adjustments of images J3 through J7 are shown in
Table 1. One can see a fairly consistent residual
error of about 2.3 pixels in these resections.
Further refinements of our camera model may
improve this situation, but at the current scale of
the modelboard photography these errors
correspond to about a three foot displacement in
ground position. Sources of error include
uncertainty in the modelboard ground control
locations, errors in the measurement of these points
in each of the modelboard images, and unmodeled
distortions resulting from the image formation
process. It remains to be seen how such errors will
effect the accuracy of cartographic feature
descriptions, such as buildings, whose models are
composed from partial object descriptions acquired
from multiple views of the scene.
An important output of the resection solution is the
precision information obtained on the orientation
parameters, which can be propagated to estimate
the precision on calculated ground coordinates,
distances, or heights. These precision estimates
will in turn allow us to more meaningfully control
and merge various operations.
The image resection parameters are used
pervasively in our research. For building
extraction from the oblique aerial imagery, the
vanishing point information is directly calculated
and exploited as described in [McGlone and
Shufelt 93]. In the stereo processing the image
orientation parameters are used to precisely
resample the images into epipolar geometry and to
calculate elevations and heights in the scene. The
incorporation of precise camera models, resection
information, and precision information into other
applications is now in progress.
The RADIUS modelboard imagery presents several
new complexities in the interpretation of aerial
imagery. The emphasis on oblique views breaks
some of the basic assumptions built into processes
that analyze and interpret near-vertical stereo pairs.
In order to establish a performance baseline for our
stereo analysis systems we processed the two
stereo pairs (J4-J5) and (J6-J7) found in the initial
release of the RADIUS modelboard dataset. We
used our standard orientation methods developed
for near-vertical imagery taken along a single
flightpath [Perlant and McKeown 90] as well as
our new image orientation system based upon the
resection results reported in the previous section.
Both pairs, (J4-J5) and (J6-J7), are relatively wide
angle stereo, with convergence angles of 60 and 25
degrees, respectively. For the examples in this
section we show results using the 60 degree pair
because it represents an extreme case with respect
to our previous work.
Most stereo systems in cartographic analysis
assume that the stereo pair is in a collinear epipolar
geometry. We use two independent stereo
matching systems of this type. The first, S1, is an
area-based method that provides good figural-
continuity and captures a sense of foreground and
background. It works in a hierarchical coarse-to-
fine fashion in order to capture as much global
continuity as possible while retaining a locally-
based process. Its results are best in smooth
textured areas, but tends to smooth (blur) abrupt
changes in depth.
The second, S2, is a feature-based method that
provides a more accurate estimate at a few
points---especially near depth discontinuities, but
requires interpolation to ''fill in the gaps.'' This
process also uses a hierarchical coarse-to-fine
approach, but matches ''waveform'' features
across (epipolar) scanlines rather than a correlation
window. To remove false matches this process
uses a inter-lintra-scanline consistency check
[McKeown and Hsieh 92].
The results of the two stereo processes are refined
using a monocular segmentation of the original
intensity image into homogeneous regions. This
process first merges the disparity results from each
stereo method using a common estimate of
''goodness'' to select the best match; however, if
there is a large disagreement between the two
methods, then both estimates are suppressed.
Within each region of the segmentation, which is
assumed to represent a single continuous patch of
surface, the disparity values are averaged and the
outliers are removed. Two different segmentations
are used to limit the formation of artifacts during
this process [McKeown and Perlant 92].
Epipolar resampling, that is, resampling a stereo
pair of images so that the epipolar lines run along
the rows of the image, is a requirement for our
stereo matchers, as it is for most existing computer
vision systems applied to aerial imagery.
Unfortunately, the resampling that is typically
performed uses approximate warping techniques
that may be adequate for vertical images but may
fail for imagery with severe obliquity. We have
implemented a rigorous epipolar reprojection
routine that transforms a given stereo pair into the
required geometry using the full orientation
parameters for the images.
As an experiment we generated epipolar aligned
imagery using two different techniques. First, we
established a baseline registration by performing a
relative orientation of the RADIUS modelboard
imagery by finding common scene points in each
of the stereo pairs. A polynomial orientation was
performed giving an approximately collinear
epipolar alignment [Perlant and McKeown 90].
The second orientation was performed using a
rigorous epipolar reprojection based upon the
modelboard resection.
Both S1 and S2 were run using both the polynomial
orientation and the resection reprojection on the
RADIUS J4 and J5 stereo pair. Figures 1 and 2
show the left and right image pairs. Figures 3 and
4 show the stereo disparity results after refinement
using the polynomial orientation. The disparity
results are encoded such that bright areas are
higher than dark areas.
Using the polynomial orientation one can easily
see areas of mismatch, especially for the area-
based S1 process. This was mostly due to the lack
of a precise alignment of the epipolar lines. In
addition, the large number of occluded regions
caused several mismatches by the feature-based S2
matcher. In many cases the correct match between
features had an opposite intensity contrast, which
violates one of the current S2 constraints. This was
less an issue of registration and due more to the
imaging geometry.
Figure 5 shows the reprojection of the original left
image in Figure 1 such that the camera axis is
perpendicular to the stereo baseline. Both the left
and right images were reprojected into epipolar
alignment. One can notice a change in shape of
the scene due to the obliquity and the angle
between the original camera axis and the stereo
baseline. In this case the change was minimal
since the baseline was nearly horizontal.
Figure 6 shows the result of s2 matching and
refinement using the resection orientation.
Although the results in Figures 4 and 6 are not
directly comparable due to the reprojection, one
can see significantly more structure in the
buildings in the upper left corner of the scene.
Figures 7 and 8 show perspective views of the
modelboard reconstruction and also highlight some
of the differences between the two orientation
techniques. Quantitative analysis of the stereo
accuracy along the lines of [Hsieh et al. 92] will be
performed over a set of test cases.
As discussed in Section 2.1 we have applied a
rigorous photogrammetric approach to the problem
of obtaining a more exact collinear epipolar
alignment of the stereo images. We still need to
determine how much tolerance our stereo
algorithms exhibit with oblique imagery. With the
larger angle oblique images, we will need to
consider methods to deal with the large occluded
areas and the large baseline-to-range ratio.
We have observed that the stereo refinement
process greatly improved the disparity results in
both of the modelboard image tests. We plan to
introduce additional sources of information in the
stereo process, such as wall and roof hypotheses
generated by monocular analysis, in order to help
guide and refine the stereo matching.
The interpretation of stereo disparity maps to
detect and delineate manmade structures contained
within is a difficult problem. Our recent research
has been addressing the detection and extraction of
buildings using stereo analysis together with
monocular cues. The goal is to produce full three-
dimensional models of complex buildings for site
model construction and update. Our approach is to
apply the cooperative-methods paradigm starting
with the results generated by the stereo analysis of
a pair of aerial images and, together with
monocular cues, mark those areas of the image that
appear to be structures.
Our first step is to obtain a set of refined stereo
estimates of the scene. This is obtained by using
the S1 and S2 stereo matching systems coupled with
disparity map refinement as described by
[McKeown and Perlant 92]. In the course of the
stereo refinement process an intensity
segmentation is produced. This segmentation is
used as the basis for subsequent processing.
The second step is to merge those segmented
regions that have approximately the same disparity
and that are adjacent. Next, those (merged)
regions having a significantly greater disparity than
their neighbors are selected. The rule applied in
this step is liberal in the sense that we would rather
produce a few false positives that miss buildings at
this point.
In order to remove some or all of the false
positives, we apply heuristics and constraints
derived from monocular cues.
Finally, the remaining clusters of buildings are re-
analyzed by searching for a best fit building model
for each cluster. In addition, the shadow regions
are again used to hypothesize the location of the
shadow casting sides of each potential building,
acting as a final cluster hypothesis verification.
Some initial results are shown for a complex
industrial scene, DC38008. Figure 9 shows the
original intensity image of the left view of the
stereo pair of aerial images, while Figure 10 shows
the refined stereo result produced by the S2
matcher and used as the initial input to the building
extraction process.
Figure 11 shows the segmentation of the scene in
Figure 9 that was generated during the stereo
refinement process in which the range of intensity
within each region is f5.
These regions are used as an initial over-
segmented view of the aerial scene and adjacent
regions are merged using the mode stereo disparity
value within the region. The threshold for merger
of adjacent regions is ttl pixel. Next individual
regions are marked as potential buildings based on
their relationship to adjacent regions. According to
the heuristic rule: (1) If the mode disparity within
the region is less than the lowest of its neighbors,
then it is not considered a building; (2) If its
disparity is greater than the highest of its
neighbors, then it is given a likelihood value of 1.0
(from a range of 0-1.6); (3) If the mode disparity is
equal to both the high and low values of its
neighbors, it is allowed to be considered a building
hypothesis and assigned a value of 1.6; Otherwise,
its likelihood is calculated by the formula:
where:
Figure 12 shows the result of accepting regions
rated at 0.5 or according to the above heuristic.
Clusters of regions that are very large (more than
6000 pixels) or small (less than 100 pixels) are
removed.? This is followed by a further restriction
that all clusters that do not have a hypothesized
shadow region to their non-sunward edges are
removed. Figure 13 shows the final result after
these restrictions.
As a result of this process many of the significant
buildings are detected, with various degrees of
accurate delineation. One way to visualize the
results is to look at the differences between a three-
dimensional ground truth description, the refined
stereo disparity map, and the three-dimensional
scene that results from using building extraction.
Figure 14 shows the original scene rendered using
a hand-generated stereo ground truth estimate
14(a), the shows the refined S2 stereo result 14(b),
and using the building hypotheses generated by
this technique 14(c).
Although our initial results are promising, we feel
that no approach will reliably detect and delineate
manmade structures solely by using stereo
disparity. As a part of the cooperative-methods
paradigm we plan to include other sources of
information such as BABE building hypotheses
[McKeown 90] and surface material classification
[Ford and McKeown 92b, Ford and McKeown
92a]. In rugged terrain or in areas with significant
tree canopy additional cues will be necessary for
both the selection and the filtering of building
hypotheses. In addition, we expect that such
monocular cues, such as those generated by BABE
will play an important role in the verification and
re-analysis of region clusters during the model
fitting and labeling process.
Automated feature extraction from aerial images is
a complex problem, and research in this domain
has illustrated the difficulties in reliably detecting
and verifying building structure. Although the
ultimate goals of our work in this area are systems
which will accurately detect and precisely
delineate man-made features in aerial photography
without human intervention, it is clear that a
combination of current extraction techniques with
some degree of user guidance has the potential to
exhibit improved performance on complex
imagery.
To date, many of the semi-automated systems
require a large portion of the detection and
delineation tasks to be performed by the user of the
system. In such systems, the user interactively
manipulates a variety of models over features in
the image, fitting the models to the features
[Hanson et al, 87, Kass et al. 87]. An altenative
paradigm suggests that another approach for
developing a high-performance system is to allow
the user to lend a guiding hand during the
execution of the feature extraction algorithms.
We have been exploring possibilities for the
application of human interaction in the extraction
process. Our current testbed for this research is
BABE, a line-corner intensity based feature
extractor [McKeown 90]. In brief, BABE proceeds
through four major phases to incrementally
generate building hypotheses. The first phase
constructs corners from lines, under the
assumption that buildings can be modeled by
straight line segments linked by (nearly) right-
angled corners. The second phase constructs
chains of edges which are linked by corners, to
serve as partial structural hypotheses. The tthird
phase uses these line-corner structures to
hypothesize boxes, parallelopipeds which may
delineate man-made features in the scene. The
fourth phase evaluates the boxes in terms of size
and line intensity constraints, and the best boxes
for each chain are kept, subject to shadow intensity
constraints similar to those proposed by [Nicolin
and Gabler 87] and [Huertas and Nevatia 88].
In recent work, we have addressed the possibility
of replacing the hypothesis evaluation routine with
a simple form of user verification, in which a
person uses the mouse to drop points on each
individual structure in the scene. Then, hypothesis
evaluation reduces to determining which boxes
produced by BABE contain points placed by the
user. This level of interaction does not place great
demands on the user, and makes effective use of
the hypothesis generation capabilities of BABE;
thus, it serves as an interesting test for an
intermediate level of man-machine interaction in
this domain.
Figure 15 shows a ground-truth hand segmentation
of a suburban scene in Washington, DC. Figure 16
shows the complete set of hypotheses generated by
BABE for this scene, and Figure 17 shows the
hypotheses verified by the shadow intensity
constraint algorithms invoked in the fully
automatic version of BABE. Figure 18 illustrates
the results of a semi-automated BABE execution, in
which the shadow verification algorithm was
replaced by user selection of three points on each
building, followed by intersection of these points
with the full set of hypotheses in Figure 16. Each
of these results was then compared on a pixel-by-
pixel basis with the ground-truth hand
segmentation to generate the statistics in Table 2.
Note that we give data for a single-point user
selection example as well; we omit the
corresponding figure for brevity.
With the simple mechanism of multiple point
selection, a user interacting with BABE can achieve
a marked improvement in building detection, at the
slight expense of accumulating errors in
background classification. This is due to line
placement errors in BABE hypotheses that are
otherwise accurate descriptions of man-made
structure. Note also, however, that the total scene
classification rate remains essentially the same in
each of the three examples. This suggests that user
interaction at the verification level trades detection
rate against overall classification precision.
Given that the initial hypothesis data produced by
BABE still fails to detect 18% of the building
structures in the scene, it should be clear that more
work is necessary on the basic feature extraction
algorithms, and we intend to continue our pursuits
in this area. User interaction at an intermediate
level appears to be a fruitful avenue for further
exploration, however, and we intend to investigate
this topic further. One key issue is the
determination of the appropriate level of
interaction between a user and a feature extraction
algorithm. We also plan to experiment with user
input at other phases in the extraction algorithms,
such as corner detection, line linking, and structure
generation.
Terrain modeling is becoming an increasingly
important issue with the advent of large-scale
distributed simulations for training, mission
rehearsal, and mission planning. Such systems
rely on efficient representations for natural terrain,
as well as manmade features such as buildings,
roads, and bridges. Our recent work in this area
has focused on the development of visualization
tools for three-dimensional data, and in the
continuation of our research in triangular irregular
networks (TINs).
WitUh tUhe increasing availability of a variety of
digital spatial data ranging from map databases,
object model descriptions, digital elevation
models, and geo-referenced imagery, there is a
need to conveniently view image and vector data to
support various aspects of our research. In many
cases these datasets are best visualized in three
dimensions. Figure 19 demonstrates the difference
between viewing terrain as an intensity mapped
height field and as an overhead shaded relief
rendering. The latter process takes into account
shading from a light source and tends to make the
surface structure more apparent. Small changes in
terrain detail are enhanced and surface slope and
aspect appear more pronounced. To support our
need for 3D display of spatial data we have
developed an X/Motif application, XRELIEF, to
allow us to visualize digital elevation models and
TINs, manual ground truth segmentations,
automated stereo results, multi-spectral results, and
digital map data (ITD, DLMS) overlaid on terrain.
Figure 20 shows a sample control panel used to
specify imagery, terrain, map overlay, and viewing
parameters. Users can create and store multiple
ordered sets of camera parameters in order to
compare results from different stages of an
extraction process. They can also compare results
from different analysis methods from a single
known viewpoint. XRELIEF has an intuitive
graphical interface for control and creation of these
camera parameters, as well as positioning of an
illumination source used for shading calculations,
and simple animation support. This interface is
shown at the lower right of Figure 20. The large
and small circles represent camera lookfrom and
lookat points, and the image displayed underneath
corresponds to the image being overlaid on the
terrain.
A digital elevation model (DEM) is a terrain model
consisting of elevation data regularly spaced on a
grid. A triangular irregular network consists of
elevation data that are irregularly spaced and are
connected into triangular facets to form a surface.
The ability of the TIN to place points irregularly
permits point density to adapt to terrain
complexity, and allows points to be placed
precisely on peaks and valley floors. The TIN
terrain model is ideally suited to real-time
rendering, as it consists of a reduced set of
polygons tailored to the underlying terrain
complexity. Previous research in TIN generation
using point selection from the DEM was described
in an earlier paper [Polis and McKeown 92]. In
this section we give a brief update on our
development of a new (and improved) method for
point selection.
Our point selection method relies on the iterative
selection of points based upon successive
approximation to the actual terrain surface. At
each iteration a dense DEM is constructed by
interpolation from the current TIN. This
approximate TIN is compared to the actual DEM
and the point or points having the greatest error in
elevation are determined. These DEM points are
added to the TIN as correction points, and a new
triangulation is generated. The triangulation is
evaluated based on a global point budget and the
residual global error. If the point budget is not
exceeded and the RMS error is still greater than a
user specified error goal, then the process is
repeated. In practice the point budget controls the
stopping conditions for the TIN generation
process.
Our previous point selection method relied on the
generation of error contours and associated medial
axes. Points would be selected from the set of
maximal contours generated at each iteration. The
new point selection is based solely upon a measure
calculated at each point in the approximation
DEM. The new process chooses fewer correction
points per iteration and as a result many more
iterations are required. However, the points
chosen are of higher quality in terms of our RMS
error metric. As a result of this a fast triangulation
is necessary, so the modified greedy triangulation
has been replaced with a Delaunay triangulation.
However, the improvement in point selection
appears to outweigh the loss in triangulation
accuracy, especially since the iterative process will
naturally add points to correct poor triangulations.
In the following section we will describe the use of
this new triangulation method to generate large-
scale TINs suitable for use in SIIMNET.
A digital elevation model constructed to support a
SMINET training exercise was provided to us by the
U.S. Army Topographic Engineering Center
(USATEC). The DEM covered an area 50
kilometers on a side (2500 square km) including
the National Training Center (NTC), Fort Irwin,
California. This area is primarily desert, with
some highly eroded mountainous areas and
intricate alluvial fans running to the desert floor.
The sheer size of the area presents significant
problems. The DEM consists of 1979s1979
points, nearly 4 million elevation posts. To
maintain the desired polygon density for the
SIMNET computer image generation systems, only
90D00 points were to be selected for the TIN.
An additional complication for the TIIN generation
process was the desire for reduced fidelity in the
mountainous areas with increased detail in the
areas of alluvial fans and on the desert floor. This
was primarily driven by the fact that mountainous
areas are not accessible to ground vehicles
(simulated or otherwise) yet, due to their height
and complexity, they tend to accumulate a large
number of TIN points. This decreases the budget
available for other areas of the terrain. An overlay
indicating the mountainous areas was provided by
USATEC, and was used to produce an importance
grid. Our initial experiment was to make the
maximum error in the mountains approximately
one fifth as large as that in the low lying areas. We
smoothed the importance grid to avoid problems
that might result from a discontinuity at the
boundary of the mountainous area. Since point
selection under our new method was based solely
upon a measure calculated at each point, it was
now possible to use the importance grid to apply a
weight to each point based upon its subjective
importance.
Figure 21 shows a shaded relief representation of
the western part of the National Training Center.
The left half shows the terrain relief using the
original digital elevation model. The right half
shows the same area using the TIN representation
for the underlying surface structure. The TIN was
generated using selective fidelity in the
mountainous areas. Using approximately 2.5% of
the original DEM points we were able to construct
a TIN with an RMS elevation error of 3.1 meters
when compared to the original DEM. The range of
elevations in the DEM was approximately 1500
meters. From a qualitative standpoint it appears
that the major topographic features are generally
preserved and that detail in the alluvial fans and
desert floor areas are also quite good. This
impression was confirmed using the SIUMNET
system at USATEC and driving an M1 tank
(simulated) through the terrain.
We have shown the utility of our new TIN
construction method for a large-scale digital
elevation model. Research issues remain in
determining how to factor more detailed mobility
information into the point selection process. We
are also interested in addressing how to integrate
small scale cartographic features, particularly roads
into a TIIN, while maintaining a limited polygon
budget.
From a pragmatic standpoint, the generation of a
TIN directly from the NTC digital elevation model
using our new point selection method would take
weeks, even on a fast (20mips) workstation. Our
initial solution was to divide the DEM into tiles
and then generate a TIN for each tile. We
maintained a restriction that TINs must match
along common boundaries. The execution time is
divided by the number of tiles, and can be further
reduced since tiles which have no common
boundary can be generated in parallel. Using this
method we were able to generate the NTC TIN
ovenight using three workstations. There are
limits to this technique since as the number of tiles
is increased, the global behavior of point selection
is greatly reduced. This can defeat the overall goal
of placing points wherever their utility is the
greatest.
Knowledge refinement is a central problem in the
field of expert systems [Buchanan and Shorliffe
84]. It refers to the progressive refinement of the
initial knowledge-base of an expert system into a
high-performance knowledge-base. For rule-based
systems, refinement implies the addition, deletion
and modification of rules in the system so as to
improve the system's empirical adequacy, ie., its
ability to reach correct conclusions in the problems
itis intended to solve [Ginsberg et al. 88].
The goal of our research effort is to understand the
methodology for refining large rule-based systems,
as well as to develop tools that will be useful in
refining such systems. The vehicle for our
investigation is SPAM, a production system (rule-
based system) for the interpretation of aerial
imagery [McKeown et al. 89, McKeown et al. 85].
It is a mature research system having over 600
productions, many of which interact with complex
geometric algorithms. A typical scene analysis
task requires between 50,000 to 400,000
production firings and an execution time of the
order of 2 to 4 cpu hours.'
Large, compute-intensive systems like SPAM
impose some unique constraints on knowledge
refinement. First, the problem of credit/blame-
assignment is complicated. It is extremely difficult
to isolate a single culprit production (or a set of
culprit productions) to blame for an error observed
in the output. Second, given the large run-time, it
is not possible to rely on extensive experimentation
for knowledge refinement.
As a result, the methodology adopted in well-
known systems such as SEEK and SEEK2 [Politakis
and Weiss 84, Ginsberg et al. 88], or KRUST [Craw
and Sleeman 91], cannot be directly employed to
refine knowledge in SPAM. Our approach is to
address this problem in a bottom-up fashion, i.e..
begin by understanding SPAM's individual phases,
and then attempt to understand the interactions
between the phases. A different set of tools is
required to allow the user to focus attention on
individual modules responsible for intermediate
results and refine them. In our work so far, we
have focused on the second phase in SPAM, local-
consistency (LCC), which applies constraints to a
set of plausible hypotheses and prunes the
hypotheses that are inconsistent with those
constraints. Furthermore, we have narrowed this
focus to refining SPAM's distance and orientation
constraints.
In working toward refining these constraints, we
posed several questions to help guide our analysis:
In the following sections we describe some of our
current efforts toward addressing these questions.
We have begun our investigation on knowledge
refinement by focusing on SPAM's second phase of
processing, LCC. This phase was chosen because
most of SPAM's time is spent in this phase, and it
showed the most potential for future growth. LCC
performs a modified constraint satisfaction
between hypotheses generated in SPAM's first
phase. In LCC, a successful application of a
constraint provides support for a pair of
hypotheses, and an unsuccessful application goes
towards filtering out that pair of hypotheses. The
distance constraint specifies allowable distance
ranges between different pairs of hypothesized
objects, e.g., two hangar buildings must occur
between 20 and 200 meters apart, while a parking
apron and a hangar building must be between 0
and 50 meters apart. In essence, each constraint in
the LCC phase classifies the pairs of hypotheses --
either the constraint supports that pair, or it does
not.
Our refinement methodology consists of three
parts: intermediate result evaluation, constraint
optimization, and embedded evaluation. The first
two methods allow the isolation and improvement
of individual constraints, while the third method
allow us to evaluate the performance of the new
knowledge in the context of the overall system
output.
In order to measure the effect of various spatial
constraints we needed to establish a database of
correct inputs and outputs for LCC. For each of the
sets of data that we run through SPAM we have a
ground-truth database containing all the objects in
the scene with their correct hypothesis labels. An
''ideal'' input to the LCC phase, a set of hypotheses
that are 100% correct, can easily be manually
generated and run through the system. Any errors
in the output are then directly attributable to the
constraints.
A set of constraint results can be generated by
allowing a user (the expert) to enumerate those
constraints that should exist between each pair of
objects in the ideal input. This is equivalent to an
''ideal'' output for LCc.
Once SPAM has processed the ideal input, the
generated output can then be compared to the ideal
output. Such a comparison is informative as it
allows a quantitative measure of error to be
computed. We can produce this comparison as a
set of confusion matrices where each matrix
represents the results for a single constraint and a
single pair of classes. These matrices contain the
usual cells (true-positives, false-positives, true-
negatives, false-negatives). An example confusion
matrix is shown in Figure 22.
A true-positive entry in the confusion matrix
indicates situations where the expert and LCC both
conclude that the constraint supports a pair of
hypotheses. A true-negative entry indicates
situations where the expert and LCC both conclude
that the constraint does not support a pair of
hypotheses. A false-positive entry is one where
LCC concludes support, while the expert does not.
A false-negative entry is one where the expert
concludes support, while LCC does not.
By examining the overlap of each histogram, we
can tell if SPAM's distance constraint is working
properly. The overlap of, for example, true
positives with false positives can tell us how the
constraint can be modified to achieve the greatest
number of true positives without introducing too
many false positives. For numeric constraints,
such as distance, we have developed an automatic
process for adjusting the constraint bounds to
generate an improved set of ranges.
Automated bounds selection is achieved by doing
an exhaustive search through the space of possible
bounds settings, evaluating each setting with an
objective function. Currently, this objective
function weighs all cells in the confusion matrix
equally and seeks to maximize the number of
elements in the diagonal cells of the matrix (true-
positives, true-negatives).
Both methods described above evaluate and
improve the performance of isolated constraints.
However, it is most important that the system's
overall output improve with the adjusted contraint
embedded within it.
We want to choose to evaluate embedded
performance at a place within the system where the
intermediate results have been used, but where
only a small amount of processing has been done
so that the credit assignment problem is avoided.
We chose to evaluate performance at the end of
SPAM's third phase, FA. This phase does grouping
based on the results of the constraints applied in
LCC. These groups of supporting hypotheses are
called fiunctional-areas (FAs).
SPAM's long run times prohibit iterative refinement
if the number of iterations required can be large.
This limitation can be avoided by appropriately
choosing experiments to run and observing the
system's behavior. In this way, we sample the
space of possible bounds settings and hence,
sample the system's output behavior.
We ran the LCC phase of SPAM with a set of hand-
labeled hypotheses and compared this to our
ground-truth. The resulting comparison histogram
was used by our bounds adjustment procedure
which generated a set of optimal settings for this
constraint. These experiments were performed on
four data sets, with each data set corresponding to
a different airport scene.
Next, we ran five experiments for each data set,
allowing SPAM to execute through it's third phase.
Each experiment corresponded to a modification of
the distance constraint bounds, as follows:
Those table entries labeled orient- are orientation
constraint modifications. For each run, we
compiled statistics on run-time, number of
production firings, number of functional-areas
generated, and number of correct and incorrect
hypotheses included in those functional-areas.
Evaluation was done by comparing each run to the
original bounds settings.
Our results are presented in Table 3. From this
table, we can make several observations. First,
from the increase in run time (from off to original),
it can be noted that the distance constraint is
having some impact on the results. The increase in
the number of correct hypotheses and the drop in
the number of incorrects reveals that this constraint
is playing a positive role.
Finding the best setting for the bounds of the
constraints is a more difficult problem. The
evaluation function for this task seems very
complex, taking into account relationships between
numbers of correctslincorrects, sizes of functional-
areas, and run time. For the Moffett data set, the
number of corrects increases, while the number of
incorrects increases, but at a slower pace. From
this we would conclude that the bounds should be
set to the maximum value. However, the same
analysis for DC National implies that the
optimized value would be best. Other larger data
sets, such as those for San Francisco National
Airport, show a similar trend. This suggests that
the bounds for the distance constraint should be
chosen on a case by case basis.
An interesting phenomena is observed as the
constraints are selectively turned off, The
generated functional-area groups get smaller, but
they do not radically change in area of coverage.
This implies that the distance constraint is
selectively applicable, i.e., it largely overlaps with
the other system constraints, but it is necessary for
the inclusion of some subset of hypotheses.
Because optimizing and then coupling these two
constraints does not produce a dramatic
improvement in results, it appears that more
constraints may be required to do a better job of
interpretation
With the recent emphasis on performance
evaluation of vision systems focused upon low and
intermediate level vision tasks, this work
establishes a data point in the area of high level
vision systems. Though our goal is to improve the
interpretations generated by SPAM, we have begun
by improving our understanding of how the
individual components of SPAM operate, and how
they interact. This will provide the foundation for
understanding the effects of modifying or adding
knowledge to the system.
We have been able to show that SPAM's distance
constraint plays a positive role in the interpretation
task. However, choosing an ''optimal'' value is
difficult, and seems to be scene dependent.
Finally, we have determined that, of the two
constraints considered thus far (distance and
orientation), the applicability of both overlaps a
great deal.
There is still much to be done. In the short term,
there are several obvious problems that we have
not addressed. First, we need to look more closely
at the applicability of the constraints and
characterize, if possible, the cases where each
constraint can be applied. Second, it is unclear if
the bounds optimization procedure we developed
will extend to non-numeric constraints. Finally,
we wish to extend the analysis to simultaneously
perform validation across multiple constraints.
Overall, we believe that it will be possible to build
a heuristic system that would automate the
knowledge refinement process, similar to the
automated system in [Ginsberg et al. 88, Politakis
and Weiss 84]. Within such a system, we would
like to discover ways not only to improve the
current constraints, but to automate methods for
determining what new knowledge may be needed.
Our work in multispectral analysis to determine
surface material properties has been focused on
basic research on demonstrating the utility of such
data for cartographic feature extraction. For many
tasks in traditional remote sensing it is clear that
having surface material information drives many
tasks in land use, environmental monitoring, and
natural resource management. Our hypothesis is
that such data can aid in manmade object
detection, delineation, and identification.
However, getting multispectral imagery at spatial
resolutions that are comparable with the high
resolution panchromatic imagery has been
difficult.
Initial work has demonstrated the utility of the
refinement of multispectral classification using
monocular panchromatic imagery, and the fusion
of stereo disparity maps with surface material
information [Ford and McKeown 92b, Ford and
McKeown 92a]. One issue is maintaining accurate
registration between the multispectral scanner data
(8 meter gsd) and the panchromatic imagery (1.3
meter gsd). Once this is accomplished a unique
hybrid three dimensional multispectral dataset can
be created and utilized for further analysis.
Our recent research has been to perform a
performance evaluation of two classification
techniques, gaussian maximum likelihood and
differential radial basis function, for surface
material classification. In order to do this
evaluation we have created several highly detailed
ground truth segmentations based upon manual
analysis of the multispectral imagery, as well as by
inspection of panchromatic imagery acquired over
the same area. Details of this work can be found in
a companion paper [Ford et al. 93] in this volume.
Our overall conclusions are that multispectral
imagery with moderate spatial resolution has great
potential to provide scene domain cues necessary
to improve the performance of cartographic feature
extraction based on panchromatic imagery with
high spatial resolution.
Our work in multispectral analysis to determine
surface material properties has been focused on
basic research on demonstrating the utility of such
data for cartographic feature extraction. For many
tasks in traditional remote sensing it is clear that
having surface material information drives many
tasks in land use, environmental monitoring, and
natural resource management. Our hypothesis is
that such data can aid in manmade object
detection, delineation, and identification.
However, getting multispectral imagery at spatial
resolutions that are comparable with the high
resolution panchromatic imagery has been
difficult.
Initial work has demonstrated the utility of the
refinement of multispectral classification using
monocular panchromatic imagery, and the fusion
of stereo disparity maps with surface material
information [Ford and McKeown 92b, Ford and
McKeown 92a]. One issue is maintaining accurate
registration between the multispectral scanner data
(8 meter gsd) and the panchromatic imagery (1.3
meter gsd). Once this is accomplished a unique
hybrid three dimensional multispectral dataset can
be created and utilized for further analysis.
Our recent research has been to perform a
performance evaluation of two classification
techniques, gaussian maximum likelihood and
differential radial basis function, for surface
material classification. In order to do this
evaluation we have created several highly detailed
ground truth segmentations based upon manual
analysis of the multispectral imagery, as well as by
inspection of panchromatic imagery acquired over
the same area. Details of this work can be found in
a companion paper [Ford et al. 93] in this volume.
Our overall conclusions are that multispectral
imagery with moderate spatial resolution has great
potential to provide scene domain cues necessary
to improve the performance of cartographic feature
extraction based on panchromatic imagery with
high spatial resolution.
We thank the unsung hackers of the Digital
Mapping Laboratory for their help in the research
reported in this paper. Steve Lacy pointed the way
toward user assisted building verification, Chris
OIson motifed his way through visualization
routines, Jeff McMahill performed multispectral
magic, and Scott Colville knows more than he'd
like about functional areas. Ed Allard, Karl
Fischer, and Mark Stemm joined the project too
recently to have done anything terribly interesting.
Computational Sensors combine computation and
signal acquisition to improve performance and pro-
vide new capabilities that were not previously pos-
sible.
They may attach analog or digital VLSI process-
ing circuits to each sensing element, exploit unique
optical design or geometrical arrangement of ele-
ments, or use the physics of the underlying material
for computation. Typically, a computational sensor
implements a distributed computing model of the
sensory data, including the case where the data are
sensed or preprocessed elsewhere.
Recognizing the importance and potential of
computational sensors, Oscar Firschein, DARPA
SISTO,requested us to organize a workshop to bring
together developers and users of computational sen-
sors. The workshop was to define the state of the
art, discuss the issues, and identify promising ap-
proaches and applications for this new technology.
The workshop was held at The University of Penn-
sylvania on May 11-12, 1992. Approximately 40
people attended from academia, government, and
industry. The workshop hosted several key presen-
tations and followed them with group discussion and
summary sessions. This workshop report presents
a summary of the state of the art in computational
sensors and recommendations for future research
programs.
In Section 2 we discuss opportunities for compu-
tational sensors. Some computational sensor exam-
ples are reviewed in Section 3. Technologies, issues,
and limitations are considered in Section 4. Section
5 discusses algorithms for computational sensors.
Recommendations for future programs are given in
the concluding section. The appendix includes a
bibliography of computational sensing created with
input from the workshop participants.
Traditionally, sensory information processing pro-
ceeds in three steps: transducing (detection), read-
out (or digitization), and processing (interpretation).
Micro-electronics technologies will spawn a new
generation of sensors which combine transducing
and processing on a single chip - a computational
Sensor.
In machine vision, the basic approach has been
to use a TV camera for sensing, to digitize the im-
age data into a frame buffer and then to process
the data with a digital computer. Apart from be-
ing expensive, large, heavy, and power-hungry, this
sense-digitize-and-then-process paradigm has fun-
damental performance disadvantages. A high band-
width is required to transfer data from the sensor to
the processor. The parallel nature of operands cap-
tured in a 2D image plane is not exploited. Also,
high latencies caused by this method, due to image
transfer times, limit the usefulness of this method
for high-speed, real-time applications. Combining
processing on silicon wafers together with detectors
will eliminate these limitations, and have the po-
tential to produce a visual sensor of low-cost, and
low-power with high-throughput and low latency.
The potential for integrating the transducing and
processing of signals has been recognized for some
time, but in the past, research and development in
this area was driven mostly by curiosity or special
use. Today, however, the advancement of VLSI
and related technologies provides opportunities for
us to harness this potential in new, broad, practi-
cal applications in image understanding, robotics,
and human-computer interfaces. Most importantly,
VLSI technologies have become available and ac-
cessible to the sensor application community where
we have recently observed a growing body of re-
search in computational sensors.
Several computational sensors have been fabri-
cated and demonstrated to perform effectively. Ana-
log vision chips have been demonstrated which can
detect a motion field, or continuously compute the
size and orientation of an object. Three dimen-
sional range sensing has been performed at a rate
of 1000 frames per second using a chip containing
an array of cells each capable of detecting and cal-
culating the timing of an intensity profile. Sensor
chips that mimic the human's fovea and peripheral
vision have been fabricated and used for pattern
recognition. Tiny lenses can be etched on silicon
to focus light efficiently on a photosensitive area,
or even to perform a geometrical transformation of
images. Resistive networks and associated circuits
on a chip can solve optimization problems for shape
interpolation.
Computational sensors are not limited to vision
use, but have applications in mechanical, chemical,
medical and other sensors. Development of mi-
cromechanical pressure sensors and accelerometers
has been underway for some time. An air-bag sen-
sor for automobiles could become one of the first
successful, mass-produced, low-cost computational
sensors, It contains a miniature accelerometer and
processing circuits in a chip. Processing could also
be combined with micro-chemical sensors to de-
tect water contamination, air pollution, and smells,
while micro-medical sensors could measure blood
chemistry, flow, and pressure.
Potential applicationslmarkets of computational
sensors are abundant:
Development of a computational sensor does not
simply mean combining sensing capability with pro-
cessing algorithms. It requires new thinking. Most
of the current vision algorithms, for example, are
strongly influenced by the fact that image data is
provided in a stream and processed by instructions.
Also, the concept of frame rate (ie., considering
a certain number of discrete frames per second) is
dominant in dealing with time varying events. How-
ever, a computational sensor can take advantage of
the inherent, two-dimensional nature of the sensory
data arrangement, the continuous time-domain sig-
nal, and the physics of the media (eg. silicon) it-
self for processing. This type of new thinking of-
ten results in a completely different, more efficient,
orders-of-magnitude faster ''algorithm''. Many of
the successful examples mentioned above and in
section 3 are the results of such new algorithms.
Finally, computational sensors can create a funda-
mental change in the approach to the sensor system
as a whole, When a sensor is bulky, expensive and
slow, it is not affordable, both economically and
technically, to place many of them within a system.
The sensor system is forced to be centralized. If
computational sensors can provide cheaper, smaller,
and faster sensing units, we can place a large num-
ber of sensors throughout a system, such as covering
the whole surface of a submersible vehicle. A new
opportunity exists to make sensor systems more dis-
tributed, reliable, and responsive.
This section reviews computational sensor architec-
tures that have emerged in recent years:
Many existing systems would fall into several of
the above categories. Representative examples of
each category are presented here.
Although most examples we give are of visual
information processing, these considerations and
techniques extend directly to measurement over the
whole spectrum of electromagnetic radiation. In
general, any other ''imaging sensors'' such as me-
chanical (e.g. tactile) or magnetic sensors, could
also benefit from lessons learned when considering
and designing computational sensors for vision ap-
plications.
The focal plane architecture tightly couples process-
ing and sensing hardware-each sensing site has a
dedicated processing element. The sensor and the
processing element (PE) are located in close phys-
ical proximity, thus reducing data transfer time to
PE'4. Each PE operates on the signal of its sen-
sor. However, depending on the algorithm, each PE
may need the signals of neighboring sensors or PE's.
This concept corresponds to the SIMD paradigm of
parallel computer architectures. In computational
sensors, the operands are readily distributed over an
array of PE's as they are being sensed.
Gruss and Kanade [26] [27] [40] at Canegie Mellon
have developed a computational sensor for range
detection based on light-stripe triangulation. The
sensor consists of an array of cells, each cell having
both a light detector and a dedicated analog-circuit
PE, The light stripe is swept continuously across the
scene to be measured. The PE in each cell monitors
the output of its associated photoreceptor, recording
a time-stamp when the incident intensity peaks. The
processing circuitry uses peak detection to identify
the stripe and an analog sample-and-hold to record
time-stamp data. Each time-stamp fixes the position
of the stripe plane as it illuminates the line-of-sight
of that cell. The geometry of the projected light
stripe is known as a function of time, as is the line-
of-sightgeometry of all cells. Thus, the 3-D location
of the imaged object points ('range pixels'') can be
determined through triangulation. The cells operate
in a completely parallel manner to acquire a frame of
3-D range data, so the spatial resolution of the range
image is determined solely by the size of the array.
In the current CMOS implementation, an array of 28
x 32 cells has been fabricated on a 7.9mm x 9.2mm
die.
Keast and Sodini [41] at MIT have designed and
fabricated a focal plane processor for image acqui-
sition, smoothing, and segmentation. The processor
is based on clocked analog CCDICMOS technol-
ogy. The light signal is acquired as an accumulated
charge. The neighboring PE's share their operands
in order to smooth data. In one iteration, each PE
sends one quarter of its charge to each of its four
neighbors. The charge meets halfway between the
pixels and mixes in a single potential well. After
mixing, the charge is split in half and returned to
the original PE, approximating Gaussian smooth-
ing. However, the segmenting circuit will prevent
this mixing if the absolute difference between the
neighboring pixels is greater than a given threshold.
A 40 x 40 array with a cell size of about 150 x 150
microns is currently being fabricated.
Some algorithms can exploit the physics of the VLSI
layers to achieve ''processing'' in a computational
sensor. Carver Mead at Caltech has developed a
set of subthreshold CMOS circuits for implement-
ing a variety of vision circuits. The best known
design is the ''Silicon'' retina, a device which com-
putes the spatial and temporal derivative of an im-
age projected onto its phototransistor array. The
photoreceptor consists of a phototransistor feeding
current into a node of a 48 by 48 element hexagonal
resistive grid with uniform resistance values R. The
photoreceptor is linked to the grid by a conductance
of value G. An amplifier senses the voltage between
the receptor output and the network potential. The
circuit computes the Laplacian of an image, while
temporal derivatives are obtained by adding a ca-
pacitor to each node.
Another example which exploits resistive grids
to achieve signal processing is the blob position and
orientation circuit developed by Standley, Horn, and
Wyattat MIT [83][84]. Light detectors are placed at
the nodes of a rectangular grid made of polysilicon
resistors, The photo-current is injected into these
nodes and the current flowing out of the perimeter
of the grid is monitored. The injected photocurrent
and the grid perimeter current are related through
Green's theorem; based on sensed perimeter cur-
rent, information to compute the first and second
moments of the blob is extracted at 5000 frameslsec.
An array of 29 s 29 cells has been fabricated on a
9.2mm x 7.9mm die.
Some computational sensors are based on the''com-
putation'' performed by virtue of the special geom-
etry or optical material of the sensor array.
The University of Pennsylvania's log-polar
sensor developed by Kreider and Van der
Spiegel [47] [48] [73] [77] in collaboration with
Sandini of University of Genova and researchers at
IMEC in Belgium has a radially-varying spatial res-
olution. A high resolution center is surrounded with
a lower resolution periphery in a design resembling
a human retina. A sensor that has a high spatial
resolution area, like a fovea in a human retina, is
often termed a foveating sensor. The image is first
mappedfrom log-polar to the Cartesian plane. There
is evidence that in biological systems this type of
mapping takes place from eye to brain. The authors
have shown that transformations involving perspec-
tive, such as optical flow and rotation, are simplified
with such a mapping. This sensor must be mechan-
ically foveated for a specific region of interest, and
current research concentrates on applying this chip
to robotics.
Bederson, Wallace,and Schwartz [7]at New York
University and Vision Application, Inc. designed a
log-polar sensor as well. The VLSI sensor itself is
in the process of being fabricated. An additional
interesting part of their system is a miniature pan-
tilt actuator called Spherical Pointing Motor (SPM)
shown. The SPM is capable of carrying and ori-
enting the sensor. It is an accurate, fast, small, and
inexpensive device with low power requirements
and is suitable for active vision applications.
Another foveating sensor has been designed by
Kosonocky, Wilder and Misra at Rutgers Univer-
sity. The objective was to design a sensor whose
foveal region(s) will be able to expand, contract and
roam in the field-of-view. The chip is, in essence,
a 512x512 square array with the ability to ''merge''
its pixels into regions, and output only one value for
each such rectangular''super pixel'', The largest su-
per pixel is an 8x8 region. There are three modes of
operation. In Variable Resolution Mode, the resolu-
tion of the entire chip can be selected from highest
to lowest, or anywhere inbetween. The Multiple
Region of Interest mode provides multiple active
windows, possibly with different resolutions, while
reading data out from the rest of the array is inhib-
ited. The third mode is a combination of the first
two modes. This third mode would resemble the
sampling of a human retina if so programmed. The
design permits multiple foveae within the retina.
The authors demonstrated significant speed-up in
data acquisition for a variety of tasks from indus-
trial inspection to target tracking.
Hexagonal sampling tessellates the frequency plane
more efficiently than rectangular sampling.' Pous-
sart and Trembley [91] at Laval designed a 200 x
200 array with a hexagonal grid. This chip facil-
itates parallel access to the data in a particular lo-
cal neighborhood. For rapid convolution, this local
neighborhood is subsampled along three principal
axes of the grid, thus reducing the data needed for
convolution in the local neighborhood of each pixel.
Their MAR (Multi-port Array Photo-Receptor sys-
tem) performs zero-crossing detection at seven spa-
tial frequencies in 16 milliseconds. Edge detection
is computed in real time.
By etching desired geometrical shapes directly into
the surface of an optical material, a designer can pro-
duce optical elements with properties that were pre-
viously impossible to achieve. This method, called
binary optics,can perform simple optical processing
before the light is detected.
As VLSI microlithographic techniques have ad-
vanced, inexpensive fabrication of binary optical
devices has become possible [93]. Veldkemp of
Lincoln Lab at MIT has developed a micro lens ar-
ray in which each lens is only 200 microns in diam-
eter. One application of such an array would be to
focus light onto tiny photodetectors thus saving sil-
icon area for processing hardware. Some of the first
applications of the idea are already on the market:
Hitachi FP-C10 HI-8 video coders use a micro-lens
array CCD, and the Sony XC-75 video camera dou-
bles the sensitivity to f8 @ 2000Lux using their Hy-
perHAD CCD structure which uses micro lenses. In
addition, binary optics devices have been applied to
automatic target recognition and space applications.
McHugh of Hughes Danbury Optical Systems ex-
perimented with binary optical techniques and found
that they can generate virtually any transformation
of an optical wave front. The first application that
used this new capability was a binary optical com-
ponent that optically mapped the log-polar plane to
the Cartesian plane. This device, in effect, samples
images at log-polar resolution and optically trans-
forms them for sensing on a Cartesian grid. This
way an optical log-polar foveating sensor is pro-
duced, while the mapping to the Cartesian plane has
become ''free of charge''.
Wolff at Johns Hopkins University uses liquid crys-
tal polarizers whose polarization angles are elec-
tronically controlled [100]. It has been reported that
by eliminating mechanical rotation of filters, switch-
ing time between different polarization angles is re-
duced, and accuracy of results is improved. Wolff
hopes to build polarization cameras with polarizers
in each element of the CCD array for acquisition of
polarized images in real-time. For specularity detec-
tion, material classification and object recognition,
color and polarization carry independent and com-
plementary information: polarization for specular-
ity, and color for diffuse surfaces and light sources.
Sensors for real-time combination of both color and
polarization images will add rich information to vi-
Sion systems.
While not strictly a computational ''sensor'', there is
a class of computational modules for sensory infor-
mation processing which exploit VLSI technologies
in a similar manner as computational sensors.
These computational modules are useful when
there is not enough space on a single chip to accom-
modate complex PE's, or the data to be processed
comes from other modules.
At Caltech, several regularization techniques have
been implemented on-chip. For example, consider
the problem of fitting a 2D surface to a set of sparse,
noisy depth measurements by imposing a ''smooth-
ness'' constraint. This method produces quadraticly
varying functions. This can be solved using simple
linear resistive networks by virtue of the fact that
the electrical power dissipated in linear networks is
quadratic in the current or voltage [71].
Mapping 2D motion algorithms onto analog chips
has turned out to be surprisingly difficult. A ro-
bust motion detection circuit implemented in ana-
log VLSIhas yet to be demonstrated, but early effort
has been made by Tanner at Caltech [88] [89]. He
successfully built and tested an 8x8 pixel chip that
outputs a single uniform velocity averaged over the
entire image. His chip reports values of x and y ve-
locity which minimize the least square error in the
image brightness constraint equation.
Bair and Koch have successfully built an ana-
log VLSI chip that computes zero crossings of the
difference of Gaussians. It takes the difference be-
tween two copies of an image, supplied by a 1-D
array of 64 photoreceptors, each smoothed by a sep-
arate linear first-order resistive network, and reports
the zero-crossings in this difference [6]. This imple-
mentation has the particular advantage of exploiting
the smoothing operation naturally performed by re-
sistive networks, and therefore avoids the burden
of additional circuitry. The network resistance and
the confidence of the photoreceptor input are inde-
pendently adjustable for each network. Also, an
adjustable threshold on the slope of zero-crossings
can be set to cause the chip to ignore weak edges
due to noise.
Binary line processes which model discontinu-
ities in intensity within the stochastic framework of
Markov Random Fields provide a method to detect
discontinuities in motion, intensity, and depth. This
is achieved by selectively imposing the smoothness
assumption. Harris and Koch have invented the
''resistive fuse'', which is the first hardware circuit
that explicitly implements line processes in a con-
trolled fashion [31]. Like a normal house fuse, a
resistive fuse operates as a linear resistor for small
voltage drop and as an open-circuit for large voltage
drops, A 20x20 rectangular grid network of fuses
has been demonstrated for smoothing and segment-
ing test images which are scanned onto the chip.
Van der Wal and Burt at David Sarnoff Research
Center developed a VLSI pyramid chip PYR [94].
Combined with external framestore, the PY R chip
is capable of computing Gaussian and Laplacian
pyramid transforms simultaneously. These trans-
forms consist of Gaussian filtering and consecutive
subsampling, and, for Laplacian, image subtraction.
The Chip has a separable 5 by 5 filter and four
1024-sample-long delay lines. Each filter tap has
a preassigned set of possible values. Coefficient
values from this set can be changed under software
control. PYR has special features such as double
precision, double sample density, image border ex-
tension and automatic timing control. At 15MHz
a single chip can compute Gaussian and Laplacian
pyramids at 44 frames/second for 512 by 480 im-
ages, PYR is implemented in digital VLSI using the
CMOS standard cell library from VLSITechnology.
Inc. Digitized image samples pass through the chip
sequentially, in raster scan order.
Successful development of a computation sensor re-
lies on careful consideration of several issues includ-
ng:
All of these issues are discussed in the following
sections.
Both digital and analog circuits can be implemented
using VLSI technology. The analog approach can
be conceptually divided into continuous-time (un-
clocked) and discrete-time (clocked) processing.
The choice of technology depends on the particu-
lar application, but several general remarks are in
order. Compared to digital, the traditional disad-
vantage of analog electronics is its susceptibility to
noise, yielding low precision. The source of this
noise can be on-chip switching electronics which
require special considerations for hybrid designs.
Also, analog electronics do not provide efficient
long-term storage; typical storage times are about
one second. On the other hand, digital processing
requires AID and DlA conversion, which usually
imposes limitations on total circuit speed. Analog
electronics are characterized by:
In general, analog hardware takes less chip area
than digital mechanisms of the same functionality.
Most participants at the workshop were experts in
analog circuitry which seems to be preferred; how-
ever, many recognized the importance of digital
electronics for computational sensing.
Analog VLSI offers two interesting advantages
for computational sensor design. First, the physical
properties of the solid-state layers and devices can
sometimes be exploited to yield elegant, new solu-
tions. One such example is to exploit the physics of
a resistive sheet (or dense grid) to compute desired
quantities.
The second interesting advantage of analog VLSI
is charge-domain processing, best exemplified by
CCD technology. which offers an area-efficient
mechanism for transferring data. In addition, cre-
ative processing schemes can be developed to pro-
cess the data in charge-domain as it is transferred.
CCD technology has already provided several useful
examples of integrated sensing and signal process-
While the VLSI computational sensor offers excit-
ing opportunities, one must be careful in deciding
which algorithms or applications will benefit from
such an implementation. At the present state of tech-
nology, successful design of working VLSI circuits,
especially analog ones, is a lengthy process.
Algorithms must be carefully selected or invented
to match the architecture to the circuitry for max-
imum performance - there are definite limitations
on circuitry and architectures. Circuitry has lim-
ited precision and storage. Until technology allows
much denser circuits (or 3D structures) for example,
there is not enough room to fabricate a complex PE
at each photo site.
Simple cell-parallel algorithms that detect local
cues or integrate local information over time or mul-
tiple channels (eg. spectrum) at each cell are most
ideal.
When a complex PE is required, processing and
sensing can take place on separate, but tightly cou-
pled (preferably on-chip) modules. The cost of
transferring data must be minimized in order to jus-
tify the use of VLSI over conventional computer
systems. CCD row-parallel transfer is one way to
perform the transfer at a reasonable speed. Also,
some algorithms do not directly exhibit parallelism
in the focal plane; they often require significant lo-
cal data storage at each PE. In stereo algorithms, for
example, optical signals are to be combined from
two different focal planes. In this case, data are
read out and processed on a separate computational
module.
There are optimizations and other techniques that
map naturally to physical processes in silicon; such
as relaxation processes implemented on resistive
grids, The advantage of these physics-based pro-
cessors over computer implementation is that they
minimize a multi-dimensional energy function by
reaching a stable state of a continuous-time system,
potentially reducing round-off error and numerical
instability from which an iterative solution by a dig-
ital computer may suffer.
In summary, the following are some general char-
acteristics of algorithms which are good candidates
for computational sensors implementation:
CMOS, Bipolar, and BiCMOS are the most avail-
able VLSI technologies. CMOS is characterized by
very dense packaging, low power consumption, and
high input impedance. Good switching properties
make it well suited for digital, switching, and hy-
brid circuits. It is widely accessible and relatively
inexpensive technology. CCD's are implemented in
MOS technology.
Bipolar technology is characterized by low noise
and fast circuitry, but consumes more power and
takes more substrate real estate. It is not as accessi-
ble to the wider research community as it probably
should be.
BiCMOS combines the advantages of both
CMOS and Bipolar technologies.
Semiconductor material other than silicon is also
available. GaAs compounds yield very high speed
circuitry and are well suited to electro-optical ap-
plications. GaAs technology is less available, how-
ever, and is considerably more expensive.
The trend in VLSI is toward smaller device ge-
ometries. This produces both smaller and faster
digital circuits and hence more functionality per unit
area. This scaling, however, is not as beneficial to
analog circuitry as to digital. Most active devices
are designed at a given size and scaling and would
not preserve desired functional features after a scale
change. Analog MOS circuits benefit more from
improvements in fabrication process quality. Fac-
tors such as oxide quality and thickness, or tighter
control of threshold voltages would greatly benefit
analog circuit performance.
Great interest has been shown in 3D VLSI. One
possibility is optical signal communication between
stacked chips. This could be accomplished with
the availability of silicon-compatible semiconduc-
tor emitters and IR detectors [90]. This technique
would also require and exploit integrated optics
capability such as binary optics. Alternatively,
a conducting feedthrough could be developed for
making distributed point-to-point electrical connec-
tions [70].
Micro fiber-optics could be used to route data in
parallel from module to module. The optical ap-
proach has the advantage of possible optical pro-
cessing during the data transmission itself, but has
the disadvantage of high power consumption and
heat dissipation. This technology has not been
developed far enough to become accessible to the
wider research community.
As VLSI technology advances and becomes acces-
sible to a wider research community, a number of
ideas that combine sensing and processing on a chip
are emerging. Many attempts, however, are too
quick to postulate miraculous chips and systems
which have little chance of ever working.
Several successful examples of computational
sensors have been driven by applications, and the
workshop participants have agreed that this will re-
main true for most successful developments. A truly
successful ''marriage'' of sensing and computation
can be done only by careful analysis of application
requirements in conjunction with implementation
technologies.
While a wide variety of applications are conceiv-
able, the following are potential applications that
have been suggested during the workshop:
An issue which received unanimous agreement
among workshop participants is the lack of ana-
log VLSIdesign tools equivalent to those for digital
design. These tools include design aids from lay-
out to testing, including extraction, verification and
simulation. Analog circuits are more sensitive to
parasitics than digital circuits. Accurate techniques
for including these parasitics in the extracted files
would reduce the number of design iterations due to
unexpected circuit behavior.
Analog modeling and simulation capabilities are
still inadequate. Much of the attention in modeling
is directed at the effects of extremely short chan-
nel lengths on MOS transistor operation. Analog
design rarely uses minimum size transistors, but is
more critically dependent upon operating under a
different bias condition: subthreshold and saturation
regions. The proper modeling of bias-dependent ca-
pacitances is critical for modeling circuit dynamics
and stability, There is little or no supportfor simulat-
ing charge-domain devices like CCD's. Statistical
modeling is an important predictive element of ana-
log design, providing assurance that the resulting
circuits will meet the prescribed design constraints.
Without it, a circuit may be functional and within
specifications for a given process model, but actual
process variation may result in an out-of-spec or
inoperable circuit.
It has been noted that a data book for standard
analog cells would be very useful. While it will be
more difficult than the digital domain, it is necessary
to develop a library of standard building blocks of
compatible electronic and sensor components with
which one can design a new computational sensor.
The MOSIS Service is a prototyping service offer-
ing fast-turnaround standard cell and full-custom
VLSI circuit development at very low cost. The
MOSIS Service, begun in 1980, provides fabrica-
tion services to government contractors, agencies,
and university classes under the sponsorship of
the Defense Advanced Research Projects Agency
(DARPA) with assistance from the National Sci-
ence Foundation (NSF). MOSIS has developed a
methodology that allows the merging of many dif-
ferent projects from various organizations onto a
single wafer. Instead of paying for the cost of mask-
making, fabrication, and packaging for a complete
run (currently between $50.000 and $80.000) MO-
SIS users pay only for the fraction of the silicon
that they use, which can cost as little as $400. Ini-
tially, the MOSIS user-base was primarily university
and government users. MOSIS' success in serving
this group of users led, in recent years, to a natu-
ral expansion into the industrial sector, with rapidly
growing use of MOSIS by commercial companies.
MOSIS foundries have also taken advantage of the
frequent prototype runs for their own needs as well
as those of their clients. MOSIS is located at the
Information Sciences Institute of the University of
Southern California (USCIISI) in Marina del Rey,
California.
The MOSIS program has been a successful mech-
anism for promoting VLSI applications. MO-
SIS' ease of access, quick turnaround, and cost-
effectiveness have afforded designers opportuni-
ties for frequent prototype iterations that otherwise
might not even have been considered. With MOSIS'
low cost for ''tiny-chip'' fabrication, silicon can be
used as a rapid prototyping vehicle. Small func-
tional building blocks can be easily fabricated and
tested before too much time is invested in building
and integrating a full system. Furthermore, many
ideas and needed intuition can be gained through
''playing'' withthese actual working chips. Success-
ful designers of existing functional computational
sensors have reported that silicon prototyping, com-
bined with higher level algorithm simulation, has
proven to be a useful system-building approach in
computational sensors.
MOSIS offers two monthly runs of a standard
2um, double-layer metal, CMOS process. One of
these runs usually includes a second layer of polysil-
icon. Typically these designs are fabricated, bonded
and returned in about two months. In addition to
these standard runs, a l.2um CMOS run goes out
about once every month and there are more infre-
quent runs at 0.Sum. Every other month includes
a low-noise 2um analog CMOS run which has op-
tions for second poly, a NPN bipolar transistor in
the n-well, and a buried channel CCD.
MOSIS's capability, however, is limited for the
research and development of computational sensors.
Quality bipolar and depletion-mode MOS devices
are unavailable. MOSIS is beginning to offer GaAs
(instead of the more usual Silicon) process runs on
a regular basis.
At this point, MOSIS does not provide a capa-
bility for optical electronics fabrication. University
researchers must rely on teaming with industries
which have the fabrication capability in this area. It
is noteworthy that both the European research com-
munity and the Japanese micro-sensor project will
have a common facilities including capabilities for
optical electronics fabrication.
Understanding semiconductor and device physics
as well as techniques for marketing custom-made
integrated circuits are essential prerequisites to de-
veloping a successful computational sensor. For
the complete success of a computational sensor, av-
enues of communication between VLSI designers,
computer vision researchers, and product develop-
ers must be developed. These groups would ex-
change information about the opportunities and dif-
ficulties in each others' fields. Vision (and other
sensor) researchers must be made aware of what is
available in VLSI technology, and VLSI designers
must understand the problems of machine vision.
This workshop was very productive. It was recom-
mended that follow-on workshops or conferences
be held.
It was proposed that universities and industries
team-up to allow students to obtain more hands-on
experience. This is an old idea that still has diffi-
culty working in practice. Namely, most students
and university professors are more likely to under-
take theoretical research than to work on the ''real
thing''. This is primarily due to the fact that deal-
ing with hardware tends to extend time in graduate
schoolfor students, and reduce the publishing rate of
professors. This problem received some attention,
and reviews of academic standards were suggested.
It was suggested that more credit should be given to
efforts which produce working prototype devices or
systems.
The body of experience and knowledge of com-
putational sensors is currently scattered over a large
number of disciplines and corresponding publica-
tions. Publications range from journals on elec-
tronic circuits and signal processing to publications
on neural networks and vision research. To ef-
fectively communicate knowledge about computa-
tional sensors, it was suggested that a new journal
be created.
Another type of cooperation is to distribute work-
ing prototype sensors in among the user community.
An excellent example is the log-polar camera proto-
type that University of Pennsylvania has offered to
share with interested researchers. This type of co-
operation is of mutual benefit to the sensor designers
as well as to application developers. Designers of
the computational sensor receive much needed feed-
back about the actual need and practical value of the
sensor, while application researchers can investigate
new areas previously limited by the absence of these
specialized devices.
In light of the previous analysis, the workshop has
recommended the following:
THE FOLLOWIING BIBLIOGRAPHY CONTAINS PA-
PERS COLLECTED DURING AND AFTER THE WORK-
SHOP BY THE CONTRIBUTIONS OF PARTICIPANTS.
In many real world applications, there is a
need to perform alignment tasks between
two objects, Two simple, generic tasks
are inserting a peg into a hole and align-
ing objects into arbitrary geometric config-
urations (e.g. robotic assembly tasks,) A
key component of this problem is position-
ing where there is little room for mechan-
ical error. The idea of precision measure-
ment (in our example, alignment) using a
mechanical device, photographic emulsions
or photo-electric sensors, has been exam-
ined in great detail by the researchers in
non-topographic photogrammetry. By us-
ing models which account for most of the
aberration and lens defects in modern lenses,
they obtain highly precise calibrations of
their camera systems, For more informa-
tion see Karara[6]. These methods are often
difficult to understand and inconvenient to
use in most robotics environments. They
usually require the minimization of several,
complex, non-linear equations of multiple
variables (of which the results are not guar-
anteed to be robust.) Other methods for
performing camera calibration for robots in-
clude the works of Tsai [16, 15], Young et.
al. [18], Bennett et, al, [1], and Holt et, al.
[4] for example.
To give the reader an idea of the align-
ment/insertion task, figure l shows our ex-
perimental setup. Off the end of the end
effector of our robot is a probe with a sharp
tip (the ''peg''.) The target in this scene is a
2mm hole in the machined aluminum block
located almost directly below the probe.
Figure 2 shows a view of the target objects
taken from the camera system. In this fig-
ure, the holes in the machined block are
more easily seen. The goal of the task is
to maneuver the probe to a position where
it is directly above the target, and then to
insert the probe into the target.
Another class of methods revolves
around the depth from motion paradigm.
This body of research tries to recover the
absolute pixel velocity for objects in image
space. Here too the researchers are search-
ing for an absolute transformation from a
known reference (the velocity of a known ob-
ject) and an unknown system (the actual,
time-varying, intensity data). The method
we propose does not require the absolute po-
sitional information that both of the afore-
mentioned systems require. It uses simple
image displacement data (generated from
the movement of the camera system) to gen-
erate an estimated position where it expects
that the object motion will be minimized
with respect to the camera movement.
Our technique takes the typical map-
ping from 3-D positions to image coordi-
nates, and instead of finding this mapping,
it recovers a property of the image coor-
dinates. The traditional mapping problem
(known as the calibration problem) deter-
mines the position of objects based on rela-
tive scale difference, perspective distortion,
and/or several other properties which ex-
ist between a calibrated system and an ob-
served system. These positional values can
be obtained from both static and dynamic
systems, These methods do not exploit the
fact that a known movement in the camera
system can result in useful motion informa-
tion in the image system without knowing
the exact calibration between the systems,
We approached the problem by asking
the following question: How can I get a
robot to perform a given task using only un-
calibrated visual input to direct the robot's
actions? In many cases, it is not neces-
sary for the robot to have a completely cal-
ibrated work area. (It is not necessary to
know the exact positions of everything in
the robotic workspace. It may be more im-
portant to know only the exact position of
certain items.) We propose a new tech-
nique, similar to the work of Sawhney [l1,
12], which will allow the robot system to
maintain an arbitrary, geometric relation-
ship with an object system, and as a result of
certain operations, the robot-object system
can 'calibrate'' itself to or ''can define its lo-
cation with respect to'' the unknown camera
system. The newness of our technique arises
from the fact that our system performs the
useful task of moving to the goal position
without ever really knowing the true loca-
tion of the camera system.
In order to perform the peg-in-hole insertion
task, we broke the task into two parts: the
alignment task and the actual insertion task.
The alignment task servos the end effector
in a plane in robot space until the alignment
condition occurs (that being when the ob-
ject to be servoed to and the end effector
lie on the same axis,) The insertion task
relies on the fact that the alignment stage
has constrained the solution to lie along a
line (thus making the insertion task simply
a one degree of freedom search.)
A simplified setup is shown in figure 3.
The task is to maneuver the end effector to
a position directly over the target position.
We started our investigation by examin-
ing what would happen if we attached some
sensing system to the rotational axis, such
that the system could image the rotational
axis, We noticed the following effect as we
servoed the rotational joint over a small an-
gle (see figure 4.) Those objects which were
further away from the axis of rotation moved
a greater distance than those points closer to
the axis, This effect is not new and is very
similar to the work done by researchers on
the analysis of the Focus of Expansion for
time to impact studies.
We make the simplifying assumption
that the objects do not change their appear-
ance as we perform the rotation. One simple
way of doing this was to use point-like tar-
gets. The point-like targets rely on the fact
that the perspective distortion is highly lo-
calized due to the fact that the targets have
a high level of spatial coherence.
Using the effect noticed above, we trans-
formed the alignment problem into one of a
positioning problem in a plane. The simpli-
fication is justified by the following observa-
tions:
To perform our peg-in-hole insertions
we also make the following assumptions:
The following constraints were not nec-
essary:
In the figure 3, the robot-camera sys-
tem is constrained to move in the plane A,
where A is defined by the circle swept by the
camera around the rotational axis, R. We
have simplified the alignment task to one of
a 2 DOF problem. The goal state is one
where the object simply rotates in the im-
age plane without translating, hence satis-
fying the alignment condition which is that
the object lies on the rotational axis, The
rotational degree of freedom is used as a free
variable for the alignment task and does not
contribute to the final alignment state (for
circularly symmetric objects).
Once an object has been selected in the
camera's view, the robot rotates the camera
around its rotational axis, R. By slowly ro-
tating the camera around its rotational axis,
we remove the correspondence problem (the
object moves only a slight bit between con-
secutive shots, therefore making the corre-
spondence between two shots trivial to com-
pute.) If the only movement in the robot-
camera system is caused by the rotation, the
object will trace out a conic section, an el-
lipse under certain conditions ', in the cam-
era system. We propose to use these ellipti-
cal parameters to recover the alignment con-
dition. One simple method requires that we
move about in the plane A, sweeping out
ellipses in camera space. The further away
the object is from the rotational axis, the
larger area is swept out by its ellipse pro-
jection into camera space. The closer we
come to aligning the object to the rotational
axis, the smaller the projected ellipses will
become. The goal in this scenario is to de-
vise a method for maneuvering the end ef-
fector's position in plane A to the position
which causes the object to project to an el-
lipse with the smallest area.
The majority of this work was inspired by
Safaee-Rad et. al. I10, 9, 14], Haralick [2],
Magee et. al, [7], Sawhney et. al, [11,
12! and Shiu et, al,[13!.
While inspired by these methods, we
have developed a new formulation for de-
riving ellipses from scattered point data, In
our current scenario, we accumulate the (%,
X projection, Y projection) triplet derived
from combining the angle made between the
end effectors zero position and the current
position of the end effector and the pro-
jection of the tracked feature into camera
space. We then parameterized the curve
traced out by the feature as:
The area enclosed by this curve (computed
using Green's Theorem) is
The full proof that the parametric curves
generated by these equations are ellipses is
contained in [17].
The problem of fitting raw data points
to elliptical data was covered in both the
Sawhney and Safaee-Rad works cited earlier.
We were concerned primarily with develop-
ing a method which did not require data
points be taken from the entire ellipse and
which could be solved linearly. In our exper-
iments, the elliptical data was taken over a
90 degree sector of the ellipse. Using only
this data, we were able to fit ellipses quite
well (see figure 5.)
This method uses a version of the simplex
method for finding local minima. We were
motivated by the fact that the solution sur-
face was fairly smooth and by the idea that
even a simple ''walking'' algorithm should
be able to find the solution. We proposed
creating a ''walker'' with three legs: a sim-
plex (for two dimensional ''walking'') requir-
ing three starting points. A simplex was
created in X-Y space from the set of three
arbitrary, non-collinear positions ((0.0, 0.0),
(0.0,50.0), and (50.0, 50.0)). The search
method using the simplex simply ''walks''
down the surface by tossing the ''leg'' which
is furthest uphill an equal amount down-
hill, Once it constrains the solution to
lie between its ''legs'' it shrinks itself and
tries ''walking'' down the surface using its
new position and new, smaller ''legs,'' This
method is similar to the Simplex method of
Nelder and Mead[8].
The implemented version of the algo-
rithm for the simplex search runs as follows:
The above algorithm tries to trap the global
minimum using large simplex movements to
surround the minima and when the minima
is trapped, it reduces its search space by
moving the point with the largest area to
the middle of the simplex (roughly reduc-
ing the bounding area by one-third). The
algorithm is repeated until the area of the
ellipse computed for a position is falls below
the area threshold.
In figure 3, we show a schematic of the sys-
tem set up for testing the new alignment
method. We mounted a Sony XC-77 CCD
camera in a bracket system off the end ef-
fector of a Puma 560 robot. The camera
was not calibrated or position constrained
when initially placed. The system was con-
trolled using RCCL and RCI [3]. The im-
ages were digitized at 256x242 resolution
and 8 bits gray scale at standard NTSC
frame rates using the PIPE parallel image
processing engine [5]. The resulting images
were thresholded to recover a simple black
object on a white background. In general,
any recovery method can be used to generi-
cally extract object information from an im-
age array. The object was positioned so
the robot would not encounter singularities
when moving to the new control positions.
Given that the only information neces-
sary to constrain the alignment is the area
of the projected ellipse on the image plane,
it is not necessary to know anything about
the geometry of the sensor setup.
In the experiment, we used the modified
simplex method (see section 4) with an ini-
tial simplex of ((0.0,0.0), (0.0, 50.0), and
(50.0,50.0)).
We built a feature tracker which as-
sumes velocity constrained object motion in
image space. At the beginning of the ex-
periment, a scene was extracted by the im-
age processor and the user was prompted to
move a pointing device to the location of the
feature. The feature extractor used a Sobel
operator with a fixed threshold to extract
the predominant feature in the selected re-
gion, The tracker would follow the feature
over consecutive image frames as long as the
feature moved only small distances.
After establishing the feature tracker,
the robot was instructed to move to the first
position, stop , and rotate its last joint 90
degrees over the course of which it would ex-
tract 16 images spaced equi-angularly with
respect to the robot's rotation.
The feature tracker tracked the move-
ment of the selected target position over the
complete 90 degrees (reporting to the con-
troller the position of the object at 16 equi-
angular positions over the duration of the
movement.)
The centroid of feature (in image coor-
dinates) was then fed to a least squares esti-
mator to recover the ellipse parameters asso-
ciated with the moving features's trajectory.
These parameters were then fed into formula
3 for computing the area of the ellipse.
The process was repeated for the re-
maining two points in the simplex. We ini-
tialized the simple simplex algorithm using
these three areas and allowed it to step its
way to the minima.
The halting condition was when the
area of the ellipse formed from a position
was 5 lpizels% The following table tabu-
lates the results of this experiment:
In the figure, the raw data is displayed
as point data while the predicted ellipses are
drawn in as solid lines.
Figure 6, we show the ellipses generated
by all 25 positions investigated. Note that
the system is not guaranteed to be mono-
tonically convergent (in terms of the number
of evaluations) but the system is convergent
none the less.
The system also can be fooled by ellipses
generated at points which are sampled very
close to one another. In the case of the fi-
nal few ellipses, noise pixels resulted in the
oddish ellipsoid calculations. A more intelli-
gent system would detect this condition and
would hypothesize about the area of the el-
lipse taking this into account. But even with
noisy data, the system reconstructs an ellip-
soid which reflects the general behavior of
the points.
In the experiment above and in figure 7,
the tracked feature was a 2mm diameter
hole. The robot system was able to place the
''peg'' (a tapered probe) within 3mm of the
hole (this using uncalibrated camera data!)
In addition, trying the same experiment 3
more times resulted in about the same re-
sult, that is: an error of about 3mm for the
insertion task. When using a 10mm diam-
eter hole, the robot system almost always
succeeds in placing the probe in the hole.
Upon closer examination, the modified sim-
plex method does converge as well as a
method should taking into account the
amount of knowledge we have given it about
this system. (See figure 8.) The modified
simplex method does suffer from the fault
of reexamining points analyzed previously.
This can be seen in the overlapping num-
bers in figure 8. The only way the simplex
can shrink itself is by covering all possible
point reflections and then after exhaustively
examining all possibilities, it determines the
best step for proceeding to the solution state
is to contract,
Notice that the simplex method suffers
from the fact that it must 'overextend'' the
simplex in all directions before coming to
the conclusion that the simplex should be
shrunk This ability allows a simplex to
normally ''jump'' over a local minima and
continue its search in a more fruitful valley.
In the case of our system, there exists only
one minima and simplex need not evaluate
all positions when it sees an increase in the
area of an ellipse after picking a new point.
This observation brings up several pos-
sible places where the algorithm for com-
puting the next position can be improved
and makes two insightful observations which
are crucial to understanding the alignment
problem. The first observation is the fact
that we are tracking the centroid of the mov-
ing object rather than the true center of the
object. In the case where the object is point-
like, the center of the object and the centroid
of the object are very close together, so the
algorithm works. But, in the case of a fairly
large object observed through a fairly wide
angle lens (take for instance: 12.5mm focal
length), the distortion of the center of an ob-
ject can be significant (on the order of > 1/2
the radius of the object) depending on the
angle the camera takes with respect to the
rotational axis.
The second observation is the fact that
by starting with several observations where
the rotational axis is far from the object po-
sition, resulting in large ellipses, we can start
with accurate estimates of the ''family'' of
ellipses over small rotations. This is in con-
trast to the smaller ellipses which (because
of numerical inaccuracies in estimating the
center of the object and problems caused by
quantization) need larger movement arcs to
adequately recover the parameters of the el-
lipse. This sweep function is a function of
the resolution of the imaging device as well
as the size of the object and position of the
object.
One possibility for increasing the effec-
tiveness of this process is to use more of the
innate properties of the ellipses generated
by the process. A more sophisticated search
procedure, one based on the physical model
of the parabolic surface which is formed by
the ellipses areas, would give more satisfac-
tory results,
In addition, productive results will
probably be gained from the analysis of
other properties of the conic sections. If the
component values of the conic sections are
traced out as a function of the rotation, the
sinusoids generated will show a phase an-
gle difference with respect to the rotation
of the end-effector. The magnitude of the
sinusoids will determine the net amount of
translation of the object with respect to the
rotational axis. These four values can prob-
ably be used as a control signal to effect a
net change to drive all four values to zero
which is a position where the sinusoids are
both in phase and at zero amplitude with
respect to the rotations: the alignment con-
dition. This technique needs to be examined
in further detail.
Another problem which must be faced
is the problem of small ellipses. When im-
age noise is of the same magnitude as the
centroid data the Least Squares fit no longer
captures the true centroid information of the
object. Remember that the object itself is
perspective transformed and the true object
center can actually be a great distance from
the objects projected center. It may be pos-
sible to use the centroid information, if we
are able to recover the varying amounts of
skew caused by perspective. It may also be
possible to recover the centroidal informa-
tion by using the parametric description of
the object and divining the focal points of
the object, the generating lines, and/or the
eccentricity of the ellipses.
The final positioning error may be im-
proved by using a set of movement primitive
vectors defined by a spiral like the logarith-
mic spiral or some member of the family of
spirals, which can exploit the properties of
containment and possibly approach with an
incremental goodness-of-fit function (which
may be a property of the spiral).
We have demonstrated a method for per-
forming a three dimensional task in essen-
tially two dimensions. The peg-in-hole ser-
voing task and the the vernier alignment
task both benefit from a method which can
constrain the initial position of the object
(to a high degree) and which can essen-
tially turn a three dimensional search prob-
lem into a two dimensional search in uni-
modal space. We have presented such a
method which converges to a solution state
even when using a very simple convergence
algorithm.
The key features/contributions of our
system:
Active camera motion that recovers image
space properties of tracked objects has
shown itself to be useful in performing
alignment tasks without the need to
calibrate the camera systems.
Changes in the relative orientation of a surface with
respect to a camera cause deformations in the im-
age of the surface. Deformations can be used to in-
fer local surface geometry from motion [Koenderink
and van Doorn, 1987; Sawhney and Banson, 1991;
Cipolla and Blake, 1992; Jones and Malik, 1992b].
Since a repeating texture pattern can be thought of
as a pattern in motion, shape from texture can also be
derived from deformations [Kanade and Kender, 1983;
Super and Bovik, 1992]. Constraints on the shape of
the uundeformed structure also allow the computation
of shape from texture [Brown and Shyvaster, 1990;
Garding, 1990].
To first order, the image deformation and translation
due to relative motion can be described using a six
parameter affine transformation Awhere
r, and r, are the image coordinates and us and vy
the image translation. This is a valid approximation
assuming local planarity and weak perspective projec-
tion IKanade and Kender, 1983]. Even in situations
where full-perspective projection must be used, it can
be shown that if the change in relative orientation of
the surface patches is small, the image projections can
again be related by an affine transform [Adiv, 1985].
The recovery of 3-D structure from the affine trans-
form requires robust local estimates of the affine pa-
rameters. Consider the case where an image patch F;
is deformed into a patch Fy (either in the same im-
age or in another image) by an unknown affine trans-
form. The problem of measuring the affine transform
is to first find the corresponding patch Fs given F4 and
second to recover the affine parameters from the two
patches, Even if the centroids of the image patches
are matched, the precise sise and shape of Fy is diffi-
cult to determine since it is a function of the unknown
deformation. If this correspondence is not precisely
done, the affine parameters will be determined incor-
rectly, Thus the problem is more difficult than in stan-
dard correspondence problems e.g. the determination
of optical fiow.
Eaisting methods using image patches usually ignore
this problem. A number of techniques assume that
the affine parameters are small and then linearise the
brightness function or filtered versions of it with re-
spect to the spatial coordinates [Bergen et alL, 1992;
Koenderink and van Doorn, 1987; Campani and Verri,
1992; Werkhoven and Koenderinck, 1990]. Thus these
methods are restricted to cases where the affine trans-
form is small which in turn requires that the 3-D mo-
tion be small. [Jones and Malik, 1992b] do not assume
that the affine transform is small. Their method, how-
ever, uses brute force search techniques and again ig-
nores the determination of precise correspondence. A
natural way to find correspondence is to use straight
lines Sawhney and Banson, 1991] or closed boundary
contours KCipolla and Blake, 1992], with the change in
the sise and shape of the enclosed area defining the
affine tranform. These methods, however, fail when
such structures are absent as in many richly textured
scenes. Further, their use has only been demonstrated
on homogeneous image regions with closed boundaries.
This paper presents a technique for reliably measur-
ing affine transforms that correctly handles the dif-
culty of corresponding deformed image patches. The
image patches are filtered using gaussians and deriva-
tives of gaussians. Measuring the affine transform is
then recast as a problem of finding the deformation
parameters of the filters rather than the patches. For
example, let F4 and Fy be related by a scale change
s. Then the output of F4 filtered with a gaussian of
o will be equal to the output of Fy filtered with a
gaussian of so. Similar relationships hold for arbitrary
affine transforms and filters described by derivatives of
gaussians. These equations are exact for any arbitrary
affine transform in arbitrary dimensions.
The second part of the paper focuses on solving for
the affine transform when it can be written as the
product of a scale change and a rotation (the solution
for the general case will be considered in future pa-
pers). For example, this situation arises in the case of
rmostly translational camera motion and shallow struc-
tures (i.e. structures whose extent in depth is small
compared to their distance from the camera [Sawhney
and Banson, 1991]).
The equation can be solved by sampling the o space.
Rather than use a brute force search technique, the
search space is sampled for a few different d' and one
of the o' is picked as the operating point. The scale is
recovered by linearising the gaussian filter with respect
to a about this operating point using the diffusion
equation. Consistency is used to establish the correct
operating point. Note that linearization is done with
respect to o' as opposed to linearisation with respect
to the image coordinates done by other methods. As
a result, scale changes of arbitrary magnitude can be
dealt with by choosing different operating points. The
rotations can also be arbitrary. In contrast, linearis-
ing with respect to the image coordinates is a valid
approximation only for small affine transforms.
The gaussian (seroth moment) equation is linear in
the scale parameter. By sampling at several scales,
an overconstrained linear system is obtained. This
is solved for scale using ingular value decomposition.
Using the first moment an equation which is nonlin-
ear with respect to scale is obtained. Again, this may
be sampled at multiple scales to provide an overcon-
strained system of equations, This non-linear system
is solved using the Gauss-Newton technique. The first
moment equation also allows the computation of the
rotation. Both the formulation and the solution are
done for arbitrary dimensions, not just 2. Experimen-
tal results are shown on both synthetic and real im-
ages attesting to the robustness and simplicity of the
method.
Notation Vectors will be represented by lowercae
letters in boldface while matrices will be represented
by uppercase letters in boldface.
We will assume that the image translation is known
and has been set to sero. Methods for finding the im-
age translation are briefly discussed in section 4.2.4.
Then the affine transform has only four deformation
parameters. It is also assumed that shading and illu-
mination effects can be ignored. These can, however,
be taken care of by incorporating an additional con-
stant factor in the equations. For simplicity, we focus
on the 2-D case although the discussion is dimension-
independent.
Our discussion is based on two observations. First, the
result ofa filtering operation on two image patches will
be different in general unless the filter is appropriately
deformed for the second image patch-the deforma-
tion being a function of the affine transform. Second,
moments of the image patches are related by simple
functions of the affine transforms, and this can be ex-
ploited to compute the affine transform.
Consider two functions Fg and Fy related by an affine
transform of the underlying coordinate system. Then
Their integrals over some finite interval are related by:
expressed succinctly as
Let s; be the scale change along the ifh dimension and
n the number of dimensions. Then det(A) = IIfs;.
This can be intuitively understood as follows. Con-
sider the 1-D case (n == 1), where the affine trans-
form reduces to a scale change. Let the function F;
be graphed on a rubber sheet. The graph of F4 is
obtained by stretching the sheet and attached coor-
dinate system. The determinant term is equal to the
stretching undergone by the coordinates. Note that
the integral of a function may also be viewed as its
seroth moment.
(3) cannot be used directly because the limits on the
right-hand side depend on the affine transform and are
therefore unknown. This crucial point has not been
handled correctly before. On the other hand, taking
the limits from -oo to oo would not preserve local-
isation. The solution to this problem is to weight the
function by another which decays rapidly-here the
gaussian is used.
We present the weighted equations analogous to (3)
first for the case where A < aR. (i,e. the affine trans-
form equals a scale change s times a rotation R), fol-
lowed by the general case.
Denote the unnormalised gaussian by
Multiply both siles of (2) by H(r,o') to obtain
From the orthonormality of rotations it follows that
which allows (5) to be rewritten as
The weighted seroth moment is therefore
where the limits are taken from -oo to oo. The factor
a: detA' can be eliminated by using normalised
gaussians
in place of H. The moment equation then becomes
The integral may be interpreted as a gaussian convo-
lution or filtering at the origin. Thus we write (11]
where r; = Ar.
(12), the weighted analog of (3), is exact and valid for
arbitrary dimensions. The problem of recovering the
affine parameters has been reduced to finding the de-
formation of a known function, the gaussian, rather
than that of the unknown brightness functions. How-
ever since (12) is invariant to rotation it can only be
used for recovering the scale (the recovery of rotation
by other means is discussed later). Note that although
the limits are infinite, since the gaussian is a rapidly
decaying function, it suffices in practice to take limits
from -4o to 4o (and correspondingly from -4so to
4so on the right-hand side).
A similar equation can be shown to hold for arbitrary
affine transforms, provided generalised gaussians are
used. Define a generalised gaussian as
where M is a symmetric positive semi-definite matriz.
Then
Thus the weighted moment equation may be written
where the ilentity det(AA)4 e det(A) has been
used. The matrix AAP is a symmetric, positive semi-
definite matriz and may therefore be written
where H. is a rotation matriz and E a diagonal matri
with entries \o',sso'..is.o' (sn 2 0). Thus
Again, to show the connections to convolution and fil-
tering, this may be written as
(18) is the analog of the sero moment equation (3),
and can be used for determining the affine transform.
The level contours of the generalised gaussian are el-
lipsoids rather than spheres. The tlt of the ellipsoid
is given by the rotation matrix while its eccentricity
is given by the matri E, which is actually a fanc-
tion of the scales along each dimension. (18) clearly
shows that to recover affine transforms by filtering,
one must deform the filter appropriately; a point ig-
nored in previous work [Bergen et al., 1992; Koen-
derink and van Doorn, 1987; Campani and Verri, 1992;
Werkhoven and Koenderinck, 1990; Jones and Malik,
1992b]. The sero moment equation (18) alone does
not permit the recovery of the complete affine matriz-
only the scales and the tilt. To find the complete affine
transform, higher order moments need to be consid-
ered. Using higher order moments also permits the
use of more overconstrained equations.
The first order moments of F; and Fy are related by
The second order moments are given by
and this may be expressed as
Note that the zeroth moment equation is a scalar equa-
tion, (20) a vector one, and (21) is a matriz equation.
As before, the moment equations (20) and (21) are not
directly usable due to the difficulty that the limits of
the patches integrated over depend on the deforma-
tion. Therefore we again employ gaussian weighted
moments, using the fact that the derivatives of gaus-
sians are closely related to moments weighted with
gaussians.
The effect of filtering with derivatives of gaussians can
be obtained by differentiating the gaussian (13). First
write r4 = Ar. Differentiating (13) gives
where
and
This equation looks different from the first moment
(20) because the first derivative of the gaussian has
been normalized. Convolving with second derivatives
of a gaussian gives
where
and
(25) and (21) are seen to be closely related; the differ-
ences are the additional term due to the gaussian in
(25) and due to normalisation.
Since convolutions with gaussians and derivatives of
gaussians are so closely related to the original weighted
moment equations, they will often be referred to as
moments in the rest of the paper.
If the value of the moments is sero, (or near zero in
practice), the moment equations are ill-conditioned
and cannot be solved. This can occur in two ways;
either the signal strength is too low (i.e. the magni-
tude of F is small) or the function F is purely even or
purely odd causing some of the moment equations to
be sero. There is little that can be done in the first
case. The latter case, however, provides insight into
the number of moment equations required to solve for
the affine parameters.
Consider first the 1-D case. It is easy to see that the
even moments of any odd function will be sero while
the odd moments of any even function are zero. Since
the seroth moment is even, and the first moment is
odd and only one parameter (the scale) needs to be
determined, these two equations suffice to find it.
The situation is a little more complicated in higher
dimensions. One way of stating the problem is to con-
sider each dimension separately. Then if a function is
odd along any dimension, its contribution to the even
moment from that dimension will be zero and hence
inferences along that dimension cannot be made. Sim-
ilarly, if a function is even along any dimension, its
contribution is sero to the odd moments along that
dimension. Note that typically a function is even or
odd only at a few points over its domain, so this may
not be a significant problem.
How many moments are required in 2-D to solve for
the affine transform? In general four affine parame-
ters need to be determined. Straightforward equation
counting seems to show that there are four even equa-
tions (1 from the seroth moment and 3 from the second
moment), and there are six odd equations (2 from the
first moment and 4 if the third moment is used). Thus
even if the function is purely even or odd, moments up
to third order suffice to solve for the affine parameters.
However, the third moment is actually not required,
since the previous analysis ignores the information
available from the deformation of a gaussian. Consider
the zeroth moment when an arbitrary affine transform
A needs to be measured. In this case, the zeroth mo-
ment may be used to find the matrix AA (i,e. 3
parameters may be computed). Accounting for the
additional information available from the deformation
of the gaussian, there are at least 6 even equations [3
from the zeroth moment and at least 3 from the sec-
ond moment) and at least 4 odd equations (from the
first moment).
The function F may also be transformed so that some
of the moments are always non-sero. For example, if
instead of the function F, the magnitude of its auto-
correlation is used, the seroth and the second moment
are always nonsero. This follows from the radial sym-
metry of the auto-correlation function-which implies
that the odd moments are all zero while the even mo-
ments are nonsero.
A different transformation uses certain algebraic tricks
to convert any function to an odd or an even func-
tion thus ensuring that every moment equation is well-
defined. For example, consider the 1-D case again.
Every function F can be written as the sum of an
even part EF and an odd part OF. The odd part OF
can be converted into an odd function by taking its
magnitude. The even part EF can be converted to
an odd function by fiipping one half of the function.
The problem is somewhat more complicated in higher
dimensions. The 2-D case will be dealt with in the
solution section.
In the remainder of this paper, only the case where
the affine transform A = aB. (i.e. a scale change and
a rotation) will be considered (see section 2.1); the
general affine transform will be considered in a later
paper. The zeroth moment equation will be dealt with
first followed by the first moment equation.
When the affine transform is described by A = aR,
(12) can be written as
where o' s ao. The important point here is that
the rotation matrix does not figure in o'. The
problem of finding the scale parameter has therefore
been converted into the problem of finding the value
of o'. Older methods [Bergen et al., 1992; Koen-
derink and van Doorn, 1987; Campani and Verri, 1992;
Werkhoven and Koenderinck, 1990; Jones and Malik,
1992b] have instead concentrated on the much more
difficult problem of trying to correspond the functions
F and F.
The equation can be solved by sampling the space of
possible values of o', filtering for each sampled value
and declaring the solution to be that value of o' for
which the above equation has smallest residual error
according to some norm. A more elegant approach
uses the fact that the affine transform can be analyti-
cally interpolated. The idea is to sample over a small
set of o' and then interpolate using a Taylor series
approximation. Consider first a given o'. The Taylor
series approximation to first order gives
where a s 1+ a. The last equality follows from the
diffusion eqquation E = ov G. This allows the con-
volution (28) to be written as
The above equation is linear in a. To find s, three
filtering operations need to be performed: two gaus-
sian filtering operations and one laplacian operation.
Note that the above equation expresses the well-known
result that a laplacian can be approximated by a dif-
ference of gaussians.
Information in an image is scale dependent. There
may be information present at several different scales
or at only one of them. A method which does not
take this into account is not likely to be robust. Thus
it is desirable to solve the above equation at several
different scales (o1). Let a set of o; be chosen. For each
such oI an equation of the form (31) may be written
giving the following system of equations
This is an overconstrained set of equations in the
unknown a. The redundancy offered by the over-
constrained problem also makes it more robust with
respect to noise.
The particular choice of the o; is to some extent ar-
bitrary although some general criteria may be speci-
fied. Too small a o; will make the system sensitive
to noise while localisation requires that o, not be too
large. The actual values are not very crucial. In prac-
tice, a set of eight different o; were chosen. They
were all spaced apart by half an octave (a factor of
1.4). The filter width = 8 o; (since the filters need to
range from -4o, to 4o2). The widths actually chosen
were (3,5,7,10,14,20,28,40) (see also lJones and Malik,
1992al).
The above system of equations was cast into the fol
lowing linear least squares problem
and was solved using Singular Value Decomposition
(SVD). It was found that the lowest filters (widths
s 3,5,7) were noisy and hence they were disregarded
(one reason may be that the laplacian is noisy when
the filter sise is small). The scale was recovered fairly
accurately using the other widths (see the experimen-
tal section for details). This set of filter widths worked
better than another one where 8 filters were used with
their o, spaced apart by a factor of 1.2; presumably
because with a larger variation in scale, there is more
information available at multiple scales.
For large s (say s 1.3) the recovered scale sometimes
tends to be poor. This is because the Taylor series
approximation is good only for a small change in o.
The problem arises because in (31) the right-hand side
is expanded around the same ot as on the left-hand
side. A better approximation is obtained by expanding
o' as close to the correct scale as possible. An example
should clarify this point. Assume that the left-hand
side uses o = oo and that the scale change a is 1.3,
then it is better to expand the right-hand side around
a4 = 1.4oa (ie. the half-octave step closest to the
actual scale) rather than expanding at oo. In this case,
(31) may therefore be modified to
where s = 1,4(1-4a'). Since filtering by a set ofo is al-
ready being performed for the overconstrained system
no additional filtering operations are required. Again,
an overconstrained system may be implemented eas-
ily, For each value of o, on the left-hand side of (34),
expand around du41 on the right-hand side.
A similar scheme may be implemented if the scale a
g 0.8 by expanding around a o; which is smaller by a
factor of 1.4.
A priori the o, around which the exxpansion should be
done is not known since the value of the scale is not
available. The solution is to expand around all three
of them (ie o, 1.4oa and o;1.4) and then pick the
correct answer to be the one which gives o' close to
the operating point. Again an example will clarify this
point. Assume that the correct scale is again 1.3 and
that the three different operating points return the
following values of s (1.25, 1.32, 1.18). Consistency
decides the correct answer here. 1.18 is inconsistent
with expanding around o/1.4and can be rejected. The
other answers are both between l and 1.4 and closer to
1.4. Therefore, the appropriate operating point to pick
is 1.4o; = c41. Experimentally, this method seems to
work well. An alternative is to compare the residual
error after SVD minimisation; this does not seem to
work as well, partly because the different errors are
not really comparable-they have different numbers
of equations. Another technique that has been tried is
to make all the equations into a single overconstrained
system and solve it using SVD -based on the answer
obtained, some of the equations may be dropped and
the system resolved.
In principle the same technique can be used to ex-
pand around nonnearest neighbor operating points ds,
[j - i| > 1, if the scale gets very large (or small). The
range of scales to be expected depends on the applica-
tion. For structure for motion, a scale change of more
than 1.4 almost never happens in practice. In find-
ing shape from texture, in principle any scale change
can occur. If the surface is smooth, it is expected that
there is likely to be a neighbouring texture patch whose
scale change is less than 2.5. In this case one should
also expand around 2.0o and 0.5o in addition to o, l.4o
and 1/1.4o. Very high scale changes are probably dif-
ficult to measure in any case because of the extreme
foreshortening that this implies. We reemphasise that,
apart from such inherent limitations, our approach can
in principle handle large magnitude affine transforms
with little approximation, whereas previous methods
were limited to small transforms.
The method does not work if the output of the gaus-
sian convolution is zero (or close to sero in practice).
This can happen either if the signal is weak or if the
signal shows odd symmetry along any dimension.
The 1-D case was dealt with in section 4.1. Here it
is shown how a function in 2-D may always be con-
verted into an even function. One cannot consider each
dimension separately for this would destroy the rota-
tional symmetry of the gaussian. Instead, the function
is decomposed into parts which are radially even E,F;
and radially odd O,Fj where
The magnitude of both fanctions (] E,Fi ] ] O,Fi ]) is
then taken. The resulting functions are both even and
the gaussian convolution is nonsero for both. The SVDD
is performed on each set of these functions separately
and the one with the lower error is then used (this
ensures that if either the even or odd components is
really small, it is ignored).
Before scale can be recovered, the two patches must
be aligned by finding the image translation. These can
be found using traditional optical fiow or displacement
schemes IAnandan, 1989]. Alternatively, the residual
of the SVD error can be used to localise the image
translation to S0.5 piels. This is done in the fol
lowing manner. The first image patch is filtered with
the set of gaussians. The second patch is filtered with
gaussians at every piel in a small window centered
at the first patch's location and the SVD computed.
That pixel for which the SVD residual is minimised is
declared to be the correct image translation. Experi-
mentally, this method was found to work satisfactorily.
Note that in general no additional filtering operations
are required since the filtering operations are done at
every point in the image anyway.
Experiments were carried out both on synthetic images
as well as a pair of real images. The first synthetic
image (Figure 1) shows a cosine wave generated by
the equation F(a,y) = 127cos(yV-+ 4,). The cosine
was picked for the following interesting properties. It
can be made even or odd at any point depending on
the value of ,, Further, there is no information along
the y direction (the so-called aperture problem). In
spite of that the scale can be recovered.
For the first experiment, A, Yas chosen to be sero, so
that the function was even. 4y == 0.2 was chosen. A
second cosine function was generated using the follow-
ig fanction F(s,y) = 127 cos(4=-+ 4,) (Figure 2).
F is rotated 90% with respect to F4 and also scaled by
the factor s. For various values of s, the scale was re-
covered using the seroth moment. The results are tab-
ulated in Table 1. The experiment was repeated with
noise added. First, uniform noise ranging from -10
to 10 was added to F4. Second, ganssian noise with
a standard deviation of 10 was added to Fy. These
results are also tabulated in Table 1. Two operating
points were used: a and 1.4o. The appropriate oper-
ating point was picked as discussed in the text.
Table 1 is to be read as follows. The first column in
Table 1 is the actual scale while column 2 shows the
recovered scale in the noise-free case. Two different
percentage errors are tabulated and they arise from the
following considerations. Assume that an object is at a
depth of z6 and after a translation T, in the s direction,
its new depth is z1 = A + T,. Then the percentage
srror i findin te 4wWO 1/= i sien bs 3
100 and this is tabulated in column 3 for the noise-
free case. On the other hand, the percentage error in
finding the quantity T,/so is given by P++100 and this
is tabulated for the noise-free case in column 4. Which
of these quantities is more important? Since the depth
A is a priori unknown, the quantity of relevance at
least in the motion case is T,fso and the corresponding
percentage error is more significant. Similar values are
tabulated when gaassian noise (columns 5,6 and 7) and
uniform noise (columns 8,9, and 10) are added.
The results show that even with noise depth recon-
struction effectively has an accuracy on the order of
several percent. The results are excellent in the noise-
free case. The percentage errors in column 3 are all less
than about 3% while even in column 4 the percentage
errors do not exceed 7%. Note that the method recov-
ers scale accurately in spite of the large rotation.
With noise added, the results are as good exxcept for
the lowest scales (1.05 and 1.10, corresponding to the
largest depths). These results for the lower scales
might be improved by using operating points separated
by ratios smaller than 1.4.
The experiment was repeated using W, = x/2 for both
images. The method failed because the function now
becomes an odd function at the origin and thus the
result of gaussian filtering is sero. However, if the
function is transformed into an even function using
the methods discussed in the text, the seroth moment
can once again be applied and the results are similar.
The experiment was repeated with random dot images.
A random dot image of sise 64 by 64 was generated
(Figure 3). The image was then affine transformed and
smoothed using a cubic interpolation scheme. For var-
ious values of the scale factor s, the scale was recovered
using the seroth moment method. The results are tab-
ulated in Table 2. The highest error in column 4 (rel-
ative depth error) is less than 9% if the smallest scale
(1.05) is ignored. Again the relative error in s (column
3) is much lower. The error is somewhat larger in this
case because the program that affine transforms the
image does interpolation which tends to destroy im-
age structure. This is more serious at the lower scales.
Finally the algorithm was tested on a pair of real im-
ages from a sequence [Sawhney and Banson, 1991].
The images were taken with a Sony ccd camera us-
ing a robot moving straight ahead. The robot moved
about 1.4 ft between frames. Since the original images
were taken with the intent of using a line based algo-
rithm, most of the objects have little intensity varia-
tion in their interior. However, the posters on the back
wall show some intensity variation and can therefore
be used. Points 1 and 2 were picked by hand in the first
image (Figure 4). The corresponding points in the sec-
ond image (Figure 5) were determined using the SVDD
residual error. For point 1, the recovered scale was
1.07 which corresponds to a distance of 1.4/(1.07 - 1)
s 20 ft. For point 2 the recovered scale was 1.06 which
corresponds to a distance of 1.4/(1.065 - 1) = 21.5 ft.
The measured distance to the back wall is 20.3 ft. The
accuracy is thus within 6%.
Again for the case where the affine transform is de-
scribed by A = aB, the first moment equation (22)
may be written
The diffusion equation applied to the derivatives of the
gaussian gives the following identity
where G,, denotes the derivative of the gaussian with
respect to the rP' coordinate. Using this identity,
the right-hand side of (37) is expanded around and
rewritten as
where R; is the P? row of R and a = 14 a, Note
that there are 2 such equations. This may be more
conveniently written as
where
and
The rotation matriz can be eliminated by taking the
dot-product (i.e. the magnitude) of both sides of [40)
and equating them. This gives
This is a polynomial equation in the unknown a. As
before several different scales o are used to give an
overconstrained system. The resulting system can be
solved using the Gauss-Newton technique [Gill et al.,
1981]. The Gauss-Newton procedure works by lin-
earizing the system around the current estimate of the
solution reducing the problem to a linear least-squares
problem. Define a vector function c(aa) where the PPA
component is given by
Then if ag is the current estimate of a, then Aa+1 =
as + ps where ps is the solution of the linear least
squares problem
where a quantity subscripted by k denotes that quan-
tity evaluated at as and J(a) is the Jacobian matriz
of c(o).
The least squares problem was solved using SVD and
convergence was found to be rapid--within a couple
of iterations. The method was tested on a sine-wave
pattern.
In two dimensions, the rotation may be computed in
the following manner. Consider (40) again. This may
be rewritten as
where
is a known quantity (since a is now known). Let
b = (b,%). Then (48) can be transformed into the
following form
where
== (cos,sin ) and 8 is the rotation angle. Us-
ios the W4enwwe co4 i(@PFi, aua B-'
-B/(detB), (48) can be transformed into the follow-
ing pair of equations each linear in the unknown sin 8.
where y (e4,eg). Such pairs of equations can be
written for every o, and the resulting linear system of
overconstrained equations can be solved using SVD for
the rotation angle 8.
Future work includes the solution for the case of the
general affine transform as well as the use of the second
moment equation. Other possibilities include the au-
tomation of the process over the entire image and the
detection of occlusions. Finally, the use of the affine
transform to find surface orientation from both texture
and motion cues will be explored.
Acknowledgements We wish to thank Al Han-
son for his useful comments on early drafts of this pa-
per. The first author also wishes to thank Harpreet
Sawhney for many fruitful discussions and for his con-
stant encouragement.
ALVINN (Autonomous Land Vehicle In A
Neural Network) [Pomerleau, 1992] has
shown that neural techniques hold much prom-
ise for the field of autonomous road following.
Using simple color image preprocessing to
create a grayscale input image and a 3 layer
neural network architecture consisting of 960
input units, 4 hidden units, and 50 output units,
ALVINN can quickly learn, using back-propa-
gation, the correct mapping from input image
to output steering direction. See Figure 1. This
steering direction can then be used to control
our testbed vehicles, the Navlab 1 [Thorpe,
1991] and a converted U.S. Army HMMWV
called the Navlab 2.
ALVINN has many characteristics which make
it desirable as a robust, general purpose road
following system. They include:
These features make ALVINN an excellent
candidate as the building block of a neural sys-
tem which can overcome some of the problems
which limit its use. The major problem this
research addresses is ALVINN's lack of ability
to learn features which would allow the system
to drive on road types other than that on which
it was trained. In addition to overcoming this
problem, the system must meet the current
needs of the autonomous vehicle community
which include:
From these requirements we have begun
developing a modular neural system, called
MANAC for Multiple ALVINN Networks In
Autonomous Control. MANIAC is composed
of several ALVINN networks, each trained for
a single road type that is expected to be
encountered during driving. See Figure 1.'This
system will allow for transparent navigation
between roads of different types by using these
pretrained ALVINN networks along with a
connectionist integrating superstructure. Our
hope is that the superstructure will learn to
combine data from each of the ALVINN net-
works and not simply select the best one.
Additionally, this system may be able to
achieve better performance than a single
ALVINN network because of the extra data
available from the different ALVINN net-
works.
The MANIAC system consists of multiple
ALVINN networks, each of which has been
pretrained for a particular road type. They
serve as road feature detectors. Output from
each of the ALVINN networks is combined
into one vector which is placed on the input
units of the MANIAC network. The output
from the ALVINN networks can be taken from
either their output or hidden units. We have
found that using activation levels from hidden
units provides better generalization results and
have conducted all of our experiments with
this connectivity. The MANIAC system is
trained off-line using the back-propagation
learning algorithm [Rumelhart, 1986] on
imagelsteering direction pairs stored from
prior ALVINN training sessions.
The architecture of a MANIAC system which
incorporates multiple ALVINN networks con-
sists of a 30x32 input unit retina which is con-
nected to two or more sets of four hidden units.
(The M1 connections in Figure 2.)This hidden
layer is connected to a second hidden layer by
the M2 connections. The second hidden layer
contains four units for every ALVIINN network
that the system is integrating. Finally, the sec-
ond hidden layer is connected to an output
layer of 50 units through the M3 connections.
All units in a particular layer are fully con-
nected to the units in the layer below it and use
the hyperbolic tangent function as their activa-
tion function. Also, a bias unit with constant
activation of 1.0 is connected to every hidden
and output unit, The architecture of a
MANAC system incorporating two ALVINN
networks is shown in Figure 2.
The topology of the input retina and M1 con-
nections of MANIAC system is identical to
that of the Al connection topology of an
ALVIINN network. See Figure 1. This allows
us to incorporate an entire MANIAC system
into one compact network because the Al con-
nection weights can be directly loaded onto the
M1 connections for a particular set of first
layer hidden units of the MANILAC network.
Simulating the entire MANILAC system, then,
does not entail data transfer from ALVINN
hidden units to MANIAC input units, but only
a basic forward propagation through the net-
work.
It is the A1 connection weights of the
ALVINN network that extract vital features
from the input image for accurate driving. So
in addition to allowing easy implementation of
the MANIAC network, the network topology
of the M1 connections allows us to capture
important weight information in the MANIAC
system that the ALVINN hidden units have
learned. These features can be interpreted
graphically in two dimensional views of the
A1 connection weight values. Typically, a net-
work trained for one lane roads learns a
matched filter that looks for the road body,
while a network trained on multi-lane roads is
sensitive to painted lines and shoulders.
Tb train the MANAC network, stored imagel
steering direction pairs from ALVIINN training
runs are collated into a large training sequence.
These pairs consist of a preprocessed 30x32
image which has been shifted and rotated to
create multiple views of the original image
along with the appropriate steering direction as
derived by monitoring the human driver during
ALVINN training. See [Pomerleau, 92] for an
in-depth discussion of the image preprocessing
and transformation techniques. After collation,
the sequence of pairs is randomly permuted so
that all exemplars of a particular road type are
not seen consecutively. The current size of this
training sequence for a two ALVINN
MANIAC network is 600. If additional
ALVIINN networks are used, 300 images per
new ALVINN network are added to the train-
ing sequence. This sequence is stored for use
in our neural network simulator.
Next, weights on each of the connections in
the MANIAC network must be initialized.
Because the MANIAC M1 connections consist
of precomputed ALVINN A1 connection
weights, they must be loaded from stored
weight files. After this is done, the M2 and M3
connection weights in the MANILAC network
are randomized. This weight set is then ready
for use as the initial starting point for learning.
To do the actual training, the network architec-
ture along with the weight set created as dis-
cussed in the previous paragraph and the
stored training sequence, are loaded into our
neural network simulator. Because the
MANLAC M1 connection weights are actually
the pretrained ALVINN weights who serve as
feature detectors, the M1 connections are fro-
2zen so that no modification during training can
occur to them. See Figure 2.
Initially, training is done using small learning
and momentum rates. These values are used
for 10 epochs. At this point they are increased
(approximately doubled) for the remainder of
training. This technique seems to prevent the
network from getting stuck in local minima
and is an adaption of a technique used in
ALVINN training.
The back-propagation learning algorithm is
used to train the network. The stored images
are placed on the input units of the MANILAC
network while a gaussian peak of activation is
centered at the correct steering direction on the
50 output units of the network. After about 60
epochs, the network has converged to an
acceptable state and its weights are saved. This
takes approximately10 minutes on a Sun
Sparcstation 2.
It should be noted that MANIAC uses the
same output vector representation as AILVINN,
This allows the output of the MANIAC net-
work to easily be compared with that of
ALVINN for quantitative study and also
allows for the use of existing software in the
MANIAC-vehicle interface.
Once the network has been trained, we use it in
our existing neural network road following
system to produce output steering directions at
approximately 10 Hz.
Empirical results of a MANAC system com-
posed of two ALVINN networks have been
encouraging. For this system, one ALVINN
network was trained to drive the vehicle on a
one lane path while the other learned to drive
on a two lane, lined, city street. The resultant
MANIAC network was able to drive on both
of these road types satisfactorily.
To determine more quantitative results, image/
steering direction pairs from the same two road
types as well as from a four lane, lined, city
street were captured. See Figure 3. Using these
stored images, ALVINN networks were
trained in the lab to drive on the one lane
paved path and the two lane, lined city street.
Also, a MANIAC network integrating the
same two ALVINN networks was trained. The
results of these experiments are summarized in
Table 1. In Table 1 the columns represent the
average error per test image for a particular
road type and the rows represent the type of
network that is being used. The errors com-
puted are of two types, SSD error and Output
Peak error. SSD error is the sum of squared
differences error while Output Peak error is the
absolute distance between the position of the
gaussian peak in the desired output activation
and the peak in the actual output activation.
SSD error can be thought of as a measure of
the network's ability to accurately reproduce
the target vector while Output Peak error is a
measure of the ability of the network to pro-
duce the correct steering direction.
The initial comparison to notice in the table is
that the ALVINN network trained for a partic-
ular road type always performs significantly
better (> 50%) than the ALVINN network
trained for the other road type when presented
test images of the type of road on which it is
trained. This is to be expected. Also notice that
the single MANIAC network, which has been
trained to respond properly to both road types,
typically compares well to the correct
ALVIINN network (within 11% in all cases).
As mentioned earlier, this amount of error is
acceptable to properly drive the vehicle.
The case of the four lane road is unique in that
neither of the ALVINN networks nor the
MANIAC network saw a road of this type. In
this case, the response of the one lane path
ALVINN network is nearly identical to when it
was presented a two lane, lined, city street.
Because this type of network typically
responds to the body of the road and the fact
that the two and four lane roads are both sig-
nificantly wider than the one lane path, ie.
have a larger body area, this response was
expected. A more interesting response is that
of the two lane road ALVINN network. It
seems to respond better to the four lane road
images than it does to the two lane road test
images. A possible explanation of why this is
occurring can be seen in Figure 3. The four
lane road and the two lane road look almost
identical. One slight difference, though, is that
the contrast of the road/offroad boundary is
slightly higher in the four lane road case than it
is in the two lane road case. This difference
could help the network localize the road better,
and because we want the vehicle to drive in the
left lane, close to the yellow line, the correct
output is identical to the two lane road case.
The most interesting result, though, is that
when presented with four lane road images,
the MANIAC network actually performs better
than either the one lane path ALVINN network
or the two lane road AILVINN network. In both
the prior cases, the MANILAC network per-
formed slightly worse than the best ALVINN
network for a particular road. This could imply
that the MANIAC network is using informa-
tion from both networks to create a reasonable
steering direction at its output. This will be
discussed more in the following section.
A central idea that this research is trying to
examine is that of improving performance and
making connectionist systems more robust by
using multiple networks - some of which
might be producing incorrect results. In our
system the key point to notice is that although
a particular ALVINN network may not be able
to drive accurately in a given situation, its hid-
den units still detect useful features in the input
image. For example, consider an ALVIINN net-
work that was trained to drive on a two lane,
lined road. The features that it learns are
important for accurate driving are the lines on
the road and the road/non-road division. Now
present this network with a paved, unlined
bike path. The ALVINN network will respond
in its output vector with two steering direction
peaks. The reason for this is that one of the
features that the network is looking for in the
input image is the delineation between road
and non-road. Because this occurs at two
places in the image of the paved bike path, the
feature detecting hidden units produce a
response which indicates that the road/non-
road edge is present at two locations. If these
hidden unit activations were allowed to propa-
gate to the output of the network, the charac-
teristic two peak response would appear.
Although in reality this is the incorrect
response, it is a consistent response to this
input stimulus. A similar scenario holds for
other ALVINN networks given input images of
road types for which they haven't been trained.
Because the response of particular ALVINN
network is consistent when presented with
similar images, the MANILAC network can use
this 'extra' data to produce a correct, perhaps
better, steering direction than a single
ALVINN network. It is possible that this is
what is happening in the case of MANILAC
driving better on the four lane road than either
of the ALVIINN networks.
There are many directions this research can
take but perhaps the most interesting is that of
developing se[f-training systems. In the cur-
rent implementation of the MANILAC system,
ALVINN networks must be trained separately
on their respective roads types and then the
MANAC system must be trained using stored
exemplars from the ALVINN training runs. If
a new ALVINN network is added to the sys-
tem, MANIAC must be retrained. It would be
desirable to have a system that, when given
initial or new ALVINN networks, created its
own training exemplars and was able to auto-
matically learn the correct MANIAC network
weights. Creating training exemplars from
existing network weights is essentially the net-
work inversion problem. Techniques such as
those developed by [Linden, 1989] may pro-
vide clues of how to do this one to many map-
ping that can create an input exemplar from an
output target. It can be argued that this task is
extremely difficult, even impossible, due to the
high dimensionality of most networks, but per-
haps it is worth taking a hard look at imple-
menting some network inversion techniques
because of the benefits that can be obtained by
having self training modular neural networks.
Another area in which modular neural systems
such as MANILAC may be useful is that of
incorporating information from different
sources. An example of this idea is to use
MANAC as a framework in which to add
sensing modalities other than video. In addi-
tion to a video camera, our testbed vehicle, the
Navlab 2, is equipped with an infrared camera
and two laser rangefinders. If these devices can
be used as input to ALVINN-like systems
which produce a steering angle as output, it is
reasonable to assume that a training technique
similar to the one used in the current video-
only MANIAC system will result in a network
which will be robust in all of the component
network domains. This could lead to highly
robust autonomous systems which could oper-
ate in a variety of situations in which current
systems fail. Driving with the same system in
both daylight and at night is an example. In
this scenario video images provide sufficient
information to drive in the daytime but at night
sensors such as infrared cameras would be
necessary. The infrared cameras need not go
unused in the day though, as their output
would provide addition information to the
modular network.
In addition to the previous areas of work, there
is much to be done with developing systems
which can allocate their resources and group
relevant features together. It has been shown
that modular neural networks can learn to allo-
cate their resources to match a given problem,
such as locating and identifying objects in an
input retina [Jacobs, 1990], while the cascade
correlation algorithm provides a way to pro-
duce appropriately sized networks. [Fahlman,
1990] By using similar techniques in a
MANIAC-like system, the need to pretrain
ALVINN networks would be eliminated. It is
not clear, though, how new information would
be incorporated into this type of system once it
has been trained.
This research has focused on developing a
modular neural system which can transpar-
ently navigate different road types by incorpo-
rating knowledge stored in pretrained
networks. Initial results from the autonomous
navigation domain are promising. Although
the system is simplistic, it provides a starting
point from which we can explore many differ-
ent areas of the connectionist paradigm such as
self-training modular networks and network
resource allocation. In addition to these areas,
autonomous navigation tasks such as multi-
modal perception can be studied.
This research was partly sponsored by
DARPA, under contracts ''Perception for Out-
door Navigation'' (contract number DACA76-
89-C-0014, monitored by the US Army Topo-
graphic Engineering Center) and ''Unmanned
Ground Vehicle System'' (contract number
DAAE07-90-C-R059, monitored by TACOM)
as well as a DARPA Research Assistantship in
Parallel Processing administered by the Insti-
tute for Advanced Computer Studies, Univer-
sity of Maryland. Many thanks also go to the
Semiautonomous Surrogate Vehicle group at
Martin Marietta, Denver, where this research
began.
Recently, there has been much research in the
field of sensor planning [Cowan and Bergman,
1989, Hutchinson and Kak, 1989, Ikeuchi and
Kanade, 1989, Tarabanis et al., 1991a]. The ba-
sic problem is that in setting up an automated
system for monitoring some process, the effec-
tiveness of the system can largely be determined
by the locations, types and configurations of the
sensors used. To manually determine these pa-
rameters on a case by case basis may not be cost
effective or accurate, and the resulting system
may not be optimal in any sense, It may be
better to have an automated system for deter-
rmining the sensor locations and parameters for
monitoring a given task.
To that end, many systems have been and are
being developed which, based on geometric mod-
els of an environment and models of the sensors,
can generate sensor locations and settings which
provide a robust view of specific features so that
the features are detectable, recognizable, mea-
surable, or meet some other task constraints. In
general, the sensors are cameras and a robust
view implies that the camera must have an un-
obstructed view of the entire feature set, which
must lie within the depth-of-field of the camera
and must be magnified to a given specification.
Sensor planning systems can then generate cam-
era locations, orientations, lens settings (focus-
ring adjustment, focal length, aperture), and in
some cases lighting plans to insure a robust view
of the features,
It is interesting to note that while research in
robot motion planning abounds, research in sen-
sor planning has focused on sensor planning for
static scenes. It is our belief that an intelligent
robot system capable of planning its own actions
should be capable of planning its own sensing
strategies. With a dynamic sensor planning sys-
tem, this goal is closer to a reality, Robots in-
volved in manufacturing or assembly can deter-
mine appropriate sensor locations, Teleopera-
tors can have the robot system guarantee robust
viewpoints during the operation, The intelligent
motion plans which researchers spend so much
effort computing can be monitored in an intelli-
gent fashion.
To that end, we have been exploring methods
of extending the sensor planning abilities of the
'-MVP'' Machine Vision Planning [Tarabanis,
1991, Tarabanis et al., 1991a] system to func-
tion in environments where objects are moving.
In particular, we focus on sensor planning in a
dynamic robotic work cell environment.
In previous work, we described a technique
for sensor planning in a dynamic environ-
ment [Abrams and Allen, 1991], which was im-
plemented using a simulated model of a simple
moving object. Here, we present a detailed anal-
ysis of the dynamic sensor planning problem and
improved versions of the original algorithms. In
addition, experimental results using a model of
a dual-robot work cell are presented in which we
automatically monitor a task in the work cell,
A complete description of the MVP system is
beyond the scope of this paper, For details,
see [Tarabanis, 1991, Tarabanis et al., 1991a,
Tarabanis et al., 1991b]. In brief, MVP takes
a constraint based description of the vision task
requirements and synthesizes what has been
termed a generalized viewpoint, which is an eight-
dimensional vector incorporating sensor loca-
tion, orientation, and lens parameters including
aperture and effective focal length. The con-
straints MVP considered in determining view-
points are depth-of-field, field-of-view, resolu-
tion, and unoccluded visibility.
MVP contains analytical relationships for the
optical task constraints (resolution, focus, field-
of-view), and uses 3-D solid geometric models
of the environment to formulate visibility con-
straints. (The geometric models are polyhedra,
both convex and concave,) The constraint equa-
tions can be thought of as defining hypersurfaces
bounding feasible regions in the 8-dimensional
parameter space of the generalized viewpoint.
These constraints are combined in an optimiza-
tion setting to produce a generalized viewpoint
which meets all task constraints with as much
margin for error in sensor placement and set-
ting as possible (i,e., as far away from all hyper-
surfaces as possible). Using CAD descriptions
of the object to be viewed and its environment,
MVP generates the visibility region for viewing
the desired features. This region is calculated
to be the total volume in space from which the
features are viewable without obstruction. This
volume is used in the optimization stage of MVP
for finding the best viewpoint.'
There are two basic cases which must be dealt
with separately in the dynamic sensor planning
problem. First is the case where the target ob-
jects, i.e, those features which must be viewed,
remain stationary and other objects, such as the
robot which is performing some operation on the
stationary part, moves. This case can arises in
teleoperation and in many manufacturing tasks
(i,e. spray-painting, spot-welding, etc.) Sec-
ond is the case where the targets to be viewed
are moving. This can also arise in teleoperation
and in other manufacturing tasks (i.e, pick-and-
place, part insertion, etc.),
The main difference between these two cases is
that in the first case, if a viewpoint is found
to be valid at some point during the task, it is
guaranteed to be valid with respect to all op-
tical constraints at all times during the task.
This is because the functions defining the op-
tical constraints only depend on the target fea-
ture locations and the sensor parameters, and
not on the positions or orientations of obstacles
in the environment, This fairly obvious, but im-
portant property allows us to ignore changes in
the optical constraints over time and focus only
on changes in the geometric parameters, ie, the
visibility constraint.
The second case is more difficult because it re-
quires an examination of how changes in the po-
sition and orientation of the target features effect
the optical parameters, particularly focus and
resolution. However, if the viewpoint is consid-
ered in terms of a coordinate frame attached to
the feature set, the target can always be con-
sidered stationary with the entire environment
considered as moving. The only limitation is
that the entire feature set must be moving as
a single rigid body, i,e. features can not move
independently, While extremely important, in-
dependently moving features are not yet handled
in this work, although it is being examined as
part of ongoing research.
To summarize, the exact problem we are deal-
ing with is one in which an accurately movable
camera is being used to monitor a task. In this
task, the actual target we are monitoring does
not move, but other ob jects in the environment,
such as a robot arm, or other mechanical parts,
move in a way which is known a priori, The
problem is to find where to place the camera, and
when and where to move the camera, so that at
all times during the task, we have a good view-
point for monitoring the task.
At a first glance, it may seem that the dynamic
sensor planning problem can be solved trivially,
The naive algorithm for computing a series of
viewpoints is as follows:
There are several problems with this approach.
First, it makes no attempt to reduce the number
of sensor placements required. Second, a view-
point is used up until the moment it becomes
invalid, or at least up until the point at which
the margin for error becomes very small. This
defeats the purpose of MVP, which is to find a
viewpoint which has as large a margin for er-
ror as possible. Worse, by the time a viewpoint
is deemed unacceptable, due to errors in sensor
placement, etc., the viewpoint may have been
invalid for some time.
The basic problem is that this technique does not
use knowledge of the motion in computing view-
points which will be valid for a long period of
time. It is conceivable that a new viewpoint will
be needed at every 2t, since objects are moving
in unaccounted for paths. A better approach,
such as the one presented below, uses its knowl-
edge of how objects in the environment move to
plan better viewpoints.
The approach being taken is a Temporal Inter-
val Search method, which is is based on the use
of swept volumes. The geometric models of the
moving objects are swept through their paths to
compute the regions in space which, during some
interval, are occupied by some moving object in
the environment. The MVP algorithms are then
run using the swept volumes for the occluding
bodies as opposed to the actual models, thus re-
ducing the dynamic sensor planning problem to
a static problem. If no viewpoint is found consid-
ering these swept objects over a time interval, a
temporal interval search is performed to find the
largest time intervals which can be monitored by
a single viewpoint. This allows us to plan a se-
ries of viewpoints and the times at which they
become feasible.
Given that we have an object O whose motion is
known over a time interval T, we define T(T. O)
to be the volume swept out by O during T. For
example, in 2 dimensions, if O is an axis-aligned
unit square moving one unit per second in the
positive direction, and T is 3 seconds, T(T. O)
is a l x 4 square. The key to using swept objects
for sensor planning (or, in fact, for any collision
avoidance problem) is that in planning around
an obstacle given by T(T, O), you guarantee that
you have avoided the actual obstacle O at any
instant in interval T, This observation was tnade
by Cameron in [Cameron, 1984] for the ''clash
detection'' (robot collision avoidance) problem.
Let V represent visibility volume for 7(T. O).
V is the set of all points (in 3-space) which give
views of the target which have no obstructions
(due to O) for the entire time interval T. If V is
a null volume, there is no single viewpoint which
would be valid for all of T. Even if V is not null,
there is no guarantee that there are viewpoints
within V which satisfy the optical constraints of
MVP.
A possible problem when using swept volumes
for collision avoidance type problems is that
sweeping an object discards all information re-
garding where the object is at any particular
moment. We present a technique for recover-
ing sufficient temporal information to plan sen-
sor lucations, If using V as a visibility volume,
MVP is unable to find a viewpoint which meets
all constraints, we conclude that T is too large
an interval to plan a single viewpoint for, given
the motion of O, We have no information con-
cerning when any particular viewpoint becomes
invalid; we only know that we can not find a
single viewpoint which is valid for the entire in-
terval. Recomputing T(T. O) for a shorter time
interval T will yield a smaller obstacle, a larger
V, and MVP may now be able to find a view-
point.
We can now present the algorithm formally. As-
sume we have a polygonal target r which we wish
to monitor during the time interval T = [to,4].
During T, there is a set of known obstacles O
through O,4, which move in known paths, The
goal is to plan a single viewpoint valid for the
entire interval, if such a point exists, or to de-
termine a sequence of viewpoints which, when
executed at the appropriate times, allow the fea-
tures to be monitored for the entire interval,
Note, this is not strictly a binary search. Step
4 above only looks at the first half of the time
interval, i,e. T; = [o./2]- The algorithm
searches for the endpoint of the first time interval
for which MVP can find one viewpoint. It does
this by examining [to.t], then [o.;2].!o./d.
and so on. Once a single viewpoint is found for,
say, the interval [t6,t,], step 5 sees to it that the
interval [t;,4] is examined. If no viewpoint is
found for this whole interval, [t4,44(-0/2] is ex-
amined, and so on, until a single viewpoint is
found for, say, the interval [t;,t;]. This proess
continues until a viewpoint has been found which
is valid until t,,, The critical times are the end-
points of the intervals, i,e, the times at which
the sensor must be moved.
The computation of swept volumes is central to
this algorithm. Depending upon the format in
which the motion is known, the computation of
swept volumes may not be expensive. If piece-
wise linear translational motion is all that is al-
lowed, then the computation of swept volumes is
certainly tractable [Weld and Leu, 1990]. How-
ever, if more general types of motion are allowed,
as in the motions which would be executed bv
a typical articulated manipulator (rotations in
particular), the exact computation of swept vol-
umes is more expensive, but not impossible. U n-
fortunately, sweeping is not closed over the set of
polyhedra when rotational motion is permitted.
An articulated robot arm moves strictly in rota-
tions about its joint axes, so the resulting swept
volumes are not polyhedral (they would contain
circular arcs, spherical patches, and other curved
surfaces). These objects would not be useable
in MVP. Korein gives an algorithm for comput-
ing polyhedral approximations [Korein, 1985] of
the swept volumes formed by the motion of ar-
ticulated robot links. These techniques can be
used to simplify the computation of the swept
volumes.
Strictly speaking, MVP directly computes vol-
umes of occlusion, not volumes of visibility. In
theory, the complement of a volume of occlusion
is a volume of visibility. In practice, the comple-
ment of a volume of occlusion with respect to the
workspace of the manipulator placing the sensor
yields the usable visibility volume. In the cur-
rent dynamic sensor planning implementation,
instead of computing a swept volume and then
computing its occlusion volume, we compute a
set of volumes of occlusion at discrete points
along the tra jectory, These volumes of occlusion
are then unioned to form the volume of occlusion
for the entire interval. This is possible because
the volume of occlusion generated by the union
of a set of obstacles (for viewing a particular tar-
get) is equal to the union of the volumes of oc-
clusion generated by each obstacle. One benefit
of this approach is that subdivisions of the time
interval do not require recomputing new swept
volumes. Instead, the appropriate subset of the
instantaneous occlusion volumes can be unioned
to approximate the volume of occlusion for any
given interval.
The result of the temporal interval search will be
a set of viewpoints and critical times at which
to execute them, However, an explicit represen-
tation of time is not required for the temporal
interval search, in which case the critical times
are not times at all but, rather, critical events,
If, for example, the motions of a robot have been
planned as a series of joint-space moves, the crit-
ical events would be joint angle values. If the
motion was planned in cartesian space, the criti-
cal events would be cartesian positions. Finally,
if the robot motion was planned on some global
time scale (perhaps avoiding other moving ob-
stacles), the critical events would be actual times
on this scale. As long as at task execution time
there is a way to determine when the critical
events arise, (i,e. by waiting for the robot to be
within some distance of the prescribed position ),
the viewpoints can be realized.
We have modelled our laboratory environment
using a CAD system (see figure 1). The model
includes two PU MA 560 robots and the object
to be monitored during the task. The first robot
(I) executes tasks, while the second robot (II)
has a camera mounted on it. In the simulated
experiment, robot I passes over the object as if
it were performing an operation on it, such as
spray-painting. During the task, robot II needs
to monitor a feature inside the object. A CAD
model of the object and the feature is shown
in figure 2. The target (ie, the feature to be
viewed) is the top face of the inner cube.
In order to compute viewpoints for monitoring
robot I's task, we need to compute the visibil-
ity volume for the object as the robot moves in
the vicinity of the object, i,e, the volume from
which the object is visible during the entire task,
In other words, we need to compute the visibility
volume for T( TaskIntervalRobotI, .) The visibil-
ity volume is computed by first computing the
volume of occlusion, and subtracting it from the
reachable work-space of robot II, in order to pre-
vent the computation of a viewpoint which is ei-
ther unreachable or has an occluded view. The
volume of occlusion is approximated using the
discrete union algorithm described earlier.
In the experiment, the robot model is stepped
through a series of positions along its planned
trajectory. At each step, the volume of occlu-
sion is computed as in the static sensor plan-
ning problem. The individual volumes of oc-
clusion are unioned together to form the vol-
ume of occlusion for the entire trajectory. In
this way, we approximate the volume of occlu-
sion for T( TaskInterval, RobotI) without explic-
itly computing T( TaskInterval, RobotI ). In fig-
ure 4 we show a discrete approximation to the
volume swept out by Robot I during its task (i,e.
T(TaskInterval, RobotI)). The volume of occlu-
sion resulting from this motion is shown in fig-
ure 5. The volume of occlusion resulting from
the walls of the part (i,e, due to self-occlusions)
is shown in figure 3. These two volumes were
unioned to form the total volume of occlusion.
An approximation to the workspace of Robot II,
the camera-carrying robot, (called the robot's
reachability volume) was generated. The total
occlusion volume was subtracted from this reach-
ability volume giving the reachable/visible vol-
ume, This volume, which containS all points in
space where the robot can position the camera
such that the target can be seen without occlu-
sion, was used in the optimization stage of MV P
in order to compute a viewpoint.
Since MVP was unable to find a valid view-
point for the entire task, the temporal interval
search was used to find subintervals for which
we can find valid viewpoints, Instead of recom-
puting the swept volumes for each subinterval
examined, the discrete approximation allows us
to union the appropriate subset of volumes of
occlusion. The subintervals found for this task
are shown in figures 6 and 7. The generated
volumes of occlusion due to the robot's motion
during each sub-interval are shown in figures 8
and 9. These volumes were again unioned with
the self-occlusion volume and subtracted from
the reachability volume forming the volumes of
reachability/visibility shown in figure 10 and 11.
These volumes were used in the optimization,
and MVP was able to compute a viewpoint for
each interval. Simulated views from these view-
points are shown in figures 12 and 13.
In this section we describe some alternate ways
of examining the both the static and dynamic
sensor planning problems. The observations and
discussions of this section are the motivation for
additional research which is currently being car-
ried out.
One can view the static sensor planning prob-
lem as a configuration space problem. Using this
view, the sensor's possible configurations are de-
scribed by the generalized viewpoint. The valid
configurations are bounded by the constrain-
ing hypersurfaces in the 8-dimensional parame-
ter space of the generalized viewpoint. However,
the combination of the highly nonlinear fashion
of the sensor constraining equations, plus the
high dimensionality of the generalized viewpoint,
standard techniques for searching configuration
spaces appear to be unpractical. This is one of
the reasons why MVP takes a numerical opti-
mization approach to searching the sensor's pa-
rameter space. However, the configuration-space
analogy will be useful in motivating other ideas
below.
Dynamic sensor planning is to static sensor plan-
ning what path-planning with stationary ob-
stacles is to path-planning with moving obsta-
cles. Erdmann and Lozano-Perez [Erdmann and
Lozano-Perez, 1987] proposed a configuration
space-time for solving such problems in two di-
mensions. They presented two approaches, one
for translating polygons and one for two-link ar-
ticulated planar arms. Their approaches focused
on the efficient construction of slices of configu-
ration space-time. The slices were chosen so as
to include easily computable time-varying con-
straints, simplifying the search from the start
configuration to the goal configuration.
In dynamic sensor planning with stationary tar-
gets, the only constraints in configuration space
which move are the boundaries of the visibil-
ity volume. Even if the obstacles are only al-
lowed restricted classes of motion, their volumes
of occlusion not only move but warp, due to the
fact that the volume of occlusion between an ob-
ject and a target depends on the relative orien-
tation of the two. Thus, the constraints which
are moving in configuration space-time are non-
rigid. This makes it very difficult to determine a
convenient way of slicing a configuration space-
time.
Another way of viewing the dynamic sensor plan-
ning problem is to segregate the positioning of
the sensor from the orienting and adjusting of
the sensor, This allows the computation of a
3-dimensional region from which all constraints
can be met (i,e, the projection into 3-space of
the set of valid 8-dimensional sensor configura-
tions). The moving polyhedral volumes of occlu-
sion generated by the moving obstacles in the en-
vironment can be considered as obstacles which
the sensor must avoid while moving in the free-
space. This reduces the sensor planning problem
to that of keeping a single point away from the
boundaries of a set of moving polyhedra. Then,
after the sensor path through 3-space has been
planned, the other 5 (optical) parameters can be
planned accordingly.
This suffers from the same problem as the pre-
vious approach, namely that the set of mov-
ing polyhedra (the volumes of occlusion ), are
non-rigid bodies, Although moving polyhedra
have been modelled and examined (i,e, [Canny,
1986, Cameron, 1984]), non-rigidly moving bod-
ies have not been examined in detail, It appears
that an examination of how the volumes of oc-
clusion change with respect to movements of the
obstacles will allow these approaches to be more
useful and appears very promising for future re-
search.
In conclusion, we have successfully extended our
MV P system to plan sensor locations in a time-
varying environment. This is notable in that to
the best of our knowledge, motion has not been
widely addressed in the sensor planning litera-
ture. The use of swept volumes which provides
a useful way to extend static planning problems
to dynamic domains. We have presented a con-
venient way to recover enough temporal infor-
mation from swept volumes to use them in plan-
ning tasks. Our immediate research plans are
to bring the results of this paper into our lab-
oratory and execute the task with the planned
viewpoints. Also, we will be examining the al-
ternative sweeping techniques presented to see if
they offer any performance improvements.
There are several open issues in dynamic sensor
planning. There is work to be done in compu-
tational geometry to characterize the changes in
a volume of occlusion as the target and occlud-
ing bodies move with respect to each other, A
similar characterization of how the optical con-
straints vary with the target's motion is also
important. Finally, it is hoped that these vari-
ous characterizations can be combined to plan a
continuous path through the sensor's parameter-
space, rather than computing a series of view-
points and critical times, This would complete
the analogy between sensor planning and config-
uration space-time based motion planning, and
allow more useful solutions tobe found to dy-
namic sensor planning problems.
This work was supported in part by DARPA
contract DACA-76-92-C-007, NSF grants IRI-
86-57151 and CDA-90-24735, North American
Philips Laboratories, Siemens Corporation and
Rockwell International.
A cell-parallel implementation greatly improves the perfor-
mance of a light-stripe range-imaging sensor[ 1, 2, 3]. Though
equivalent to conventional light-striping from optical and ge-
ometrical standpoints, cell-parallel light-stripe sensors incor-
porate a fundamental improvement in the range measurement
process. As a result, the acquired range data is more robust and
more accurate. Furthermore, range image acquisition time is
made independent of the number of data points in each frame.
By fully exploiting the capability of VLSI to both sense and
process information, we have built a smart sensor that acquires
a complete frame of 10-bit range image data in a millisecond.
Range information is crucial to many robotic applications.
A range image is a 2-D array of pixels, each of which rep-
resents the distance to a point in the imaged scene. Many
techniques for the direct measurement of range images have
been developed[4]. Of these, the light-stripe methods have
proven to be among the most robust and practical.
Fig. 1 illustrates the principle on which a light-stripe sensor
is based. The scene to be imaged is lit by a stripe - a plane of
light formed by fanning a collimated source in one dimension.
The stripe is projected in a known direction using a precisely
controlled mirror. When viewed by an imaging sensor, 1t ap-
pears as a contour which follows the profile of objects. The
shape of this contour encodes range information. In particu-
lar, if projector and imaging sensor geometry are known, the
distance to every point lit by the stripe can be determined via
triangulation.
A conventional light-striperange sensor buildsa range image
using a ''step-and-repeat'' procedure. A stripe is projected onto
a scene, as described above, and one column of range image
data is measured. The stripe is stepped to a new position and
the process is repeated until the entire scene has been scanned.
Unfortunately, step-and-repeat implementations are slow. In
order to build a complete range image using data from N stripe
positions, N intensity images are required. The total time TP*'
to acquire the range frame is
Assuming TM4 e 1/30second and N s: 100, 7P*'
3.3seconds is required.
The frame time of a step-and-repeat sensor has been im-
proved by imposing additional structure on the light source.
For example, the gray-coded sources used by Inokuchi[5] re-
duce the factor of N in (1) to log, N. However, achievable
frame rates are still too slow and the fundamental problem
remains - range frame time increases with spatial resolution.
The cell-parallel technique is an elegant modification of
the basic light-stripe algorithm. The technique is a dynamic
one, with time an important aspect of the range measurement
process[6].
Consider the geometry of a three-pixel, single-row cell-
parallel range sensor, seen from above in Fig. 2. In the fig-
ure, the stripe plane is perpendicular to the page. The stripe is
quickly swept across the scene from right to left, briefly illumi-
nating object features. A sensing element, say S4, monitors the
light intensity I; returned to it along a fixed line-of-sight ray
Ry. When the position of the stripe is such that it intersects Ry
at a point on the surface of an object, a ''flash'' will be observed
by the sensing element.
Range to the object is measured by recording the time t;
at which the flash is seen. The location of the stripe as a
function of time is known because its projection angle 8; (t)
is controlled by the system. The ''time-stamp'' t; acquired by
the sensing element measures the position of the stripe when
its light is reflected back to the sensor. The three-dimensional
coordinates of one object point are uniquely determined at the
intersection of the line-of-sight ray R2 with the stripe plane at
8;, (t;) on the surface of the object.
A sensor which collects a dense range image is formed by
arranging identical sensing elements into a two-dimensional
array. The cells of the array work in parallel, gathering a
range image during a single pass of the light stripe. The time
required to acquire the range frame is independent of its spatial
resolution -
The frame time 7P''' af a cell-parallel sensor is set by the
bandwidth of the photo-receptor used in its sensing elements.
Very highframe rates (1/7P'P')can be achieved. The photodi-
odes used in our cell design have bandwidthinto the megahertz.
They can detect a stripe moving at angular velocities in excess
of 6, 000rpm.
Cell-parallel system geometry can be described using homo-
geneous coordinate transformations[7, 8]. Referring to Fig. 3.
the origin of the frame Og is placed at the optical center of
the imager. The stripe is a half-plane which radiates out from
an axis-of-rotation aligned with the y-axis of the frame and
passing through the point
Stripe rotation 8; is measured counter-clockwise about its axis
when viewed from the positive direction and defined to be
zero when the stripe lies in the yz-plane. In a homogeneous
representation, a plane is described in terms of a column vector
P that satisfies the scalar product xP e: 0, where x is a ho-
mogeneous point that lies in P. In the sensor coordinate frame
defined above, the stripe plane is modeled in terms of b and 8;
The position xg = (as,9s, 4s) of a sensing element on the
sensor image plane defines the line-of-sight ray Rg. The para-
metric equation for a line in three dimensions is used to repre-
sent Rg as
wbere n = ll1 = y//W4G2. The tne parameer
r, when normalized by rm;, is simply the distance along Rg
measured from Og heading toward the object.
The point of intersection xo, between the stripe and the
line-of-sight, is found by solving xP; = 0 for r:
In the coordinate frame of the sensor, this point is
Thus, the 3-D position xo of imaged object points can be
recovered from the scalar distance measurement r.
A practical implementation of the cell-parallel range imag-
ing algorithm requires a smart sensor - one in which optical
sensing is local to the required processing. Silicon VLSI tech-
nology provided the means for building such a sensor.
Fig.4summarizes the operation of elements in the smart cell-
parallel sensor array. Functionally, each must convert light
energy into an analog voltage, determine the time at which
the voltage peaks and remember the time at which the peak
occurred.
The multi-pixelcell-parallel range sensor we have developed
is shown in Fig. 5. This chip consists of 896 sensing elements
arranged in a 28 x 32 array. It was fabricated using a 2pm
p-well CMOS, double-metal, double-poly process and mea-
sures 9.2mm x 7.9mm (width x height). Of the total 73 mm'
chip area, the sensing element array takes up 59 mmf, read-
out column-select circuitry 0.37 mmf and the output integrator
0.06mm%. The remaining 14 mmf is used for power bussing,
signal wiring, and die pad sites.
The architecture chosen for the range sensing elements
is shown in Fig. 6. Areas of interest in the diagram in-
clude the photo-receptor (PDiode), the photo-current trans-
impedance amplifier (PhotoAmp), threshold comparison stage
(n2Comp), stripe event memory (RSFlop), time-stamp track-
and-hold circuitry (PGateI/CCell) and cell read-out logic
(PGateO/TokenCell).
In operation, sensing elements cycle between two phases -
acquisition and read out.
During the acquisition phase, each sensing element imple-
ments the cell-parallel procedure of Fig. 4. The photodiode
within a cell monitors light energy reflected back from the
scene. Photocurrent output is amplified and continuously com-
pared to an external threshold voltage Vth. When photorecep-
tor output exceeds this threshold, the ''stripe-detected'' latch in
the cell is tripped. The value of the time-stamp voltage at that
instant is held on the capacitor CCell, recording the time of
the stripe detection.
The acquisition phase is synchronized with stripe motion and
ends when the stripe completes its scan. At that time, the array
sensing elements recorded a range image in the form of held
time-stamp values. This raw range data must now be read from
the chip.
A time-multiplexed read-out scheme off loads range image
data in raster order through a single chip pin. One bit of token
state is passed through the sensing element array, selecting
cells for output. Dual nlp-transistor pass gate structures are
used throughout the time-stamp data path. They permit the use
of rail-to-rail time-stamp voltages, maximizing the dynamic
range of the analog time-stamp data.
One of the more challenging aspects of the cell design in-
volved the circuitry which detected the stripe.
A photodiode forms the light sensitive area within each cell.
This diode is a vertical structure, built using the n-substrate
as the cathode and the p-well of the CMOS process as the
anode. An additional p'' implant, driven into the well, reduces
the surface resistivity of the anode and increases the device
bandwidth.
The non-linear transimpedance amplifier of Fig. 7 was a key
element of the sensor cell design. Reflected light from the
swept stripe source generates nano-amp photo-current pulses
and thus a very high-gain amplifier is required to convert this
current into a usable voltage. In addition, very little die area
could be devoted to photo-current amplification if cell area
was to be kept small. The three transistor amplifier design
of Fig. 7 satisfies both requirements. Its logarithmic transfer
characteristic provides freedom from output saturation even
when input light levels vary over several orders of magnitude.
The output rise-time of photodiodelamplifier test structures in
response to a stripe was measured to be a few microseconds.
Analog signal processing techniques played an important
role in the design of this smart sensor. As shown in Fig. b,
sensing elements use analog circuitry to amplify the photo-
current, to detect the stripe and to record the per-cell time-
stamp information. Stripe timing is represented in analog form
as a 0-5V sawtooth broadcast to all cells of the array. This
allowed the time-stamp value to be stored as charge on the l pf
capacitor within each cell. The digital equivalent of latching
a count into a multi-bit register would be significantly larger
in area and would require that the digital time-stamp counters
run during the acquisition phase. Thus, analog processing kept
cell area small and minimized digital switching noise during
photo-current measurements in the acquisition phase.
The 28 x 32element VLSI sensor prototype described in the
previous section was incorporated into the light-stripe range
system shown in Fig. 8. System components visible in the pho-
tograph include (from the left) the stripe generation assembly,
the VLSI sensor chip and its interface electronics, a calibration
target and the 3-DOF positioning system. Table I provides
details of the configuration shown.
Calibration provides the complete specification of system ge-
ometry necessary for converting cell time-stamp data into range
images. Two sets of calibration parameters must be measured.
First, 3-D sensor chip geometry and optical parameters must
be measured - the imager model. Next, a mapping between
time-stamp values 8; and distance r for all sensing elements is
developed - the stripe model.
This method measures component model geometry using
reference objects, manipulated in the sensor's field of view with
an accurate 3-DOF (degree of freedom) positioningdevice. The
following two-step procedure is used (Fig. 3):
A planer target out of which a triangular hole has been cut as
shown in Fig.9 is used to map out sensing element line-of-sight
rays. The target is mounted on the positioner so that its surface
is parallel to the world-ay plane.
A single 3-D point on the line-of-sight of a particular sensing
element is found as follows. The target is moved to some z-
position in world coordinates and held. The bottom edge of the
triangular hole is located by moving the target around in a and
y as indicated in Fig. 9. When a small motion in either z or y
causes a large change in the time-stamp value reported by the
cell, occlusion of the line-of-sight at an edge of the triangular
cut is indicated.
Once many points along the bottom edge are located, a line,
known to lie in the plane of the target, is fit. The location of
the top edge is found in a similar fashion. The intersection of
the top and bottom edge lines define one 3-D point that lies on
the cell's line-of-sight. A number of these points are located
by moving the target in z and repeating the process. The line-
of-sight for a single cell can then be identified by fitting a 3-D
line to these points. Experimental data from the calibration of
one sensing element's line-of-sight is shown in Fig. 10.
Mapping the line-of-sight rays for all 896 sensing elements
in this manner is too time consuming. In practice, line-of-sight
information is measured for 25 cells, evenly spaced in a 5 grid.
The geometry of the remaining cells is approximated using a
pinhole-camera model.
The pinhole-camera model[ 11] constrains all sensing ele-
ment line-of-sight rays to pass through a single point focus of
expansion at the optical center of the camera. Fig. ll graph-
ically illustrates the process. Sensing element locations are
assumed to lie in some sensor plane, at locations evenly spaced
in a 2-D grid on the plane. Eleven model parameters must be
determined that identify the transformation matrix Tgw and the
geometry of the the sensor plane. A least-squares procedure
is used to fit pinhole-model parameters to line-of-sight infor-
mation measured in the first calibration step. Imager model
geometry is now fully calibrated.
Unfortunately, calibration of the imager model via line-of-
sight measurement is not suitable for use outside of the labo-
ratory environment. ''One-at-a-time'' measurement of sensing
element geometry, as outlined above, is slow and cumbersome.
We are developing a faster, more precise method for imager
model calibration. In this new calibration method, the 3-DOF
positioning system is replaced with a liquid crystal display
(LCD) mask that need only be accurately positioned along one
degree of freedom. The LCD mask is used to define precise
black-and-white images that are ''seen'' by the range sensor.
The method relies on intensity image information, measuring
geometry through analysis of reference object images[9].
The LCD mask is placed between a diffuse planer target
and sensor chip at a known position and is backlit by shining
the system stripe source on the planer target. The pattern
displayed on the LCD forms a black-and-white image on the
sensor. Only illuminated sensing elements will latch the stripe-
detected condition (Section III-B). A single-bitintensity image
is derived by identifying the time-stamp output of illuminated
sensing elements.
Sensing element line-of-sight geometry is found by varying
the LCD mask pattern in a controlled fashion. For example, a
circular pattern, whose 3-D center is known, can be projected.
A calibration point is found by measuring the 2-D location of
this circle's center in the intensity image returned by sensor.
Additional calibration data is measured by varying the position
of the circle on the LCD mask and the position of the LCD
along ss. Also, by measuring the center different radii of the
circle at a fixed position, we can compensate for the low spatial
resolution of the current sensor. The new sensor chip design,
discussed in Section VII, returns multi-bit intensity image data
which further assists imager geometry calibration.
Use of the LCD mask significantly reduces the time required
to perform imager-model calibration. In the previous method,
two edges of a triangular hole had to be mapped out, viaaccurate
back-and-forth movement, in order to yield a single calibration
point. In the new method, one calibration point is measured
from a single LCD-generated pattern without mechanical X -Y
movement. Precise calibration of the low-spatial resolution
range sensor is possible because high-precision patterns are
generated by the LCD mask.
The use of an LCD mask to project precise 2-D patterns
has application beyond the calibration of our light-stripe range
sensor. For example, this technique could be used to assist
more traditional camera calibration procedures or to present
training data to image-based neural net systems. LCD displays
have several advantages over CRT displays for applications
like these - they are fast, they are static (not refreshed), and
they form images which are stable and well defined.
The second part of the calibration procedure determines the
mapping between time-stamp data and range along all sensing
element line-of-sightrays. As shown in Fig. 12, a planer target
with no hole replaces the target used in step one. The new
target is held at a known world-z position, parallel to the azy
plane, and time-stamp readings @g from all sensors are recorded.
This process is repeated for many z positions. Using this
information, the function which maps cell time-stamp values
8; into line-of-sight distance r for each sensing element is
approximated by fitting a parabola to each. Experimental data,
showing the fitted r verses 8g functions for several sensing
elements, is shown in Fig. 13. Calibration of the cell-parallel
range sensor is now complete.
The quality of the range data produced by the cell-parallel
range sensor was measured by holding a planer target at a
known world-z position with the 3-DOF positioning device. In
the experimental setup, the world-z axis heads almost directly
toward the sensor with the zw = 0 pointroughly 500mm away.
Analog time-stamp values from the sensor array were digitized,
using a 12-bit analog-to-digital converter (A/D), and recorded
for 1, 000 trials. Light-stripe sweep (acquisition phase) time
for each scan was 3 msec.
A histogram of the range data reported by one cell is plotted
in Fig. 14. The horizontal axis represents the digitized time-
stamp value, converted to world-z distance via the calibration
model. Data for six world-z positions are combined in this
plot. The vertical axis shows the number of times (plotted
logarithmically), out of the 1, 000 trials, that the sensing ele-
ment reported that world-z distance. The sharpness of each
peak is an indication of the stability (repeatability) of the range
measurements.
Averaged statistical data for 25 evenly-spaced sensing ele-
ments is plotted in Fig. 15. In order to measure accuracy and
repeatability, the position of the target, as reported by the cell-
parallel sensor, is compared to the actual target z position. The
''boxed'' points in the plot represent the mean absolute error,
expressed as a fraction of the world-z position and averaged
for the 25 elements at zw. One standard deviation of ''spread'',
also normalized with zw, is shown () above and below each
box.
The experiments show the mean measured range value to be
within 0.5 mm at the maximum 500 mm z - an accuracy of
O.1%. The aggregate distance discrepancy between world and
measured range values remains less than 0.5 mm over the entire
360 mm to 500 mm z range. The cell-parallel sensor repeatabil-
ity is found by computing the standard deviation of the distance
measurements. The measured repeatability of histogram data
is less than 0.5 mm -- 0.1% at the maximum 500 mm posi-
tioner translation. The 0.5 mm repeatability decreases with
the distance to the sensor - essentially with the slope of the
time-stamp to distance mapping function (Fig. 13).
Fig. 16 shows a wire-frame representation of one 28 x 32
range image produced by the sensor. The imaged object is the
cup shown in the figure, approximately 80 mm in diameter at its
openingand 80 mm high. The range sensor islookingdirectly at
the object from a distance of 500 mm. The viewpointof the plot
is at a point directly above the optical center of the sensor. The
complete range image was acquired during a 3 msec stripe scan.
The intersection points of the wire-frame plot are positioned on
cell line-of-sight rays at the measured distance along the ray
and the focus of expansion is located in front of the cup. Thus,
the smaller ''squares'' represent object surface patches closer
to the sensor. This is opposite the manner in which straight
perspective would make an object with a grid painted on it
appear, and at first glance gives the false impression that the
''mold'' used to make the cup has been imaged.
The curved smooth front surface of the object is clearly
visible in the range data. The 20 mm handle of the cup is
readily distinguished, as is the planer background behind the
cup. The curved surface of the object halfway down the cup
directly across from the bottom of its handle includes a slight
shift of the wire-frame. The imaged cup is slightly narrower at
its base by about 2 mm. The cell-parallel sensor is measuring
this small 3-D feature at the 500 mm object distance.
A summary of the cell-parallel sensor system performance
is given in Table II.
A second-generation implementation of the light-stripe sen-
sor array has been fabricated. This new chip, seen in Fig. 17,
incorporates several advantages over the first design. The die
area of the new cell, shown in Fig. 18, is 216um x 216um,
40% smaller than that of the cells of the first-generation sensor
(photoreceptor area has been kept constant). Stripe detection is
done in a more robust manner and range data read-out circuitry
has been simplified. In addition, the new cell provides a means
to record and read out the value of the peak intensity seen when
it acquires a range data sample. The peak intensity informa-
tion provides a direct measure of scene reflectance because
stripe output power is known and distance to the object point is
measured. In addition, the availability of intensity information
allows for efficient sensor calibration (Section V-B).
Peak detection is done using the circuit of Fig. 19. Operation
of the circuit is straightforward. The source following transis-
tor 2, enables capacitor C) to track the rising intensity input
voltage transitions. No path is provided for C,, to discharge
when photoreceptor output transitions downward. At the end
of a scan, the largest intensity reading observed will be held.
Stripe detection is easily accomplished by comparing the peak-
intensity value V; with the amplified photodiode output V,.
When V, falls below the V;, the output from the comparator
is used to record a time-stamp value.
Using Spice[ 10], operation of of the second-generation sens-
ing element design was simulated. The simulation results are
plotted in Fig. 20. The output from the peak-following cir-
cuit ILSCELL.30 acts as a dynamic threshold for each cell,
replacing the externally applied global threshold of the first-
generation design (Section III-B). Comparator input offset
mismatch made setting a global threshold level, valid for all
cells in the array, difficult. Thus, stripe detection is made more
robust by this modification. In addition, the ''true'' peak de-
tection of the new design provides better quality range data
because the new stripe detection scheme identifies the location
of the peak in time more accurately than simple thresholding.
The peak-intensity value held within the second-generation
cell is an important artifact of the ranging process and, in the
new design, is provided as an additional sensing element out-
put. The illumination source in the system, the stripe, is of
known power. Intensity reduction from 1/r-type losses can be
accounted for because range to the object is measured. 'The
intensity value therefore provides a direct measure of scene
reflectance properties at the stripe wavelength. It is an image
aligned perfectly with range readings from the cell array.
The area in each cell dedicated to time-stamp read out is
much smaller in the new design. Direct addressing of the cell
to be read, using row and column selects, eliminates the token
state necessary in the first-generation design. The N x M array
is read using N row select lines and M column select lines.
A given cell is enabled for read out by asserting the row and
column select lines that correspond to the location of the cell
in the array. The two-level bus hierarchy has been maintained,
however, to keep bus loading at a minimum. The area savings
of the new read selection method has made cell area of the
second-generation design smaller despite the additional peak
detection circuitry.
We have presented the design and construction of a very
high-performance range-imaging sensor. This sensor acquires
a complete 28 x 32range-data frame in a few milliseconds. Its
range accuracy and repeatability were measured to be less than
O.5mm on average at half-meter distances. The success of this
implementation can be attributed to the use of a VLSI smart
sensor methodology that allowed a practical implementation of
the cell-parallel technique.
While the advantages of processing at the pointsensing have
been advocated by many, few practical smart-sensor imple-
mentations have been demonstrated. The cell-parallel range
imager presented here bridges the gap between smart sensor
theory and practice, demonstrating the impact that the smart
sensor methodology can have on robotic perception systems,
like automated inspection and assembly tasks.
Smart VLSI-based sensors, like the high-speed range im-
age sensor presented here, will be key components in future
industrial applications of sensor-based robotics.
Easing the dichotomy of two-level stores is currently
the most challenging problem in persistent object
stores, since transferring and translating data be-
tween the in-memory and the backing store degrades
the overall performance of persistent applications.
One way whereby we can ease this problem is clus-
tering. Clustering algorithms attempt to store co-
related objects within the same or in a neighbouring
unit (of transfer) on the backing store and thus re-
duce the number of accesses when such objects are
retrieved.
In recent years the problem of optimal clustering
has attracted a lot of attention. Most of the new
clustering algorithms are aimed at supporting object-
oriented database systems [Ben90] [Cha89]. How-
ever, the emergence of new complex data models and
database systems such as hypertext [Nie90] present
new requirements from the persistent object store.
In particular, the main requirement is to support a
dynarmic graph object space. The underlying persis-
tent object store, in a hypertext system for example,
is represented by a graph whose nodes represent ob-
jects and links (i.e edges) between the nodes repre-
sent references between objects. This graph is highly
dynamic, i.e. nodes (objects) and edges (references)
may be added, deleted and updated arbitrarily and
frequently. Clustering of such a dynamic graph is
a complicated task. Moreover, the access patterns to
the underlying persistent object store can be changed
from one application to another.
Our main goal in searching for an optimal clustering
algorithm is to support a dynamic persistent object
store environment such as a hypertext database en-
vironment. In [Tuv92] we present the main require-
ments for such a store to support hypertext. Naviga-
tion is no longer the only access to the said store but
it is augmented with support for associative queries
and free text retrieval operations. Thus the access
patterns to the underlying persistent object store are
not based solely on the inter-structure of the persis-
tent object store (i.e. a digraph object space), but
also on the content of the objects.
Existing clustering algorithms use different types of
information as input for finding the clusters in persis-
tent object stores. Some algorithms, such as [Hor87],
use a static strategy whereby objects of the same type
are grouped together in the same cluster. In [Ban88]
the algorithm relies upon the directed acyclic graph
structure of the underlying object store and on the
expected traversal operations on it. More seman-
tic information is provided in clustering algorithms
which aim at supporting object-oriented databases
such as [Cha89] [Ben90]. These algorithms exploit
the knowledge of the class hierarchy of the database
and the operations performed on these classes. The
semantic clustering [Sha90] proposes to use knowl-
edge of the programmer in the clustering algorithm.
ObServer [Hor87] supports dynamic clustering by us-
ing the actual values of the objects. In this algorithm
an object remains in its original cluster as long as its
content satisfies the cluster's specification. In Cac-
tis [Dre90] statistics are kept which count how many
times each relationship between two objects is tra-
versed. Frequently traversed objects are candidates
to be clustered together. The relationships between
objects is also the input in [[Tsa91] and [Ker?0] but
these algorithms ignore the way this information is
gathered and reclustering is required as a result of
changes in the relationships between the objects in
the underlying persistent object store. None of the
above-mentioned algorithms provides for incremen-
tal learning of the clustering algorithm based on the
actual usage of the persistent object store.
Herein we present a new approach to clustering in
the context of dynamic changes in the access pat-
terns to the persistent object store. In particular, we
concentrate on incremental learning of the clustering
algorithm that allows us to capture the dynamic as-
pects of the persistent object store, and on learning
from the actual usage of the persistent object store,
i.e. the actual access patterns to the persistent object
store. This approach provides the support to employ
dynamic clustering whereby the optimal persistent
object placement is decided at the time the object is
to be transferred from the in-memory to the backing
store.
The rest of the paper is organied as follows.
In Section 2 we present an overview of the Self.
Organising Feature Map (SOFM) algorithm and dis-
cuss the motivation behind the use of the SOFM for
optimal clustering in a persistent object store. In
Section 3 we show how SOFM can be applied to the
object placement problem in a persistent object store.
Experimental results of Kohonen's SOFM are given
in Section 4. Finally, in Section 5, we conclude the
paper and discuss directions for further research.
A typical topology of the Kohonen network is
presented in Figure 1. The architecture of
the network consists of two layers. The in-
put layer is a one-dimensional array of N nodes,
and the output layer is a two-dimensional array
of M output nodes which are used to form fea-
ture maps, Every input node is fully connected
to every output node via a variable connection
weight.
The basic idea behind the SOFM learning is as fol-
lows : Assume an input data set X, whereby each
a E K is a vector of N dimensionality, Rf, and
assume a set of connections [m1 : mr E 9,i s:
1,3,.-.,M} between the input layer and the output
layer whereby each connection vector m; is associated
with the node iin the output layer. Let d(z, m;) be
some distance measurement between z and m, such
as the Euclidean distance, and let N, be the neigh-
bourhood of the output node c, which consists of the
node c together with a set of nodes around the node
SOFM is an iterative algorithm whereby each iter-
ation is a two-step process. In the first step, which
is called selection of the best matching output node,
the algorithm compares a new selected input vector
a E X7 with each m, and finds the best matching
node c such that d(z,m,) 5 d(z, m),i = 1,2,..,M.
In the second step, which is called learning step,
the connections m; which are associated with the
nodes in the neighbourhood N, are updated such
that d(z,m), i E N,, is decreased, i.e. the up-
dates increase the matching between the nodes in the
neighbourhood N, toward the input vector z, At the
end of this algorithm the output nodes are organized
into neighbourhoods (i.e clusters) which act as fea-
ture classifiers on the input data set.
As discussed earlier in the paper our aim is to find
an algorithm which identifies clusters in a dynamic
persistent object store such as a hypertext database
system. The algorithm must employ incremental
learning where the computational cost is minimized,
and which takes into account the actual usage of the
persistent object store. We decided to use Kohonen's
SOFM whose properties we believe satisfy the above
requirements.
Herein, we enumerate some of the main properties of
Kohonen's SOFM and the way in which they support
the clustering problem.
We now describe the way by which we model the
clustering problem in a persistent object store envi-
ronment and how Kohonen's SOFM algorithm can be
applied to it. We assume that each persistent object
in the persistent object store is associated with an
invariant unique identity, called object identity. The
object identity is the means by which objects refer to
each other and allows sharing of objects. In [Tuv92]
we realize the concept of object identity by using log-
ical identities rather than physical addresses in order
to allow mobility of objects within the store; this al-
lows clustering of any set of objects.
As mentioned earlier, the underlying persistent ob-
ject store can be modelled as a graph. Based on this
graph, many researchers, model the clustering prob-
lem as a graph partitioning problem. The advan-
tage of this approach is that one can use existing
algorithms, which solve the graph partitioning prob-
lem, such as the heuristic algorithm of Kernighan-
Lin [Ker70] and optimisation algorithm using sim-
ulated annealing [Joh89]. However, in an example
given later in the paper, we show that it is more accu-
rate to view the persistent object store together with
the accesses to that store as a hypergraph, since the
accesses to the persistent object store are not based
solely on the structure of the said graph but also can
be set-oriented, whereby a set of retrieved objects do
not necessarily follow the said graph structure.
Before we give a formal description of the cluster-
ing problem we first introduce the concepts of feature
and co-reference with regard to the persistent object
store.
A feature is any concept which is used for learning
by the clustering algorithm. A persistent object is a
feature. Eowever, a feature can be any other concept
such as a type in a type system or even special values
or terms which are expected to be important with
regard to clustering organization.
A co-reference is a a set of features. Mostly co-
references represent actual answers to queries posed
to the persistent object store but they can also rep-
resent answers to expected queries. An example of
co-reference can be the query to retrieve a father and
his children or even retrieval of the whole farmily.
We next model the problem of clustering in a per-
sistent object store as the problem of node parti-
tioning in a hypergraph. Let P (F,V,Q,I) be
a weighted hypergraph, where F is the set of nodes
whereby each node corresponds to a different feature
over P, V is the set of all persistent objects in P with
t g F, Q is the set of hyperedges of P, where each
g 6 Q is a non-empty subset of F (i.e. c F)
and each hyperedge corresponds to a co-reference (or
access) over P, and I is an infinite set of identities,
Each feature (i.e. node) is associated with an invari-
ant unique id i E I, which is the means by which
features can be referenced, and each hyperedge, (44
is associated with a weight, denoted by ut, which
counts the number of times that the corresponding
co-reference, i, is the answer to queries which have
been posed to P,
For a given P we then define the node partitioning
as M non-empty disjoints subsets of V, [V,.-.,Vu)
(i,e. V, n V 6, i f j), and which cover the set V
(ie, V = J,, %.). We further defne the cost of a
co-referenc< i bY (40 = wcEC5,, %4, here k; i 1
if g;n Vj y %, and 0 otherwise.
For a given P and a positive integer K [V], in the
optimal clustering we are looking for a node parti-
tioning of P whereby ]V.] gK, i = 1,.-,M, and such
that it minimizes the total cost of all co-references in
2.e, mnlmie )= E}5i(i).
Kohonen's SOFM algorithm applied to the above
problem now follows :
We define O to be a special set of answers to queries
which are used for the classification of objects in a
given organized feature map. In each o' @ O only
one object (i,e feature) j is *1' (i,e o[ = 0 Vk y
), and o} = 1). Each feature corresponds to one
o' E O. At the end of the learning process we apply
the set of access patterns O in order to classify the
different objects into their clusters. Moreover, after
this learning session, the neural network can continue
learning from new incoming queries by applying steps
3,4 and 5 in the above algorithm. This provides for
incremental learning of the clustering algorithm in a
persistent object store.
The following is an example which illustrates the
representation of the clustering problerm as a hyper-
graph and which shows that the hypergraph repre-
sentation of the clustering problem is more accurate
than the graph representation of the said problem.
Let P be a persistent object store consisting of six
objects labelled from 1 to 6, and let Q be a set of
seven different accesses to these objects which are :
where each tuple is an answer to a query and the num-
bers represent the retrieved objects. The set Q in Fig-
ure 2a in its vectorial representation for the use in Ko-
honen's soFM ls : 4'(a) = (1,1,1,0,0,0), ;() =
(1,4,4,1,0,o), P(e) = (,1,4,4,1,o), *(4) =
(A,o,1,o,0,1), P(+) = (,0,o,1,1,o)i P() =
(0,0,0,0,1,1)'; and '(0)= (0,0,4,1,0,1). Figure
2a depicts the hypergraph representation of the above
persistent object store where the nodes correspond to
the objects and the hyperedges correspond to the dif
ferent accesses, and Figure 2b depicts the graph rep-
resentation of the said persistent object store, where
the access (1,2,3) is replaced by the three edges (1,2),
(1,3) and (2,3). In an environment with the con-
straint that a cluster can contain 3 objects (nodes)
at most (i,e K = 3), it is easy to figure out that the
best partitioning i (1,2,3) and (4,5,6) which is the
solution for the hypergraph partitioning with K:3.
However, the graph partitioning fails to give the best
clustering, since the graph in Figure 2b is cubic, which
implies that any partitioning can be applied.
Since there are no known benchmarks for node parti-
tioning of large and dynamic hypergraphs, we use the
following example in order to illustrate the use of our
algorithm. Figure 3 depicts a hypergraph represen-
tation of part of the London underground travel map
consisting of 25 nodes, labelled from 1 to 25, each of
which corresponds to a different train station, and 9
hyperedges, labelled from 1 to 9, which correspond
to the different train lines. Table 1 presents the hy-
peredges of the said hypergraph together with their
labels, their corresponding train line names and fi-
nally with their incident node sets.
In the said hypergraph we are looking for a node
partitioning subject to the constraint that the maxi-
mum number of nodes in each partition is 5, i.e. K=5.
We experimented different initial configurations in or-
der to ensure a stable solution for the required clus-
tering. The total cost of all given hyperedges (i.e.
co-references), c(0), is used as an indicator for the
efficiency of any resulting clustering. In the above
problem we are looking for a solution with c(Q) g 24.
Since the cardinality of the node set and the hyper-
edge set of the said hypergraph are both small, we
use a small network topology for our experiments,
e.g. the output layer consists of a linear array of 10
nodes.
As mentioned earlier, after the learning phase we next
employ a classification phase based upon the mea-
surement of the distance between each of the features
to the output layer of the trained network and orga-
nising the features in an ordered list. The resulting
features ordered list is then used for partitioning the
25 features into 5 disjoint partitions, whereby the first
5 features are assigned to the first partition followed
by the next 5 features assigned to the second parti-
tion, and so on.
Table 2 presents the resulting clustering of an ex-
periment with the following initial configuration :
= 0.1,r = 5, and the number of iterations is 8100.
The goals of this paper were two-fold; firstly, we
showed the potential of using neural network learning
paradigms such as Kohonen's SOFM in a database
system and, secondly, we presented a new approach
for incremental learning of clustering in a persistent
object store that captures the dynamic aspects of the
underlying persistent object store.
The aim of the presented algorithm is to identify
classes (i.e. clusters) of objects from a set of objects
based on the access patterns to these objects. Fur-
thermore, our algorithm can use in its initial stage a
priori knowledge on the underlying persistent object
store, such as the database schema or inter-relations
between types of objects, in order to employ more
effective clustering.
In our model of the clustering problem we focused
on the dynamic aspects of the access patterns to the
persistent object store and ignored many other im-
portant factors, such as the size of the objects, which
may affect the efficiency of the resulting clustering.
However, at the final stage of our algorithm, after the
clusters are defined and found, one can employ any
known technique, such as the heuristic graph parti-
tioning of Kernighan-Lin [Ker70], for further refine-
ment of a given cluster.
A promising direction in solving general combina-
torial optimization problerms such as graph partition-
ing and, in particular, in solving the optimal cluster-
ing problem is to combine the neural network model
with a simulated annealing algorithm. For example,
in Kohonen's SOFM we can modify the best match-
ing step by using an energy function (cost function),
which takes into account arguments such as the num-
ber of elements in the different clusters, rather than
just use the Euclidean distance measurement.
Finally, we suggest the use of a neural network
model to improve performance in a database system.
Clustering is only one aspect of a wide range of po-
tential problems that the above model can solve. For
example, an efficient algorithm for replication of ob-
jects in a persistent object store can use an overlap-
ping clustering algorithm similar to our proposed al-
gorithm. Moreover, the knowledge of the underlying
clusters in the persistent object store is important in
order to employ efficient buffering policies, such as
prefetching of objects as well as clever indexing tech-
niques.
The authors wish to express their appreciation to the
referees for their useful comments.
For the past three decades, there has been a
mounting interest among researchers in the
problems related to machine simulation of
human reading. Character recognition has
attracted an immense research interest not only
because of the very challenging nature of the
problem, but also because it provides a means
for automatic processing of large volumes of
data such as postal codes [1], office automa-
tion [2, 3], and other business and scientific
applications [4, 5].
The different areas covered under the
general term character recognition fall into
either On-line or Off-line categories, each
having its own hardware and recognition
algorithms. In On-line character recognition,
the computer recognizes the symbols as they
are drawn [6, 7, 8]. The most common writing
surface is the digitizing tablet, which typically
has a resolution of 200 points per inch and a
sampling rate of 100 points per second.
Off-line recognition, is performed after the
writing or printed is completed. Optical
Character Recognition (OCR) deals with he
recognition of optically processed character
rather than magnetically processed ones. In a
typical OCR system, input characters are read
and digitized by an optical scanner. Each
character is then located, segmented and
resulting matrix is fed into a preprocesssor for
smoothing, noise reduction, and size
normalization [9, 10, 11, 12, 13].
This paper proposes a new structural
technique for the recognition of printed Latin
text. The technique can be divided into five
major steps. First, is the digitization step, in
which the original image is transformed into a
binary image utilizing a 300 dpi scanner.
Second, is a preprocessing step in which the
thinning algoritthm. Third, the skeleton of the
binary image is thinned using a parallel image
is traced and a binary tree is constructed.
Features are extracted from the binary tree as
pattern primitives, such as straight lines,
curves, and others. A partitioned dictionary is
then used to identify the character, and merged
characters are segmented. Figure 1 depicts a
block diagram of this system.
Extensive work has been conducted on the
digitization or binarization process [14, 15].
The binarization algorithm employed in this
work, however, is similar to the method
appearing in [16]. A 300 dpi scanner is used to
digitize the image using an 8-bit grey scale.
The majority of printed text is approximately 8
point size and this gives an image frame of
between 30 and 35 pixels in hight, a
satisfactory resolution for this system.
Individual pixels are then converted to a binary
value by a thresholding technique.
Old documents, carbon copies, and text
printed on recycled paper generally suffer grey
scale shading problems and a more complex
approach can be employed which adjusts thhe
threshold level according to the background
'white level' of the image [17], both for the
entire document and localised areas.
The preprocessing step involves operations on
the digitized image that are intended to reduce
noise and increase the ease of extracting
structural features. This involves cleaning the
image, extraction of individual character
images, and thinning of these images.
This is a reduction in the noise that the process
of binarization produces. The sharp edge
which the human eye sees between the black
printed letter and white page usually translates
upon scanning into a blurred grey scale edge of
several pixels wide. Thresholding can then
create a ragged edge in this region,
contributing to noise in the image.
Each pixel, P, has eight neighbor pixels
which are numbered P0 to P7 in a clockwise
fashion starting from the north neighbor. The
sum of the eight pixels surrounding P is
termed B(P) and is defined below.
To lessen the effects of this noise, any
black pixel P wiihh B(P) c 4 is whitened and a
white pixel P wih1 B(P) > 6 is blackened.
There is a large volume of literature on
thinning algorithms of both the sequential and
parallel nature. Parallel approaches allow a
thinning of the entire image simultaneously
while sequential algorithms use a process on
the pixels which are dependent on previous
operations. The algorithm implemented in this
system is a parallel method based on the Guo
and Hall [18] A1 method. Figure 2 shows an
original scanned image and the resulting
skeleton following application of the thinning
algorithm.
Figure 3 gives an example of some
problems that occur during thinning. Due to
the nature of the thinning algorithm, ine
segments of typically short length can
sometimes be formed which do not contribute
to the structure of the character (Figure 3a.).
Without knowledge of the overall topology of
the character, it is difficult to ascertain
whether these hairs' are important to the
structure of the image. Figure 3.h shows some
of the difficulties which occur with serif fonts
following the thinning process.
The aim ot this step is to extract the individual
character images from the document image.
The algorithm involves a separation of an
entire printed line by horizontally scanning for
bounding white lines. Individual character
images are then extracted by similar vertical
projection. The problem of merged characters
is dealt with in a later step.
This step builds, from the pre-processed
image, a binary tree which contains
information describing the structure of the
image.
An algorithm implementing a 3x3 window is
used to trace along the path of the skeleton,
recording the structural information of the
traced path. A path is described as a
tracingbetween junction or end points, where
an end point has a single neighbor and a
junction point 3 or more neighbors (and
multiple paths exist).(Figure 4)
This path is stored in a node of the binary
tree; where a choice of paths to trace exists, a
left and right node are formed beneath the
current one and their respective paths traced
out. A priority system is used which favours
certain directions over others (without this, the
window would trace the skeleton in random
directions). It is obtained by experience and
experimental results.
The starting point for tracing the skeleton
is based on several criteria. The image is
divided into 3 horizontal regions and the top
and bottom region are searched for end points
or junction points. This ensures that the
starting point does not split a path into two
subpaths. If no such points are found, such as
with the letter o', then the left most pixel of the
entire image is selected as starting point.
This path is stored in a node of the binary
tree; where a choice of paths to trace exists, a
left and right node are formed beneath the
current one and their respective paths traced
Out.
The structural information saved for each path
traced is the following:
Additional information stored concerning the
entire character image includes:
The completed tracing results in the
segmentation of the character into paths or
strokes which will latter be formed into
primitives. For the thinned image given earlier,
experimental results produced the binary tree
in figure 7.
Upon completion of the binary tree, a
smoothing step allows redundancies and noise
to be removed from the tree. The smoothing of
the binary tree is designed to minimize the
number of nodes in the tree and minimize the
Freeman code chain. Loops whose paths
contain multiple nodes are identified and then
compressed to single node. Redundancies in
the Freeman chain code are smoothed and
noise is reduced. At points of change in the
Freeman code, a vector averaging algorithm
produces results as shown in figure 7. Ttw
result is a minimal smoothing function which
produces segment-like code with a minimum
unit length of 3 and eliminates paths of length
The structural information in the binary tree
allows the formation of pattern primitives, or
subpatterns, which are used to describe thhe
original image. There are two main primitives
described in this system: straight lines and
curves. A path may be described by a single
primitive or by multiple primitives. The
structural information in the tree is converted
to these primitives using the following
definitions.
Breakpoint (separator); divides a path into
subpaths more easily described by primitives.
A breakpoint has at least one of two possible
conditions:
Straight Line: has its usual geometric
definition as two points in sequence within a
path. A point in a freeman chain can be
defined as a change in the freeman code. Lines
can be distinguished from curves in two ways:
the length of a line segment is significant in
comparison to the length of the path, or the
path contains only two points.
Figure 8c illustrates the sectioning of a line
drawing into its primitives.
The recognition phase involves the use of a
partitioned dictionary to obtain a match
between the image tree, which describes an
unknown symbol, and a specific Latin
character.
The dictionary consists of three main partitions
which allow easier classification of the
characters; those characters which contain a
loop(s), curve(s), and those with straight line
segments only. These partitions are further
sub-divided into sub-partitions depending on
particular structural features. The lowest
partition consists of a a linked list of word
trees which conform to the partition features
where each word tree contains a structural
description of a single character in the Latin
alphabet. The word tree is constructed in a
similar fashion to the tree built to represent the
unknown character. Each node, hence,
contains a primitive pattern along with other
structural information. The links in the tree
indicates connectivity of primitives. Once the
image tree (the description of the unidentified
character) has been classified into a partition,
a match is achieved by a comparison of each
word tree with the image tree.
The matching occurs in this manner. A
traversal of the image tree is performed,
searching for a match to the structural
information contained in head node of the word
tree. Figure 10 shows an example of a match
for the unidentified tree describing the letter
'A'.
For the head node only, primitive line
segments may be matched to line segments of
opposite direction (e.g. Es may be matched
with W4). If a match is found, the equivalent
node in the image tree becomes the temporary
head of that tree.
A preorder traversal is then performed on
the word tree. Each node encountered in the
traversal of the word tree must equate to a
node in the unidentified tree; the match
includes primitives, certain structural
information, as well as connectivity, but with
the following re-definitions to the image tree.
The example image tree would be
restructured according to the above rules as in
tigure 11.
Every node in the word tree must have an
equivalent in the imagetree in order for a
recognition to occur. Extra nodes left
unmatched in the image tree indicate that the
recognized letter is a sub-pattern of the image
tree. This indicates several possibilities:
The segmentation of merged or touching
characters remains a difficult process in any
OCR systems. The technique used for
recognizing characters in this paper simplifies
the problem considerably. Merged characters
which are singly connected and whose contact
point occurs at natural junction points of the
letters will be separated and recognized
according to rule 2 in the above section
without any further processing. An example
would be the two letters 'ar', connected by the
bottom serifs. Letters whose contact point does
not occur at natural junction points creates a
splitting of primitives in one or both
characters. The characters 'lu' merged at the
left most serif of 'u' would split the description
of letter 'l' into two short line segments instead
of a single long one. To overcome this
problem, equivalence relationships can he used
in the traversal of the word tree (e.g. two
connected short line segments of the same
direction are equivalent to a single long
segment).
Several other more difficult problems need
to be dealt with. Multiply-connected characters
(two or more contact points) will create an
incorrect loop primitive which must be
segmented. Merged characters whose image
forms a new character are also difficult to
counter. The merging of 'rn', for instance,
cannot be separated on lower level information
alone.
The advantage of the proposed
segmentation method lies in the fact that
characters are first identified before being split
from the merged image. Many current
techniques rely on statistical methods which
indicate several likely segmentation points
based upon selected topological information.
The image is segmented, an attempt is made to
recognize the image, and upon failure requires
re-segmentation. Such methods are
computationally expensive because they are
essentially blind to the entire topological
information of the image. By recognizing a
character before segmentation, it is ensured
that the segmentation point is in fact correct.
This paper presented a combined structural
and classification approach for the recognition
of multi-font Latin texts. The intuitive idea
behind this technique is to extract certain
features which are purely structural such as
curves, straight lines, etc. in a manner similar
to that which human beings describe
characters geometrically and recognize them
based on a classification which uses partitions
in a dictionary. In addition, this approach
adopted is rapid for feature extraction,
recognition, and segmentation.
Results to date with a small set of
characters in different fonts have teen
encouraging. More extensive testing and
results will be presented during the conference.
All algorithms in this paper were written in
C++ and run on an IBM pc.
Throughout the information processing
industry there is a need for rapid, effective
access to information in databases containing
text, formatted data, graphics, and other mul-
timedia (e.g., engineering drawings, sound,
and video). Furthermore, as conversion of
large volumes of paper documents and techni-
cal manuals into electronic raster format
accelerates, the need for fast, accurate index-
ing of optically stored raster images becomes
increasingly important. Typically such files
are indexed manually at great expense, while
search methods are crude. Improved retrieval
methods are needed. The Automated
Document l Image Indexing and Retrieval
(ADIIR) research project seeks to address
these needs with: new techniques for captur-
ing and indexing images; new algorithms for
effective retrieval from large text and image
databases; and intelligent interface design,
including integrated text and image retrieval.
In this paper we give a short description of the
projecl as a whole with an emphasis on the
document image analysis portion and a brief
account of our performance at the Text
Retrieval Conference (TREC).
The first long-term objective of the ADIIR
project is to develop effective, computation-
ally implemented retrieval algorithms that can
efficiently scale up for very large text and
image retrieval systems. The second is to
develop an automatic indexing engine that
will capture key elements of a scanned docu-
ment based on document class and common
zone patterns, eliminating costly manual
indexing. The third is to develop indexing
and retrieval algorithms that can take advan-
tage of structural information contained in
documents, e.g., those formatted in the
Standard Generalized Markup Language
(CSGML).
ln support of these objectives we are
developing a retrieval system/testbed incor-
porating;
The ideal text retrieval system retrieves doc-
uments based on an understanding of the
meaning of the query and the document. Due
to ambiguity in the use of words and concepts,
keyword retrieval falls far short of such an
ideal [1, 2]. Furthermore, scanned documents,
or those in SGML format, provide structural
and layout information of which a retrieval
system should take advantage. All such
[Insent Fig. 1]
The first stage of this research has
involved: a) analytical work developing and
implementing the individual retrieval algo-
rithms and the methodology for combining
them in an overall ranking algorithm; b)
acquisition of large test collections of paper,
image, SGML-tagged, and ASCII documents;
c) evaluation and acquisition of commercial
products for scanning/OCR and SGMIL pars-
ing, and of university research prototypes for
document retrieval [5, 6]. The second stage,
presently underway, is to build the proto-
information, however, provides at best clues
to document relevance. Thus, a probabilistic
approach is warranted.
Figure 1 shows the prototype ADIIR
retrieval system. The two arcs from the
incoming document stream indicate that some
documents are in ASCII format, others are
paper, requiring scanning and optical charac-
ter recognition (OCR). The analysis system
provides automatic indexing. Natural lan-
guage processing (NLP) is provided by an
NLP module created using PRC'4 NLP shell,
PAKTUS [3, 4]. The intent is to provide con-
cept-based indexing with broad, shallow,
domain-independent NILP without extensive
handcrafting. Analyzed documents are stored
in an object-oriented repository customized
for document retrieval, LEND [5]. LEND
supports various types of data including a
comprehensive computerized lexicon for
English derived from machine readable dic-
tionaries, factual domain knowledge, large
digital, hyper-linked, and the multimedia
archives from which retrieval takes place.
Finally, the retrieval system matches docu-
ment representations derived by the analysis
System to query representations derived from
the user.
typeltestbed system. These stages include the
following tasks.
Research has shown that different retrieval
models retrieve different sets, only slightly
overlapping, of more or less equally relevant
documents [7, 8]. Accordingly, a major thrust
of this project is to implement the
Combination of Expert Opinion (CEO)
methodology [9] for combining the results of
multiple probabilistic retrieval models into an
optimal ranking of documents. Each retrieval
model can be viewed as a retrieval expert.
The initial models examined will be the prob-
abilistic indexing model of Maron and Kuhns
[10] and the inverse document frequency
model 11, 12].
In the Bayesian formulation of the CEO
problem [ 13] a decision maker is interested in
some parameter or event; and helshe has a
prior, or initial, distribution or probability for
that parameter or event. The decision maker
revises the distribution upon consulting sev-
eral experts, each with his/her own distribu-
tion or probability for the parameter or event.
To effect this revision, the decision maker
must assess the relative expertise of the
experts and their interdependence, both with
each other and the decision maker. The
experts' distributions are considered as data by
the decision maker, which is used to update
the prior distribution.
For automatic document retrieval, the
retrieval system is the decision maker and
different retrieval algorithms, or models, are
the experts [9, 14]. This is referred to as the
upper level CEO. At the lower level the prob-
abilities of individual features, e.g., terms,
within a particular retrieval model can be
combined using CEO. In lower level CEO he
retrieval model is the decision maker and the
term probabilities are viewed as lower level
experts. The probability distributions sup-
plied by these lower level experts can be
updated, according to Bayes theorem, by user
relevance judgments for retrieved documents.
These same relevance judgments also give the
system a way to evaluate the performance of
each model, both in the context of a single
search of several iterations and over all
searches to date. These results can be used in
a statistically sound way to weight the contri-
butions of the models in the combined proba-
bility distribution used to rank the retrieved
documents.
Current retrieval systems provide little sup-
port in query formulation. User models allow
a system to adapt to each user, enabling
mixed-initiative interaction [ 15]. In this pro-
ject relevance feedback algorithms [ 16] will
be investigated.
Fox, in collaboration with this research, is
investigating techniques for rapid processing
of queries against very large text databases
using minimal perfect hash functions [ 17].
These are especially important for accessing
data on optical storage, since they guarantee
collision-free hashing, thus reducing the num-
ber of seeks required on inherently slower
optical storage devices. These hashing tech-
niques are embedded in LEND. The integra-
tion of LEND with ADIIR is planned later this
year.
Standard test collections are small compared
to today's terabyte databases. It is uncertain
how retrieval techniques will scale up.
Furthermore, these collections are not well
suited to testing iterative, feedback-based
retrieval, nor to image retrieval. We are
obtaining access to large document text and
image collections for our testbed. Through
participation in the Text Retrieval Conference
TTREC), we have acquired an approximately
half million document, 2 gigabyte corpus of
ASCII text [18].
As previously explained, documents enter the
ADIIR system as paper or as electronic files,
such as, SGML-tagged ASCII text or raster
image files. If a document enters the system
in paper format, the first step is to scan it into
raster format and store the image files into an
appropriate directory structure. Because each
page of the paper document is treated as a
single image file after the scanning process,
the directory structure should logically repre-
sent these page files as a document. The next
step is to index each document image.
Document image indexing is the ability to
segment, identify, and tag specific zones on
an image. These zones could be document
titles, abstracts, summaries, or any objects on
a page that require identification. After each
zone is located it is used to create a tagged
text representation of the document, through
the use of optical character recognition and
other techniques. At this point the paper doc-
ument will be transformed into a representa-
tion similar to an SGML-tagged ASCII docu-
ment. The algorithms to perform the pro-
cesses are currently the focus of much
research at various sites. The process currently
utilized to index an image requires manual
intervention at three points: 1) indices
creation; 2) indices to image link creation; 3)
OCR correction. The goal of automatic doc-
ument image indexing in ADIIR is to reduce
manual involvement as much as possible. The
remainder of this section will focus on several
subtasks: evaluation of common-off-the-shelf
(COTS) OCR products, automatic indexing
of images and ASCII text, establishment of an
expert thesaurus database, and
implementation of reference tables to improve
OCR accuracy.
The first subtask has been to evaluate optical
character recognition products based upon
specific criteria. The recognition engine has
to operate on a UNID platform, be capable of
performing OCR on zones of an image, and
provide a robust application programmer's
interface. After literature review, product
demonstrations, and meetings with several
companies; the Calera Recognition Systems
product was selected. This engine operates on
many platforms, including the Sun SPARC,
and has a Sun developers kit for application
programmers. In addition, Calera performed
at the highest accuracy rate according to a
report on the accuracy of OCR devices, based
on a study conducted by the Information
Science Research Institute at the University of
Nevada, Las Vegas [ 19].
The second subtask is to automatically index
image documents and ASCII text. Most
recent work on task 5 has focused on this
subtask. For explanation purposes, this task is
broken down into 7 steps:
The PRC Digital Image Library (DIL) is at the
heart of the automatic document image index-
ing process. It provides the ability to control
and manipulate image files without concern
for the physical attributes of image files or
with special imaging hardware. The library
file services provide application routines for
reading, writing, and manipulating a wide
range of image formats including: CALS
28002 Type 1 and 2, TIFF, PCX, DCX, patent
trademark (proprietary to the United States
Patent and Trademark Office) and encapsu-
lated postscript. The library conversion ser-
vices support conversion to and from CCITT
Group IV(G4), CCITT Group IV uled (G4T),
Run Length List (RLL) and Binary Bit map.
The library also provides transformation ser-
vices such as: scaling and rotating; inversion
and mirror; copy, cut and paste; and drawing
operations. The ability to deskew, despeckle,
erase, fill, crop, and annotate are also part of
the library.
Our initial large collection of document
images comes from the United States Patent
and Trademark Office (USPTO). As the
contractor for management of the USPTO's
text and imaging system, the Automated
Patent System, the selection of a patent image
collection was sensible. Patent images consist
of large amounts of text, drawings and tables.
They have a cover page, listing key informa-
tion about the patent such as, title, abstract,
inventors, and filing date. The general layout
is dual column with numbers imbedded
between the columns to ease reading. They
can be provided in 150 dpi or 300 dpi, but file
formats are not standard. The image file con-
sists of Group IV compressed data, but with
no file header to identify dpi. The PRC
Digital Image Library supports the PTO file
format.
Once a document image has entered the
ADIIR system, by scanning or from an on-line
collection, a deskewing and despeckling
algorithm is initiated. The algorithm removes
noise and corrects the skew of the image. A
skewed image will cause incorrect zoning or a
failure in OCR. Zone segmentation begins
once despeckling and deskewing are com-
plete. This is a three step process, known as
X-Y Tree partitioning, beginning with vertical
segmentation, horizontal segmentation, and
finally resegmentation of identified zones
I20
To begin the process each page of a doc-
ument is opened by the PRC Digital Image
Library and decompressed into a memory
buffer. A vertical pass (phase 1) of the image
is performed to mark the y coordinates of each
zone. These coordinates are determined by
ANDing each run length list (rll) line with a
rll mask. If several consecutive white lines
are found, and they fall outside a collection-
dependent threshold, hen the y2 coordinate of
the previous zone is recorded as well as the yl
coordinate for the new zone. For example, in
a typical newspaper article, stories are sepa-
rated by several lines. If the lines that sepa-
rate the articles are larger then a defined
height, then the end of the previous article is
marked and the beginning of the new one
recorded.
Once the vertical pass of the image is
complete, the zones are processed through the
horizontal segmenter (phase 2). The process
is similar to the vertical pass, except the indi-
vidual zones identified in phase 1, are seg-
mented to determine the x coordinates. If a
gap larger then the horizontal threshold is
found, then the x2 coordinate of the zone is
recorded. Following the previous example,
once the vertical pass has identified the indi-
vidual articles in a column of newspaper text,
the horizontal pass will mark the width of
each column. During this process a zone may
be further segmented (phase 3) to reveal addi-
tional zones. This is analogous to scanning a
newspaper page from left to right and finding
two columns of text on a newspaper page
instead of one column. If a new zone is iden-
tified (column), it must reenter the vertical
segmentation step to find its height.
When this process is complete the x and y
zone coordinates for each page image are
placed in memory. Finally, the density of
each zone, the ratio of black to white pixels, is
calculated and stored with each zone. These
densities will be used to assist in the identifi-
cation of text, table, and drawing zones. Once
all zones have been located and their densities
determined, optical character recognition
(OCR) is applied.
Calera's OCR engine consists of four phases:
segmentation, image recognition, ambiguity
resolution, and document analysis [21].
Segmentation is the process of locating indi-
vidual characters, while image recognition
takes a look at each individual character and
classifies it without the use of contextual
information. Ambiguity resolution attempts
to resolve any image recognition errors with
the use of contextual information (i.e. known
word forms). Document analysis attempts to
assign tags to entities such as paragraphs and
columns on an image. Once OCR is applied
to each zone, the resulting text is returned.
This text becomes another attribute of the
respective zone. The text is used to assist in
assigning a type to each zone; and in the cre-
ation of tags for the text document.
The next step in document image indexing is
the assignment of a zone type such as, abstract
or title, to each zone, which can be done, for
example, by utilizing a basic keyword search
on the text [22]. If a keyword is found, then
the zone can be tagged with the appropriate
identifier. For example, a search on the text
of a zone may result in the match on the key-
word ''Abstract'', thus the zone will be give
the identifier ''ABSTRACT''. This process
alone will not always result in a correctly
tagged zone, thus structural knowledge is nec-
essary. Structural knowledge consists of
rules, such as: a title is always in upper case; a
title is always a specific pitch; and a title usu-
ally occurs at the top of the page. Processing
using structural knowledge is referred to as a
publication-specific, because the rules for
document layout are predefined [23]. Zone
density can also assist in the typing of zones.
When all zones have been identified and
assigned types, the process of generating a
text document and an outline document is ini-
tiated. The text document includes all of the
ASCII text that was extracted during the OCR
process along with the tags such as
''ABSTRACT'', Uhat identify the text. This
text document will be the input for the auto-
matic indexing process. The outline docu-
ment is a PRC proprietary file that provides
the ability to navigate through the logical
document image. The document is organized
much like an outline in that it includes items
such as: a table of contents, list of figures, list
of effective pages, and pages of the image.
The table of contents could be expanded,
when selected, to reveal key sections such as
the title, abstract, and summary. By selecting
the abstract, the image file containing the
abstract is displayed. The PRC Outline
Documentor is the application that provides
the ability to manipulate and navigate through
the outline. It is described in more detail in
section 2.1.5.4.
Text documents that are outputs of the zone
process (step 6) are utilized to create the
indices for the various retrieval models dis-
cussed in Task 1 (Multiple Retrieval Models).
The exact indexing process used depends
upon the text retrieval system performing the
indexing. A typical system begins by break-
ing the text document into tokens or concepts.
Each concepts is examined to determine its
type or classification (proper noun, word,
date, number) and marked accordingly. When
complete, each term is compared against a
stop-word list. Words identified as being on
the list are not stored. Once stop-words are
removed, terms are stemmed to their root
form. The stemmed words are then assigned
weights, generally determined by the term
frequency in the document[24]. Once this is
complete an index to the text collection is
built. This index will be used for the retrieval
of relevant documents based on a user query.
Different models (vector space, boolean,
probabilistic) use varying indexing schemes
therefore, the multiple retrieval models
explained in Task 1 will differ in their index-
ing procedures. Each indexing engine will
store a special key (document ID) used to
identify the related document image during
retrieval.
The third subtask is to establish an expert the-
saurus database and implement reference
tables to improve OCR accuracy. The key
terms of a particular domain form the basis for
the thesaurus. Reference tables are also based
on expert knowledge of the domain. They are
used to verify andlor correct the OCR inter-
pretation of special names and titles. Typical
reference tables may be parts lists, glossaries,
phone directories, or organizational charts.
This subtask will be implemented once the
second subtask has been completed.
When a user requests to display a document,
the PRC Outline Documentor is initialized. It
provides an object-like approach to the man-
agement, manipulation, and presentation of
diverse data types and locations of documents
in a rule-based outline. Within one structure,
the user has easy access and control of
images, text, and fonts. It provides the ability
to navigate through an outline (created in the
automatic document image indexing task), by
promoting, demoting, or adding and deleting
items. The user can control the presentation
of the outline by expanding or collapsing
entries. If an item, e.g., abstract or main text,,
is selected for viewing, the Outline
Documentor will interface with the PRC
Digital lmage Editor to display the item. The
Digital Image Editor offers a complete range
of image viewing and manipulation functions
for documents including display of multiple
images, scaling, pan, scroll, rotate, zoom, and
reverse video. Annotations can be added to
the image with the ability to add 16 layers of
color markup without modification of the
original image. Each layer supports text and
graphics markup (lines, freehand line, arrows,
circle, rectangles, polygons). Finally, raster
editing features are available to copy, cut and
paste full or partial images, erase, crop, and
for image cleanup with despeckle and deskew.
An object-oriented design has been com-
pleted and implemented for the Combination
of Expert Opinion (CEO) module. The
SMART retrieval system, which has provided
the framework for implementing the retrieval
models, was acquired from Cornell
University. We have also acquired Fulcrum's
Ful/Text system, which may be used in addi-
tion to the SMART system. We have
obtained the LEND system from Professor
Edward Fox of Virginia Polytechnic
University and State University and the
INQUERY system from Professor Bruce
Croft of the University of Massachusetts,
Amherst, but have not yet integrated them
with our prototype system. The Calera optical
character recognition engine has been pur-
chased and installed. The PRC Digital Image
Library has been implemented and the United
States Patent image collection has been pro-
cured. The capability to identify zones of
interest using the X-Y tree partitioning algo-
rithm and the ability to OCR those zones has
been accomplished. The algorithm to identify
the types of zones (abstract or title) has been
partially completed, but it only employs
textual clues for identification and not struc-
tural information. Likewise, the classifying of
zones (text, draying, table) by usage of the
density routine has been achieved to a limited
extent. However, the use of other structural
clues to assist in this process has not been
implemented. The creation of a PRC outline
document and a tagged text document has
been attained.
On the retrieval side, we plan to integrate the
various retrieval systems mentioned above, as
well as the natural language understanding
system, PAKTUS (PRC Adaptive
Knowledge-based Text Understanding
System) with the PRC document imaging
products described in section 2.1.5 to com-
plete the ADIIR system. The ranked outputs
of the different retrieval systems will be
combined using the CEO algorithm. On the
document image analysis side, the zone iden-
tification process will be enhanced to include
a run length smoothing algorithm (RLSA)
[25] and a connected pixel analysis [26].
These enhancements will assist with the iden-
tification of imbedded zones and improve the
accuracy of the X-Y tree partitioning routine.
The use of structural data to improve zone
identification and classification is also
planned. The despeckling and deskewing
algorithm currently requires user assistance,
thus the ability to automatically perform these
functions is a desirable feature. The tagged
text document created after zone identification
and typing will eventually be replaced with a
SGML-tagged document. Lastly, the estab-
lishment of an expert thesaurus database and
implementation of reference tables to improVe
OCR accuracy will be implemented after the
zzOne identification routine has been opti-
mized.
PRC in collaboration with Professor Edward
Fox and his colleagues at Virginia Polytechnic
Institute and State University (VPI&SU) par-
ticipated in the first TREC conference [27,
28]. The CEO algorithm developed for
ADIIR was used by PRC to combine the
results of different retrieval methods supplied
by VPI&SU. Since these methods, such as p-
norm, are expressed in terms of correlations
rather than probability distributions, it was
necessary to extend the CEO algorithm to
handle correlations. So far this extension has
been handled in a heuristic fashion. Due to
delays in obtaining necessary equipment we
were not able to process the full set of TREC
queries against the full set of data.
Nevertheless our 11-point average (based on
averaging precision at 1l points of recall, the
standard overall TREC measure), was within
the range of the top three scores of systems
completing the full task using manually-gen-
erated ad hoc queries. It should be pointed
out that the TREC task was experimental in
nature and that there were many factors, such
as the delays many sites experienced in
obtaining equipment, which preclude too
much weight being placed on the scores
received by the various systems.
Since TREC, we have experimented with
weighting the different methods combined
based on their performance with the TREC
data, i.e., to determine an upper bound for per-
formance based on knowledge of each
method's performance on the actual test data.
We have used four different weighting
schemes: the 1l-point average, precision at
0.00 recall, precision at 0. 10 recall, and
unweighted (i.e., our official TREC results).
So far none of the weighting schemes has
produced better results than the unweighted
scheme. Two immediate explanations sug-
gest themselves. First, using overall averages
may not be too useful. Second, our simple
implementation of CEO assumes indepen-
dence among the methods. To examine the
first problem we intend to try weighting the
methods on a topic by topic basis rather than
by overall averages. Again this would be a
retrospective upper bound experiment. In
terms of the CEO approach [9] using only
overall averages would be analogous to using
only feedback from past searches, while using
topic-specific weights would correspond to
receiving feedback over several iterations of
the same search. We propose to investigate
the second problem by analyzing the overlap
of pairs of runs of the various methods to
determine dependence and thus perform CEO
without the independence
The ADIIR system provides a proto-
typeltestbed for research in information
retrieval systems which can take documents
input in paper or electronic format and that
can utilize layout or other structural informa-
tion, e.g., given by SGMIL tags. It is designed
to work effectively and efficiently for very
large databases. Further it seeks to address
the problem of different retrieval models
retrieving very different sets of more or less
equally relevant documents, by providing a
methodology, CEO, for the combination of
the results of different weighted retrieval
models into an overall optimal ranking.
Nearest neighbor (NN) classifiers with su-
perior recognition accuracy have at last be-
come practicable due to advances in com-
puting technology. Although their excellent
asymptotic properties are well known, and
both optimal and heuristic pruning algo-
rithms have been extensively investigated,
to the authors' knowledge this is the first
report on using NN in a large-scale OCR
application.
Recognition of scanned handprinted
digits is the objective of continuing re-
search. For up-to-date surveys, see [1-
5]. Applications include zip-code recog-
nition, facsimile coversheet interpretation,
form reading, and data entry,
A recent conference [6] sponsored by
the United States Bureau of the Census
and conducted by the National Institute of
Standards and Technology reported a wide
variation in recognition performance: one
participant reported a 98.2% accuracy, with
most in the 96.0% to 96.1% range. Most of
these digit recognition systems were trained
on the same NIST database used in the ex-
periments reported here.
The following work will focus exclu-
sively on the classification of isolated hand-
printed digits, Several versions of the NN
classifier are considered. The motivation for
using NN is its low error rate, simplicity,
and minimal training requirements, How-
ever, NN classifiers are slow and require a
large memory, Fortunately, several meth-
ods exist to prune the NN classifier (Section
3), reduce memory requirements by 8(0%,
and increase classification speed by a factor
of 5.
We first investigate the dependence of
the error rate on the number of training
samples and on I, the number of nearest
neighbours, performing experiments using
two distinct sets of features: contour tan-
gents [7] (derived by smoothing and sam-
pling the chain code description of the char-
acter ), and Zernike moments [8] (computed
by determining the inner product of the
character bitmap with the Zernike kernel
of the form ,44(p.8) = R,,44(p)ei* where
=-- - sS'24E
The NN classifier utilizing tangents is more
accurate, with a substitution error rate of
1.64%.
Next we turn our attention to the error-
reject trade-off. The highest possible accep-
tance rate may be valuable in some appli-
cations such as information retrieval with
robust word-matching, but substitution er-
rors are undesirable since they are diffi-
cult to detect or correct in the absence of
numeric context. If throughput is com-
puted to include both automated classifi-
cation and human proofreading and correc-
tion, then a very low substitution rate with
a relatively high rejection rate may actu-
ally increase throughput [9]. By combin-
ing the two NN classifiers (one using tan-
gents, the other using Zernike moments) a
substitution error rate of 0.27% is obtained.
An even lower substitution rate of 0.035%
can be obtained by using a unanimous vote
acceptance criterion (Section 4) at the ex-
pense of a rejection rate of 18.41%,
The data considered in these experiments is
from the NIST-3 database of hand-printed
characters [10]. The database was gen-
erated using 2100 writers distributed geo-
graphically in proportion to the population
density of the United States to compensate
for geographic variations in hand-printing
style. The writers are from the Bureau
of the Census and can be considered mo-
tivated (i,e., this database represents fair
hand-printing). No restriction was placed
on the writing implement, which ranges
from wide felt-tipped pens to hard sharp-
pointed penciis. This leads to a consid-
erable variation in stroke thickness, which
sometimes causes characters to touch (these
experiments consider only well-segmented,
isolated characters). There is also a wide
range in character size, tilt, and skew. This
database contains approximately 250,000
hand-printed digits scanned at 300 dots per
inch. Of this total, we will use 118.000 sam-
ples for training and 119,121 samples for
testing. Care was taken to ensure that the
test set was not compromised in the follow-
ing experiments.
The database was partitioned to form
disjoint training and evaluation sets: no
writer has samples in both sets, A small
portion of the database was reserved for fu-
ture research. Approximately 10.500 touch-
ing characters were not included in the ex-
periments. We partitioned the database as
follows:
Database: 252,000 digits (2100 writers)
The reference sets used in our experi-
ments are subsets of the Training Set, and
the tes se1s are subsets of the t.valuation
Set.
The NN rule, introduced in 1967 [11], as-
signs the label of the nearest pattern in the
reference set to the unknown pattern, Let
a 1 be a labelled reference pattern, se-
lected from the reference set A = a;}L,.
where .lI is the number of reference pat-
terns, and 9! is the pattern space, Let y E 91
be the unknown pattern. Then the nearest
neighbour of y, denoted a44. is given by
where d(-) is a distance measure, In other
words, 4/4i, is the pattern in A which min-
imizes d(r, y). The nearest neighbour rule
assigns the label of 44 to unknown y.
where l( ) is the label assignment.
The most interesting theoretical prop-
erty of the NN rule [12] is that under very
mild regularity assumptions on the under-
lying statistics, the large-sample error rate
incurred is less than twice the Bayes er-
ror rate. The Bayes decision rule achieves
minimum1 error rate but requires complete
knowledge of the underlying statistics [l1],
[13]. More precisely [12], the error rate of
the NN, denoted PN N, is bounded by
where c is the number of categories, and Fg
is the Bayesian error. This bound applies to
the asymptotic case, and little can be said
about the non-asymptotic behaviour of the
N N classifier (i,e., real pattern recognition
applications ).
T he NN classifier uses each sample in
the reference set as a model, Unlike clus-
tering techniques, the N N does not attempt
to capture global characteristics of the data,
R athher, thhe boundaries between classes are
defined locallv as the Voronoi boundarv
letween adjacent reference patterns, N N
classifiers do not require any training (just
store the patterns ), but they do require a
large memory and are slow at query-time,
In sparse spaces, a large number of refer-
ence patterns are often required to capture
the variations in patterns within a category.
The NN classifier has high accuracy in com-
parison to other techniques [14], and is of-
ten used to benchmark classification perfor-
Inance,
The KN N classifier is similar to the NN
classifier, except that the nearest I neigh-
bours are determined, and the unknown
pattern is assigned the label most prevalent
among them. Let [zaa,}{., be the K pat-
terns in reference set which are nearest
to unknown y, The nearest neighbour set
W, is defined recursively by
where P; = [444a,1,; are the i nearest
neighbours to y, and A/ P, is the reference
set A less P;. The K-nearest-neighbour
rule assigns to y the label most frequently
represented among the h nearest samples
(majority vote).
The KN N rule reduces the influence of
local category information in favour of in-
formation contained in a wider neighbour-
hood of the unknown pattern. This dimin-
ishes the importance of an individual ref-
erence sample in favour of its neighbours,
The asymptotic error rate of the KN N rule
is nearer to the Bayes error rate than that
of NN [12]. As K - oc, the performance
of KN N approaches that of the Bayes de-
cision rule, For sparse data, however, few
theoretical results apply, In dense spaces,
the KN N is a good approximation to the
Bayes decision rule, while in sparse spaces,
category averagitng will often decrease the
accuracy of the KNN classifier.
In order to gauge the relative merits
of the NN and KNN classifiers, a series of
small-scale experiments were conducted us-
ing hand-printed digits, The reference set
consists of 10. (000 patterns, and the test set
consists of 4, 000 patterns, Each category
is equally represented. The reference set
contains samples from 100 writers, and the
test set contains samples from1 40 writers
not used in building the reference set, i,e.
a *blind test''.
Table 1 shows the performance of the
K N N using tangents and Zernike moments.
Each feature set is considered in isolation.
In both cases, accuracy on the test set de-
creased as K increased. This result in-
dicates that the small-sample performance
oof KN N is inconsistent with its asymptotic
performance (which predicts an increase in
KN N accuracy with increasing ). The
K N N can be viewed as an attempt to es-
timate the a posteriori probabilities from
samples [12, page 104]. It is therefore not
surprising that it falters in sparse pattern
spaces which are insufficient to model the
class probability densities. The results in
Table 1 support the use of I = 1 for clas-
sifying hand-printed digits, We have ob-
served that several of the errors of N N.
Tangents classifier are correctly identified
by the NN-Zernike moments classifier, and
vice versa, motivating the combination of
the classifier outputs (Section 5).
A1othier experiment examines how NN
classifier performance changes with an ex-
panding reference set (Table 2). A test set
of 4.000 samples was used, while the size of
the reference set was varied from 1, 000 to
50. 000 samples. The classification accuracy
improves as the sample size increases, indi-
cating that the category boundaries are be-
coming better defined with increasing sam-
ple size. This experiment also lends cre-
dence to the notion that the single most im-
portant variable in hand-printed digit clas-
sification is the size of the training set.
A drawback of NN classifiers is the large
amount of storage and computation in-
vOlved d uue to t he app)arent need to store
all the sample data. Pruniag (also know n
as thinning or editing) is an attempt to
store onlv a fraction of the data without
substantialiy degrading classifier accuracy,
Pruning removes superfluous patterns from
the training set, specifically those patterns
which do not affect the decision boundaries
(i,e,, patterns interior to homogeneous re-
gions of the pattern space). Pruning ac-
celerates classification and reduces memorv
requirements,
T wo methods of pruning are consid-
ered here: (1) a geometric method (Voronoi
pruning), which preserves the NN deci-
sion boundaries but is computationally
prohibitive for higher-dimensional pattern
spaces, and (2) a heuristic method (based
op Hart's condensed nearest neighbour rule
[13]), which is computationally tractable al-
though it does not always preserve the orig-
inal decision boundaries. Experiments in-
dicate that 79% of the training set can be
pruned with only a 0.077. deterioration in
recognition accuracy,
A pruning algorithm based on the geometry
of the pattern space is now discussed [15],
[16]. This method uses the Voronoi diagram
of the sample data,
Definition. Voronoi Diagram [17'
T)e Voronoi diagram of a set of points,
called sites, is a partition of R' that assigns
a surrounding region of ''nearby'' points to
each of the sites (see Figure 1(b)). Each re-
gion is the d-polytope containing all points
lving nearer to one site than to any other.
Formally, the (nearest-set ) Voronoi diagram
of the set A,; = [4.2.---- <4} of n sites in
R'' is the set of n convex regions,
for 1 S i S n.
Algorithm. Voronoi Pruning (VP)
The VP algorithm preserves the NN de-
cision boundary precisely, since the NN de-
cision boundarv is contained in the Voronoi
diagram, and only those data points which
do not affect the NN decision boundary are
removed from the data set (see Figure 1).
nfortunately, there are several problems
associated with VP. The V P algorithm does
not alwavs produce a minimal representa-
tion. The algorithmic complexity is on the
order of (O(dn log n), where n is the sam-
ple size, and d is the dimensionality [15].
[16]. For higher dimensional spaces, and
for large sample sizes, the complexity is
prohibitive. As well, VP treats all regions
equally and does not give precedence to
hard-to-recognize nonhomogeneous regions
in the pattern space.
In order to reduce the computational
complexity of the VP algorithm, an op-
timization is considered in [17]. The
worst-case computational complexity is
O(3/log n), while the expected com-
plexity is O(ES,,nlog n), where S,, is the
number of dual simplexes in the Voronoi
+s=. >. = (, , ) 6. = E. =
the probability of a sammple being interior
to the Voronoi decision region (for further
explanation, see [17). This complexity is
still intractable for large data sets in high-
dimensional spaces. Sub-optimal geometric
approximations to VP are somewhat faster.
These methods do not preserve the N N de-
cision boundaries, but do hiave a lower com1-
putational complexity, For example, the
Gabriel Algorithm [15], has a worst-case
complexity of O(dn*).
The data sets considered here consist of
approximately 10 samples, and the dimen-
sionality is d = 64 for tangents, and d s 56
for Zernike moments, The computational
complexity of partitioning this set of data is
;pP or the VP algorithm, 10%% (estimate)
for the optimized VP algorithm, and 10
for the Gabriel algorithm, All these meth-
ods are computationally too intensive for
the available com puter resources, which are
capable of approximately 10% computations
per second. Note that even the Gabriel al-
gorithm would take 10? seconds (approxi-
mately 30 years) to complete! Therefore,
we next examine heuristic pruning algo-
rithms which are more tractable.
The condensed nearest-neighbour (CN N )
rule is a heuristic method of reducing the
size of the training set without significantly
affecting the performance of the NN classi-
fier. Popularized by Hart [13], this method
has several interesting variations [18], [19],
[20]. Our own variation yields a more
conservative pruning algorithm which pre-
serves nearlv all of the NN decision informa-
tion. For clarity, we will first outline Hart's
original CNN rule.
A lgorithm. Condensed Nearest Neigh-
bour Rule. [13]
The (CN N rule attempts to preserve the
NN decision boundaries by identifying ho-
mogeneous regions of the pattern space,
whhere homogeneity is measured using a
nearest neighbour rule (step 2). The CNN
rule is sequential and order-dependent. In
general, the condensed data set is not able
to preserve the NN decision boundaries,
as substantiated bv the results in Table 3.
A modified CN N rule uses an interesting
muethod to determine the decision bound-
aries: for each pattern z, the nearest neigh-
bour to ar from another class, denoted y, is
found. Pattern y must be close to the deci-
sion boundary [18], although the set of all
y is not sufficient to define the N N decision
boundary,
We now outline a novel variation of the
CN N which considers the nearest neigh-
bours of each data pattern. A pattern r
is pruned from the data set only when all
I of its neighbours are of the same cate-
gory (unanimous vote criterion ). This al-
gorithm updates the reference set until all
the pruned samples are correctly classified.
Algorithm. Modified Condensed Nearest
Neighbour Rule. (MCN)
The modified condensed nearest neigh-
bour rule is illustrated in Figure 2. The
computational complexity of MCN is ap-
proximately O(dn*) (note: an application
of MCN using 118,000 samples takes ap-
proximately 48 (CPD hours). The MCN rule
retains a larger portion of the reference set
than does the (CN N rule, and it preserves
most of the NN decision boundaries (see Ta-
ble 3). A modification to the MCN based
on [18], which we call TCN. is derived by
changing step (2): delete from the refer-
ence set every sample which is not the near-
est neighbour of a different class for some
other sample. Thhe T(CN is shown to reduce
the data set by a larger fraction than either
CN N or MCN. but TCN does not perform
as well in test conditions. Since we are in-
terested in maintaining a ligh recognition
rate, we will use the MCN rule in further
pruning experiments.
Table 4 shows the performance of MCN
for various sized training sets with I s 15.
(The contour tangents feature set is used.)
Other MCN experiments, conducted using
I = 5 and K = 10, suggest that smaller
values of I do not lead to as good a repre-
sentation of the decision boundary. A value
of K s 15 provides a reasonable tradeoff
between size of condensed set and test per-
formance. Similar results were noted for
Zernike moments, with a pruning rate of
76.73% for K = 15 when 118,000 samples
were used. Note that the classification of
the test set was conducted with K s 1
using either the full training sets or the
pruned reference sets,
As the sample size increases, the prun-
ing performance improves. For 118.000
data samples, the training set is reduced by
79% with a loss of accuracy of only 0.07%.
The data considered in these experiments is
from the NIST-3 database of hand-printed
characters, described in Section 1.1, The
writers in the test set have not been used
in any manner whatsoerer in the train-
ing of these classifiers. The reference and
test patterns are the feature vectors derived
from the original bit maps of the hand-
printed digits. Two feature sets are con-
sidered: contour tangents, and Zernike mo-
ments,
An N N classifier for each feature set was
tested in isolation (see Table 5). When
using a training set of 118.000 patterns
(condensed using the MCN algorithm with
I = 15), the NN classifier with K 1
has a recognition rate of 98.36% when using
tangents, compared to 96.71% when using
Zernike moments.
The results show that the pruned ref-
erence set derived from a large number of
writers outperforms a larger reference set
derived from fewer writers,
A simple rejection criterion is now de-
scribed: If the K-nearest neighbours are of
the same class, then the result is accepted,
otheruise the result is rejected, For suitable
values of K, very low substitution rates can
be achieved in the relatively homogeneous
portions of pattern space isolated by this
rejection rule (see Table 6). For instance,
we can achieve a substitution rate of (0.33%
with 4.71% rejections using a K = 5 unan-
imous vote criterion (tangents ).
WVe now consider a classification method
that exploits the availability of several NN
classifiers, each using a different set of fea-
tures. The output of the classifiers are
combined using the following rule: If the
K-nearest neighhbours for all the classifiers
are of the same class, then the result is ac-
cepted, othe rwise the result is rejected,
Combining the tangent based and
Zernike moment based classifiers leads to
a test performance of 0.27% substitutions
with a rejection rate of 4.11% for K = 1
(i,e. the combined classifier has an accu-
racy of 99.T3% on non-rejected characters).
For I = 5, this method leads to a test per-
formance of 0.035% substitutions with a re-
jection rate of 18.41% (i,e, the combined
classifier has an accuracv of 99.965%. on ac-
cepted characters ). The results for K = 1
to 8 appear in Table 6.
The tradeoff between rejection and sub-
stitution errors is illustrated in Figure 3.
The interesting properties of the error-
reject curve, including convexity, initial
slope, and the integral relation between the
error rate and the reject rate in optimal
classifiers, are explored in [21].
It is interesting to note that both near-
est neighbour classifiers (with K = 1) are
wrong 0.65% of the time. If neither classi-
fier is correct, it is difficult to determine the
true output identity (but see [22] on com-
bined classification using rankings ). There-
fore, 0.65% of the characters are hard to
recognize. This may be due to character
shape ambiguities, inadequate features, in-
sufficiently robust classifier, or the limited
size of the reference set.
N N classification is as powerful as it is
simple. Very high recognition accuracy
(98.36%) is possible using a sufficiently
large training set. Pruning of the NN data
set can lead to a speed-up by a factor of 5.
A substitution rate lower than 0.04% can be
achieved by combining two nearest neigh-
bour classifiers. Four errors per 10,000 dig-
its surpasses the accuracy of most human
proof-readers.
These experiments demonstrate the ad-
vantages of using the NN classification rule.
T'he main disadvantage is operating speed.
This disadvantage is quickly disappearing,
and it is certain that in the foreseeable fu-
ture computing machines of greater speed
will increase throughlput (currently 50 char-
acters per minute) to perhaps 1000 char-
acters per minute. Hence, in the situa-
tion where extremely high performance is
required in the absence of context, it is clear
the Nearest Neighbour rule is best (NN
$1).
This work is supported in part by NSERC
grant OGP0004234. The authors wish
to thank Theodore C. Yapo of Rensse-
laer Polytechnic Institute for providing the
Voronoi diagram display software used in
Figures 1 and 2.
We are building an experimental page
reader [l] that is easily adaptable to various
languages and writing systems. A strongly
language-free page reader must cope with
variability of many kinds, such as sym-
bol sets, font styles, page layouts, linguis-
tic context, output encodings, and so forth.
We have adopted a strategy in which geo-
metric page layout [2, 3] is performed early,
before symbol recognition and contextual
analysis. While in many Western writing
systems (e.g. Latin and Greek), textlines
are generally horizontal, some East Asian
writing systems (e.g, Japanese and Korean)
allow vertical textlines and even allow both
orientations to occur in the same page.
Thus, we felt the need for an algorithm
to infer the textline orientation within iso-
lated blocks of text, if possible without
prior knowledge of the language, symbol
set, text sizes, or the number or location
of the textlines.
Layout analysis occurs as follows. Black
S-connected components are first extracted,
the image is corrected for skew and shear
[4], and blocks of text are isolated by ana-
lyzing the structure of the white space [5, 6].
Then for each text block, the subject algo-
rithm is used to infer the textline orienta-
tion. Once orientation is known, connected
components are formed into textlines and
organized into the logical reading order ap-
propriate for the language and orientation.
A typesetting convention which appears
to be universal across writing systems is
that characters are printed more tightly
within a textline than between textlines.
The heuristic to be described in Section 2
attempts to exploit this convention by ex-
amining distance relationships among con-
nected components in the image. The tech-
nique uses computational geometry algo-
rithms to achieve a worst-case asymptotic
runtime of O(n log n), where n is the num-
ber of black connected components in the
block image.
Few papers in the research literature
address this problem directly; most pa-
pers covering such writing systems assume
a fixed orientation [7, 8]. Tsuji [9] uses
a top-down strategy for image segmenta-
tion using recursive pro jection profiles with
hand-crafted rules for stopping and back-
tracking from a depth-first search. Leaves
of the resulting structure tree are elements
such as text lines, ruled lines, and non-
text areas. Once individual text lines are
found, their orientation is determined, pre-
sumably by a simple rule such as aspect
ratio. Spitz [10] describes a system capa-
ble of processing pages containing both En-
glish and Japanese text. Page segmentation
is accomplished by searching for horizontal
followed by vertical rivers of white space.
Lines of Japanese text with vertical orienta-
tion are represented as individual columns.
One approach to inferring textline ori-
entation which comes to mind is to make
use of pixel pro jections. After accurate
skew-correction, the image artwork could
be pro jected in both directions and the
energy computed. The projection direc-
tion producing the larger energy should be
the text orientation. However, this sim-
ple method is not robust, for example it
fails on a single tall column of horizontal
fixed-pitch text. Alternatively, the projec-
tions could be used to estimate the domi-
nant spacing, with the larger spacing taken
to be the textline orientation. Two prob-
lems are immediate - non-regular spacing
makes finding the dominant spacing diffi-
cult, and a reasonable sample size is needed
(say, 5 textlines). The latter problem leads
to special case tests and analysis for smaller
blocks.
The algorithm to infer textline orientation
accepts a list of connected components. It
is assumed these connected components are
printed in a single orientation, and that the
characters of the writing system are nomi-
nally detached (for example Japanese, Chi-
nese, Korean). It is not necessary that
the components be previously corrected for
skew or that a character of the writing sys-
tem be comprised of a single glyph.
The steps in the heuristic are:
The intent of the first step is to ignore
fragments that are much too large or small
to be characters, based on a rough estimate
of expected point sizes. This reduces prob-
lems caused by small spatially correlated
noise such as lines of dirt fragments along
page borders and gutters,
The next step reduces each connected
component to a single point in the plane
- we use the center of its bounding box.
These points define vertices in a fully con-
nected undirected graph; the edges of the
graph are labeled with the Euclidean dis-
tance between its endpoints. As we will
show, this fully-connected graph need not
be explicitly constructed.
Most of the computation of the heuris-
tic is due to step 3. The objective is to
construct a tree from the fully connected
graph containing all vertices such that the
total length of the tree is minimum. The
MST tends to connect vertices (i.e. con-
nected components) from within the same
textline, with a few edges joining compo-
nents between textlines. Efficient construc-
tion of the MST is described in Section 2.1.
Inferring textline orientation is then a
matter of finding the dominant orientation
of the edges in the MST. A coarse histogram
of edge orientation from 0' to 180% is main-
tained as edges are added to the tree (Sec-
tion 2.1). Once complete, we sweep over
the histogram to find the window contain-
ing the largest energy (details later in Sec-
tion 3). If sufficient energy is contained
in a window with its center near (0%, it
is decided the text is organized into hor-
izontal textlines; likewise, if the window
is centered near 9(0%, vertical textlines are
assumed; otherwise, the algorithm returns
''uncertain''. Note that the technique is tol-
erant of skew since it is not required that
the edges be oriented exactly horizontal or
vertical.
The MST problem for general graphs was
first solved independently by Prim [11], Di-
jkstra [12], and Kruskal [13], each demon-
strating a polynomial time algorithm. The
best algorithm to date has a run time of
O(e), where e is the number of edges in
the graph[14]. Solving the Euclidean MST
(EMST) problem would then require O(n')
time, since e = (n*- n)/2, Fortunately, the
EMST problem has metric properties which
can be exploited. (The following facts are
well-established in the computational ge-
ometry literature, but may be unfamiliar to
some readers in the document image anal-
ysis community.)
The Voronoi diagram is a fundamen-
tal structure in computational geometry.
Given a set of points in the plane, the
Voronoi diagram produces a partition of
the plane into regions such that for each
point p,, there is a region defining the lo-
cus of points closer to p; than to any other
point. Adding a straight-line segment be-
tween points sharing an edge in the Voronoi
diagram produces a triangulation' of the
points known as the Delaunay triangula-
tion. Figure 1 shows a few points in the
plane with the Voronoi diagram and the De-
launay triangulation.
The Delaunay triangulation contains all
edges of the EMST and no more than 3n- 6
edges altogether (by planarity). This sim-
plifies the problem of finding the MST of a
fully connected graph with (n'- n)f2 edges
to finding the MST of the Delaunay trian-
gulation which contains no more than 3n -6
edges. Algorithms to compute the Voronoi
diagram and Delaunay triangulation have
been studied for two decades, An asymp-
totically efficient algorithm due to Fortune
[15] uses the sweepline paradigm and pro-
duces the Delaunay triangulation of a set
of n points in the plane in time O(n log n).
The algorithm is also efficient with respect
to storage, requiring only O(n) space.
Cheriton and Tarjan [14] proposed an
MST algorithm using a data structure rep-
resenting a forest of subtrees which are
merged until a single tree remains (the
MST). They further proposed a strategy for
the selection of the subtrees in such a way
that when applied to the Delaunay triangu-
lation, the MST can be found in time linear
in n, The initial queue of subtrees is the list
of vertices, so it is easy to show the storage
requirement of the algorithm is O(n).
These results together give an EMST
algorithm running in O(n log n) time and
O(n) space.
The algorithm described in the previous
section has been implemented and inte-
grated into our experimental page reader.
Figure 2 shows an image of a page of Chi-
nese. The textlines in the two main blocks
are oriented vertically, while the page num-
ber in the upper left is horizontal.
Our block segmentation algorithm pro-
duces these 3 blocks of text. The block
containing only the page number is clearly
determined to be horizontal since all (two)
edges of the MST are near 0%. The other
blocks are more interesting. Figure 3 il-
lustrates the EMST for the upper block of
text, along with the histogram of MST edge
orientation. As can be seen from the his-
togram, a large fraction of the energy is
centered around 90'; this block of text is
clearly oriented vertically.
Chinese and Japanese orthographies do
not delimit words with white space (Fig-
ure 2). This actually helps our algorithm
since there are no large word breaks to
)ump. Figure 3 is representative of the ef-
fectiveness of the technique on these writ-
ing systems. The Korean system, however,
is more challenging. Figure 4 shows a small
block of Korean text written in horizontal
lines.
Hangul is the native alphabet used to
write the Korean language. Hangul has
only 24 letters which are combined in a
two-dimensional fashion to form composite
Hangul symbols. Each composite symbol
corresponds roughly to a syllable of the lan-
guage. Often the individual letters of a syl-
lable are detached and are therefore seen
as multiple connected components. Words
in the language are delineated with white
Space and since a composite is an entire
syllable, words tend to be short (with re-
spect to the composites). This results in a
relatively large number of inter-word gaps
which may be as large as inter-textline dis-
tances.
Figure 5 (a) shows the Delaunay trian-
gulation for the fully connected graph rep-
resenting the Hangul text. The triangu-
lation contains 473 edges; the fully con-
nected graph contains more than 13,000
edges. Figure 5 (b) shows the resulting
EMST while (c) gives the distribution of
edge orientations.
The evidence provided by the edge dis-
tribution is not nearly so clear in this case.
Experiments have shown it is helpful to
weight an edge's contribution by the area
(i,e, number of black pixels) of its connected
components, In this way the distribution is
less influenced by small fragments of sym-
bols which may result in edges at wild ori-
entations. The ''energy'' at histogram bin
i is defined as (%/ 32;;)', where 6; is the
count at bin i, This has the effect of sharp-
ening peaks and dampening noise in the
histogram. Our implementation sweeps a
window of 320% over the histogram and re-
quires that 65% of the energy be contained
in a single window in order to believe the
associated decision. Given this implemen-
tation, over 72% of the histogram energy
shown in (c) is contained within the window
around (0%, Therefore the correct decision of
horizontal textlines is made.
Figure 6 shows a small block of set-solid
English text. The right justification and
constant pitch font cause large, irregular
inter-word gaps, Figure 7 shows the EMST
for this block of text, along with the his-
togram of edge orientation. The large inter-
word gaps, resulting in a high percentage
of inter-textline edges, is offset somewhat
by the longer words of English. Over 80%
of the histogram energy is contained within
the window around (0,
Figure 8 shows a block produced from a
Japanese newspaper by the page segmenta-
tion algorithms. (Our system does not at-
tempt to separate text from non-text prior
to this stage.) Also shown are the resulting
EMST and edge histogram. Here the pre-
processing step is helpful as it throws out
several of the very large connected compo-
nents, The window with maximum energy
is centered near 60 and contains 45% of the
energy. The algorithm returns ''uncertain'',
leaving the decision of what to do with this
block to higher level control.
The algorithm has been tested on a
database of over 100 pages covering the
following writing systems: Chinese, Dan-
ish, English, Japanese, Korean, Russian,
Sinhalese, Thai, Tibetan, Ukranian, Viet-
namese, and Yiddish. Pages were selected
with preference given to complex, multi-
column layouts (e.g. newspapers). Pages
with tabular data and line-graphics were
accepted. Table 1 shows a confusion ma-
trix summarizing results over all blocks
produced by our page segmentation algo-
rithms. The correct orientation was deter-
mined by hand on a block basis, An answer
of ''uncertain'' was the desired result when
the algorithm was given a block of noise or
fragments of a non-text region. Overall, the
algorithm deduced the correct orientation
95% of the time.
The majority of the uncertain blocks
incorrectly labeled vertical actually con-
tained nothing but noise, typically located
at the left and right margins of the page.
These blocks tended to be tall and narrow,
producing EMST edges oriented near 90%.
Most of the horizontal blocks for which the
algorithm was uncertain contained narrow
columns of tabular data. The EMST edges
between the few characters in each textline
could not offset the edges joining textlines.
Ignoring for the moment blocks of noise,
purely tabular data, and the rare case of a
block of mixed orientation, there were a to-
tal of 638 horizontal and 447 vertical blocks
of text within the 100 page images. There
were no confusions among the orientations
but 8 of the horizontal blocks and 1 vertical
block were labeled uncertain (7 of the 8 hor-
izontal errors were on horean text). Over-
all, the algorithm correctly labeled 99% of
the clean, non-tabular, text blocks.
Figure 9 shows the CPI7 time spent ex-
ecuting this algorithm over a range of block
sizes. The times were measured using stan-
dard UNID prof tools on a Silicon Graph-
ics Computer Systems Power Series Model
4D/480S minicomputer with a 40 MH2 IP7
processor. For reference, the block of Fig-
ure 3 has 400 points and the Hangul text
of Figure 4 contains 160 connected compo-
nents, The average block size over the 100
page test database was 141 points, while
the largest block had 1315 connected com-
ponents (the data points above this were
produced by turning off block segmentation
and running the algorithm over the entire
page).
While the EMST describes a convenient set
of distance relationships among the con-
nected components, other relationships are
available. The k-nearest neighbors (k-NN)
is a long-popular approach in pattern recog-
nition. Rather than finding the MST in
step 3 of our algorithm, we could find the
K-NN for each point and examine the dis-
tribution of these edges in step 4.
Experiments with this approach have
shown that k 1 does not provide enough
information and that k 2 4 provides too
much data due to the edges which almost
certainly connect distinct textlines. k = 2
or k s 3 produces reasonable results, in
most cases comparable to the MST. Inter-
estingly, the MST gave better results on
proportionally spaced alphabetic text. The
k-NN seemed to cross textlines often due to
the large fraction of wide connected com-
ponents resulting from touching characters.
This is an indication of the robustness of
the MST approach.
Hashizume et al. [16] use the 1-NN pairs
between connected components to estimate
the dominant skew angle. In a method sim-
ilar to ours, a histogram of the direction
vectors defined by the 1-N N is constructed.
The histogram peak is found and the local
weighted average taken to be the dominant
skew angle.
O'Gorman uses the k-NN information
for a variety of page layout tasks[17]. He
analyzes the structure and edge length and
angle of the k-NN (k typically 5) graph to
merge connected components into words,
lines, and blocks, estimating document
skew, point size, and line spacing along the
way. For our more specialized application,
we have been able to avoid an analysis of
the MST structure, relying instead on the
orientation of individual edges only.
This paper has described a simple heuristic
to infer textline orientation given a block of
image artwork. The technique makes use
of constructs from computational geometry
for speed. The algorithm has been imple-
mented and integrated into our experimen-
tal page reader, It has been tested on a
database of over 100 pages in a dozen writ-
ing systems with an accuracy rate of over
95%, failing primarily on non-text blocks
and tabular data where spacing is not reg-
ular.
Our algorithm assumes that the charac-
ters of the target writing system are nomi-
nally detached from one another. The algo-
rithm has also been tested on a few pages
of Arabic and Nepali (written using the De-
Vanagari script), two writing systems where
characters making up a word are typically
connected. Performance on Arabic was us-
able, but not so on Nepali text (attached
units are even longer than in Arabic, with
more white space between units). Perhaps
in this case the metric on which the MST
is constructed should be the minimum dis-
tance between bounding boxes, rather then
the Euclidean distance between their cen-
ters.
A limitation of this technique is that it
relies entirely on the page segmentation al-
gorithm to provide blocks of a single ori-
entation. If page segmentation produces a
block of mixed orientation, the algorithm
happily returns the dominant orientation,
or at best, an indication of uncertainty. It
may be necessary to analyze the structure
of the MST to verify all text is of the same
orientation and to split text blocks when
necessary.
Our downstream geometric page lay-
out algorithms, such as line finding, pitch
estimation, and word finding, have been
generalized to support vertical text in a
completely analogous manner to horizontal
text. Text is maintained internally accord-
ing to the logical reading order for the par-
ticular language. System output is in this
order, regardless of the orientation or read-
ing direction of the original material.
Steven Fortune provided code for his
sweepline algorithm to construct the Delau-
nay triangulation of points in the plane. I
am very grateful to Henry Baird for helpful
comments on this paper.
